

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="https://s2.loli.net/2023/07/15/AhaZCquL51QoPpk.png">
  <link rel="icon" href="https://s2.loli.net/2023/07/15/AhaZCquL51QoPpk.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="喵老师">
  <meta name="keywords" content="">
  
    <meta name="description" content="本节主要介绍生成模型中的变分扩散模型(VDM)，会论述其概率模型、变分下界ELBO、模型训练、损失函数的三种等价关系、采样过程等内容。">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习-6-生成模型4-变分扩散模型">
<meta property="og:url" content="http://example.com/2024/05/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-6-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B4-%E5%8F%98%E5%88%86%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/index.html">
<meta property="og:site_name" content="喵老师&#39;s Blog">
<meta property="og:description" content="本节主要介绍生成模型中的变分扩散模型(VDM)，会论述其概率模型、变分下界ELBO、模型训练、损失函数的三种等价关系、采样过程等内容。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2023/07/26/tDhLZgxV4HaIizX.jpg">
<meta property="article:published_time" content="2024-05-23T03:36:01.000Z">
<meta property="article:modified_time" content="2024-06-03T07:54:04.589Z">
<meta property="article:author" content="喵老师">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://s2.loli.net/2023/07/26/tDhLZgxV4HaIizX.jpg">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>深度学习-6-生成模型4-变分扩散模型 - 喵老师&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.5","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="喵老师's Blog" type="application/atom+xml">
</head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>喵老师&#39;s Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/Daily/" target="_self">
                <i class="iconfont icon-music"></i>
                <span>日常</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('https://s2.loli.net/2023/07/17/Myx8RtuvNLKFgiZ.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="深度学习-6-生成模型4-变分扩散模型"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-05-23 11:36" pubdate>
          2024年5月23日 中午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          22k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          186 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">深度学习-6-生成模型4-变分扩散模型</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="变分扩散模型vdm">变分扩散模型(VDM)</h1>
<p>  在上一节关于变分自编码的介绍中，我们已经讨论到了具有多层隐变量以及马尔可夫性质的变分自编码模型(MHVAE)，其基本形式与我们今天要介绍的变分扩散模型(Variational
Diffusion Models)已经非常相似，在 MHVAE 的基础上，
VDM的主要改进有三个方面：</p>
<ul>
<li><strong>隐变量 <span class="math inline">\(z\)</span>
的维度:</strong> VDM将隐变量 <span class="math inline">\(z\)</span>
的维度设置成与数据 <span class="math inline">\(x\)</span> 一致。<br>
</li>
<li><strong>编码器 <span class="math inline">\(q(z|x)\)</span>
的分布:</strong> 每个时刻 <span class="math inline">\(t\)</span>
的编码器 <span class="math inline">\(q(z_{t}|z_{t-1})\)</span>
不再是一个需要学习的分布，而是由前一时刻所输入 <span class="math inline">\(z_{t-1}\)</span> 为中心的高斯分布。<br>
</li>
<li><strong>隐变量高斯分布的参数:</strong> 隐变量高斯分布的参数随时间
<span class="math inline">\(t\)</span> 改变，经过 <span class="math inline">\(T\)</span> 步后最终变为标准高斯分布。</li>
</ul>
<p>  我们来尝试理解一下这些改进的 motivations。在 VAE 中，我们首先将数据
<span class="math inline">\(x\)</span> 由数据空间通过编码器 <span class="math inline">\(q_{\phi}(z|x)\)</span> 映射到隐空间中，隐变量
<span class="math inline">\(z\)</span> 的维度比 <span class="math inline">\(x\)</span>
要小，这一步的目的是希望隐变量能够抽象出数据 <span class="math inline">\(x\)</span>
的一般分布特征，而忽略掉特殊细节；隐变量 <span class="math inline">\(z\)</span> 的分布为高斯分布 <span class="math inline">\(N(z;
\boldsymbol{\mu}_{\phi}(x),\sigma_{\phi}^{2}(x)\boldsymbol{I})\)</span>，参数由解码器计算出。同时我们希望该高斯分布与标准高斯分布的
KL Divergence
尽可能小，即尽可能相似，这一方面是因为我们希望提升模型的泛化性能，避免模型学习到的模式过于单一，另一方面是因为在采样时我们需要从标准高斯分布采样出隐变量
<span class="math inline">\(z\)</span>，再由解码器生成新的样本 <span class="math inline">\(x'\)</span>，在训练时要求隐变量 <span class="math inline">\(z\)</span>
的分布与标准高斯分布尽可能相似也是希望能够与采样过程匹配。但事与愿违，由于训练目标的对抗性，隐变量的分布无法与高斯分布非常相似，另外单个隐变量对于分布特征的抽象能力也十分有限，这造成原始
VAE 所生成的图片大多非常模糊，效果不佳。<br>
  MHVAE
采用了多层次隐变量的架构，通过叠加多个隐变量，使得编码器一步一步地将原始数据的分布特征抽象出来，再通过解码器一步一步对隐变量进行解码，生成新样本。从一步到多步，虽然计算过程变得更复杂，但模型的学习数据分布的能力会变得更强，经过多次编码，隐变量的分布特征变得越发不明显(抽象)，其与标准高斯分布的相似程度也会越高，这样就能更加匹配采样过程。但
MHVAE
也具有缺陷，它虽然改善了分布不匹配问题，但由于训练目标的对抗性，仍无法彻底解决这个问题。同时，由于存在多个参数化的编码器与解码器，模型参数量较多，模型的训练需要很长时间。
  现在我们来讨论 VDM 的想法，既然采样过程是先从标准高斯分布采样出 <span class="math inline">\(z\)</span> ，再经过解码器生成新样本 <span class="math inline">\(x'\)</span>，VAE 与 MHVAE 均是希望应该将数据
<span class="math inline">\(x\)</span>
编码到与标准高斯分布相似的隐变量分布，以匹配采样过程，主要的困难在于很难学习出能够实现这一过程的编码器
<span class="math inline">\(q_{\phi}(z|x)\)</span>。VDM
的想法便是，既然编码器很难学，那干脆不学了，人为设定编码器，通过更长的步骤，将数据
<span class="math inline">\(x\)</span>
逐步编码到近似标准高斯分布(随机噪声)。既然要将数据 <span class="math inline">\(x\)</span>
逐步编码为近似噪声，那干脆采用逐步加噪的方法，这种想法最为简洁，即：</p>
<p><span class="math display">\[x_{t} = \sqrt{\alpha_{t}} x_{t-1} +
\sqrt{1-\alpha_{t}}\epsilon,\quad \epsilon \sim N(\epsilon;
\boldsymbol{0,I})\]</span></p>
<p>  其中，<span class="math inline">\(x_{0}\)</span> 表示初始的数据
<span class="math inline">\(x\)</span>，<span class="math inline">\(x_{1:T}\)</span>
表示编码后的隐变量，对应于MHVAE中的 <span class="math inline">\(z_{1:T}\)</span>，基于这种形式，我们自然需要假设隐变量的维度与数据一致。同时，由于马尔可夫性质，<span class="math inline">\(x_{t}\)</span> 的分布自然是以 <span class="math inline">\(x_{t-1}\)</span>
为中心的高斯分布(不考虑系数)。<br>
  这样设置的好处是显而易见的，在这种条件下，编码器没有参数要学习，是一个线性过程，速度较快，则可以用更长的加噪步骤使得最终得到的隐变量分布
<span class="math inline">\(q(x_{T}|x_{T-1})\)</span>
与噪声更加接近。同时，加噪的过程也可以视为将数据 <span class="math inline">\(x\)</span>
的分布特征进行抽象，例如一张清晰的猫的图片 <span class="math inline">\(x_{0}\)</span>
在经过多次加噪后，只能看见模糊的猫的轮廓了，这也是对猫的图片的分布特征的一种压缩与抽象。解码器则是从随机噪声生成新样本，与编码过程互逆。在正向加噪过程中已经产生了每个步骤加噪前与加噪后的图片对，如果解码器能够训练成编码器的逆过程，即利用正向过程得到的图片对，基于加噪后的图片预测噪声，从而得到加噪前的图片，则可以完成逐步去噪的过程，生成与原始图片相似的新样本。以猫的图片的例子类比，这个过程是从猫的一般特征(模糊)，去生成细节更加丰富的猫的图片(清晰)。这个过程大大改善了以往
VAE 所存在的分布不匹配问题。这便是我理解的 VDM 在 MHVAE 基础上的
motivations，接下来我们具体来介绍 VDM 的细节。</p>
<h2 id="概率模型">概率模型</h2>
<p>  VDM 的概率图与 MHVAE 基本相同，其概率图如下图1所示：</p>
<center>
<img src="https://s2.loli.net/2024/05/27/F7RPJpD53LSaEVq.png" srcset="/img/loading.gif" lazyload width="80%" height="80%">
<div data-align="center">
Image1: VDM 概率图
</div>
</center>
<p>  其中 <span class="math inline">\(x_0\)</span> 表示原始数据，<span class="math inline">\(x_{1:T}\)</span> 表示隐变量。由前文的讨论可知，VDM
的前向编码过程是一个不需要学习的逐步加噪过程：</p>
<p><span class="math display">\[\begin{align}
    x_{t} = \sqrt{\alpha_{t}} x_{t-1} +
\sqrt{1-\alpha_{t}}\epsilon,\quad \epsilon \sim N(\epsilon;
\boldsymbol{0,I}) \tag{1}
\end{align}\]</span></p>
<p>  其中 <span class="math inline">\(\alpha_{t}\)</span> 是随层次 <span class="math inline">\(t\)</span> 变化的常数(潜在可学习)。这样第 <span class="math inline">\(t\)</span> 层的编码器便是以 <span class="math inline">\(\sqrt{\alpha_{t}} x_{t-1}\)</span>
为均值的高斯分布。同时，与 MHVAE 一样，VDM
各层的转移概率分布也满足马尔可夫性质，故有：</p>
<p><span class="math display">\[\begin{align}
    q(x_{1:T}|x_{0}) &amp;= \prod_{t=1}^{T}q(x_{t} | x_{t-1}) \tag{2} \\
    q(x_{t} | x_{t-1}) &amp;= N(x_{t}; \sqrt{\alpha_{t}} x_{t-1},
(1-\alpha_{t})\boldsymbol{I}) \tag{3}
\end{align}\]</span></p>
<p>  从前文的第三个假设中，我们可以得知，最终层次的隐变量 <span class="math inline">\(x_{T}\)</span> 的先验分布为标准高斯分布。VDM
的逆向去噪过程需要通过参数化的解码器 <span class="math inline">\(p_{\theta}(x_{t-1}|x_{t})\)</span>，逐步将图片由高斯噪声
<span class="math inline">\(x_{T}\)</span>，还原回原始数据 <span class="math inline">\(x_{0}\)</span>。通过马尔可夫性质，我们可以写出 VDM
的联合分布：</p>
<p><span class="math display">\[\begin{align}
    p(x_{0:T}) &amp;= p(x_{T})\prod_{t=1}^{T}p_{\theta}(x_{t-1}|x_{t})
\tag{4} \\
    p(x_{T}) &amp;= N(x_{T};\boldsymbol{0,I}) \tag{5} \\
\end{align}\]</span></p>
<p>  与 MHVAE 不同的是，在 VDM 中，我们只需要学习解码器的参数 <span class="math inline">\(\boldsymbol{\theta}\)</span>。当训练完成后，采样过程便是先从标准高斯分布
<span class="math inline">\(p(x_{T})\)</span> 中采样出高斯噪声 <span class="math inline">\(x_{T}'\)</span>，再通过学习到的各层解码器
<span class="math inline">\(p_{\theta}(x_{t-1}|x_{t})\)</span> 经过
<span class="math inline">\(T\)</span> 步解码后，生成新的数据 <span class="math inline">\(x_{0}'\)</span>。</p>
<h2 id="变分下界elbo">变分下界(ELBO)</h2>
<p>  与 MHVAE 一样，VDM 同样是对似然函数的变分下界进行优化。</p>
<p><strong>VDM's ELBO</strong></p>
<p><span class="math display">\[\begin{align}
    \log{p(x)} \ge \mathbb{E}_{q(x_{1:T}|x_{0})} \left[
\log{\frac{p(x_{0:T})}{q(x_{1:T}|x_{0})}} \right] \tag{6}
\end{align}\]</span></p>
<p><strong><span class="math inline">\(Proof\)</span></strong></p>
<p><span class="math display">\[\begin{align}
    \log{p(x)} &amp;= \log{p(x)} \int q(x_{1:T}|x_{0})dx_{1:T} \notag \\
    &amp;= \int \log{p(x)} q(x_{1:T}|x_{0})dx_{1:T} \notag \\
    &amp;= \mathbb{E}_{q(x_{1:T}|x_{0})} \left[ \log{p(x)} \right]
\notag \\
    &amp;= \mathbb{E}_{q(x_{1:T}|x_{0})} \left[
\log{\frac{p(x_{0:T})q(x_{1:T}|x_{0})}{p(x_{1:T}|x_{0})q(x_{1:T}|x_{0})}}
\right] \notag \\
    &amp;= \mathbb{E}_{q(x_{1:T}|x_{0})} \left[
\log{\frac{p(x_{0:T})}{q(x_{1:T}|x_{0})}} \right] +
\mathbb{E}_{q(x_{1:T}|x_{0})} \left[
\log{\frac{q(x_{1:T}|x_{0})}{p(x_{1:T}|x_{0})}} \right] \notag \\
    &amp;= \mathbb{E}_{q(x_{1:T}|x_{0})} \left[
\log{\frac{p(x_{0:T})}{q(x_{1:T}|x_{0})}} \right] +
D_{KL}(q(x_{1:T}|x_{0}) || p(x_{1:T}|x_{0})) \tag{7} \\
    &amp;\ge \mathbb{E}_{q(x_{1:T}|x_{0})} \left[
\log{\frac{p(x_{0:T})}{q(x_{1:T}|x_{0})}} \right] \notag
\end{align}\]</span></p>
<p>  由以上证明的(7)式我们可以得知，似然函数与ELBO之间的差为 <span class="math inline">\(q(x_{1:T}|x_{0})\)</span> 与 <span class="math inline">\(p(x_{1:T}|x_{0})\)</span> 之间的 KL
Divergence，其表示给定原始数据 <span class="math inline">\(x_{0}\)</span>
后，编码过程的联合分布与解码过程的联合分布之间的KL距离。最大化 ELBO
等价于最小化这个 KL
Divergence。这个距离越小，则说明正向加噪与逆向去噪越匹配，模型的生成效果越好。进一步地，利用马尔可夫性质，我们可以将这个
KL Divergence 分解成三项：</p>
<p><span class="math display">\[\begin{align}
    &amp; D_{KL}(q(x_{1:T}|x_{0}) || p(x_{1:T}|x_{0})) \tag{8}\\
    =&amp;\sum_{t=1}^{T-1}\mathbb{E}_{q(x_{t-1},x_{t+1} | x_{0})} \left[
D_{KL}(q(x_{t}|x_{t-1}) || p_{\theta}(x_{t}|x_{t+1})) \right]
\tag{consistency term}\\
    &amp;+ \mathbb{E}_{q(x_{T-1}|x_{0})}\left[ D_{KL}(q(x_{T}|x_{T-1})
|| p(x_{T})) \right] \tag{prior matching term}\\
    &amp;- \mathbb{E}_{q(x_{1}|x_{0})}\left[
\log{\frac{p_{\theta}(x_{0}|x_{1})}{p(x_{0})}} \right]
\tag{reconstruction term}
\end{align}\]</span></p>
<p><strong><span class="math inline">\(Proof\)</span></strong></p>
<p><span class="math display">\[\begin{align}
    &amp; D_{KL}(q(x_{1:T}|x_{0}) || p(x_{1:T}|x_{0})) \notag \\
    &amp; = \mathbb{E}_{q(x_{1:T}|x_{0})}\left[
\log{\frac{q(x_{1:T}|x_{0})}{p(x_{1:T}|x_{0})}} \right] \notag\\
    &amp; = \mathbb{E}_{q(x_{1:T}|x_{0})} \left[
\log{\frac{p(x_{0})\prod_{t=1}^{T}q(x_{t}|x_{t-1})}{p(x_{T})\prod_{t=1}^{T}p_{\theta}(x_{t-1}|x_{t})}}
\right] \notag \\
    &amp; = \mathbb{E}_{q(x_{1:T}|x_{0})} \left[
\log{\frac{p(x_{0})q(x_{T}|x_{T-1})\prod_{t=1}^{T-1}q(x_{t}|x_{t-1})}{p(x_{T})p_{\theta}(x_{0}|x_{1})\prod_{t=1}^{T-1}p_{\theta}(x_{t}|x_{t+1})}}
\right] \notag \\
    &amp; =
\sum_{t=1}^{T-1}\mathbb{E}_{q(x_{t-1},x_{t},x_{t+1}|x_{0})}\left[
\log{\frac{q(x_{t}|x_{t-1})}{p_{\theta}(x_{t}|x_{t+1})}} \right] +
\mathbb{E}_{q(x_{T-1},x_{T}|x_{0})}\left[
\log{\frac{q(x_{T}|x_{T-1})}{p(x_{T})}} \right] -
\mathbb{E}_{q(x_{1}|x_{0})}\left[
\log{\frac{p_{\theta}(x_{0}|x_{1})}{p(x_{0})}} \right] \notag \\
    &amp; =
\sum_{t=1}^{T-1}\underbrace{\mathbb{E}_{q(x_{t-1},x_{t+1}|x_{0})}\left[
D_{KL}(q(x_{t}|x_{t-1}) || p_{\theta}(x_{t}|x_{t+1}))
\right]}_{consistency \ term} +
\underbrace{\mathbb{E}_{q(x_{T-1}|x_{0})}\left[ D_{KL}( q(x_{T}|x_{T-1})
|| p(x_{T})) \right]}_{prior \ matching \ term}  -
\underbrace{\mathbb{E}_{q(x_{1}|x_{0})}\left[
\log{\frac{p_{\theta}(x_{0}|x_{1})}{p(x_{0})}} \right]}_{reconstruction
\ term} \notag \\
\end{align}\]</span></p>
<p>  我们对 KL Divergence 进行了分解，得到了 consistency term, prior
matching term, reconstruction term 三项。 最小化 KL Divergence
等价于最小化这三项，即使得 consistency term, prior matching term
尽可能小，reconstruction term
尽可能大。我们先不详细解释这三项的含义，接下来我们同样对 VDM 的 ELBO(6)
进行分解，我们会发现 ELBO 分解后的结果与 KL Divergence
几乎一致，只是符号相反：</p>
<p><span class="math display">\[\begin{align}
   &amp; \mathbb{E}_{q(x_{1:T}|x_{0})} \left[
\log{\frac{p(x_{0:T})}{q(x_{1:T}|x_{0})}} \right] \tag{9} \\
   =&amp; \mathbb{E}_{q(x_{1}|x_{0})}[\log{p_{\theta}(x_{0}|x_{1})}]
\tag{reconstruction term} \\
   -&amp; \mathbb{E}_{q(x_{T-1}|x_{0})}\left[ D_{KL}( q(x_{T}|x_{T-1})
|| p(x_{T})) \right] \tag{prior matching term}\\
   -&amp; \sum_{t=1}^{T-1}\mathbb{E}_{q(x_{t-1},x_{t+1}|x_{0})}\left[
D_{KL}(q(x_{t}|x_{t-1}) || p_{\theta}(x_{t}|x_{t+1})) \right]
\tag{consistency term} \\
\end{align}\]</span></p>
<p><strong><span class="math inline">\(Proof\)</span></strong></p>
<p><span class="math display">\[\begin{align}
    &amp; \mathbb{E}_{q(x_{1:T}|x_{0})} \left[
\log{\frac{p(x_{0:T})}{q(x_{1:T}|x_{0})}} \right] \notag \\
    &amp; = \mathbb{E}_{q(x_{1:T}|x_{0})} \left[
\log{\frac{p(x_{T})\prod_{t=1}^{T}p_{\theta}(x_{t-1}|x_{t})}{\prod_{t=1}^{T}q(x_{t}|x_{t-1})}}
\right] \notag \\
    &amp;= \mathbb{E}_{q(x_{1:T}|x_{0})} \left[
\log{\frac{p(x_{T})p_{\theta}(x_{0}|x_{1})\prod_{t=2}^{T}p_{\theta}(x_{t-1}|x_{t})}{q(x_{T}|x_{T-1})\prod_{t=1}^{T-1}q(x_{t}|x_{t-1})}}
\right] \notag \\
    &amp;= \mathbb{E}_{q(x_{1:T}|x_{0})} \left[
\log{\frac{p(x_{T})p_{\theta}(x_{0}|x_{1})\prod_{t=1}^{T-1}p_{\theta}(x_{t}|x_{t+1})}{q(x_{T}|x_{T-1})\prod_{t=1}^{T-1}q(x_{t}|x_{t-1})}}
\right] \notag \\
    &amp;= \mathbb{E}_{q(x_{1:T}|x_{0})} \left[
\log{\frac{p(x_{T})p_{\theta}(x_{0}|x_{1})}{q(x_{T}|x_{T-1})}} \right] +
\mathbb{E}_{q(x_{1:T}|x_{0})}\left[ \log{\prod_{i=1}^{T-1}}
\frac{p_{\theta}(x_{t}|x_{t+1})}{q(x_{t}|x_{t-1})} \right] \notag \\  
    &amp;= \mathbb{E}_{q(x_{1}|x_{0})}[\log{p_{\theta}(x_{0}|x_{1})}] +
\mathbb{E}_{q(x_{T-1},x_{T}|x_{0})} \left[
\log{\frac{p(x_{T})}{q(x_{T}|x_{T-1})}} \right] +
\sum_{i=1}^{T-1}\mathbb{E}_{q(x_{1:T}|x_{0})}\left[ \log{
\frac{p_{\theta}(x_{t}|x_{t+1})}{q(x_{t}|x_{t-1})}} \right] \notag \\
    &amp;=
\underbrace{\mathbb{E}_{q(x_{1}|x_{0})}[\log{p_{\theta}(x_{0}|x_{1})}]}_{reconstruction
\ term} -
\underbrace{\mathbb{E}_{q(x_{T-1}|x_{0})}[D_{KL}(q(x_{T}|x_{T-1}) ||
p(x_{T}))]}_{prior \ matching \ term} -
\sum_{i=1}^{T-1}\underbrace{\mathbb{E}_{q(x_{t-1},x_{t+1}|x_{0})}\left[
D_{KL}(q(x_{t}|x_{t-1}) || p_{\theta}(x_{t} || x_{t+1}))
\right]}_{consistency \ term} \notag \\
\end{align}\]</span></p>
<p>  由以上的推导我们将 ELBO 分解为与 KL Divergence
相似的三项，只是符号相反，这也从另一个方面说明了最大化 ELBO
实际上等价于最小化 KL Divergence。最大化 ELBO 的过程是使得
reconstruction term 尽可能大，prior matching term, consistency term
尽可能小。接下来以 ELBO
的分解结果为例，我们尝试理解一下这三项的含义。</p>
<ul>
<li><strong>reconstruction term:</strong>
重构项。这一项的含义是将原始数据 <span class="math inline">\(x_{0}\)</span> 编码一次后得到隐变量 <span class="math inline">\(x_{1}\)</span> 后再通过解码器还原回 <span class="math inline">\(x_{0}\)</span>，所得到的对数似然。这一项在 VAE
中也存在，这一项的值越大，表明数据在编码与解码后与原始数据更相似，即生成的效果更好。<br>
</li>
<li><strong>prior matching term:</strong>
先验匹配项。这一项的含义是最终隐变量 <span class="math inline">\(x_{T}\)</span> 的后验分布 <span class="math inline">\(q(x_{T}|x_{T-1})\)</span> 与其先验分布 <span class="math inline">\(p(x_{T})\)</span> 之间的 KL Divergence
的期望。这一项越小，则先验与后验越匹配，说明编码过程得到的最终隐变量的分布与标准高斯分布越接近，更能匹配采样过程。<br>
</li>
<li><strong>consistency term:</strong>
一致性项。这一项的含义是从前向和后向两个过程努力使 <span class="math inline">\(x_{t}\)</span>
处的分布保持一致。如图2所示，对于每一个中间时间步 <span class="math inline">\(t\)</span>，从噪声图像中得到的去噪图片的分布 <span class="math inline">\(p_{\theta}(x_{t}|x_{t+1})\)</span>
应该与从干净图像中得到的相应加噪步骤得到的图片的分布 <span class="math inline">\(q(x_{t}|x_{t-1})\)</span> 相匹配，这在数学上通过KL
Divergence得到了体现。这一项越小，说明解码器 <span class="math inline">\(p_{\theta}(x_{t}|x_{t-})\)</span>
被训练的越好。</li>
</ul>
<center>
<img src="https://s2.loli.net/2024/05/29/Aq8Eb5kvrWURtNX.png" srcset="/img/loading.gif" lazyload width="80%" height="80%">
<div data-align="center">
Image2: 一致性
</div>
</center>
<p>  通过以上的推导，VDM 的 ELBO
均分解成了期望的形式，我们可以使用蒙特卡洛方法来对这些项进行近似，然而，实际上使用我们刚才推导出的(9)式来优化ELBO可能是次优的；因为一致性项在每个时间步
<span class="math inline">\(t\)</span> 上都被计算为两个随机变量 <span class="math inline">\(x_{t-1},x_{t+1}\)</span>
的期望值，因此其蒙特卡洛估计的方差有可能高于每个时间步仅使用一个随机变量估计的项。由于它是通过对
<span class="math inline">\(T-1\)</span>
个时间步求和来计算的，因此对于大的T值，ELBO的最终估计值可能具有很高的方差。<br>
  为了改善这个问题，我们尝试对 (9)
式做一些变形。基于马尔可夫性与贝叶斯公式，我们可以得到以下等式：</p>
<p><span class="math display">\[\begin{align}
    q(x_{t}|x_{t-1}) = q(x_{t}|x_{t-1},x_{0}) =
\frac{q(x_{t-1})q(x_{t}|x_{0})}{q(x_{t-1}|x_{0})} \tag{10}
\end{align}\]</span></p>
<p>  通过(10)式，我们可以将 VDM 的 ELBO 分解为如下形式：</p>
<p><span class="math display">\[\begin{align}
   &amp; \mathbb{E}_{q(x_{1:T}|x_{0})} \left[
\log{\frac{p(x_{0:T})}{q(x_{1:T}|x_{0})}} \right] \tag{11} \\
   =&amp; \mathbb{E}_{q(x_{1}|x_{0})}[\log{p_{\theta}(x_{0}|x_{1})}]
\tag{reconstruction term} \\
   -&amp; D_{KL}( q(x_{T}|x_{0}) || p(x_{T}))  \tag{prior matching
term}\\
   -&amp; \sum_{t=2}^{T}\mathbb{E}_{q(x_{t}|x_{0})}\left[
D_{KL}(q(x_{t-1}|x_{t},x_{0}) || p_{\theta}(x_{t-1}|x_{t})) \right]
\tag{denoising matching term} \\
\end{align}\]</span></p>
<p><strong><span class="math inline">\(Proof\)</span></strong></p>
<p><span class="math display">\[\begin{align}
    &amp; \mathbb{E}_{q(x_{1:T}|x_{0})} \left[
\log{\frac{p(x_{0:T})}{q(x_{1:T}|x_{0})}} \right] \notag \\
    &amp;= \mathbb{E}_{q(x_{1:T}|x_{0})} \left[
\log{\frac{p(x_{T})\prod_{t=1}^{T}p_{\theta}(x_{t-1}|x_{t})}{\prod_{t=1}^{T}q(x_{t}|x_{t-1})}}
\right] \notag \\
    &amp;= \mathbb{E}_{q(x_{1:T}|x_{0})} \left[
\log{\frac{p(x_{T})p_{\theta}(x_{0}|x_{1})\prod_{t=2}^{T}p_{\theta}(x_{t-1}|x_{t})}{q(x_{1}|x_{0})\prod_{t=2}^{T}q(x_{t}|x_{t-1})}}
\right] \notag \\
    &amp;= \mathbb{E}_{q(x_{1:T}|x_{0})} \left[
\log{\frac{p(x_{T})p_{\theta}(x_{0}|x_{1})}{q(x_{1}|x_{0})}} +
\log{\prod_{t=2}^{T}\frac{p_{\theta}(x_{t-1}|x_{t})}{q(x_{t}|x_{t-1},x_{0})}}
\right] \notag \\
    &amp;= \mathbb{E}_{q(x_{1:T}|x_{0})} \left[
\log{\frac{p(x_{T})p_{\theta}(x_{0}|x_{1})}{q(x_{1}|x_{0})}} +
\log{\prod_{t=2}^{T}\frac{p_{\theta}(x_{t-1}|x_{t})}{\frac{q(x_{t-1}|x_{t},x_{0})\cancel{q(x_{t}|x_{0})}}{\cancel{q(x_{t-1}|x_{0})}}}}
\right] \notag \\
    &amp;= \mathbb{E}_{q(x_{1:T}|x_{0})} \left[
\log{\frac{p(x_{T})p_{\theta}(x_{0}|x_{1})}{\cancel{q(x_{1}|x_{0})}}}
+\log{\frac{\cancel{q(x_{1}|x_{0})}}{q(x_{T}|x_{0})}} +
\log{\prod_{t=2}^{T}\frac{p_{\theta}(x_{t-1}|x_{t})}{q(x_{t-1}|x_{t},x_{0})}}
\right] \notag \\
    &amp;= \mathbb{E}_{q(x_{1}|x_{0})}[\log{p_{\theta}(x_{0}|x_{1})}] -
\mathbb{E}_{q(x_{T}|x_{0})}\left[ \log{\frac{q(x_{T}|x_{0})}{p(x_{T})}}
\right] -\sum_{t=2}^{T} \mathbb{E}_{q(x_{t},x_{t-1}|x_{0})}\left[
\log{\frac{q(x_{t-1}|x_{t},x_{0})}{p_{\theta}(x_{t-1}|x_{t})}} \right]
\notag \\
    &amp;=
\underbrace{\mathbb{E}_{q(x_{1}|x_{0})}[\log{p_{\theta}(x_{0}|x_{1})}]}_{reconstruction
\ term} - \underbrace{D_{KL}(q(x_{T}|x_{0}) || p(x_{T}))}_{prior \
matching \ term} -
\sum_{t=2}^{T}\underbrace{\mathbb{E}_{q(x_{t}|x_{0})}[D_{KL}(q(x_{t-1}|x_{t},x_{0})
|| p_{\theta}(x_{t-1}|x_{t}))]}_{denoising \ matching \ term} \notag \\
\end{align}\]</span></p>
<p>  reconstruction term 与 prior matching term
的含义与(9)基本一致。差别较大的是(9)式中的 consistency term 与
(11)式中的 denoising matching term。与 consistency term 相比，denoising
matching term 中的每一时间步 <span class="math inline">\(t\)</span>，只需要计算一个随机变量 <span class="math inline">\(x_{t}\)</span>
的期望，显著改善了蒙特卡洛估计的方差较大的问题。同时，最小化 denoising
matching term 意味着在每一个时间步 <span class="math inline">\(t\)</span>，通过解码器去噪后的数据的分布 <span class="math inline">\(p_{\theta}(x_{t-1}|x_{t})\)</span>
与真实加噪过程中加入噪声前的图片的分布 <span class="math inline">\(q(x_{t-1}|x_{t},x_{0})\)</span> 相匹配，即 KL
Divergence
尽可能小。这一项越小，说明解码器每一步预测噪声的能力越强，即从一般抽象特征去生成更加细节的特征的能力越强，生成图片与原始图片就会越相似，这一过程如图3所示。</p>
<center>
<img src="https://s2.loli.net/2024/05/29/FAxezibV6GnNpv3.png" srcset="/img/loading.gif" lazyload width="80%" height="80%">
<div data-align="center">
Image3: 去噪匹配
</div>
</center>
<h2 id="损失函数">损失函数</h2>
<p>  在前文中，我们推导了 VDM's ELBO 的理论形式(11)，通过最大化 ELBO
来近似最大化对数似然，得到待估参数 <span class="math inline">\(\theta\)</span>。现在我们要利用 VDM
的假设条件，根据 ELBO
的理论形式，来得到具体用于模型训练的损失函数。<br>
  通过前文的分析，我们可以将VDM 的损失函数写作如下的三部分:</p>
<p><span class="math display">\[\begin{align}
    \boldsymbol{L}(\theta) = -ELBO = \mathbb{E}_{q}\left[
\underbrace{D_{KL}(q(x_{T}|x_{0}) || p(x_{T}))}_{L_{T}} +
\sum_{t=2}^{T}\underbrace{D_{KL}(q(x_{t-1}|x_{t},x_{0}) ||
p_{\theta}(x_{t-1}|x_{t}))}_{L_{t-1}} -
\underbrace{\log{p_{\theta}(x_{0}|x_{1})}}_{L_{0}} \right] \tag{12}
\end{align}\]</span></p>
<p>  接下来我们来逐个讨论这三项的具体形式。</p>
<h3 id="先验匹配损失-l_t">先验匹配损失 <span class="math inline">\(L_{T}\)</span></h3>
<p>  这一项是衡量最终隐变量 <span class="math inline">\(X_{T}\)</span>
的先验分布与后验分布的相似程度，其中 <span class="math inline">\(p(x_{T})\)</span> 是标准高斯分布，当给定数据 <span class="math inline">\(x_{0}\)</span> 后， <span class="math inline">\(x_{T}\)</span> 的后验分布 <span class="math inline">\(q(x_{T}|x_{0})\)</span> 可以不依赖于参数 <span class="math inline">\(\theta\)</span> 计算出，故 <span class="math inline">\(L_{T}\)</span>
损失函数中相当于常数，可以不考虑。</p>
<h3 id="去噪匹配损失-l_t-1">去噪匹配损失 <span class="math inline">\(L_{t-1}\)</span></h3>
<p>  去噪匹配损失 <span class="math inline">\(L_{t-1}\)</span>
在损失函数中占主导地位，其衡量了每个时间步 <span class="math inline">\(t\)</span>，编码器的去噪后得到的图片与加噪过程中该时刻的真实图片的相似程度。在
<span class="math inline">\(L_{t-1}\)</span> 中我们最主要是需要计算
<span class="math inline">\(D_{KL}(q(x_{t-1}|x_{t},x_{0}) ||
p_{\theta}(x_{t-1}|x_{t}))\)</span>，对于其中的编码器 <span class="math inline">\(q(x_{t-1}|x_{t},x_{0})\)</span>，由贝叶斯公式以及马尔可夫性质可以得到：</p>
<p><span class="math display">\[\begin{align}
    &amp; q(x_{t-1}|x_{t},x_{0}) =
\frac{q(x_{t}|x_{t-1},x_{0})q(x_{t-1}|x_{0})}{q(x_{t}|x_{0})} =
\frac{q(x_{t}|x_{t-1})q(x_{t-1}|x_{0})}{q(x_{t}|x_{0})} \tag{13} \\
    &amp; q(x_{t}|x_{t-1}) = N(x_{t}; \sqrt{\alpha_{t}}x_{t-1},
(1-\alpha_{t})\boldsymbol{I}) \tag{14}
\end{align}\]</span></p>
<p>  在前文中，我们已经得知了正向加噪过程满足递推公式：</p>
<p><span class="math display">\[x_{t} = \sqrt{\alpha_{t}} x_{t-1} +
\sqrt{1-\alpha_{t}}\epsilon,\quad \epsilon \sim N(\epsilon;
\boldsymbol{0,I})\]</span></p>
<p>  利用递推公式，我们可以计算出 <span class="math inline">\(q(x_{t}|x_{0})\)</span> 所满足的高斯分布：</p>
<p><span class="math display">\[\begin{align}
    q(x_{t}|x_{0}) = N(x_{t}; \sqrt{\bar{\alpha}_{t}}x_{0}, (1 -
\bar{\alpha}_{t})\boldsymbol{I}),\quad \bar{\alpha}_{t} =
\prod_{i=1}^{t}\alpha_{i} \tag{15} \\
\end{align}\]</span></p>
<p><strong><span class="math inline">\(Proof\)</span></strong></p>
<p><span class="math display">\[\begin{align}
    x_{t} &amp;= \sqrt{\alpha_{t}} x_{t-1} +
\sqrt{1-\alpha_{t}}\epsilon_{t-1}^{*} \notag \\
    &amp;= \sqrt{\alpha_{t}} \left( \sqrt{\alpha_{t-1}} x_{t-2} +
\sqrt{1-\alpha_{t-1}}\epsilon_{t-2}^{*} \right) +
\sqrt{1-\alpha_{t}}\epsilon_{t-1}^{*} \notag \\
    &amp;= \sqrt{\alpha_{t}\alpha_{t-1}} x_{t-2} +
\sqrt{\alpha_{t}-\alpha_{t}\alpha_{t-1}}\epsilon_{t-2}^{*} +
\sqrt{1-\alpha_{t}}\epsilon_{t-1}^{*} \notag \\
    &amp;= \sqrt{\alpha_{t}\alpha_{t-1}} x_{t-2} +
\sqrt{\sqrt{\alpha_{t}-\alpha_{t}\alpha_{t-1}}^{2} +
\sqrt{1-\alpha_{t}}^{2}}\epsilon_{t-2} \notag \\
    &amp;= \sqrt{\alpha_{t}\alpha_{t-1}} x_{t-2} + \sqrt{1 -
\alpha_{t}\alpha_{t-1}}\epsilon_{t-2} \notag \\
    &amp;= \dotsb \notag \\
    &amp;= \sqrt{\prod_{i=1}^{t}\alpha_{i}}x_{0} +
\sqrt{1-\prod_{i=1}^{t}\alpha_{i}} \epsilon_{0} \notag \\
    &amp;= \sqrt{\bar{\alpha}_{t}}x_{0} +
\sqrt{1-\bar{\alpha}_{t}}\epsilon_{0} \sim N(x_{t};
\sqrt{\bar{\alpha}_{t}}x_{0}, (1 - \bar{\alpha}_{t})\boldsymbol{I})
\notag \\
\end{align}\]</span></p>
<p>  利用 (15) 式，我们可以得到 <span class="math inline">\(x_{t-1}\)</span> 的分布：</p>
<p><span class="math display">\[\begin{align}
    q(x_{t-1}|x_{0}) = N(x_{t-1}; \sqrt{\bar{\alpha}_{t-1}}x_{0}, (1 -
\bar{\alpha}_{t-1})\boldsymbol{I}) \tag{16} \\
\end{align}\]</span></p>
<p>  联立(14)、(15)、(16)式，我们可以发现(13)式的分子分母均为高斯分布，由高斯分布的联合分布与边际分布均为高斯分布可知，
<span class="math inline">\(q(x_{t-1}|x_{t},x_{0})\)</span>，同样满足高斯分布，现在我们需要利用(13)式来计算其均值与方差，通过计算我们可以得到如下结论：</p>
<p><span class="math display">\[\begin{align}
    q(x_{t-1}|x_{t},x_{0}) = N(x_{t-1}; \mu_{q}(x_{t},x_{0}),
\Sigma_{q}(t)) \tag{17} \\
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
    \mu_{q}(x_{t},x_{0}) &amp;=
\frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})x_{t} +
\sqrt{\bar{\alpha}_{t-1}}(1-\alpha_{t})x_{0}}{1-\bar{\alpha}_{t}},\quad
\Sigma_{q}(t) = \sigma_{q}^{2}(t)\boldsymbol{I} =
\frac{1-\alpha_{t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_{t}}\boldsymbol{I}
\tag{18} \\
\end{align}\]</span></p>
<p><strong><span class="math inline">\(Proof\)</span></strong></p>
<p><span class="math display">\[\begin{align}
    q(x_{t-1}|x_{t},x_{0}) &amp;=
\frac{q(x_{t}|x_{t-1})q(x_{t-1}|x_{0})}{q(x_{t}|x_{0})} \notag \\
    &amp;= \frac{N(x_{t}; \sqrt{\alpha_{t}}x_{t-1},
(1-\alpha_{t})\boldsymbol{I})N(x_{t-1}; \sqrt{\bar{\alpha}_{t-1}}x_{0},
(1 - \bar{\alpha}_{t-1})\boldsymbol{I})}{N(x_{t};
\sqrt{\bar{\alpha}_{t}}x_{0}, (1 -
\bar{\alpha}_{t})\boldsymbol{I}),\quad \bar{\alpha}_{t}} \notag \\
    &amp; \propto \exp \left(-\frac{1}{2}\left[
\frac{(x_{t}-\sqrt{\alpha_{t}}x_{t-1})^2}{1-\alpha_{t}} + \frac{(x_{t-1}
- \sqrt{\bar{\alpha}_{t-1}}x_{0})^2}{1 - \bar{\alpha}_{t-1}} -
\frac{(x_{t} - \sqrt{\bar{\alpha}_{t}}x_{0})^2}{1 - \bar{\alpha}_{t}}
\right] \right)  \notag \\
    &amp;= \exp\left( -\frac{1}{2}\left[
\frac{(-2\sqrt{\alpha_{t}}x_{t}x_{t-1}+\alpha_{t}x_{t-1}^{2})}{1-\alpha_{t}}
+ \frac{(x_{t-1}^{2}-2\sqrt{\bar{\alpha}_{t-1}}x_{t-1}x_{0})}{1 -
\bar{\alpha}_{t-1}} + C(x_{t},x_{0}) \right] \right)  \notag \\
    &amp; \propto \exp\left( -\frac{1}{2}\left[
\frac{1-\bar{\alpha}_{t}}{(1-\alpha_{t})(1-\bar{\alpha}_{t-1})}x_{t-1}^{2}
- 2\left( \frac{\sqrt{\alpha_{t}}x_{t}}{1-\alpha_{t}} +
\frac{\sqrt{\bar{\alpha}_{t-1}}x_{0}}{1-\bar{\alpha}_{t-1}}
\right)x_{t-1} \right] \right)   \notag \\
    &amp;= \exp\left( -\frac{1}{2}\left(
\frac{1-\bar{\alpha}_{t}}{(1-\alpha_{t})(1-\bar{\alpha}_{t-1})}
\right)\left[ x_{t-1}^{2}-2\frac{\left(
\frac{\sqrt{\alpha_{t}}x_{t}}{1-\alpha_{t}} +
\frac{\sqrt{\bar{\alpha}_{t-1}}x_{0}}{1-\bar{\alpha}_{t-1}}
\right)(1-\alpha_{t})(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_{t}}x_{t-1}
\right] \right)  \notag \\
    &amp;= \exp\left( -\frac{1}{2}\left(
\frac{1}{\frac{(1-\alpha_{t})(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_{t}}}
\right)\left[
x_{t-1}^{2}-2\frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})x_{t} +
\sqrt{\bar{\alpha}_{t-1}}(1-\alpha_{t})x_{0}}{1-\bar{\alpha}_{t}}x_{t-1}
\right] \right)  \notag \\
    &amp; \propto N(x_{t+1};
\underbrace{\frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})x_{t} +
\sqrt{\bar{\alpha}_{t-1}}(1-\alpha_{t})x_{0}}{1-\bar{\alpha}_{t}}}_{\mu_{q}(x_{t},x_{0})},
\underbrace{\frac{(1-\alpha_{t})(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_{t}}\boldsymbol{I}}_{\Sigma_{q}(t)=\sigma_{q}^{2}(t)\boldsymbol{I}})
\notag \\
\end{align}\]</span></p>
<p>  通过以上的推导，我们得到了加噪过程中<span class="math inline">\(x_{t-1}\)</span>所满足的高斯分布，要计算 <span class="math inline">\(L_{t-1}\)</span> 式中的 KL
Divergence，我们还需要去噪过程中 <span class="math inline">\(x_{t-1}\)</span> 的分布。<br>
  为了使得去噪过程与加噪过程尽可能匹配，我们同样将去噪过程建模为高斯过程，即
<span class="math inline">\(p_{\theta}(x_{t-1}|x_{t})\)</span>
满足高斯分布。去噪过程的高斯分布的方差与对应的加噪过程的方差一致，而均值是由参数化的神经网络计算得出。对于均值的计算，VDM将
<span class="math inline">\(p_{\theta}(x_{t-1}|x_{t})\)</span>
所满足的高斯分布的均值设定为与 <span class="math inline">\(q(x_{t-1}|x_{t},x_{0})\)</span> 具有相同的形式，即
<span class="math inline">\(\mu_{q}(x_{t},x_{0})\)</span>，但在去噪过程中是没有给定
<span class="math inline">\(x_{0}\)</span>
的，故神经网络在均值计算中的实际作用是输出 <span class="math inline">\(x_{0}\)</span> 的预测值 <span class="math inline">\(\hat{x}_{\theta}(x_{t-1},t)\)</span> ，从而得到
<span class="math inline">\(p_{\theta}(x_{t-1}|x_{t})\)</span>
所满足的高斯分布的均值。以上建模过程总结的数学表达式如下：</p>
<p><span class="math display">\[\begin{align}
    p_{\theta}(x_{t-1}|x_{t}) = N(x_{t-1}; \mu_{\theta}(x_{t},t),
\Sigma_{p}(t))  \tag{19}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
    \mu_{\theta}(x_{t},t) &amp;=
\frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})x_{t} +
\sqrt{\bar{\alpha}_{t-1}}(1-\alpha_{t})\hat{x}_{\theta}(x_{t-1},t)}{1-\bar{\alpha}_{t}},\quad
\Sigma_{p}(t) = \Sigma_{q}(t) =
\frac{1-\alpha_{t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_{t}}\boldsymbol{I}
\tag{20} \\
\end{align}\]</span></p>
<p>  通过以上的建模，去噪匹配损失 <span class="math inline">\(L_{t-1}\)</span> 中的 KL Divergence
实际上是计算两个方差相同的高斯分布的 KL
Divergence，这使得问题变得非常简单，因为高斯分布的 KL Divergence
是有显式表达式的，其表达式如下所示：</p>
<p><span class="math display">\[\begin{align}
    D_{KL}(N(x;\mu_{x},\Sigma_{x}) || N(y;\mu_{y},\Sigma_{y})) =
\frac{1}{2}\left[ \log{\frac{|\Sigma_{y}|}{|\Sigma_{x}|}}-d +
tr(\Sigma_{y}^{-1}\Sigma_{x}) + (\mu_{y} -
\mu_{x})^{T}\Sigma_{y}^{-1}(\mu_{y}-\mu_{x}) \right] \tag{21}
\end{align}\]</span></p>
<p>  其中，<span class="math inline">\(d\)</span>
是高斯分布的维度。结合(17)、(18)、(19)、(20)、(21)式，我们现在可以来计算
<span class="math inline">\(L_{t-1}\)</span> 中的 KL Divergence
的具体表达式了，其结果如下：</p>
<p><span class="math display">\[\begin{align}
    D_{KL}(q(x_{t-1}|x_{t},x_{0}) || p_{\theta}(x_{t-1}|x_{t})) =
\frac{1}{2\sigma_{q}^{2}(t)}\frac{\bar{\alpha}_{t-1}(1-\alpha_{t})^{2}}{(1-\bar{\alpha}_{t})^{2}}
\left[ ||\hat{x}_{\theta}(x_{t-1},t) - x_{0}||_{2}^{2} \right] \tag{22}
\end{align}\]</span></p>
<p><strong><span class="math inline">\(Proof\)</span></strong></p>
<p><span class="math display">\[\begin{align}
    &amp; D_{KL}(q(x_{t-1}|x_{t},x_{0}) || p_{\theta}(x_{t-1}|x_{t}))
\notag \\
    &amp;= D_{KL}(N(x_{t-1}; \mu_{q}, \Sigma_{q}(t)) || N(x_{t-1};
\mu_{\theta}, \Sigma_{p}(t)))  \notag \\
    &amp;= \frac{1}{2}\left[
\log{\frac{|\Sigma_{q}(t)|}{|\Sigma_{q}(t)|}}-d +
tr(\Sigma_{q}(t)^{-1}\Sigma_{q}(t)) + (\mu_{q} -
\mu_{\theta})^{T}\Sigma_{q}(t)^{-1}(\mu_{q}-\mu_{\theta}) \right] \notag
\\
    &amp;= \frac{1}{2}\left[ \log{1}-d + d + (\mu_{q} -
\mu_{\theta})^{T}\Sigma_{q}(t)^{-1}(\mu_{q}-\mu_{\theta}) \right] \notag
\\
    &amp;= \frac{1}{2}\left[(\mu_{q} -
\mu_{\theta})^{T}(\sigma_{q}^{2}(t)\boldsymbol{I})^{-1}(\mu_{q}-\mu_{\theta})
\right] \notag \\
    &amp;= \frac{1}{2\sigma_{q}^{2}(t)} \left[ ||\mu_{q} - \mu_{\theta}
||_{2}^{2} \right] \notag \\
    &amp;=  \frac{1}{2\sigma_{q}^{2}(t)} \left[||
\frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})x_{t} +
\sqrt{\bar{\alpha}_{t-1}}(1-\alpha_{t})x_{0}}{1-\bar{\alpha}_{t}}-
\frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})x_{t} +
\sqrt{\bar{\alpha}_{t-1}}(1-\alpha_{t})\hat{x}_{\theta}(x_{t-1},t)}{1-\bar{\alpha}_{t}}||_{2}^{2}\right]\notag
\\
    &amp;= \frac{1}{2\sigma_{q}^{2}(t)} \left[||
\frac{\sqrt{\bar{\alpha}_{t-1}}(1-\alpha_{t})}{1-\bar{\alpha}_{t}}
(\hat{x}_{\theta}(x_{t-1},t) - x_{0}) ||_{2}^{2}\right]  \notag \\
    &amp;=
\frac{1}{2\sigma_{q}^{2}(t)}\frac{\bar{\alpha}_{t-1}(1-\alpha_{t})^{2}}{(1-\bar{\alpha}_{t})^{2}}
\left[ ||\hat{x}_{\theta}(x_{t-1},t) - x_{0}||_{2}^{2} \right] \notag \\
\end{align}\]</span></p>
<p>  对于(12)式中的期望，可以在每个训练批次中使用 Monte Carlo estimate
方法来估计。通过 (22) 式 我们可以得知，VDM
在逆向去噪过程中，每一步的优化目标都是在给定噪声图片 <span class="math inline">\(x_{t-1}\)</span> 的情况下，预测出原始图片 <span class="math inline">\(x_{0}\)</span>。这种损失函数的设定方式可以总结为<strong>预测原始数据</strong>，在之后的章节我们会讨论扩散模型损失函数的另外两种等价形式，分别为<strong>预测噪声</strong>以及<strong>分数匹配</strong>。</p>
<h3 id="重构似然损失-l_0">重构似然损失 <span class="math inline">\(L_{0}\)</span></h3>
<p>  对于损失函数中的 <span class="math inline">\(L_{0}\)</span> 项，在
DDPM 的原始论文[2]
中，采用了一个独立的离散编码器。这是因为在之前的加噪去噪步骤中，我们都将图片数据的取值由
<span class="math inline">\(\{ 0,1,\dotsb,255 \}\)</span>
的离散数值映射到 <span class="math inline">\([-1,1]\)</span>。这一方面是使数据标准化，便于神经网络处理；另一方面是因为采样过程是从标准高斯分布中进行采样，再由解码器逐步去噪，故需要在训练时的加噪去噪过程对离散数据进行映射后来匹配采样时的数值范围。<br>
  具体来讲，由之前步骤训练得到的参数化的神经网络，以及去噪得到的数据
<span class="math inline">\(x_{1}\)</span>，我们可以同样可以得到 <span class="math inline">\(p_{\theta}(x_{0}|x_{1})\)</span> 所满足的高斯分布
<span class="math inline">\(N(x_{0};
\mu_{\theta}(x_{1},1),\sigma_{1}^{2})\)</span>，但在这里我们不能像去噪过程一样，直接使用该高斯分布来计算
<span class="math inline">\(x_{0}\)</span> 的对数似然，这是因为原始数据
<span class="math inline">\(x_{0}\)</span>
是离散数据，而高斯分布本身是连续的，直接使用高斯分布来计算离散对数本事是不够准确的。在
DDPM 的原始论文[2]
中，作者采用了一个离散边界函数计算高斯分布在离散区间上的积分，从而精确计算离散数据的对数似然。其计算公式如下：</p>
<p><span class="math display">\[\begin{align}
    p_{\theta}(x_{0}|x_{1}) = \prod_{i=1}^{d}
\int_{\delta_{-}(x_{0}^{i})}^{\delta_{+}(x_{0}^{i})} N(x_{0};
\mu_{\theta}(x_{1},1),\sigma_{1}^{2}) dx \tag{22} \\
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
    \delta_{+}(x_{0}^{i}) = \left \{
\begin{array}{l}
\infty &amp; if \ x = 1 \\
x + \frac{1}{255} &amp; if \ x &lt; 1 \\
\end{array} \right. \quad \delta_{-}(x_{0}^{i}) = \left \{
\begin{array}{l}
-\infty &amp; if \ x = -1 \\
x - \frac{1}{255} &amp; if \ x &gt; -1 \\
\end{array} \right.
\tag{23}
\end{align}\]</span></p>
<p>  其中，<span class="math inline">\(i\)</span> 表示数据 <span class="math inline">\(x_{0}\)</span> 的每一个维度，<span class="math inline">\(\delta_{+}(x_{0}^{i}),\delta_{-}(x_{0}^{i})\)</span>
表示每个数据维度的离散值的上下边界。由于在做数据映射时，是使用 <span class="math inline">\(x' = (2x - 255) / 255\)</span>
将数据取值范围由 <span class="math inline">\(\{ 0,1,\dotsb,255
\}\)</span> 映射到 <span class="math inline">\([0,1]\)</span>的，故
<span class="math inline">\(x' \pm \frac{1}{255} = [2(x \pm 0.5)
-255]/255\)</span>，故(23)式设置的离散边界实际上是等价于在离散值上下分别加上0.5后再计算高斯分布的积分。使用这种方法可以较为精确的计算离散数据的对数似然。</p>
<h2 id="总结">总结</h2>
<p>  在这篇博客中，我们首先讨论了 VDM 相较于之前的 MHVAE
又做了哪些假设，以及做出这些假设的 motivations。之后与 VAE
类似，我们讨论了 VDM 的 ELBO
的理论表达式，其可以分解为三项，在此基础上，我们计算了 ELBO
三个分解项在损失函数中的具体表达式。<br>
  但实际上，DDPM
在训练中并不是使用<strong>预测原始数据</strong>，即(22)式作为损失函数，而是使用
<strong>预测噪声</strong>
作训损失函数，在之后的一些研究中，例如基于分数的生成模型，则是使用
<strong>分数匹配</strong>
作为损失函数，这三种扩散模型的损失函数实际上是等价的，这一点由于篇幅的限制我们不再做过多的讨论。在下一节，我们将重点讨论三种损失函数的等价关系，如何做训练以及采样，以及扩散模型系数该如何设置或学习。</p>
<h2 id="reference">Reference</h2>
<p><strong>[1] Paper: Luo C. Understanding diffusion models: A unified
perspective[J]. arXiv preprint arXiv:2208.11970, 2022.</strong><br>
<strong>[2] Paper: Ho J, Jain A, Abbeel P. Denoising diffusion
probabilistic models[J]. Advances in neural information processing
systems, 2020, 33: 6840-6851.</strong><br>
<strong>[3] Video: 想不出来昵称又想改, 扩散模型-Diffusion
Model【李宏毅2023】, Blibili</strong><br>
<strong>[4] Blog: 苏剑林, 生成扩散模型漫谈(1-3), 科学空间</strong></p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="category-chain-item">深度学习</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>深度学习-6-生成模型4-变分扩散模型</div>
      <div>http://example.com/2024/05/23/深度学习-6-生成模型4-变分扩散模型/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>喵老师</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2024年5月23日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/05/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-5-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B3-%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81/" title="深度学习-5-生成模型3-变分自编码">
                        <span class="hidden-mobile">深度学习-5-生成模型3-变分自编码</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"OLzAG6UdDYLLDlQvyrFhe7ho-gzGzoHsz","appKey":"IuY7HMC7xJw1qmdlkBlrfMRq","path":"window.location.pathname","placeholder":null,"avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    
  
</div>

  
  
    <!-- 备案信息 ICP for China -->
    <div class="beian">
  <span>
    <a href="http://beian.miit.gov.cn/" target="_blank" rel="nofollow noopener">
      京ICP证123456号
    </a>
  </span>
  
    
      <span>
        <a
          href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=12345678"
          rel="nofollow noopener"
          class="beian-police"
          target="_blank"
        >
          
            <span style="visibility: hidden; width: 0">|</span>
            <img src="/img/police_beian.png" srcset="/img/loading.gif" lazyload alt="police-icon"/>
          
          <span>京公网安备12345678号</span>
        </a>
      </span>
    
  
</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
