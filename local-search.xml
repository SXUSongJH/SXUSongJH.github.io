<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Optimal Transport-2.Wassertein Distance</title>
    <link href="/2024/02/15/Optimal%20Transport-2.Wassertein%20Distance/"/>
    <url>/2024/02/15/Optimal%20Transport-2.Wassertein%20Distance/</url>
    
    <content type="html"><![CDATA[<h1>Wassertein Distance</h1><p>  在上一节 Monge-Kantorovich Problem 中，我们介绍了最优传输问题。最优传输一个重要的应用是它可以用来衡量分布之间的距离，从而将距离的概念由点与点之间拓展到分布与分布之间。本节我们将介绍分布之间的距离定义，即 Wasserstein Distance，以及为什么其能够用于表示分布之间的距离。</p><h2 id="Metric-Properties-on-Probility-Space">Metric Properties on Probility Space</h2><p>  在上一节中，我们从概率视角描述了最优传输问题。设 $X,Y$ 是服从分布 $\boldsymbol{\alpha},\boldsymbol{\beta}$ 的两个随机变量，运输矩阵为 $\boldsymbol{P}$，成本矩阵为 $\boldsymbol{C}$，则分布 $\boldsymbol{\alpha},\boldsymbol{\beta}$ 之间的最优传输问题可以被定义为：</p><p>$$L_{\boldsymbol{C}}(\boldsymbol{\alpha},\boldsymbol{\beta}) := \min_{\boldsymbol{P} \in \boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta})} \left&lt; \boldsymbol{P},\boldsymbol{C} \right&gt; = \min_{(X,Y)} { \mathbb{E}_{(X,Y)}(c(X,Y)): X \sim \boldsymbol{\alpha}, Y \sim \boldsymbol{\beta} }$$</p><p>  $L_{\boldsymbol{C}}(\boldsymbol{\alpha},\boldsymbol{\beta})$ 的含义是将分布 $\boldsymbol{\alpha}$ 传输到分布 $\boldsymbol{\beta}$ 所花费的最小成本，我们很自然地就会想到 $L_{\boldsymbol{C}}(\boldsymbol{\alpha},\boldsymbol{\beta})$ 也许能够表示分布 $\boldsymbol{\alpha}$ 和 $\boldsymbol{\beta}$ 之间的距离或相似度。当然，要说明这个问题，我们需要证明函数 $L_{\boldsymbol{C}}$ 满足概率空间中距离函数的性质。<br>  设分布 $\boldsymbol{\alpha},\boldsymbol{\beta} $ 取自概率空间 $\mathcal{X}$，$W(\boldsymbol{\alpha},\boldsymbol{\beta})$ 是分布 $\boldsymbol{\alpha},\boldsymbol{\beta}$ 之间的距离函数，如果函数 $W$ 满足:</p><ul><li><strong>非负性(Non-negativity):</strong> 对 $\forall \boldsymbol{\alpha},\boldsymbol{\beta} \in \mathcal{X}, W(\boldsymbol{\alpha},\boldsymbol{\beta}) \ge 0.$</li><li><strong>同一性(Identity of Indiscernibles):</strong> $W(\boldsymbol{\alpha},\boldsymbol{\beta})=0$ 当且仅当 $\boldsymbol{\alpha} = \boldsymbol{\beta}.$</li><li><strong>对称性(Symmetry):</strong> $\forall \boldsymbol{\alpha},\boldsymbol{\beta} \in \mathcal{X}, W(\boldsymbol{\alpha},\boldsymbol{\beta}) = W(\boldsymbol{\beta},\boldsymbol{\alpha}).$</li><li><strong>三角不等式(Triangle Inequality):</strong> $\forall \boldsymbol{\alpha},\boldsymbol{\beta},\boldsymbol{\gamma} \in \mathcal{X}, W(\boldsymbol{\alpha},\boldsymbol{\gamma}) \leq W(\boldsymbol{\alpha},\boldsymbol{\beta})+W(\boldsymbol{\beta},\boldsymbol{\gamma}).$</li></ul><p>  学者们通过研究发现，当对成本矩阵 $\boldsymbol{C}$ 设置一些条件后，可以使得概率空间中最优传输问题的解 $L_{\boldsymbol{C}}(\boldsymbol{\alpha},\boldsymbol{\beta})$ 满足距离函数的性质，从而使得其可以用于衡量分布之间的距离。</p><h2 id="Wasserstein-Distance">Wasserstein Distance</h2><h3 id="Definition">Definition</h3><p>  我们首先来定义离散分布下的 Wasserstein Distance。设 $\boldsymbol{\alpha},\boldsymbol{\beta} \in \sum_{n}:={ \boldsymbol{x} \in \mathbb{R}^{n}<em>{+}: \boldsymbol{x^{T}}\mathbf{1}</em>{n}=1 }$，设矩阵 $\boldsymbol{D} \in \mathbb{R}^{n \times n}$ 是一个度量矩阵，即矩阵 $\boldsymbol{D}$ 满足:<br><strong>(1)</strong> $\boldsymbol{D} \in \mathbb{R}^{n \times n}<em>{+};$<br><strong>(2)</strong> $\boldsymbol{D}</em>{i,j}=0$，当且仅当 $i=j$;<br><strong>(3)</strong> $\boldsymbol{D}$ 是对称矩阵;<br><strong>(4)</strong> $\forall i,j,k \in { 1,\dotsb,n}, \boldsymbol{D}<em>{i,k} \leq \boldsymbol{D}</em>{i,j}+\boldsymbol{D}<em>{j,k}$.<br>令成本矩阵 $\boldsymbol{C} = \boldsymbol{D}^{p}= \left[  \boldsymbol{D}</em>{i,j}^{p} \right]<em>{n \times n} \in \mathbb{R}^{n \times n}</em>{+}(p \ge 1)$，定义：</p><p>$$W_{p}(\boldsymbol{\alpha},\boldsymbol{\beta}) := L_{\boldsymbol{D}^{p}}(\boldsymbol{\alpha,\boldsymbol{\beta}})^{1/p}$$</p><p>则称 $W_{p}(\boldsymbol{\alpha},\boldsymbol{\beta})$ 为概率分布 $\boldsymbol{\alpha},\boldsymbol{\beta}$ 之间的 <strong>p-Wasserstein 距离</strong>。<br>  现在来证明$W_{p}$可以作为概率空间$\sum_{n}$上的距离函数。</p><h3 id="Proof">Proof</h3><p>$$W_{p}(\boldsymbol{\alpha},\boldsymbol{\beta}) = L_{\boldsymbol{D}^{p}}(\boldsymbol{\alpha,\boldsymbol{\beta}})^{1/p} = \left( \min_{\boldsymbol{P} \in \boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta})} \left&lt; \boldsymbol{P},\boldsymbol{D}^{p} \right&gt; \right)^{\frac{1}{p}}$$</p><p>其中 $\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta}) = { \boldsymbol{P} \in \mathbb{R}^{n \times n}<em>{+} : \boldsymbol{P}\mathbf{1}</em>{n}=\boldsymbol{\alpha} \quad and \quad \boldsymbol{P^{T}}\mathbf{1}<em>n=\boldsymbol{\beta} }$.<br>  要证明 $W</em>{p}$ 可以作为概率空间$\sum_{n}$上的距离函数，则需要证明 $W_{p}$ 满足概率空间中距离函数的性质，即非负性、同一性、对称性、三角不等式。<br>  <strong>(1) 非负性证明</strong><br>   $\boldsymbol{P},\boldsymbol{D}^{p} \in \mathbb{R}^{n \times n}<em>{+} \Rightarrow \left&lt;  \boldsymbol{P},\boldsymbol{D}^{p} \right&gt;=\sum</em>{ij}\boldsymbol{P}<em>{ij}\boldsymbol{D}^{p}</em>{ij} \ge 0 \Rightarrow W_{p}(\boldsymbol{\alpha}, \boldsymbol{\beta}) \ge 0.$<br>  <strong>(2) 同一性证明</strong><br>  由度量矩阵的性质可知:  $\boldsymbol{D}<em>{i,i}=0, \forall i \in { 1,\dotsb,n }$，则有 $\boldsymbol{D}</em>{i,i}^{p}=0$，即成本矩阵 $\boldsymbol{D}^{p}$ 的对角线元素均为零。<br>  当 $\boldsymbol{\alpha}=\boldsymbol{\beta}$ 时，可行域 $\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\alpha}) = { \boldsymbol{P} \in \mathbb{R}^{n \times n}<em>{+} : \boldsymbol{P}\mathbf{1}</em>{n}=\boldsymbol{P^{T}}\mathbf{1}_n=\boldsymbol{\alpha} }$，则 $\boldsymbol{P}^{*}=diag(\boldsymbol{\alpha}) \in \boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\alpha})$，此时：</p><p>$$\left&lt;  \boldsymbol{P}^{*}, \boldsymbol{D}^{p} \right&gt;=\sum_{i}\boldsymbol{\alpha}<em>{i}\boldsymbol{D}</em>{i,i}^{p}=0 \Rightarrow W_{p}(\boldsymbol{\alpha},\boldsymbol{\alpha})=0$$</p><p>  当 $W_{p}(\boldsymbol{\alpha},\boldsymbol{\beta})=0$ 时，由于成本矩阵 $\boldsymbol{D}^{p}$ 的非对角线元素均大于零，故运输矩阵 $\boldsymbol{P}$ 的非对角线元素均为零，即运输矩阵 $\boldsymbol{P}$ 为对角矩阵，$\boldsymbol{P}=\boldsymbol{P}^{T}$. 此时有 $\boldsymbol{P}\mathbf{1}_{n}=\boldsymbol{P^{T}}\mathbf{1}<em>n$，即 $\boldsymbol{\alpha}=\boldsymbol{\beta}$.<br>  <strong>(3) 对称性证明</strong><br>  设 $\boldsymbol{P}^{*}$ 为$W</em>{p}(\boldsymbol{\alpha},\boldsymbol{\beta})$所对应的最优运输矩阵，则有:</p><p>$$W_{p}(\boldsymbol{\alpha},\boldsymbol{\beta})=\left&lt;  \boldsymbol{P}^{*},\boldsymbol{D}^{p} \right&gt;^{\frac{1}{p}}$$</p><p>  由于成本矩阵 $\boldsymbol{D}^{p}$ 是对称矩阵，故有：</p><p>$$W_{p}(\boldsymbol{\alpha},\boldsymbol{\beta})=\left&lt;  \boldsymbol{P}^{<em>},\boldsymbol{D}^{p} \right&gt;^{\frac{1}{p}}=\left&lt;  \boldsymbol{(P^{</em>})^{T}},\boldsymbol{D}^{p} \right&gt;^{\frac{1}{p}}$$</p><p>  $\boldsymbol{(P^{<em>})^{T}}\mathbf{1}_{n}=\boldsymbol{\beta}, \boldsymbol{P}^{</em>}\mathbf{1}_{n}=\boldsymbol{\alpha} \Rightarrow \boldsymbol{(P^{*})^{T}} \in \boldsymbol{U}(\boldsymbol{\beta},\boldsymbol{\alpha})$. 由于 $\boldsymbol{U}(\boldsymbol{\beta},\boldsymbol{\alpha})$ 与 $\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta})$ 中的运输矩阵是对应转置的关系，故有：</p><p>$$W_{p}(\boldsymbol{\beta},\boldsymbol{\alpha})=\left( \min_{\boldsymbol{P} \in \boldsymbol{U}(\boldsymbol{\beta},\boldsymbol{\alpha})} \left&lt; \boldsymbol{P},\boldsymbol{D}^{p} \right&gt; \right)^{\frac{1}{p}}=\left&lt;  \boldsymbol{(P^{*})^{T}},\boldsymbol{D}^{p} \right&gt;^{\frac{1}{p}}$$</p><p>$$\Rightarrow W_{p}(\boldsymbol{\alpha},\boldsymbol{\beta}) = W_{p}(\boldsymbol{\beta},\boldsymbol{\alpha})$$<br>  <strong>(4) 三角不等式性质证明</strong><br>  设 $\boldsymbol{\gamma} \in \sum_{n}$, 现证明：$W_{p}(\boldsymbol{\alpha},\boldsymbol{\gamma}) \leq W_{p}(\boldsymbol{\alpha},\boldsymbol{\beta})+W_{p}(\boldsymbol{\beta},\boldsymbol{\gamma})$.<br>  设 $\boldsymbol{P}$ 是 $W_{p}(\boldsymbol{\alpha},\boldsymbol{\beta})$ 所对应的最优运输矩阵，$\boldsymbol{Q}$ 是 $W_{p}(\boldsymbol{\beta},\boldsymbol{\gamma})$ 所对应的最优运输矩阵，则有</p><p>$$\begin{split}<br>W_{p}(\boldsymbol{\alpha},\boldsymbol{\beta}) &amp;= \left&lt;  \boldsymbol{P},\boldsymbol{D}^{p} \right&gt;^{\frac{1}{p}} = \left(\sum_{ij}\boldsymbol{P}<em>{ij}\boldsymbol{D}^{p}</em>{ij}\right)^{\frac{1}{p}}  \<br>W_{p}(\boldsymbol{\beta},\boldsymbol{\gamma}) &amp;= \left&lt;  \boldsymbol{Q},\boldsymbol{D}^{p} \right&gt;^{\frac{1}{p}} = \left(\sum_{ij}\boldsymbol{Q}<em>{ij}\boldsymbol{D}^{p}</em>{ij}\right)^{\frac{1}{p}}  \<br>\end{split}$$</p><p>  定义：</p><p>$$\tilde{\boldsymbol{\beta}} = [\tilde{\boldsymbol{\beta}}<em>{j}],\quad \tilde{\boldsymbol{\beta}}</em>{j} = \left { \begin{array}{lr}<br>\boldsymbol{\beta}<em>{j}, \quad\boldsymbol{\beta}</em>{j} &gt; 0 \<br>1, \quad\boldsymbol{\beta}_{j} = 0<br>\end{array} \right.$$</p><p>$$\boldsymbol{S} := \boldsymbol{P}diag(1/\tilde{\boldsymbol{\beta}})\boldsymbol{Q} \in \mathbb{R}^{n \times n}_{+}$$</p><p>  则有：</p><p>$$\begin{split}<br>\boldsymbol{S}\mathbf{1}<em>{n} &amp;= \boldsymbol{P}diag(1/\tilde{\boldsymbol{\beta}})\boldsymbol{Q}\mathbf{1}</em>{n}=\boldsymbol{P}diag(1/\tilde{\boldsymbol{\beta}})\boldsymbol{\beta}  \<br>&amp;= \boldsymbol{P}\boldsymbol{[\boldsymbol{\beta}<em>{j}/\tilde{\boldsymbol{\beta}}</em>{j}]<em>{n}} = \boldsymbol{P}\mathbf{1}</em>{Supp(\boldsymbol{\beta})} = \boldsymbol{P}\mathbf{1}_{n} \<br>&amp;= \boldsymbol{\alpha}<br>\end{split}$$</p><p>  同理可得：$\boldsymbol{S}^{T}\mathbf{1}_{n}=\boldsymbol{\gamma}$，则可以得到:  $\boldsymbol{S} \in \boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\gamma})$.</p><p>$$\begin{split}<br>W_{p}(\boldsymbol{\alpha}, \boldsymbol{\gamma}) &amp;= \left( \min_{\boldsymbol{P} \in \boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\gamma})} \left&lt; \boldsymbol{P},\boldsymbol{D}^{p} \right&gt; \right)^{\frac{1}{p}} \leq \left&lt; \boldsymbol{S},\boldsymbol{D}^{p} \right&gt;^{\frac{1}{p}}  \<br>&amp;= \left(  \sum_{ik}\boldsymbol{D}<em>{ik}^{p}\boldsymbol{S}</em>{ik} \right)^{\frac{1}{p}} = \left(  \sum_{ik}\boldsymbol{D}<em>{ik}^{p}\sum</em>{j}\frac{\boldsymbol{P}<em>{ij}\boldsymbol{Q}</em>{jk}}{\tilde{\boldsymbol{\beta}}<em>{j}} \right)^{\frac{1}{p}} = \left(  \sum</em>{ijk}\boldsymbol{D}<em>{ik}^{p}\frac{\boldsymbol{P}</em>{ij}\boldsymbol{Q}<em>{jk}}{\tilde{\boldsymbol{\beta}}</em>{j}} \right)^{\frac{1}{p}}  \<br>&amp; \leq \left(  \sum_{ijk}(\boldsymbol{D}<em>{ij}+\boldsymbol{D}</em>{jk})^{p}\frac{\boldsymbol{P}<em>{ij}\boldsymbol{Q}</em>{jk}}{\tilde{\boldsymbol{\beta}}<em>{j}} \right)^{\frac{1}{p}} \leq \left(  \sum</em>{ijk}\boldsymbol{D}<em>{ij}^{p}\frac{\boldsymbol{P}</em>{ij}\boldsymbol{Q}<em>{jk}}{\tilde{\boldsymbol{\beta}}</em>{j}} \right)^{\frac{1}{p}} + \left(  \sum_{ijk}\boldsymbol{D}<em>{jk}^{p}\frac{\boldsymbol{P}</em>{ij}\boldsymbol{Q}<em>{jk}}{\tilde{\boldsymbol{\beta}}</em>{j}} \right)^{\frac{1}{p}}  \<br>&amp;= \left(  \sum_{ij}\boldsymbol{D}<em>{ij}^{p}\boldsymbol{P}</em>{ij}\sum_{k}\frac{\boldsymbol{Q}<em>{jk}}{\tilde{\boldsymbol{\beta}}</em>{j}} \right)^{\frac{1}{p}} + \left(  \sum_{jk}\boldsymbol{D}<em>{jk}^{p}\boldsymbol{Q}</em>{jk}\sum_{i}\frac{\boldsymbol{P}<em>{ij}}{\tilde{\boldsymbol{\beta}}</em>{j}} \right)^{\frac{1}{p}}  \<br>&amp;= \left(  \sum_{ij}\boldsymbol{D}<em>{ij}^{p}\boldsymbol{P}</em>{ij} \right)^{\frac{1}{p}} + \left(  \sum_{jk}\boldsymbol{D}<em>{jk}^{p}\boldsymbol{Q}</em>{jk} \right)^{\frac{1}{p}}  \<br>&amp;= W_{p}(\boldsymbol{\alpha},\boldsymbol{\beta}) + W_{p}(\boldsymbol{\beta},\boldsymbol{\gamma})  \<br>\end{split}$$</p><p>  故有：</p><p>$$W_{p}(\boldsymbol{\alpha},\boldsymbol{\gamma}) \leq W_{p}(\boldsymbol{\alpha},\boldsymbol{\beta})+W_{p}(\boldsymbol{\beta},\boldsymbol{\gamma})$$</p><p>  综上所述，$W_{p}$可以作为概率空间$\sum_{n}$上的距离函数。</p><h2 id="Ground-Cost">Ground Cost</h2><p>  证明了$W_{p}$可以作为概率空间$\sum_{n}$上的距离函数。接下来我们就可以考虑如何定义度量矩阵 $\boldsymbol{D}$，从而生成成本矩阵$\boldsymbol{C}$, 得到成本矩阵$\boldsymbol{C}$后，我们便可以来计算分布 $\boldsymbol{\alpha},\boldsymbol{\beta}$ 之间的 Wasserstein 距离。当我们在欧式空间中考虑最优传输问题时，一种常用的生成成本矩阵$\boldsymbol{C}$的方法是 <strong>Ground Cost</strong>。<br>  Ground Cost 使用原始分布与目标分布的取值之差的 $L_2$ 范数来定义度量矩阵 $\boldsymbol{D}$，容易验证矩阵 $\boldsymbol{D}$ 满足度量矩阵的性质，然后使用度量矩阵的平方生成成本矩阵 $\boldsymbol{C}$，即 $\boldsymbol{C}=\boldsymbol{D}^2$，故 Ground Cost 是欧式空间中的一种 2-Wasserstein 距离。<br>  仍然考虑离散分布 $\boldsymbol{\alpha}, \boldsymbol{\beta}$，设:</p><p>$$\boldsymbol{\alpha} = \begin{bmatrix}<br>\alpha_1 \<br>\alpha_2 \<br>\vdots \<br>\alpha_n \<br>\end{bmatrix}, \quad \boldsymbol{\beta} = \begin{bmatrix}<br>\beta_1 \<br>\beta_2 \<br>\vdots \<br>\beta_n \<br>\end{bmatrix}$$</p><p>则分布 $\boldsymbol{\alpha},\boldsymbol{\beta}$ 的分布列可以写成：</p><style>.center {  width: auto;  display: table;  margin-left: auto;  margin-right: auto;}</style><p align="center"><font face="黑体" size="2."></font></p><div class="center"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">1</th><th style="text-align:center">2</th><th style="text-align:center">$\cdots$</th><th style="text-align:center">n</th></tr></thead><tbody><tr><td style="text-align:center">p</td><td style="text-align:center">$\alpha_1$</td><td style="text-align:center">$\alpha_2$</td><td style="text-align:center">$\cdots$</td><td style="text-align:center">$\alpha_n$</td></tr></tbody></table><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">1</th><th style="text-align:center">2</th><th style="text-align:center">$\cdots$</th><th style="text-align:center">n</th></tr></thead><tbody><tr><td style="text-align:center">p</td><td style="text-align:center">$\beta_1$</td><td style="text-align:center">$\beta_2$</td><td style="text-align:center">$\cdots$</td><td style="text-align:center">$\beta_n$</td></tr></tbody></table></div>  <p>定义度量矩阵$\boldsymbol{D}$为离散分布$\boldsymbol{\alpha},\boldsymbol{\beta}$取值之差的$L_2$范数：</p><p>$$\boldsymbol{D} = [\boldsymbol{D}<em>{ij}]</em>{n \times n}=[ ||i-j||<em>{2} ]</em>{n \times n} = \begin{bmatrix}<br>0 &amp; \cdots &amp; ||1-n||<em>{2}  \<br>\vdots &amp; &amp; \vdots \<br>||n-1||</em>{2} &amp; \cdots &amp; 0 \<br>\end{bmatrix}$$</p><p>定义成本矩阵$\boldsymbol{C}$为度量矩阵$\boldsymbol{D}$的平方：</p><p>$$ \boldsymbol{C} = \boldsymbol{D}^{2} = [\boldsymbol{D}<em>{ij}^{2}]</em>{n \times n} = \begin{bmatrix}<br>0 &amp; \cdots &amp; ||1-n||<em>{2}^{2}  \<br>\vdots &amp; &amp; \vdots \<br>||n-1||</em>{2}^{2} &amp; \cdots &amp; 0 \<br>\end{bmatrix}$$</p><p>概率分布 $\boldsymbol{\alpha}, \boldsymbol{\beta}$ 之间的 Wasserstein 距离可以被定义为：</p><p>$$W_{2}(\boldsymbol{\alpha},\boldsymbol{\beta}) = L_{\boldsymbol{C}}(\boldsymbol{\alpha,\boldsymbol{\beta}})^{1/2} = \left( \min_{\boldsymbol{P} \in \boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta})} \left&lt; \boldsymbol{P},\boldsymbol{C} \right&gt; \right)^{\frac{1}{2}}$$</p><h2 id="Example">Example</h2><p>  我们用一个实际的例子来展示如何基于 Ground Cost 来计算离散分布之间的 Wasserstein 距离。我们将使用Python中专门用于OT问题的库 <strong>POT</strong> 来完成这个实例的计算。首先导入所需要的包：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> ot<br></code></pre></td></tr></tbody></table></figure><p>  假设离散分布 $\boldsymbol{\alpha}, \boldsymbol{\beta} \in \sum_{5}:={ \boldsymbol{x} \in \mathbb{R}^{5}<em>{+}: \boldsymbol{x^{T}}\mathbf{1}</em>{5}=1 }$：</p><p>$$\boldsymbol{\alpha} = \begin{bmatrix}<br>0.1 \<br>0.3 \<br>0.2 \<br>0.1 \<br>0.3 \<br>\end{bmatrix}, \quad \boldsymbol{\beta} = \begin{bmatrix}<br>0.1 \<br>0.3 \<br>0.2 \<br>0.3 \<br>0.1 \<br>\end{bmatrix}$$</p><p>  画出离散分布 $\boldsymbol{\alpha}, \boldsymbol{\beta}$ 的概率分布直方图：</p><pre><code class="language-python">def pbar(x,y,color,title):    plt.bar(x,y, width=1, color=color,alpha=0.7)    plt.title(title)    plt.xlabel('Value')    plt.ylabel('Probability')# 1*2 plotdef multiplot(x,y_1,y_2):    plt.figure(figsize=(10,4))    plt.subplot(1,2,1)    pbar(x,y_1, color="blue", title='alpha distribution')    plt.subplot(1,2,2)    pbar(x,y_2, color="green", title='beta distribution')    plt.show()# values of probalility distributionx = np.array([1,2,3,4,5])# probability vectora = np.array([0.1,0.3,0.2,0.1,0.3])b = np.array([0.1,0.3,0.2,0.3,0.1])# draw distribution barplotmultiplot(x,a,b)</code></pre><p>得到的图像为：</p><center>    <img src="https://s2.loli.net/2024/03/04/amv6pIF52GMkEds.png" width="60%" height="60%">    <div align="center">Image1: 原始分布与目标分布的直方图</div></center>  <br><p>  基于 Ground Cost 我们可以定义成本矩阵 $\boldsymbol{C}$，相应的代码为：</p><pre><code class="language-python"># ground costdef ground_cost(n):    C = np.zeros((n, n))    for i in range(n):        for j in range(n):            x = i+1            y = j+1            C[i][j] = (x-y)**2    return CC = ground_cost(5)</code></pre><p>  得到的成本矩阵$\boldsymbol{C}$为：</p><p>$$\boldsymbol{C} = \begin{bmatrix}<br>0 &amp; 1 &amp; 4 &amp; 9 &amp; 16 \<br>1 &amp; 0 &amp; 1 &amp; 4 &amp; 9 \<br>4 &amp; 1 &amp; 0 &amp; 1 &amp; 4 \<br>9 &amp; 4 &amp; 1 &amp; 0 &amp; 1 \<br>16 &amp; 9 &amp; 4 &amp; 1 &amp; 0 \<br>\end{bmatrix}$$</p><p>则概率分布 $\boldsymbol{\alpha}, \boldsymbol{\beta}$ 之间的 Wasserstein 距离可以被定义为：</p><p>$$W_{2}(\boldsymbol{\alpha},\boldsymbol{\beta}) = L_{\boldsymbol{C}}(\boldsymbol{\alpha,\boldsymbol{\beta}})^{1/2} = \left( \min_{\boldsymbol{P} \in \boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta})} \left&lt; \boldsymbol{P},\boldsymbol{C} \right&gt; \right)^{\frac{1}{2}}$$</p><p>  我们可以使用POT库的API来求解离散分布 $\boldsymbol{\alpha},\boldsymbol{\beta}$ 之间的最优传输矩阵 $P^{*}$ 以及 Wasserstein 距离 $W_{2}(\boldsymbol{\alpha},\boldsymbol{\beta})$，其代码如下：</p><pre><code class="language-python"># optimal transport matrixP = ot.emd(a, b, C)# wasserstein distencewasserstein_distence = ot.emd2(a, b, C)print(P.round(4))print(round(np.sqrt(wasserstein_distence),4))</code></pre><p>求解结果如下：</p><p>$$\boldsymbol{P}^{*} = \begin{bmatrix}<br>0.1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \<br>0 &amp; 0.3 &amp; 0 &amp; 0 &amp; 0 \<br>0 &amp; 0 &amp; 0.2 &amp; 0 &amp; 0 \<br>0 &amp; 0 &amp; 0 &amp; 0.1 &amp; 0 \<br>0 &amp; 0 &amp; 0 &amp; 0.2 &amp; 0.1 \<br>\end{bmatrix},\quad W_{2}(\boldsymbol{\alpha},\boldsymbol{\beta})=0.4472$$</p><h2 id="Reference">Reference</h2><ul><li><strong>[1] Book: Peyré G, Cuturi M. Computational optimal transport[J]. Center for Research in Economics and Statistics Working Papers, 2017 (2017-86).</strong></li></ul>]]></content>
    
    
    <categories>
      
      <category>最优传输理论</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Optimal Transport-1.Monge-Kantorovich Problem</title>
    <link href="/2024/02/07/Optimal%20Transport-1-Monge-Kantorovich%20Problem/"/>
    <url>/2024/02/07/Optimal%20Transport-1-Monge-Kantorovich%20Problem/</url>
    
    <content type="html"><![CDATA[<h1 id="monge-kantorovich-problem">Monge-Kantorovich Problem</h1><p>  最优传输理论(Optimal TransportTheroy)是应用数学的一个分支，主要研究的是概率分布之间的最优转移方式以及相关的距离度量。它的核心思想是通过最小化两个概率分布之间的转移成本，来定义这两个分布之间的距离。<br>  最优传输问题的历史可以追溯到18世纪。著名法国数学家 Monge在1781年提出了最优传输问题的早期形式，即Monge-Problem。他关注的是如何以最小的成本将一个土堆移动到另一个位置，这被看作是最优传输理论的起源。时至今日，很多介绍最优传输理论的科普文章依然喜欢使用“土堆移动、推土机”等例子来论述最优传输理论。Monge的工作奠定了这一理论的基础。另外一个对最优传输理论的发展具有关键作用的是前苏联数学家Kantorovich。Kantorovich在20世纪40年代从实际经济资源分配问题中重新导出了最优传输问题，他基于测度论对最优传输问题进行了严格的定义，并推导出了一系列重要的理论成果。Kantorovich的工作对最优传输理论的数学形式和理论基础的建立起到了关键作用。由于在线性规划和资源分配方面的重要贡献，Kantorovich于1975年获得诺贝尔经济学奖。此外还有多位数学家对最优传输理论的发展做出了重要贡献，Wassertein、Villani、Berg等数学家从理论、应用、数值计算等方面丰富了最优传输理论。<br>  最优传输理论的应用涵盖多个领域：</p><ul><li><strong>1.图像处理和计算机视觉:</strong>在图像处理中，最优传输理论被用来衡量图像之间的相似性，从而进行图像匹配、图像检索等任务。在计算机视觉领域，它被用于解决图像生成和变换的问题。<br></li><li><strong>2.机器学习:</strong>在机器学习中，最优传输理论被应用于生成模型、领域自适应等问题，以提高模型的泛化性能。<br></li><li><strong>3.经济学和金融学:</strong>在经济学中，最优传输理论被用于研究资源的分配和经济结构的演变。在金融学中，它可以用来量化不同资产之间的差异和联系。<br></li><li><strong>4.统计学和信息论:</strong>最优传输理论在统计学和信息论中也有广泛的应用，尤其是在测度空间中定义概率分布之间的距离。</li></ul><p>  总的来说，最优传输理论在多个领域都展现了广泛的应用，其在处理概率分布之间的关系和相似性的能力为研究者提供了有力的工具。</p><h2 id="monge-problem">Monge Problem</h2><p>  GaspardMonge(1746-1818)是法国著名的数学家和物理学家，他在数学、物理学和工程学领域做出了突出的贡献，被认为是现代微分几何的奠基人之一，对现代科学和数学的发展产生了深远的影响。<br>  在18世纪，第一次工业革命正在欧洲如火如荼地进行着，经济与军事活动催生了大量的数学、物理理论的诞生。最优传输理论的萌芽便来源于军事活动中所遇到的实际问题。彼时，英国与法国正在争夺欧洲霸权，为此进行了一系列的战争。在过去的战争中，防御工事的修建至关重要，修建防御工事需要将大量的土堆从某地转移到阵地，并堆砌成确定的形状。修建工事需要消耗大量的人力物力，这便催生了一个实际问题，<strong>如何将某一形状的土堆移动到另一个地方并堆成另一种形状，使得这整个过程中所消耗的资源最少。</strong><br>  Monge此时正在法国军队中担任工程研究人员，负责为军队提供有关土木工程和防御工事的重要建议。Monge敏锐地注意到了这个问题，并对其展开了研究。1781年，Monge发表了著作——<strong>Mémoiresur la théorie de déblais et deremblais(关于挖掘和填充的备忘录).</strong>Monge在著作中对该问题进行了论述，并提出了一种解决方案。<br>  Monge首先对所要研究的问题做出了一系列假设：</p><ul><li><strong>所要运输的土堆都是由质量相等的不可再分的分子所构成的，且土堆中分子的分布是均匀的。</strong></li><li><strong>每个分子的运输价格都是相等的，与该分子的重量和它所被传输的距离成正比。</strong></li><li><strong>总的运输价格是所运输的每个分子的质量乘以分子所传输的距离的总和。</strong></li></ul><p>基于以上的假设，Monge提出了"前向运输法"，并给出了从一维到三维的实例。</p><h3 id="点到点的最优传输">点到点的最优传输</h3><p>  我们首先来考虑最简单的两点运输情形，如下图1所示：</p><center><img src="https://s2.loli.net/2024/02/09/HvhSCsApP1a5Q38.jpg" width="40%" height="60%"><div data-align="center">Image1: 两点运输</div></center><p>A区域中有两个分子需要运输到B区域的指定位置，总共有两种运输方案，图1中分别用绿色与橙色的箭头表示。由三角形两边之和大于第三边可知，橙色方案的运输距离要大于绿色方案，又因为分子的质量是相同的，所有我们可以很容易得到，绿色方案的运输效率要高于橙色方案。<br>  再来考虑多点的情形，如下图2所示</p><center><img src="https://s2.loli.net/2024/02/09/vkGiCydSJEOT7BF.jpg" width="60%" height="60%"><div data-align="center">Image2: 多点运输</div></center><p>我们需要将点1、2、3运输到点4、5、6处，图2中给出了A、B、C三种运输方案。首先来比较方案A、B，方案A，B的不同在于点1、3到点5、6的运输路线，基于上文两点运输的思考，我们可以得知B方案的运输效率要高于方案A，同理，C方案的运输效率要高于B方案。<br>  基于以上的思考，Monge认为点到点的运输要遵循"前向法"，即运输路线不存在交叉。Monge同时将这种思想拓展到更高维的情形。</p><h3 id="平面到平面的最优运输">平面到平面的最优运输</h3><p>  现在考虑将某一平面区域运输到另一个平面区域，基于“前向法”，Monge给出了如下图3所示的运输方案：</p><center><img src="https://s2.loli.net/2024/02/09/2JhmZywrsRd9zpF.jpg" width="60%" height="60%"><div data-align="center">Image3: 平面运输</div></center><p>现在需要将平面区域Ⅰ中的分子运输到平面区域Ⅱ，<strong>Monge认为区域Ⅰ中的A、C两点要运输到区域Ⅱ的B、D两点，直线AB、CD相较于O点，由O点出发可以确定两条边界射线<span class="math inline">\(l_3,l_4\)</span>。在锥型区域中，可以用很多条射线将运输平面进行分割。例如夹角"非常小"的射线<span class="math inline">\(l_1,l_2\)</span>与区域Ⅰ相交于<span class="math inline">\(M_1、M_3、H_1、H_3\)</span>，与区域Ⅱ相交于<span class="math inline">\(M_4、M_6、H_4、H_6\)</span>，分割出区域<span class="math inline">\(M_1M_3H_1H_3\)</span>和<span class="math inline">\(M_4M_6H_4H_6\)</span>，根据"前向法"，区域Ⅰ中<span class="math inline">\(M_1M_3H_1H_3\)</span>的分子要运输到区域Ⅱ中<span class="math inline">\(M_4M_6H_4H_6\)</span>中，同时运输时也要遵循“前向法”，即橙色区域要运输到橙色区域，绿色区域要运送到绿色区域。由无数个分割出的小区域依据“前向法”进行运输，这种运输方式的所消耗的资源最少。</strong><br>  Monge借助微分几何的知识证明了这些从O点出发的运输射线都垂直于某一曲线R，并通过解析几何的知识求出了曲线R的解析式，以及最优运输的映射方程<span class="math inline">\(y =T(x)\)</span>。基于“前向法”，Monge同时也对三维运输的映射方程进行了求解，具体的求解过程这里不作详细的介绍，有兴趣的读者可自行查阅相关资料。</p><center><img src="https://s2.loli.net/2024/02/11/FG4H59woPsLAJMy.png" width="60%" height="60%"><div data-align="center">Image4: Monge著作中的作图</div></center><h3 id="待解决的问题">待解决的问题</h3><p>  虽然Monge借助“前向法”给出了最优传输问题的一种解决方法，但他同时也在著作中承认，实际的运输问题是非常复杂的，他的解法只是一种非常理想化的方法，并且这种方法也存在着缺陷，主要有以下几个方面:</p><ul><li><strong>1.被运输的每个分子的质量可能是不同的。</strong></li><li><strong>2.每个区域的密度有可能不同，即面积相同的区域所含有的分子数量可能不相等。</strong></li><li><strong>3.当目标区域是非凸区域时，“前向法”在中间区域的映射方程是无解的。</strong></li></ul><p>  在Monge-Problem被提出后，后世的学者也不断地对这一问题进行研究，但并没有取得突破性的进展。</p><h2 id="kantorovich-relaxation">Kantorovich Relaxation</h2><p>  19世纪-20世纪初，集合论、测度论、概率论等数学分支得到了充分的发展壮大，Monge-Problem的解决也迎来了转机。前苏联数学家kantorovich在解决Monge-Problem中起到了不可忽视的作用。然而，有趣的是，Kantorovich最初在解决这个问题时，并没有意识到自己所面对的问题与Monge-Problem之间的关联，直到将近10年后，Kantorovich才在一篇新的论文中阐述了自己的方法可以解决Monge-Problem。<br>  1938年，圣彼得堡胶合板托拉斯的研究人员找到圣彼得堡大学数学系，想要圣彼得堡大学的数学家们帮助解决在生产中所遇到的一个实际问题。<strong>托拉斯有八台不同类型的机器，每台机器能够生产五种不同型号的胶合板。每种类型的机器生产这五种胶合板的效率不同，各种不同的胶合板的产量是由现有的需求量决定的。为了在最短的时间内生产这些胶合板，应该如何给每种类型的机器分配生产任务？</strong><br>  与高深的数学理论相比，这看起来是一个非常简单的数学问题，仅仅只需要求解方程组而已，然而要求出这个问题的最优解却并不是一个简单的工作，因为当时有关线性规划的理论还没有被提出。彼时，26岁的Kantorovich正在圣彼得堡大学担任教职，这个问题最终交由他来研究。1939年，Kantorovich发表了论文《数学方法中的问题理论》，以经济资源分配与运输问题为背景，正式提出了"最优传输问题"，这一工作被认为是线性规划理论的先驱性工作。1947年，kantorovich发表论文《关于数学规划问题的一般理论》，正式提出了线性规划理论及其一般解法。</p><h3 id="最优传输问题">最优传输问题</h3><p>  我们来考虑一个实际问题，有 n 个生产木材的工厂，生产的木材需要供应给m 个城市，每个工厂每个月的产能是固定的，记为 <span class="math inline">\(\boldsymbol{a}, \boldsymbol{a} \in\mathbb{R}^{n}\)</span>；每个城市每个月木材的需求量也是固定的，记为<span class="math inline">\(\boldsymbol{b}, \boldsymbol{b} \in\mathbb{R}^{m}\)</span>。记 <span class="math inline">\(\boldsymbol{P} =[p_{ij}]_{n \times m} \in \mathbb{R}^{n \times m}_{+}\)</span>表示运输方案，其中 <span class="math inline">\(p_{ij}\)</span> 表示第 i个工厂运输到第 j 个城市的木材量。记 <span class="math inline">\(\boldsymbol{C} = [c_{ij}]_{n \times m} \in\mathbb{R}^{n \times m}_{+}\)</span> 表示运输成本，其中 <span class="math inline">\(c_{ij}\)</span>表示将单位木材从第i个工厂运输到第j个城市的成本。记：</p><p><span class="math display">\[\boldsymbol{U}(\boldsymbol{a},\boldsymbol{b}) =\{ \boldsymbol{P} \in \mathbb{R}^{n \times m}_{+}: \boldsymbol{P}\mathbf{1}_{m}=\boldsymbol{a} \quad and \quad \boldsymbol{P^{T}}\mathbf{1}_{n}=\boldsymbol{b} \}\]</span></p><p>  则最优传输问题可以被定义为：</p><p><span class="math display">\[L_{\boldsymbol{C}}(\boldsymbol{a},\boldsymbol{b}) \quad \overset{\text{def.}}{=} \quad\min_{\boldsymbol{P} \in \boldsymbol{U}(\boldsymbol{a},\boldsymbol{b})}\left&lt; \boldsymbol{C},\boldsymbol{P}\right&gt;_{F}=\sum_{i,j}p_{ij}c_{ij}\]</span></p><p><span class="math inline">\(L_{\boldsymbol{C}}(\boldsymbol{a},\boldsymbol{b})\)</span>即为最优运输方案。由于目标函数是线性函数，且<span class="math inline">\(\boldsymbol{U}(\boldsymbol{a},\boldsymbol{b})\)</span>为 <span class="math inline">\(n+m\)</span>个等式定义的凸多胞体，故最优传输问题是一个典型的线性规划问题。</p><h3 id="概率视角">概率视角</h3><p>  Kantorovich使用测度论定义了被运输的对象。在Monge的理论中，运输是确定性的，即每个分子都不可再分，且会被整体被运输到另一个位置，<strong>Kantorovich放松了这种确定性条件，他认为每个分子是可以进行切分的，一个分子所包含的质量可以被运输到不同的位置，这种运输是具有概率性的</strong>。</p><h4 id="离散概率分布运输问题">离散概率分布运输问题</h4><p>  设 <span class="math inline">\(X, Y\)</span>是两个服从多项分布的随机变量，取值于 <span class="math inline">\(\{1,2,\dotsb, d \}\)</span>。<span class="math inline">\(X,Y\)</span>的概率分布 <span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta}\)</span>取自概率单纯形 <span class="math inline">\(\sum_{d}:=\{ x \in\mathbb{R}^{d}_{+}: \boldsymbol{x^{T}}\mathbf{1}_{d}=1\}\)</span>。运输矩阵 <span class="math inline">\(\boldsymbol{P} \in\mathbb{R}^{d \times d}_{+}\)</span>。记:</p><p><span class="math display">\[\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta}):= \{ \boldsymbol{P} \in \mathbb{R}^{d \times d}_{+} :\boldsymbol{P}\mathbf{1}_{d}=\boldsymbol{\alpha} \quad and \quad\boldsymbol{P^{T}}\mathbf{1}_d=\boldsymbol{\beta} \}\]</span></p><p>从概率视角看，集合 <span class="math inline">\(\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta})\)</span>包含了随机变量<span class="math inline">\(X,Y\)</span>所有可能的联合分布<span class="math inline">\(\pi(X,Y)\)</span>，即矩阵<span class="math inline">\(\boldsymbol{P}=[p_{ij}]_{d \times d} =[\pi(x=i,y=j)]\)</span>。设成本矩阵为<span class="math inline">\(\boldsymbol{C} \in \mathbb{R}^{d \timesd}_{+}\)</span>。则多项分布 <span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta}\)</span>之间的最优传输问题可以被定义为：</p><p><span class="math display">\[L_{\boldsymbol{C}}(\boldsymbol{\alpha},\boldsymbol{\beta}):= \min_{\boldsymbol{P} \in\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta})} \left&lt;\boldsymbol{P},\boldsymbol{C} \right&gt; = \min_{(X,Y)} \{\mathbb{E}_{(X,Y)}(c(X,Y)): X \sim \boldsymbol{\alpha}, Y \sim\boldsymbol{\beta} \}\]</span></p><p><span class="math inline">\((X,Y)\)</span>表示取值于<span class="math inline">\(\mathcal{X} \times\mathcal{Y}\)</span>的联合分布。</p><h4 id="连续概率分布运输问题">连续概率分布运输问题</h4><p>  连续分布的运输问题与离散问题相似，不同点在于随机变量 <span class="math inline">\(X,Y\)</span> 服从的是连续分布 <span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta}\)</span>，最优运输同样可以定义为：</p><p><span class="math display">\[L_{\boldsymbol{C}}(\boldsymbol{\alpha},\boldsymbol{\beta}):= \min_{(X,Y)} \{ \mathbb{E}_{(X,Y)}(c(X,Y)): X \sim\boldsymbol{\alpha}, Y \sim \boldsymbol{\beta} \}\]</span></p><h3 id="局部前向法">局部前向法</h3><p>  通过求解离散分布和连续分布的最优传输问题，我们可以发现最优传输方案与Monge的"前向法"存在联系，下图5是最优传输的结果实例：</p><center><img src="https://s2.loli.net/2024/02/12/BZ8uThM5CcbQzm4.png" width="60%" height="60%"><div data-align="center">Image5: 概率分布最优传输结果实例</div></center><p>从右图离散分布的最优运输结果来看，最优运输是满足“局部前向法"的，即在满足边界分布的条件下，遵循前向运输法则。</p><h2 id="references">References</h2><ul><li><strong>[1] Book: Peyré G, Cuturi M. Computational optimaltransport[J]. Center for Research in Economics and Statistics WorkingPapers, 2017 (2017-86).</strong><br></li><li><strong>[2] Lecture: 李向东. 最优传输理论及其应用.BIMSA</strong><br></li><li><strong>[3] Paper: Cuturi M. Sinkhorn distances: Lightspeedcomputation of optimal transport[J]. Advances in neural informationprocessing systems, 2013, 26.</strong></li></ul>]]></content>
    
    
    <categories>
      
      <category>最优传输理论</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>机器学习-7-线性判别分析</title>
    <link href="/2024/02/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-7-%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/"/>
    <url>/2024/02/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-7-%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<h1 id="线性判别分析">线性判别分析</h1><p>  线性判别分析(Linear DiscriminantAnalysis，简称LDA)是一种在机器学习和统计学中常用于分类和降维的方法。它的主要目标是在特征空间中找到一个合适的投影方向，将高维数据点投影到低维空间中，使得这些数据点易于分类。LDA在特征选择、降维和模式识别等领域都有广泛的应用。  线性判别分析最早由著名统计学家<span class="math inline">\(RA.Fisher\)</span>于1936年提出，他的工作被认为是<span class="math inline">\(LDA\)</span>的奠基。线性判别分析经过多个阶段的发展，从最初的二分类问题到多分类问题，以及对不同数据类型的适应，一直在模式识别与机器学习领域中发挥着重要作用。同时，他也启发了其他降维和分类方法的发展。</p><h2 id="基本思想">基本思想</h2><p>  线性判别分析的基本思想为：在<span class="math inline">\(n\)</span>维特征空间中，找到一个最佳的投影方向，使得在将训练集中的数据点投影到该方向上后，类别间的散度较大，类别内的散点较小，这样我们可以在该投影方向上找到一个分界点，能够对训练数据集完全正确分类。对于新的实例，将其投影到最佳投影方向上，利用分界点对其进行分类。为了寻找到最佳的投影方向，需要设置与类间散度和类内散度有关的损失函数，使得在最小化损失函数的过程中，类间散度增大而类内散度减小，这样最终得到的投影方向便是最佳投影方向。线性判别分析的基本思想可用下图1来描述：</p><center><img src="https://s2.loli.net/2023/10/09/g2xVFLyehT3D8NS.jpg" width="60%" height="60%"><div data-align="center">Image1: 线性判别分析的基本思想</div></center><h2 id="模型">模型</h2><p>  线性判别分析可以应用于二分类问题或者多分类问题，这里我们主要讨论呢二分类问题下的线性判别模型。</p><p><strong>输入</strong></p><ul><li><strong>输入空间:</strong> <span class="math inline">\(\mathcal{X} =\mathbb{R}^{n}\)</span><br></li><li><strong>输入实例:</strong> <span class="math inline">\(x =\begin{bmatrix}  x^{(1)},x^{(2)},\dots,x^{(n)} \\ \end{bmatrix} \in\mathcal{X}\)</span></li></ul><p><strong>输出</strong></p><ul><li><strong>输出空间:</strong> <span class="math inline">\(\mathcal{Y} =\{-1,+1\}\)</span><br></li><li><strong>输出实例:</strong> <span class="math inline">\(y \in\mathcal{Y}\)</span></li></ul><p>  其中输出空间<span class="math inline">\(\mathcal{Y}\)</span>只包含+1和-1的一个集合，+1与-1分别代表二分类问题中的正类<span class="math inline">\(C_1\)</span>与负类<span class="math inline">\(C_2\)</span>。输出实例<span class="math inline">\(y\)</span>代表输出实例<span class="math inline">\(x\)</span>的类别。</p><p><strong>数据集</strong><br>  感知机模型的训练数据集<span class="math inline">\(T_{train}\)</span>为：</p><p><span class="math display">\[T_{train}=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}\]</span></p><p>  其中，<span class="math inline">\(x_i \in\mathcal{X}=\mathbb{R}^{n},y_i \in\mathcal{Y}=\{+1,-1\},i=1,2,\dots,N\)</span>，<span class="math inline">\(N\)</span>表示训练数据的样本容量。</p><p><strong>模型</strong><br>  设最佳投影方向为<span class="math inline">\(\hat{w}\)</span>，且<span class="math inline">\(||\hat{w}||_{2}=1\)</span>，正类与负类的样本均值点分别为：</p><p><span class="math display">\[\bar{x}_{C_1}=\frac{1}{N_1}\sum_{x_i \inC_1}x_i,\space \bar{x}_{C_2}=\frac{1}{N_2}\sum_{x_i \inC_2}x_i\]</span></p><p>  其中，<span class="math inline">\(N_1\)</span>表示训练集中正类样本的样本容量，<span class="math inline">\(N_2\)</span>表示训练集中负类样本的样本容量。将<span class="math inline">\(\bar{x}_{C_1}\)</span>与<span class="math inline">\(\bar{x}_{C_2}\)</span>投影到最佳投影方向<span class="math inline">\(\hat{w}\)</span>后的投影距离分别为<span class="math inline">\(\hat{w}^{T}\bar{x}_{C_1}\)</span>和<span class="math inline">\(\hat{w}^{T}\bar{x}_{C_2}\)</span>，设置分界点<span class="math inline">\(threshold\)</span>为：</p><p><span class="math display">\[threshold =\frac{\hat{w}^{T}\bar{x}_{C_1}+\hat{w}^{T}\bar{x}_{C_2}}{2}\]</span></p><p>  对于新的实例点<span class="math inline">\(x\)</span>，同样将其投影到最佳投影方向<span class="math inline">\(\hat{w}\)</span>，则投影距离为<span class="math inline">\(\hat{w}^{T}x\)</span>，其类别的判断准则为：</p><p><span class="math display">\[\hat{y} = \left \{\begin{array}{rcl}+1, &amp; {\hat{w}^{T}x &gt; threshold}\\-1, &amp; {\hat{w}^{T}x \leq threshold}\\\end{array} \right.\]</span></p><p><strong>假设空间</strong><br>  模型的假设空间<span class="math inline">\(\mathcal{H}\)</span>实际上是特征空间中所有的投影方向：</p><p><span class="math display">\[\mathcal{H} = \{w \vert w \in\mathbb{R}^{n}\}\]</span></p><p>  此时模型的参数空间<span class="math inline">\(\Theta =\mathcal{H}\)</span>.</p><h2 id="策略">策略</h2><p>  前文提到，线性判别分析的最佳投影方向要满足使得训练集样本在投影后有类间散度大，而类内散度小的特点，我们可以据此来设定损失函数。<br>  设特征空间中任意投影方向为<span class="math inline">\(w\)</span>，且<span class="math inline">\(||w||_{2}=1\)</span>，则特征空间中的数据点<span class="math inline">\(x\)</span>在<span class="math inline">\(w\)</span>上的投影距离为<span class="math inline">\(w^{T}x\)</span>.将训练集中的数据点投影到该方向上，令<span class="math inline">\(z_i=w^{T}x_i\)</span>，则训练数据集中正类与负类在投影到<span class="math inline">\(w\)</span>后的平均投影距离分别为：</p><p><span class="math display">\[\bar{z}_1 =\frac{1}{N_1}\sum_{i=1}^{N_1}z_i=\frac{1}{N_1}\sum_{x_i \inC_1}w^{T}x_i\]</span></p><p><span class="math display">\[\bar{z}_2 =\frac{1}{N_2}\sum_{i=1}^{N_2}z_i=\frac{1}{N_2}\sum_{x_i \inC_2}w^{T}x_i\]</span></p><p>  <span class="math inline">\(\bar{z}_1\)</span>与<span class="math inline">\(\bar{z}_2\)</span>的差的绝对值表示投影后正类数据与负类数据的中心点之间的距离，我们可以据此来表示类间散度，这个距离越大，说明投影后两个数据整体分离地越远。为了求导的方便，我们用平方代替绝对值，这样我们可以定义类间散度：</p><p><span class="math display">\[S_{be}=(\bar{z}_1-\bar{z}_2)^2\]</span></p><p>  在考虑类间散度的同时，我们也希望投影后，同一个类别的数据尽量聚拢，即类内散度较小，我们可以用投影距离的组内方差来描述类内散度。投影后正类和负类数据点的投影距离的组内方差分别为：</p><p><span class="math display">\[S_1 =\frac{1}{N_1}\sum_{i=1}^{N_1}(z_i-\bar{z}_1)(z_i-\bar{z}_1)^T=\frac{1}{N_1}\sum_{x_i\in C_1}(w^T x_i-\frac{1}{N_1}\sum_{x_i \in C_1}w^{T}x_i)(w^Tx_i-\frac{1}{N_1}\sum_{x_i \in C_1}w^{T}x_i)^T\]</span></p><p><span class="math display">\[S_2 =\frac{1}{N_2}\sum_{i=1}^{N_2}(z_i-\bar{z}_2)(z_i-\bar{z}_2)^T=\frac{1}{N_2}\sum_{x_i\in C_2}(w^T x_i-\frac{1}{N_2}\sum_{x_i \in C_2}w^{T}x_i)(w^Tx_i-\frac{1}{N_2}\sum_{x_i \in C_2}w^{T}x_i)^T\]</span></p><p>  我们希望正类与负类样本在投影后的组内方差均较小，因此我们可以将类内散度定义为：</p><p><span class="math display">\[S_{in}=S_1+S_2\]</span></p><p>  根据判别分析的基本思想，我们希望投影后数据点有类间散度大，类内散度小的特点，因此我们可以将损失函数定义为：</p><p><span class="math display">\[L(w)=-\frac{(\bar{z}_1-\bar{z}_2)^2}{S_1+S_2}\]</span></p><p>  当我们最小化损失函数<span class="math inline">\(L(w)\)</span>时，可以在增大类间散度的同时，减小类内散度。损失函数的最小值点<span class="math inline">\(\hat{w}\)</span>便是我们要寻找的最佳投影方向。<br>  我们对类间散度与类内散度做一下化简，以简化损失函数，便于优化。</p><p><span class="math display">\[\begin{align*}    \bar{z}_1-\bar{z}_2 &amp;=  \frac{1}{N_1}\sum_{x_i \inC_1}w^{T}x_i-\frac{1}{N_2}\sum_{x_i \in C_2}w^{T}x_i  \\    &amp;= w^T \left(\frac{1}{N_1}\sum_{i=1}^{N_1}x_i-\frac{1}{N_2}\sum_{i=1}^{N_2}x_i\right) \\    &amp;= w^T(\bar{x}_{C_1}-\bar{x}_{C_2})\end{align*}\]</span></p><p><span class="math display">\[\begin{align*}    S_1 &amp;= \frac{1}{N_1}\sum_{x_i \in C_1}(w^Tx_i-\frac{1}{N_1}\sum_{x_i \in C_1}w^{T}x_i)(w^Tx_i-\frac{1}{N_1}\sum_{x_i \in C_1}w^{T}x_i)^T \\    &amp;=\frac{1}{N_1}\sum_{i=1}^{N_1}w^{T}(x_i-\bar{x}_{C_1})(x_i-\bar{x}_{C_1})^{T}w  \\    &amp;= w^{T} \left(\frac{1}{N_1}\sum_{i=1}^{N_1}(x_i-\bar{x}_{C_1})(x_i-\bar{x}_{C_1})^{T}\right)w\end{align*}\]</span></p><p>  记<span class="math inline">\(S_{C_1}=\frac{1}{N_1}\sum_{i=1}^{N_1}(x_i-\bar{x}_{C_1})(x_i-\bar{x}_{C_1})^{T}\)</span>，表示投影前正类的组内方差，则投影后正类的组内方差为：</p><p><span class="math display">\[S_1=w^{T}S_{C_1}w\]</span></p><p>  同理可得：</p><p><span class="math display">\[S_2=w^{T}S_{C_2}w\]</span></p><p><span class="math display">\[S_{C_2}=\frac{1}{N_1}\sum_{i=1}^{N_1}(x_i-\bar{x}_{C_1})(x_i-\bar{x}_{C_1})^{T}\]</span></p><p>  则类内散度可以化简为：</p><p><span class="math display">\[S_1+S_2=w^{T}S_{C_1}w+w^{T}S_{C_2}w=w^{T}(S_{C_1}+S_{C_2})w\]</span></p><p><strong>损失函数</strong>   化简后，最终得到的损失函数为：</p><p><span class="math display">\[\begin{align*}    L(w)&amp;=-\frac{w^{T}(\bar{x}_{C_1}-\bar{x}_{C_2})(\bar{x}_{C_1}-\bar{x}_{C_2})^{T}w}{w^{T}(S_{C_1}+S_{C_2})w}\\    &amp;= -\frac{w^{T}Aw}{w^{T}Bw}  \\\end{align*}\]</span></p><p>  其中，<span class="math inline">\(A=(\bar{x}_{C_1}-\bar{x}_{C_2})(\bar{x}_{C_1}-\bar{x}_{C_2})^{T},B=S_{C_1}+S_{C_2}\)</span>.</p><h2 id="算法">算法</h2><p>  我们需要解决的优化问题为：</p><p><span class="math display">\[\min_{w}L(w)=-\frac{w^{T}Aw}{w^{T}Bw}\]</span></p><p>  对<span class="math inline">\(w\)</span>求一阶偏导：并令其为零：</p><p><span class="math display">\[\begin{align*}    \frac{\partial L(w)}{\partial w} &amp;= -\frac{\partial(w^{T}Aw)(w^{T}Bw)^{-1}}{\partial w} \\    &amp;= (2Aw)(w^{T}Bw)^{-1}-(w^{T}Aw)(w^{T}Bw)^{-2}(2Bw)=0  \\\end{align*}\]</span></p><p><span class="math display">\[\RightarrowAw(w^{T}Bw)-(w^{T}Aw)(Bw)=0\]</span></p><p><span class="math display">\[\Rightarrow(w^{T}Aw)Bw=Aw(w^{T}Bw)\]</span></p><p><span class="math display">\[\Rightarrow w=\frac{w^{T}Bw}{w^{T}Aw}B^{-1}Aw\]</span></p><p>  由于我们只需要求得最佳投影方向，而不需要关系其长度，因此有：</p><p><span class="math display">\[w \varpropto B^{-1}Aw\]</span></p><p>  表示<span class="math inline">\(w\)</span>的方向与<span class="math inline">\(B^{-1}Aw\)</span>一致。由于<span class="math inline">\(Aw=(\bar{x}_{C_1}-\bar{x}_{C_2})(\bar{x}_{C_1}-\bar{x}_{C_2})^{T}w\)</span>，而<span class="math inline">\((\bar{x}_{C_1}-\bar{x}_{C_2})^{T}w \in\mathbb{R}\)</span>为标量，故有：</p><p><span class="math display">\[w \varproptoB^{-1}(\bar{x}_{C_1}-\bar{x}_{C_2})\]</span></p><p>  <span class="math inline">\(\becauseB=S_{C_1}+S_{C_2}\)</span>，因此我们求得的最佳投影方向为：</p><p><span class="math display">\[w \varpropto(S_{C_1}+S_{C_2})^{-1}(\bar{x}_{C_1}-\bar{x}_{C_2})\]</span></p><p><span class="math display">\[\hat{w} =\frac{(S_{C_1}+S_{C_2})^{-1}(\bar{x}_{C_1}-\bar{x}_{C_2})}{||(S_{C_1}+S_{C_2})^{-1}(\bar{x}_{C_1}-\bar{x}_{C_2})||_2}\]</span></p><h2 id="线性判别分析实例及python实现">线性判别分析实例及Python实现</h2><p>  首先生成训练数据。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Data Generation</span><br>np.random.seed(<span class="hljs-number">520</span>)<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">data_generation</span>(<span class="hljs-params">n,basepoint,ylabel</span>):<br>    x1 = np.random.random((n,))+np.array([basepoint[<span class="hljs-number">0</span>]]*n)<br>    x2 = np.random.random((n,))+np.array([basepoint[<span class="hljs-number">1</span>]]*n)<br>    y = np.array([ylabel]*n)<br>    data = pd.DataFrame({<span class="hljs-string">"x1"</span>:x1,<span class="hljs-string">"x2"</span>:x2,<span class="hljs-string">"y"</span>:y})<br>    <span class="hljs-keyword">return</span> data<br><br><span class="hljs-comment"># positive data</span><br>positivedata = data_generation(n=<span class="hljs-number">30</span>,basepoint=(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>),ylabel=<span class="hljs-number">1</span>)<br><span class="hljs-comment"># negative data</span><br>negativedata = data_generation(n=<span class="hljs-number">20</span>,basepoint=(<span class="hljs-number">2</span>,<span class="hljs-number">1</span>),ylabel=<span class="hljs-number">0</span>)<br><span class="hljs-comment"># train data</span><br>train_data = pd.concat([positivedata,negativedata])<br>train_data = shuffle(train_data)<br>train_data.index = <span class="hljs-built_in">range</span>(train_data.shape[<span class="hljs-number">0</span>])<br>train_data.head(<span class="hljs-number">5</span>)<br><br><span class="hljs-comment"># train data scatter plot</span><br>plt.scatter(x=positivedata[<span class="hljs-string">"x1"</span>],y=positivedata[<span class="hljs-string">"x2"</span>],marker=<span class="hljs-string">"o"</span>,c=<span class="hljs-string">"green"</span>,label=<span class="hljs-string">"positive data"</span>)<br>plt.scatter(x=negativedata[<span class="hljs-string">"x1"</span>],y=negativedata[<span class="hljs-string">"x2"</span>],marker=<span class="hljs-string">"x"</span>,c=<span class="hljs-string">"red"</span>,label=<span class="hljs-string">"negative data"</span>)<br>plt.xlabel(<span class="hljs-string">"x1"</span>)<br>plt.ylabel(<span class="hljs-string">"x2"</span>)<br>plt.xlim((<span class="hljs-number">0</span>,<span class="hljs-number">4</span>))<br>plt.ylim((<span class="hljs-number">0</span>,<span class="hljs-number">4</span>))<br>plt.xticks(np.arange(<span class="hljs-number">0</span>,<span class="hljs-number">4</span>,<span class="hljs-number">0.5</span>))<br>plt.yticks(np.arange(<span class="hljs-number">0</span>,<span class="hljs-number">4</span>,<span class="hljs-number">0.5</span>))<br>plt.legend()<br></code></pre></td></tr></tbody></table></figure><center><img src="https://s2.loli.net/2024/02/03/YtmOUQ8PNdGD1Mi.png" width="60%" height="60%"><div data-align="center">Image2: 训练数据</div></center><p>  利用前文所提出的算法，计算线性判别模型的参数 <span class="math inline">\(w\)</span> .</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">LDA_param_solving</span>(<span class="hljs-params">train_data</span>):<br>    positive_data = train_data[train_data[<span class="hljs-string">"y"</span>]==<span class="hljs-number">1</span>]<br>    negative_data = train_data[train_data[<span class="hljs-string">"y"</span>]==<span class="hljs-number">0</span>]<br><br>    pos_X = np.array(positive_data.iloc[:,:-<span class="hljs-number">1</span>])<br>    neg_X = np.array(negative_data.iloc[:,:-<span class="hljs-number">1</span>])<br><br>    pos_X_mean = np.mean(pos_X,axis=<span class="hljs-number">0</span>)<br>    pos_X_var = np.cov(pos_X,rowvar=<span class="hljs-literal">False</span>)<br>    neg_X_mean = np.mean(neg_X,axis=<span class="hljs-number">0</span>)<br>    neg_X_var = np.cov(neg_X,rowvar=<span class="hljs-literal">False</span>)<br><br>    w = np.dot(np.linalg.inv(pos_X_var+neg_X_var),pos_X_mean-neg_X_mean)<br>    w = w/np.linalg.norm(w)<br><br>    <span class="hljs-keyword">return</span> w.<span class="hljs-built_in">round</span>(<span class="hljs-number">8</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">LDA_threshold</span>(<span class="hljs-params">train_data</span>):<br>    w = LDA_param_solving(train_data)<br><br>    positive_data = train_data[train_data[<span class="hljs-string">"y"</span>]==<span class="hljs-number">1</span>]<br>    negative_data = train_data[train_data[<span class="hljs-string">"y"</span>]==<span class="hljs-number">0</span>]<br>    pos_X = np.array(positive_data.iloc[:,:-<span class="hljs-number">1</span>])<br>    neg_X = np.array(negative_data.iloc[:,:-<span class="hljs-number">1</span>])<br>    pos_X_mean = np.mean(pos_X,axis=<span class="hljs-number">0</span>)<br>    neg_X_mean = np.mean(neg_X,axis=<span class="hljs-number">0</span>)<br><br>    t = (np.dot(w,pos_X_mean)+np.dot(w,neg_X_mean))/<span class="hljs-number">2</span><br><br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">round</span>(t,<span class="hljs-number">8</span>)<br><br>w_hat = LDA_param_solving(train_data=train_data)<br>threshold = LDA_threshold(train_data)<br></code></pre></td></tr></tbody></table></figure><p>  得到的模型参数及threshold为:</p><p><span class="math display">\[w = \begin{bmatrix}    -0.8235 \\    0.5673 \\\end{bmatrix}, \quad threshlod=-0.4689\]</span></p><p>  画出模型的决策边界：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python">x1_line = np.linspace(-<span class="hljs-number">3</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1000</span>)<br>x2_line = (w_hat[<span class="hljs-number">1</span>]/w_hat[<span class="hljs-number">0</span>])*x1_line<br>theta = math.atan(w_hat[<span class="hljs-number">1</span>]/w_hat[<span class="hljs-number">0</span>])<br>threshold_x1 = -math.cos(-theta)*threshold<br>threshold_x2 = math.sin(-theta)*threshold<br>decision_boundary_x2 = (-w_hat[<span class="hljs-number">0</span>]/w_hat[<span class="hljs-number">1</span>])*x1_line+(threshold_x2+(w_hat[<span class="hljs-number">0</span>]/w_hat[<span class="hljs-number">1</span>])*threshold_x1)<br>plt.figure(figsize=(<span class="hljs-number">7</span>, <span class="hljs-number">7</span>))<br>plt.scatter(x=threshold_x1,y=threshold_x2,marker=<span class="hljs-string">"p"</span>,c=<span class="hljs-string">"black"</span>,label=<span class="hljs-string">"threshold"</span>,s=<span class="hljs-number">100</span>,alpha=<span class="hljs-number">1</span>)<br>plt.scatter(x=positivedata[<span class="hljs-string">"x1"</span>],y=positivedata[<span class="hljs-string">"x2"</span>],marker=<span class="hljs-string">"o"</span>,c=<span class="hljs-string">"green"</span>,label=<span class="hljs-string">"positive data"</span>)<br>plt.scatter(x=negativedata[<span class="hljs-string">"x1"</span>],y=negativedata[<span class="hljs-string">"x2"</span>],marker=<span class="hljs-string">"x"</span>,c=<span class="hljs-string">"red"</span>,label=<span class="hljs-string">"negative data"</span>)<br>plt.plot(x1_line,x2_line,c=<span class="hljs-string">"blue"</span>,label=<span class="hljs-string">"Best Project Direction"</span>)<br>plt.plot(x1_line,decision_boundary_x2,c=<span class="hljs-string">"purple"</span>,label=<span class="hljs-string">"decision boundary"</span>)<br>ax = plt.subplot()<br>ax.spines[<span class="hljs-string">'top'</span>].set_visible(<span class="hljs-literal">False</span>)<br>ax.spines[<span class="hljs-string">'right'</span>].set_visible(<span class="hljs-literal">False</span>)<br>ax.spines[<span class="hljs-string">'bottom'</span>].set_position((<span class="hljs-string">'data'</span>,<span class="hljs-number">0</span>))<br>ax.spines[<span class="hljs-string">'left'</span>].set_position((<span class="hljs-string">'data'</span>,<span class="hljs-number">0</span>))<br>plt.xlim((-<span class="hljs-number">3</span>,<span class="hljs-number">3</span>))<br>plt.ylim((-<span class="hljs-number">3</span>,<span class="hljs-number">3</span>))<br>plt.xticks(np.arange(-<span class="hljs-number">3</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>))<br>plt.yticks(np.arange(-<span class="hljs-number">3</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>))<br>plt.legend(loc=<span class="hljs-string">"lower left"</span>)<br></code></pre></td></tr></tbody></table></figure><center><img src="https://s2.loli.net/2024/02/03/v5zwfZsA6dYotO1.png" width="60%" height="60%"><div data-align="center">Image3: 决策边界</div></center><h2 id="参考">参考</h2><p><strong>[1] Video: bilibili,shuhuai008,线性判别分析</strong><br><strong>[2] Blog: CSDN,SongGu1996,线性判别分析(Linear DiscriminantAnalysis，LDA)</strong></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>机器学习-6.隐马尔可夫模型</title>
    <link href="/2023/12/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-6-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/"/>
    <url>/2023/12/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-6-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="隐马尔可夫模型">隐马尔可夫模型</h1><p>  隐马尔可夫模型(hidden Markovmodel,HMM)是一种用于对时许数据建模的概率图模型。它主要应用于对观察序列的概率分布进行建模，这些观察序列背后存在一个不可见的状态序列。HMM的主要思想可以总结如下：</p><ul><li><strong>状态和观察:</strong>HMM包含两种类型的变量，即隐藏的状态序列和可见的观察序列。状态序列表示系统内部的状态，而观察序列是我们可以观察的外部现象。<br></li><li><strong>马尔可夫性质:</strong>HMM假设状态序列是一个马尔可夫链，即系统的未来状态只依赖于当前状态，而与过去的状态无关。这意味着在给定当前当前状态下，未来状态与过去状态的信息是独立的。</li><li><strong>状态转移概率:</strong>HMM用状态转移概率描述系统从一个状态转移到另一个状态的可能性。这些概率被组织成状态转移矩阵，矩阵的元素表示从一个状态转移到另一个状态的概率。</li><li><strong>观察概率:</strong>对于每个状态，HMM定义了生成每个观察值的概率分布。这些概率被组织成观察概率矩阵。</li><li><strong>初始概率:</strong>HMM还需要定义系统在初始时刻处于每个状态的概率，这些概率称为初始概率。</li><li><strong>前向算法和后向算法:</strong>HMM使用前向算法和后向算法来计算给定观测序列的概率。<br></li><li><strong>Baum-Welch算法:</strong>用于无监督学习的算法，通过观察序列来调整模型参数，使其更好地匹配观察数据。Baum-Welch算法本质上就是EM算法。</li></ul><p>  隐马尔可夫模型在各个领域都具有重要的应用，包括<strong>时序数据建模，语音识别，自然语言处理，生物信息学等</strong>。总体而言，HMM在多个领域中都发挥着关键的作用，为时序数据建模和分析提供了灵活而强大的工具。</p><h2 id="基本概念">基本概念</h2><h3 id="马尔可夫链mc">马尔可夫链(MC)</h3><p>  设有随机序列 <span class="math inline">\(S =\{S_1,S_2,\dots,S_t,S_{t+1},\dots \}\)</span>，若 <span class="math inline">\(S_{t+1}\)</span> 只依赖于前一时刻 <span class="math inline">\(S_t\)</span>，不依赖于 <span class="math inline">\(S_1,S_2,\dots,S_{t-1}\)</span>，即：</p><p><span class="math display">\[P(S_{t+1} | S_1,S_2,\dots,S_t)=P(S_{t+1}| S_t)\]</span></p><p>  则称随机序列<span class="math inline">\(S\)</span>为马尔可夫链(Markov Chain)。</p><h3 id="隐马尔可夫模型的定义">隐马尔可夫模型的定义</h3><p>  <strong>定义1(隐马尔可夫模型)</strong>隐马尔可夫模型是关于时序数据的概率模型，描述由一个隐藏的马尔可夫链随机生成不可观测的状态序列，再由各个状态生成一个观测从而产生观测序列的过程。隐藏的马尔可夫链随机生成的状态的序列，称为状态序列(statesequence)；每个状态生成一个观测，而由此产生的观测的随机序列，称为观测序列(observationsequence)。序列的每一个位置又可以看作是一个时刻。</p><p>  HMM模型可以用如下的概率图表示：</p><center><img src="https://s2.loli.net/2023/11/26/w4LvBDr9m7oXkf3.jpg" width="80%" height="60%"><div data-align="center">Image1: HMM模型的概率图</div></center><h3 id="模型参数">模型参数</h3><p>  隐马尔可夫模型由<strong>初始概率分布</strong>、<strong>状态转移概率分布</strong>、以及<strong>观测概率分布</strong>确定。下面我们来介绍这些模型参数的含义，在这之前先做一些符号定义。</p><p><strong>状态序列</strong><br>  设 <span class="math inline">\(I\)</span>为状态序列，<span class="math inline">\(Q\)</span> 是所有可能状态的集合，记为：</p><p><span class="math display">\[Q=\{q_1,q_2,\dots,q_N\},\quadI=\{i_1,i_2,\dots,i_{T}\}, \forall i \in Q\]</span></p><p>其中，<span class="math inline">\(N\)</span>是可能的状态数。</p><p><strong>观测序列</strong><br>  设 <span class="math inline">\(O\)</span> 是 状态序列 <span class="math inline">\(I\)</span> 所对应的观测序列，<span class="math inline">\(V\)</span> 是所有可能的观测的集合，记为：</p><p><span class="math display">\[V = \{v_1,v_2,\dots,v_{M}\},\quadO=\{o_1,o_2,\dots,o_{T}\},\forall o \in V\]</span></p><p><strong>状态转移概率矩阵</strong><br>  设 <span class="math inline">\(A\)</span> 为状态转移概率矩阵：</p><p><span class="math display">\[A = [a_{ij}]_{N \times N}\]</span></p><p>其中，</p><p><span class="math display">\[a_{ij} = P(i_{t+1}=q_{j} |i_{t}=q_{i}),\quad i,j=1,2,\dots,N\]</span></p><p>即系统在时刻 <span class="math inline">\(t\)</span> 处于状态 <span class="math inline">\(q_{i}\)</span> 的条件下在时刻 <span class="math inline">\(t+1\)</span> 转移到状态 <span class="math inline">\(q_{j}\)</span> 的概率。</p><p><strong>观测概率矩阵</strong><br>  设 <span class="math inline">\(B\)</span> 是观测概率矩阵：</p><p><span class="math display">\[B=[b_{j}(k)]_{N \times M}\]</span></p><p>其中，</p><p><span class="math display">\[b_{j}(k)=P(o_{t}=v_{k} |i_{t}=q_{j}),\quad k=1,2,\dots,M;\quad j=1,2,\dots,N\]</span></p><p>即系统在时刻 <span class="math inline">\(t\)</span> 处于状态 <span class="math inline">\(q_{j}\)</span> 的条件下生成观测 <span class="math inline">\(v_{k}\)</span> 的概率。</p><p><strong>初始状态概率向量</strong><br>  设 <span class="math inline">\(\pi\)</span> 是初始状态概率向量：</p><p><span class="math display">\[\pi = \begin{bmatrix}    \pi_1,\pi_2,\dots,\pi_N\end{bmatrix}^{T}\]</span></p><p>其中，</p><p><span class="math display">\[\pi_{i}=P(i_{1}=q_{i}),\quadi=1,2,\dots,N\]</span></p><p>  隐马尔可夫模型由<strong>初始状态概率向量<span class="math inline">\(\pi\)</span></strong>、<strong>状态转移概率矩阵<span class="math inline">\(A\)</span></strong>、<strong>观测概率矩阵<span class="math inline">\(B\)</span></strong>决定。因此，隐马尔可夫模型的参数<span class="math inline">\(\lambda\)</span>可用三元符号表示，即：</p><p><span class="math display">\[\lambda = (A,B,\pi)\]</span></p><p>  <span class="math inline">\(A,B,\pi\)</span>称为隐马尔可夫模型的三要素。</p><h2 id="模型假设">模型假设</h2><p>  隐马尔可夫模型有两个基本假设：<br>  (1) 齐次马尔可夫性假设，即假设隐藏的马尔可夫链在任意时刻 <span class="math inline">\(t\)</span>的状态只依赖于前一时刻的状态，与其他时刻的状态及观测无关，也与时刻 <span class="math inline">\(t\)</span> 无关：</p><p><span class="math display">\[P(i_{t} |i_{t-1},\dots,i_{1};o_{t-1},\dots,o_{1})=P(i_{t} | i_{t-1}),\quadt=1,2,\dots,T\]</span></p><p>  (2)观测独立性假设，即假设任意时刻的观测只依赖于该时刻的马尔可夫链的状态，与其他观测及状态无关：</p><p><span class="math display">\[P(o_{t} |i_{t},\dots,i_{1};o_{t-1},\dots,o_{1})=P(o_{t} | i_{t})\]</span></p><h2 id="基本问题">基本问题</h2><p>  隐马尔可夫模型有3个基本问题，包括概率计算问题、学习问题、解码问题。</p><h3 id="概率计算问题">(1) 概率计算问题</h3><p>  给定模型参数 <span class="math inline">\(\lambda=(A,B,\pi)\)</span>和观测序列 <span class="math inline">\(O=\{o_1,o_2,\dots,o_{T}\}\)</span>，计算在给定模型参数<span class="math inline">\(\lambda\)</span> 的条件下观测序列 <span class="math inline">\(O\)</span> 出现的概率 <span class="math inline">\(P(O | \lambda)\)</span>。</p><h3 id="学习问题">(2) 学习问题</h3><p>  已知观测序列 <span class="math inline">\(O=\{o_1,o_2,\dots,o_{T}\}\)</span>，估计模型参数<span class="math inline">\(\lambda=(A,B,\pi)\)</span>，使得在该模型下观测序列概率<span class="math inline">\(P(O | \lambda)\)</span> 最大，即：</p><p><span class="math display">\[\hat{\lambda}=\arg\max_{\lambda} P(O |\lambda)\]</span></p><p>即用极大似然估计的方法估计参数。</p><h3 id="解码问题">(3) 解码问题</h3><p>  已知模型参数 <span class="math inline">\(\lambda=(A,B,\pi)\)</span>和观测序列 <span class="math inline">\(O=\{o_1,o_2,\dots,o_{T}\}\)</span>，求给定观测序列条件下概率<span class="math inline">\(P(I | O)\)</span> 最大的状态序列 <span class="math inline">\(I=\{i_1,i_2,\dots,i_{T}\}\)</span>，即：</p><p><span class="math display">\[\hat{I} = \arg\max_{I} P(I |O)\]</span></p><p>根据所预测的<span class="math inline">\(I\)</span>的时刻不同，解码问题又可分为预测问题与滤波问题：</p><ul><li>预测问题：<span class="math inline">\(\hat{i}_{t+1} = \arg\maxP(i_{t+1} | o_1,\dots,o_{t})\)</span><br></li><li>滤波问题：<span class="math inline">\(\hat{i}_{t} = \arg\max P(i_{t}| o_1,\dots,o_{t})\)</span></li></ul><h2 id="概率计算问题-1">概率计算问题</h2><p>  已知模型参数 <span class="math inline">\(\lambda=(A,B,\pi)\)</span>和观测序列 <span class="math inline">\(O=\{o_1,o_2,\dots,o_{T}\}\)</span>，计算 <span class="math inline">\(P(O | \lambda)\)</span>。概率计算问题主要有<strong>直接计算法、前向计算法、后向计算法</strong>。</p><h3 id="直接计算法">直接计算法</h3><p>  直接计算法的思路是通过列举所有可能的长度为 <span class="math inline">\(T\)</span> 状态序列 <span class="math inline">\(I=\{i_1,i_2,\dots,i_{T}\}\)</span>，求各个状态序列<span class="math inline">\(I\)</span> 与观测序列 <span class="math inline">\(O=\{o_1,o_2,\dots,o_{T}\}\)</span> 的联合概率<span class="math inline">\(P(O,I |\lambda)\)</span>，然后对所有可能的状态序列求和，得到 <span class="math inline">\(P(O | \lambda)\)</span>。计算过程如下：</p><p><span class="math display">\[P(O | \lambda) = \sum_{I}P(I,O |\lambda)=\sum_{I}P(O | I,\lambda)P(I | \lambda)\]</span></p><p><span class="math display">\[\begin{split}    P(I | \lambda) &amp;= P(i_1,i_2,\dots,i_{T} | \lambda) \\    &amp;= P(i_{T} | i_1,\dots,i_{T-1};\lambda)P(i_1,\dots,i_{T-1} |\lambda) \\    &amp;= P(i_{T} | i_{T-1};\lambda)P(i_1,\dots,i_{T-1} | \lambda) \\    &amp;= \left( a_{i_{T-1}i_{T}} \right) \left(\pi_{i_1}a_{i_{1}i_{2}} \dotsb a_{i_{T-2}i_{T-1}} \right) \\    &amp;= \pi_{i_1}a_{i_{1}i_{2}}a_{i_{2}i_{3}} \dotsb a_{i_{T-1}i_{T}}\end{split}\]</span></p><p><span class="math display">\[\begin{split}    P(O | I,\lambda) &amp;= P(o_1,o_2,\dots,o_{T} |i_1,i_2,\dots,i_{T};\lambda) \\    &amp;= b_{i_1}(o_1)b_{i_2}(o_2) \dotsb b_{i_T}(o_T)\end{split}\]</span></p><p><span class="math display">\[\begin{split}    P(O | \lambda) &amp;= \sum_{I}P(O | I,\lambda)P(I | \lambda) \\    &amp;=\sum_{i_1,i_2,\dots,i_T}\pi_{i1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2)\dotsb a_{i_{T-1}i_{T}}b_{i_{T}}(o_{T})\end{split}\]</span></p><p>  直接计算法的思路非常直观，容易理解，但缺点是计算量很大，是 <span class="math inline">\(O(TN^{T})\)</span>阶的，随着时间的推移呈指数型增长，这种算法在实际中是不可取的。实际上，在概率计算问题中，我们更常用的是前向计算法和后向计算法。</p><h3 id="前向计算法">前向计算法</h3><p>  在导出前向算法之前，我们首先来定义<strong>前向概率:</strong></p><p><span class="math display">\[\alpha_{t}(i)=P(o_1,o_2,\dots,o_{t};i_{t}=q_{i} |\lambda)\]</span></p><p>前向概率表示在给定模型参数 <span class="math inline">\(\lambda\)</span> 的条件下，到时刻 <span class="math inline">\(t\)</span> 部分观测序列为 <span class="math inline">\(o_1,o_2,\dots,o_{t}\)</span> 且状态为 <span class="math inline">\(q_{i}\)</span>的概率。则观测序列概率可以表示为：</p><p><span class="math display">\[P(O |\lambda)=\sum_{i=1}^{N}P(o_1,o_2,\dots,o_{T};i_{T}=q_{k} |\lambda)=\sum_{i=1}^{N}\alpha_{T}(k)\]</span></p><p>  前向计算法的主要思想是递推地求得前向概率 <span class="math inline">\(\alpha_{t}(i)\)</span> 及观测序列概率 <span class="math inline">\(P(O |\lambda)\)</span>，前向计算法的过程可用下图理解：</p><center><img src="https://s2.loli.net/2023/11/27/ry1pAg8RZwhTGkV.jpg" width="60%" height="50%"><div data-align="center">Image2: 前向递推</div></center><p>  接下来我们需要找到 <span class="math inline">\(\alpha_{t}(i)\)</span> 和 <span class="math inline">\(\alpha_{t+1}(j)\)</span> 之间的递推关系式：</p><p><span class="math display">\[\begin{split}    \alpha_{t+1}(j) &amp;= P(o_1,\dots,o_{t},o_{t+1};i_{t+1}=q_{j} |\lambda) \\    &amp;=\sum_{i=1}^{N}P(o_1,\dots,o_{t},o_{t+1};i_{t}=q_{i},i_{t+1}=q_{j} |\lambda) \\    &amp;= \sum_{i=1}^{N} P(o_{t+1} |o_1,\dots,o_{t};i_{t}=q_{i},i_{t+1}=q_{j};\lambda)P(o_1,\dots,o_{t};i_{t}=q_{i},i_{t+1}=q_{j}| \lambda) \\    &amp;=\sum_{i=1}^{N}P(o_{t+1}|i_{t+1}=q_{j};\lambda)P(i_{t+1}=q_{j}|o_1,\dots,o_{t};i_{t}=q_{i};\lambda)P(o_1,\dots,o_{t};i_{t}=q_{i}| \lambda) \\    &amp;= \sum_{i=1}^{N}P(o_{t+1}|i_{t+1}=q_{j};\lambda)P(i_{t+1}=q_{j}| i_{t}=q_i;\lambda)P(o_1,\dots,o_{t};i_{t}=q_{i} | \lambda) \\    &amp;= \sum_{i=1}^{N}b_{j}(o_{t+1})a_{ij}\alpha_{t}(i)\end{split}\]</span></p><p>  当 <span class="math inline">\(t=1\)</span> 时，有：</p><p><span class="math display">\[\alpha_{1}(i)=\pi_{i}b_{i}(o_1)\]</span></p><p>  综上所述，前向计算法的递推关系式可以总计为：</p><p><span class="math display">\[\begin{split}    \alpha_{1}(i) &amp;= \pi_{i}b_{i}(o_1),\quad i=1,2,\dots,N \\    \alpha_{t+1}(j) &amp;=b_{j}(o_{t+1})\sum_{i=1}^{N}a_{ij}\alpha_{t}(i),\quad j=1,2,\dots,N\end{split}\]</span></p><h4 id="前向算法">前向算法</h4><p>  <strong>观测序列概率的前向算法</strong><br>  输入：隐马尔可夫模型的参数 <span class="math inline">\(\lambda=(A,B,\pi)\)</span>，观测序列 O；<br>  输出：观测序列概率 <span class="math inline">\(P(O |\lambda)\)</span>。<br>  (1) 初值</p><p><span class="math display">\[\alpha_{1}(i)=\pi_{i}b_{i}(o_1),\quadi=1,2,\dots,N\]</span></p><p>  (2) 递推 <span class="math inline">\(\quad\)</span> 对 <span class="math inline">\(t=1,2,\dots,T-1,\)</span></p><p><span class="math display">\[\alpha_{t+1}(j) =b_{j}(o_{t+1})\sum_{i=1}^{N}a_{ij}\alpha_{t}(i),\quadj=1,2,\dots,N\]</span></p><p>  (3) 终止</p><p><span class="math display">\[P(O |\lambda)=\sum_{i=1}^{N}\alpha_{T}(i)\]</span></p><p>  前向算法的计算复杂度为 <span class="math inline">\(O(TN^{2})\)</span>，优于直接计算法。</p><h3 id="后向计算法">后向计算法</h3><p>  后向计算法与前向计算法大致相同，不同点在于后向计算法是从后向前递推。我们首先来定义<strong>后向概率:</strong></p><p><span class="math display">\[\beta_{t}(i) =P(o_{t+1},o_{t+2},\dots,o_{T} | i_{t}=q_{i};\lambda)\]</span></p><p>后向概率表示在给定模型参数 <span class="math inline">\(\lambda\)</span>，系统到时刻 <span class="math inline">\(t\)</span> 状态为 <span class="math inline">\(q_{i}\)</span> 的条件下，从 <span class="math inline">\(t+1\)</span> 到 <span class="math inline">\(T\)</span> 的部分观测序列为 <span class="math inline">\(o_{t+1},o_{t+2},\dots,o_{T}\)</span>的概率。则观测序列概率可以表示为：</p><p><span class="math display">\[\begin{split}    P(O | \lambda) &amp;= P(o_1,o_2,\dots,o_{T} | \lambda) \\    &amp;= \sum_{i=1}^{N} P(o_1,o_2,\dots,o_{T};i_{1}=q_{i} | \lambda)\\    &amp;= \sum_{i=1}^{N} P(o_1,o_2,\dots,o_{T} |i_{1}=q_{i};\lambda)P(i_1=q_{i} | \lambda) \\    &amp;= \sum_{i=1}^{N} P(o_1 |o_2,\dots,o_{T};i_{1}=q_{i};\lambda)P(o_2,\dots,o_{T} |i_{1}=q_{i};\lambda)\pi_{i} \\      &amp;= \sum_{i=1}^{N}P(o_1 | i_{1}=q_{i};\lambda)\beta_{1}(i)\pi_{i}\\    &amp;= \sum_{i=1}^{N} \pi_{i} b_{i}(o_1) \beta_{1}(i)\end{split}\]</span></p><p>  后向计算法的主要思想是递推地求得后向概率 <span class="math inline">\(\beta_{t}(i)\)</span> 及观测序列概率 <span class="math inline">\(P(O |\lambda)\)</span>，后向计算法的过程可用下图理解：</p><center><img src="https://s2.loli.net/2023/11/27/9jd7FOnlkuo2rM4.jpg" width="60%" height="60%"><div data-align="center">Image3: 后向递推</div></center><p>  之后我们来导出 <span class="math inline">\(\beta_{t}(i)\)</span> 和<span class="math inline">\(\beta_{t+1}(j)\)</span>之间的递推关系式：</p><p><span class="math display">\[\begin{split}    \beta_{t}(i) &amp;= P(o_{t+1},\dots,o_{T} | i_{t}=q_{i};\lambda) \\    &amp;= \sum_{j=1}^{N}P(o_{t+1},\dots,o_{T};i_{t+1}=q_{j} |i_{t}=q_{i};\lambda) \\    &amp;= \sum_{j=1}^{N}P(o_{t+1},\dots,o_{T} |i_{t+1}=q_{j},i_{t}=q_{i};\lambda)P(i_{t+1}=q_{j} | i_{t}=q_{i};\lambda)\\    &amp;= \sum_{j=1}^{N}P(o_{t+1},\dots,o_{T} |i_{t+1}=q_{j};\lambda)a_{ij} \\    &amp;= \sum_{j=1}^{N}P(o_{t+1} |o_{t+2},\dots,o_{T};i_{t+1}=q_{j};\lambda)P(o_{t+2},\dots,o_{T} |i_{t+1}=q_{j};\lambda)a_{ij} \\    &amp;= \sum_{j=1}^{N}P(o_{t+1} | i_{t+1}=q_{j};\lambda)\beta_{t+1}(j) a_{ij}  \\    &amp;= \sum_{j=1}^{N} b_{j}(o_{t+1})a_{ij} \beta_{t+1}(j)\end{split}\]</span></p><p>  当 <span class="math inline">\(t=T\)</span>时，给定初始的后向概率：</p><p><span class="math display">\[\beta_{T}(i) = 1,\quadi=1,2,\dots,N\]</span></p><p>  综上所述，后向计算法的递推关系式可以总计为：</p><p><span class="math display">\[\begin{split}    \beta_{T}(i) &amp;= 1,\quad i=1,2,\dots,N \\    \beta_{t}(i) &amp;= \sum_{j=1}^{N} b_{j}(o_{t+1})a_{ij}\beta_{t+1}(j),\quad t=T-1,\dots,1;i=1,\dots,N\end{split}\]</span></p><h4 id="后向算法">后向算法</h4><p><strong>观测序列概率的后向算法</strong><br>  输入：隐马尔可夫模型参数 <span class="math inline">\(\lambda=(A,B,\pi)\)</span>，观测序列 <span class="math inline">\(O\)</span>;<br>  输出：观测序列概率 <span class="math inline">\(P(O |\lambda)\)</span>。<br>  (1) 初值</p><p><span class="math display">\[\beta_{T}(i) = 1,\quadi=1,2,\dots,N\]</span></p><p>  (2) 递推 <span class="math inline">\(\quad\)</span> 对 <span class="math inline">\(t=T-1,T-2,\dots,1\)</span></p><p><span class="math display">\[\beta_{t}(i) = \sum_{j=1}^{N}b_{j}(o_{t+1})a_{ij} \beta_{t+1}(j),\quad i=1,\dots,N\]</span></p><p>  (3) 终止</p><p><span class="math display">\[P(O | \lambda) = \sum_{i=1}^{N} \pi_{i}b_{i}(o_1) \beta_{1}(i)\]</span></p><p>  后向算法的计算复杂度为 <span class="math inline">\(O(TN^{2})\)</span>，与前向算法相同，优于直接计算法。</p><h2 id="学习问题-1">学习问题</h2><p>  隐马尔可夫模型的学习，根据训练数据是包括观测序列和对应的状态序列还是只有观测序列，可以分为监督学习与无监督学习。监督学习主要利用极大似然法来估计模型参数，无监督学习则是利用<span class="math inline">\(Baum-Welch\)</span> 算法，也就是 <span class="math inline">\(EM\)</span> 算法来估计参数。</p><h3 id="监督学习方法">监督学习方法</h3><p>  假设已给训练数据包含 <span class="math inline">\(S\)</span>个长度相同的观测序列和对应的状态序列 <span class="math inline">\(\{(O_1,I_1),(O_2,I_2),\dotsb,(O_S,I_S)\}\)</span>，可以利用极大似然估计法来估计隐马尔可夫模型的参数。</p><p>  <strong>1.转移概率 <span class="math inline">\(a_{ij}\)</span>的估计</strong><br>  设样本中时刻 <span class="math inline">\(t\)</span> 处于状态 <span class="math inline">\(q_i\)</span> 且时刻 <span class="math inline">\(t+1\)</span> 转移到状态 <span class="math inline">\(q_j\)</span> 的频数为 <span class="math inline">\(A_{ij}\)</span>，那么状态转移概率 <span class="math inline">\(a_{ij}\)</span> 的估计为：</p><p><span class="math display">\[\hat{a}_{ij}=\frac{A_{ij}}{\sum_{j=1}^{N}A_{ij}},\quadi=1,2,\dotsb,N;\quad j=1,2,\dotsb,N\]</span></p><p>  <strong>2.观测概率 <span class="math inline">\(b_{j}(k)\)</span>的估计</strong><br>  设样本中状态为 <span class="math inline">\(q_j\)</span> 并观测为 <span class="math inline">\(v_k\)</span> 的频数是 <span class="math inline">\(B_{jk}\)</span>，那么状态为 <span class="math inline">\(q_j\)</span> 观测为 <span class="math inline">\(v_k\)</span> 的概率 <span class="math inline">\(b_{j}(k)\)</span> 的估计为</p><p><span class="math display">\[\hat{b}_{j}(k)=\frac{B_{jk}}{\sum_{k=1}^{M}B_{jk}},\quadj=1,2,\quad,N;\quad k=1,2,\dotsb,M\]</span></p><p>  <strong>3.初始状态概率 <span class="math inline">\(\pi_{i}\)</span>的估计</strong><br>  设样本中初始状态为 <span class="math inline">\(q_i\)</span> 的频数为<span class="math inline">\(Q_i\)</span>，则初始状态概率 <span class="math inline">\(\pi_i\)</span> 的估计为</p><p><span class="math display">\[\hat{\pi}_{i}=\frac{Q_{i}}{S},\quadi=1,2,\dotsb,N\]</span></p><h3 id="无监督学习方法">无监督学习方法</h3><p>  虽然监督学习的方法操作十分简便，也非常容易理解，但监督学习需要对训练数据进行标注，而人工标注训练数据往往代价很高，因此，有时就会利用无监督学习的方法。无监督学习所使用的算法为<span class="math inline">\(Baum-Welch\)</span>，实际上为 <span class="math inline">\(EM\)</span> 算法。   假设给定训练数据只包含 <span class="math inline">\(S\)</span> 个长度为 <span class="math inline">\(T\)</span> 的观测序列 <span class="math inline">\(\{O_1,O_2,\dotsb,O_S\}\)</span>而没有对应的状态序列，目标是学习隐马尔可夫模型 <span class="math inline">\(\lambda=(A,B,\pi)\)</span>的参数。我们将观测序列数据看作观测数据 <span class="math inline">\(O\)</span>，状态序列数据看作不可观测的隐数据 <span class="math inline">\(I\)</span>，那么隐马尔可夫模型实际上是一个含有隐变量的概率模型</p><p><span class="math display">\[P(O | \lambda)=\sum_{I}P(O |I,\lambda)P(I | \lambda)\]</span></p><p>它的参数学习可以由 <span class="math inline">\(EM\)</span>算法实现。</p><p>  <strong>1.确定完全数据的对数似然函数</strong><br>  所有的观测数据写成 <span class="math inline">\(O=(o_1,o_2,\dotsb,o_{T})\)</span>，所有隐数据写成<span class="math inline">\(I=(i_1,i_2,\dotsb,i_{T})\)</span>，完全数据为<span class="math inline">\((O,I)=(o_1,o_2,\dotsb,o_{T};i_1,i_2,\dotsb,i_{T})\)</span>。完全数据的对数似然函数为：</p><p><span class="math display">\[L(\lambda) = \log{P(O,I |\lambda)}\]</span></p><p>  <strong>2.EM 算法的 E 步：求 <span class="math inline">\(Q\)</span>函数 <span class="math inline">\(Q(\lambda,\bar{\lambda})\)</span></strong><br>  由 <span class="math inline">\(Q\)</span> 函数的定义得</p><p><span class="math display">\[\begin{split}    Q(\lambda,\bar{\lambda}) &amp;= E_{I}[\log{P(O,I|\lambda)} |O,\bar{\lambda}]  \\    &amp;=\sum_{I}\frac{\log{P(O,I |\lambda)}P(O,I|\bar{\lambda})}{P(O|\bar{\lambda})}\end{split}\]</span></p><p>其中，<span class="math inline">\(\bar{\lambda}\)</span>是隐马尔可夫模型当前的估计值，<span class="math inline">\(\lambda\)</span>是下一步要极大化的隐马尔可夫模型参数。由于 <span class="math inline">\(P(O | \bar{\lambda})\)</span>为常数，对优化没有影响，可以舍去；同时由概率计算中的直接计算法可得：</p><p><span class="math display">\[P(O,I |\lambda)=\pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2) \dotsba_{i_{T-1}i_{T}}b_{i_{T}}(o_{T})\]</span></p><p>于是函数 <span class="math inline">\(Q(\lambda,\bar{\lambda})\)</span> 可以写成：</p><p><span class="math display">\[\begin{split}    Q(\lambda,\bar{\lambda}) &amp;=\sum_{I}\log{P(O,I|\lambda)P(O,I|\bar{\lambda})} \\    &amp;= \sum_{I} P(O,I | \bar{\lambda}) \log{[\pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2) \dotsba_{i_{T-1}i_{T}}b_{i_{T}}(o_{T}) ]}  \\    &amp;= \sum_{I} P(O,I | \bar{\lambda}) \log{\left[\pi_{i_1}\left(\prod_{t=1}^{T-1}a_{i_{t}i_{t+1}}\right)\left(\prod_{t=1}^{T}b_{i_t}(o_t) \right) \right]} \\    &amp;=\sum_{I}P(O,I | \bar{\lambda}) \left[\log(\pi_{i})+\sum_{t=1}^{T-1}\log(a_{i_{t}i_{t+1}})+\sum_{t=1}^{T}\log(b_{i_t}(o_t))\right] \\    &amp;=\sum_{I}\log(\pi_{i})P(O,I | \bar{\lambda})+\sum_{I}\left(\sum_{t=1}^{T-1}\log(a_{i_{t}i_{t+1}}) \right)P(O,I | \bar{\lambda}) \\    &amp;+ \sum_{I}\left( \sum_{t=1}^{T}\log(b_{i_t}(o_t)) \right)P(O,I| \bar{\lambda})\end{split}\]</span></p><p>式中求和都是对所有数据的序列总长度 <span class="math inline">\(T\)</span> 进行的。通过观察 <span class="math inline">\(Q(\lambda,\bar{\lambda})\)</span> 的计算式可以看出<span class="math inline">\(Q(\lambda,\bar{\lambda})\)</span> 的第一项<span class="math inline">\(\sum_{I}\log(\pi_{i})P(O,I |\bar{\lambda})\)</span> 只与参数 <span class="math inline">\(\lambda\)</span> 中的初始概率 <span class="math inline">\(\pi\)</span> 有关；第二项 <span class="math inline">\(\sum_{I}\left(\sum_{t=1}^{T-1}\log(a_{i_{t}i_{t+1}}) \right)P(O,I |\bar{\lambda})\)</span> 只与参数 <span class="math inline">\(\lambda\)</span> 中的状态转移概率矩阵 <span class="math inline">\(A\)</span> 有关；第三项 <span class="math inline">\(\sum_{I}\left( \sum_{t=1}^{T}\log(b_{i_t}(o_t))\right)P(O,I | \bar{\lambda})\)</span> 只与参数 <span class="math inline">\(\lambda\)</span> 中的观测概率矩阵 <span class="math inline">\(B\)</span> 有关。因此，可以令：</p><p><span class="math display">\[\begin{split}    Q_{1}(\pi,\bar{\lambda}) &amp;= \sum_{I}\log(\pi_{i})P(O,I |\bar{\lambda}) \\    Q_{2}(A,\bar{\lambda}) &amp;= \sum_{I}\left(\sum_{t=1}^{T-1}\log(a_{i_{t}i_{t+1}}) \right)P(O,I | \bar{\lambda}) \\    Q_{3}(B,\bar{\lambda}) &amp;= \sum_{I}\left(\sum_{t=1}^{T}\log(b_{i_t}(o_t)) \right)P(O,I | \bar{\lambda})\end{split}\]</span></p><p>则 <span class="math inline">\(Q(\lambda,\bar{\lambda})\)</span>可以写成：</p><p><span class="math display">\[Q(\lambda,\bar{\lambda})=Q_{1}(\pi,\bar{\lambda})+Q_{2}(A,\bar{\lambda})+Q_{3}(B,\bar{\lambda})\]</span></p><p>  <strong>3.EM 算法的 M 步：极大化 <span class="math inline">\(Q\)</span> 函数 <span class="math inline">\(Q(\lambda,\bar{\lambda})\)</span> 求模型参数 <span class="math inline">\(A,B,\pi\)</span></strong><br>  通过极大化 <span class="math inline">\(Q(\lambda,\bar{\lambda})\)</span> 可以得到模型参数<span class="math inline">\(A,B,\pi\)</span> 的估计值。<br>  <strong>(1) 估计初始状态概率 <span class="math inline">\(\pi\)</span></strong><br>  <span class="math inline">\(Q(\lambda,\bar{\lambda})\)</span> 中只有<span class="math inline">\(Q_{1}(\pi,\bar{\lambda})\)</span>与初始状态概率 <span class="math inline">\(\pi\)</span>有关，因此优化问题可以写成：</p><p><span class="math display">\[\begin{split}    &amp; \max_{\pi} \quad Q_{1}(\pi,\bar{\lambda}) \\    &amp; \space s.t. \quad \sum_{i=1}^{N} \pi_{i}=1 \\\end{split}\]</span></p><p>优化问题的拉格朗日函数：</p><p><span class="math display">\[L(\pi,\gamma)=\sum_{I}\log(\pi_{i})P(O,I| \bar{\lambda})+\gamma \left( \sum_{i=1}^{N}\pi_{i}-1\right)\]</span></p><p>由费马定理得：</p><p><span class="math display">\[\frac{\partial{L(\pi,\gamma)}}{\partial{\pi_{i}}}= \frac{P(O,i_1=i | \bar{\lambda})}{\pi_{i}}+\gamma=0,\quadi=1,2,\dotsb,N\]</span></p><p>得到：</p><p><span class="math display">\[\gamma\pi_{i}=-P(O,i_1=i|\bar{\lambda})\]</span></p><p>两边同时对 <span class="math inline">\(i\)</span> 求和得：</p><p><span class="math display">\[\begin{split}    &amp; \gamma\sum_{i=1}^{N}\pi_{i} =-\sum_{i=1}^{N}P(O,i_{1}=i|\bar{\lambda})=P(O | \bar{\lambda}) \\    &amp; \Rightarrow \gamma = P(O | \bar{\lambda}) \\\end{split}\]</span></p><p>从而得到 <span class="math inline">\(\pi_{i}\)</span>的估计值为：</p><p><span class="math display">\[\hat{\pi}_{i}=\frac{P(O,i_1=i|\bar{\lambda})}{P(O| \bar{\lambda})}\]</span></p><p>  <strong>(2) 估计状态转移矩阵 <span class="math inline">\(A\)</span></strong><br>  <span class="math inline">\(Q(\lambda,\bar{\lambda})\)</span> 中只有<span class="math inline">\(Q_{2}(A,\bar{\lambda})\)</span>与状态转移概率矩阵 <span class="math inline">\(A\)</span>有关，因此优化问题可以写成：</p><p><span class="math display">\[\begin{split}    &amp; \max_{A} \quad Q_{2}(A,\bar{\lambda}) \\    &amp; \space s.t. \quad \sum_{j=1}^{N}a_{ij}=1 \\\end{split}\]</span></p><p>优化问题的拉格朗日函数：</p><p><span class="math display">\[\begin{split}    L(A,\gamma) &amp;= \sum_{I}\left(\sum_{t=1}^{T-1}\log(a_{i_{t}i_{t+1}}) \right)P(O,I |\bar{\lambda})+\gamma\left( \sum_{j=1}^{N}a_{ij}-1 \right) \\    &amp;=\sum_{t=1}^{T-1}\sum_{i=1}^{N}\sum_{j=1}^{N}\left(P(O,i_{t}=i,i_{t+1}=j|\bar{\lambda})\log{a_{ij}} \right)+\gamma\left(\sum_{j=1}^{N}a_{ij}-1 \right)\end{split}\]</span></p><p>由费马定理得：</p><p><span class="math display">\[\frac{\partial{L(A,\gamma)}}{\partial{a_{ij}}}=\frac{\sum_{t=1}^{T-1}P(O,i_{t}=i,i_{t+1}=j|\bar{\lambda})}{a_{ij}}+\gamma=0,\quadi,j=1,\dotsb,N\]</span></p><p>得到：</p><p><span class="math display">\[\gammaa_{ij}=-\sum_{t=1}^{T-1}P(O,i_{t}=i,i_{t+1}=j|\bar{\lambda})\]</span></p><p>两边同时对 <span class="math inline">\(j\)</span> 求和：</p><p><span class="math display">\[\begin{split}    &amp;\gamma\sum_{j=1}^{N}a_{ij}=-\sum_{t=1}^{T-1}\sum_{j=1}^{N}P(O,i_{t}=i,i_{t+1}=j|\bar{\lambda})\\    &amp; \Rightarrow \gamma =-\sum_{t=1}^{T-1}P(O,i_{t}=i|\bar{\lambda})\end{split}\]</span></p><p>从而得到状态转移概率矩阵元素 <span class="math inline">\(a_{ij}\)</span> 的估计值为：</p><p><span class="math display">\[\hat{a}_{ij}=\frac{\sum_{t=1}^{T-1}P(O,i_{t}=i,i_{t+1}=j|\bar{\lambda})}{\sum_{t=1}^{T-1}P(O,i_{t}=i|\bar{\lambda})}\]</span></p><p>  <strong>(3) 估计观测概率矩阵 <span class="math inline">\(B\)</span></strong><br>  <span class="math inline">\(Q(\lambda,\bar{\lambda})\)</span> 中只有<span class="math inline">\(Q_{3}(B,\bar{\lambda})\)</span>与观测概率矩阵 <span class="math inline">\(B\)</span>有关，因此优化问题可以写成：</p><p><span class="math display">\[\begin{split}    &amp; \max_{B} \quad Q_{3}(B,\bar{\lambda}) \\    &amp; \space s.t. \quad \sum_{k=1}^{M}b_{j}(v_{k})=1 \\\end{split}\]</span></p><p>优化问题的拉格朗日函数：</p><p><span class="math display">\[\begin{split}    L(B,\gamma) &amp;= \sum_{I}\left( \sum_{t=1}^{T}\log(b_{i_t}(o_t))\right)P(O,I | \bar{\lambda})+\gamma\left( \sum_{k=1}^{M}b_{j}(v_{k})-1\right) \\    &amp;=\sum_{t=1}^{T}\sum_{j=1}^{N}\left(P(O,i_{t}=j|\bar{\lambda})\log{b_{j}(o_{t})} \right)+\gamma\left(\sum_{k=1}^{M}b_{j}(v_{k})-1 \right)\end{split}\]</span></p><p>由费马定理得：</p><p><span class="math display">\[\frac{\partial{L(B,\gamma)}}{\partial{b_{j}(v_{k})}}=\frac{\sum_{t=1}^{T}P(O,i_{t}=j|\bar{\lambda})I(o_{t}=v_{k})}{b_{j}(v_{k})}+\gamma=0,\quadj=1,\dotsb,N;k=1,\dotsb,M\]</span></p><p>得到：</p><p><span class="math display">\[\gammab_{j}(v_{k})=-\sum_{t=1}^{T}P(O,i_{t}=j|\bar{\lambda})I(o_{t}=v_{k})\]</span></p><p>两边同时对 <span class="math inline">\(k\)</span> 求和：</p><p><span class="math display">\[\begin{split}    &amp;\gamma\sum_{k=1}^{M}b_{j}(v_{k})=-\sum_{k=1}^{M}\sum_{t=1}^{T}P(O,i_{t}=j|\bar{\lambda})I(o_{t}=v_{k})\\    &amp; \Rightarrow \gamma = -\sum_{t=1}^{T}P(O,i_{t}=j|\bar{\lambda})\end{split}\]</span></p><p>从而得到观测概率矩阵元素的 <span class="math inline">\(b_{j}(k)\)</span> 的估计值为：</p><p><span class="math display">\[\hat{b}_{j}(k)=\frac{\sum_{t=1}^{T}P(O,i_{t}=j|\bar{\lambda})I(o_{t}=v_{k})}{\sum_{t=1}^{T}P(O,i_{t}=j|\bar{\lambda})}\]</span></p><h4 id="baum-welch-算法">Baum-Welch 算法</h4><p>  通过以上的推导，我们可以总结出无监督学习下隐马尔可夫参数估计的一种算法，其被称为Baum-Welch 算法，本质上是 EM 算法在隐马尔可夫模型学习中的具体实现。</p><p>  <strong>Baum-Welch 算法</strong><br>  输入：观测数据 <span class="math inline">\(O=(o_1,o_2,\dotsb,o_{T})\)</span>；<br>  输出：隐马尔可夫模型参数 <span class="math inline">\(\lambda=(A,B,\pi)\)</span>。<br>  (1) 初始化。对 <span class="math inline">\(n=0\)</span>，选取 <span class="math inline">\(a_{ij}^{(0)},b_{j}^{(0)}(k),\pi_{i}^{(0)}\)</span>，得到模型<span class="math inline">\(\lambda^{(0)}=(A^{(0)},B^{(0)},\pi^{(0)})\)</span>。<br>  (2) 递推。对 <span class="math inline">\(n=1,2,\dotsb\)</span>，</p><p><span class="math display">\[a_{ij}^{(n+1)}=\frac{\sum_{t=1}^{T-1}P(O,i_{t}=i,i_{t+1}=j|\lambda^{(n)})}{\sum_{t=1}^{T-1}P(O,i_{t}=i|\lambda^{(n)})}\]</span></p><p><span class="math display">\[b_{j}^{(n+1)}(k)=\frac{\sum_{t=1}^{T}P(O,i_{t}=j|\lambda^{(n)})I(o_{t}=v_{k})}{\sum_{t=1}^{T}P(O,i_{t}=j|\lambda^{(n)})}\]</span></p><p><span class="math display">\[\pi_{i}^{(n+1)}=\frac{P(O,i_1=i|\lambda^{(n)})}{P(O| \lambda^{(n)})}\]</span></p><p>  (3) 终止。得到模型参数 <span class="math inline">\(\lambda^{(n+1)}=(A^{(n+1)},B^{(n+1)},\pi^{(n+1)})\)</span></p><h2 id="解码问题-1">解码问题</h2><p>  解码问题主要是研究给定观测序列下最有可能出现的状态序列。已知模型参数<span class="math inline">\(\lambda=(A,B,\pi)\)</span> 和观测序列 <span class="math inline">\(O=(o_1,o_2,\dotsb,o_{T})\)</span>，求某一观测序列<span class="math inline">\(I=(i_1,i_2,\dotsb,i_{T})\)</span>使得条件概率 <span class="math inline">\(P(I | O)\)</span>最大。隐马尔可夫模型中的解码问题主要使用维特比算法。</p><h3 id="维特比算法">维特比算法</h3><p>  维特比算法实际是用动态规划(dynamicprogramming)解隐马尔可夫模型解码问题，即用动态规划求概率最大路径(最优路径)。这时一条路径对应着一个状态序列。维特比算法的思想可以用下图表示：</p><center><img src="https://s2.loli.net/2023/12/06/E87VbUDGMmWHFJp.jpg" width="60%" height="60%"><div data-align="center">Image3: 最优路径</div></center><p>  根据动态规划的原理，最优路径具有这样的特性：<strong>如果最优路径在<span class="math inline">\(t\)</span> 时刻通过结点 <span class="math inline">\(i_{t}^{*}\)</span>，那么这一路径从结点 <span class="math inline">\(i_{t}^{*}\)</span> 到终点 <span class="math inline">\(i_{T}^{*}\)</span> 的部分路径，对于从 <span class="math inline">\(i_{t}^{*}\)</span> 到 <span class="math inline">\(i_{T}^{*}\)</span>的所有可能的部分路径来说，必须是最优的。</strong>依据这一原理，我们只需要从时刻 <span class="math inline">\(t=1\)</span>开始，递推地计算在时刻 <span class="math inline">\(t\)</span> 状态为<span class="math inline">\(i\)</span>的各条部分路径的最大概率，直至得到时刻 <span class="math inline">\(t=T\)</span> 状态为 <span class="math inline">\(i\)</span> 的各条路径的最大概率。时刻 <span class="math inline">\(t=T\)</span> 的最大概率即为最优路径的概率 <span class="math inline">\(P^{*}\)</span>，最优路径的终结点 <span class="math inline">\(i_{T}^{*}\)</span>也同时得到。之后，为了找出最优路径的各个结点，从终结点 <span class="math inline">\(i_{T}^{*}\)</span> 开始，由后向前逐步求得结点<span class="math inline">\(i_{T-1}^{*},\dotsb,i_{1}^{*}\)</span>，得到最优路径<span class="math inline">\(I^{*}=(i_{1}^{*},i_{2}^{*},\dotsb,i_{T}^{*})\)</span>。这就是维特比算法。</p><p>  首先导入两个变量 <span class="math inline">\(\delta\)</span> 和<span class="math inline">\(\psi\)</span>。定义在时刻 <span class="math inline">\(t\)</span> 状态为 <span class="math inline">\(i\)</span> 的所有单个路径 <span class="math inline">\((i_1,i_2,\dotsb,i_{t})\)</span>中概率最大值为：</p><p><span class="math display">\[\delta_{t}(i)=\max_{i_1,i_2,\dotsb,i_{t-1}}P(i_{t}=i,i_{t-1},\dotsb,i_1;o_{t},\dotsb,o_1| \lambda),\quad i=1,2,\dotsb,N\]</span></p><p>  由定义可得变量 <span class="math inline">\(\delta\)</span>的递推公式：</p><p><span class="math display">\[\begin{split}    \delta_{t+1}(i) &amp;=\max_{i_1,i_2,\dotsb,i_{t}}P(i_{t+1}=i,i_{t},\dotsb,i_1;o_{t+1},\dotsb,o_1| \lambda) \\    &amp;= \max_{1 \leq j \leq N}[\delta_{t}a_{ji}]b_{i}(o_{t+1}),\quadi=1,2,\dotsb,N; \space t=1,2,\dotsb,T-1\end{split}\]</span></p><p>  定义在时刻 <span class="math inline">\(t\)</span> 状态为 <span class="math inline">\(i\)</span> 的所有单个路径 <span class="math inline">\((i_1,i_2,\dotsb,i_{t-1},i)\)</span>中概率最大的路径的第 <span class="math inline">\(t-1\)</span>个结点为</p><p><span class="math display">\[\psi_{t}(i)=\arg\max_{1 \leq j \leqN}[\delta_{t-1}(j)a_{ji}],\quad i=1,2,\dotsb,N\]</span></p><p>  <strong>维特比算法</strong><br>  输入：模型参数 <span class="math inline">\(\lambda=(A,B,\pi)\)</span>和观测 <span class="math inline">\(O=(o_1,o_2,\dotsb,o_{T})\)</span>；<br>  输出：最优路径 <span class="math inline">\(I^{*}=(i_{1}^{*},i_{2}^{*},\dotsb,i_{T}^{*})\)</span>.<br>  (1) 初始化</p><p><span class="math display">\[\delta_{1}(i)=\pi_{i}b_{i}(o_1),\quadi=1,2,\dotsb,N\]</span></p><p><span class="math display">\[\psi_{1}(i)=0,\quadi=1,2,\dotsb,N\]</span></p><p>  (2) 递推。对 <span class="math inline">\(t=2,3,\dotsb,T\)</span></p><p><span class="math display">\[\delta_{t}(i)=\max_{1 \leq j \leqN}[\delta_{t-1}(j)a_{ji}]b_{i}(o_{t}),\quad i=1,2,\dotsb,N\]</span></p><p><span class="math display">\[\psi_{t}(i)=\arg\max_{1 \leq j \leqN}[\delta_{t-1}(j)a_{ji}],\quad i=1,2,\dotsb,N\]</span></p><p>  (3) 终止</p><p><span class="math display">\[P^{*}=\max_{1 \leq i \leq N}\delta_{T}(i)\]</span></p><p><span class="math display">\[i_{T}^{*}=\arg\max_{1 \leq i \leqN}[\delta_{T}(i)]\]</span></p><p>  (4) 最优路径回溯。对 <span class="math inline">\(t=T-1,T-2,\dotsb,1\)</span></p><p><span class="math display">\[i_{t}^{*}=\psi_{t+1}(i_{t+1}^{*})\]</span></p><p>求得最优路径 <span class="math inline">\(I^{*}=(i_{1}^{*},i_{2}^{*},\dotsb,i_{T}^{*})\)</span>。</p><h2 id="参考">参考</h2><p><strong>[1] Book: 李航,统计学习方法(第2版)</strong><br><strong>[2] Book: 董平,机器学习中的统计思维(Python实现)</strong><br><strong>[3] Video: bilibili,简博士,隐马尔可夫系列</strong><br><strong>[4] Video: bilibili,shuhuai008,隐马尔可夫系列</strong></p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>矩阵分析-6.矩阵的等价与相似</title>
    <link href="/2023/11/20/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-6-%E7%9F%A9%E9%98%B5%E7%9A%84%E7%AD%89%E4%BB%B7%E4%B8%8E%E7%9B%B8%E4%BC%BC/"/>
    <url>/2023/11/20/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-6-%E7%9F%A9%E9%98%B5%E7%9A%84%E7%AD%89%E4%BB%B7%E4%B8%8E%E7%9B%B8%E4%BC%BC/</url>
    
    <content type="html"><![CDATA[<h1 id="矩阵的等价与相似">矩阵的等价与相似</h1><h2 id="矩阵等价">矩阵等价</h2><h3 id="定义">定义</h3><p>  设矩阵<span class="math inline">\(A,B \in \mathbb{F}^{m \timesn}\)</span>，若存在可逆矩阵 <span class="math inline">\(P \in\mathbb{F}^{n \times n}, Q \in \mathbb{F}^{m \times m}\)</span>，使得<span class="math inline">\(AP=QB\)</span>，则称矩阵<span class="math inline">\(A,B\)</span>等价。</p><p>[注]：由于<span class="math inline">\(AP=QB\)</span>，且<span class="math inline">\(Q\)</span>可逆，可得 <span class="math inline">\(Q^{-1}AP=B\)</span>. <strong>因此矩阵<span class="math inline">\(B\)</span>是由矩阵<span class="math inline">\(A\)</span>进行有限次初等变换后得到的新矩阵.</strong></p><h3 id="几何意义">几何意义</h3><p>  令：</p><p><span class="math display">\[P = \begin{bmatrix}    p_1,p_2,\dots,p_n\end{bmatrix}, p_{i} \in \mathbb{F}^{n}, i=1,2,\dots,n\]</span></p><p><span class="math display">\[Q = \begin{bmatrix}    q_1,q_2,\dots,q_{m}\end{bmatrix}, q_j \in \mathbb{F}^{m},i=1,2,\dots,m\]</span></p><p>  <span class="math inline">\(\because AP=QB\)</span>，故有：</p><p><span class="math display">\[A\begin{bmatrix}    p_1,p_2,\dots,p_n \\\end{bmatrix}=\begin{bmatrix}    q_1,q_2,\dots,q_m\end{bmatrix}B\]</span></p><p>  矩阵<span class="math inline">\(A \in \mathbb{F}^{m \timesn}\)</span>，可视为线性映射：</p><p><span class="math display">\[\begin{split}    \mathbb{F}^{n} &amp; \rightarrow \mathbb{F}^{m} \\     x &amp; \rightarrow y=Ax  \end{split}\]</span></p><p>  入口基：<span class="math inline">\(\begin{bmatrix}  p_1,p_2,\dots,p_n\end{bmatrix}\)</span>，出口基：<span class="math inline">\(\begin{bmatrix}  q_1,q_2,\dots,q_{m}\end{bmatrix}\)</span>，由线性映射的概念可以得到矩阵等价的几何意义是：<strong>线性映射<span class="math inline">\(A\)</span>在入口基<span class="math inline">\(P\)</span>和出口基<span class="math inline">\(Q\)</span>下的矩阵表示为<span class="math inline">\(B\)</span>.</strong></p>]]></content>
    
    
    <categories>
      
      <category>矩阵分析</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>机器学习-5.逻辑回归</title>
    <link href="/2023/11/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-5-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <url>/2023/11/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-5-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</url>
    
    <content type="html"><![CDATA[<h1 id="逻辑回归">逻辑回归</h1><p>  逻辑回归(LogisticRegression)是一种用于解决二分类问题的机器学习方法。虽然逻辑回归在名称中含有“回归”一词，但其实际上主要用于分类问题。逻辑回归的主要思想是将由线性回归所得的值通过一个逻辑函数映射到0和1之间的概率值，从而将线性回归模型转化为一个分类模型。逻辑回归最早是由统计学家和生物学家使用，用于建立生物学实验结果与概率的关系。后来，随着计算机科学的发展，逻辑回归成为机器学习领域的重要算法之一，被广泛用于分类问题。逻辑回归的优势在于简单易懂、计算效率高，特别适用于大规模数据集。然而，在处理复杂非线性关系的问题上，逻辑回归可能受到限制，这时候更复杂的模型（如支持向量机、深度学习等）可能更为适用。</p><h2 id="基本思想">基本思想</h2><p>  逻辑回归的基本思想是通过逻辑函数(也称为sigmoid函数)将线性回归的结果映射为一个概率值，然后利用概率值解决二分类问题。逻辑回归的基本思想中包含着三个主要因素：<strong>线性回归</strong>、<strong>逻辑函数</strong>、<strong>决策边界</strong>。<br>  线性回归很好理解，给定一个输入的特征向量 <span class="math inline">\(x=\begin{bmatrix}  x_1,x_2,\dots,x_n \\\end{bmatrix}^{T}\)</span>，以及权重向量 <span class="math inline">\(w=\begin{bmatrix}  w_1,w_2,\dots,w_n \\\end{bmatrix}^{T}\)</span>，以及偏置项 <span class="math inline">\(b\)</span>，可以计算线性回归结果：</p><p><span class="math display">\[z =w^{T}x+b=w_1x_1+w_2x_2+\dots+w_nx_n+b\]</span></p><p>  逻辑函数<span class="math inline">\(f\)</span>的作用是将线性回归的结果映射到一个概率值，即：</p><p><span class="math display">\[f: z \rightarrow p \in[0,1]\]</span></p><p>在实际中，我们的逻辑函数一般为<span class="math inline">\(sigmoid\)</span>函数，其函数形式为：</p><p><span class="math display">\[sigmoid(z)=\frac{1}{1+e^{-z}}\]</span></p><p>其函数图像为：</p><center><img src="https://s2.loli.net/2023/11/15/4rRHc3iUGn2FDKx.jpg" width="60%" height="60%"><div data-align="center">Image1: sigmoid函数图像</div></center><p>  sigmoid函数的主要优点如下：</p><ul><li><strong>输出范围为(0,1):</strong>sigmoid函数的输出范围在0和1之间，这与概率的范围一致。这使得逻辑回归的输出可以被解释为属于某个类别的概率。</li><li><strong>可导性:</strong>sigmoid函数是可导的，这使得使用梯度下降等优化算法来最小化损失函数成为可能。梯度下降等优化方法对于机器学习模型的训练非常重要，而Sigmoid函数的可导性使得模型参数可以通过梯度下降等优化方法进行有效地更新。<br></li><li><strong>单调递增性:</strong>sigmoid函数是单调递增的，这意味着输入变量的增加必然导致输出的增加。这一特性有助于模型学习输入特征与输出概率之间的关系，使得模型更容易收敛。</li><li><strong>数学上平滑:</strong>sigmoid函数的平滑性质有助于在优化过程中避免梯度爆炸或梯度消失的问题，这在深度学习等领域尤为重要。</li><li><strong>对异常值的鲁棒性:</strong>sigmoid函数在极端值上趋于饱和，对于一些异常值的影响相对较小。这有助于模型对于噪声或异常值的鲁棒性。</li></ul><p>  决策边界是指当我们通过逻辑函数得到概率值 <span class="math inline">\(p\)</span> 后，我们如何判别实例<span class="math inline">\(x\)</span>属于哪一个类别。当我们在面对的是二分类问题时，设<span class="math inline">\(y \in \{0,1\}\)</span> 表示实例<span class="math inline">\(x\)</span>的类别，概率值<span class="math inline">\(p\)</span>表示条件概率：</p><p><span class="math display">\[p = P(y=1 | x)\]</span></p><p>则我们选用的决策边界为：</p><p><span class="math display">\[\hat{y}= \left \{\begin{array}{rcl}1, &amp; {p &gt;0.5}\\0,&amp; {p \leq 0.5}\\\end{array} \right.\]</span></p><p>  其中，<span class="math inline">\(\hat{y}\)</span>为实例<span class="math inline">\(x\)</span>的预测类别。</p><h2 id="模型">模型</h2><p><strong>输入</strong></p><ul><li><strong>输入空间:</strong> <span class="math inline">\(\mathcal{X}\in \mathbb{R}^{n}\)</span>.<br></li><li><strong>输入实例:</strong> <span class="math inline">\(x =\begin{bmatrix}  x^{(1)},x^{(2)},\dots,x^{(n)} \\ \end{bmatrix}^T \in\mathcal{X}\)</span>.</li></ul><p>  其中，输入空间<span class="math inline">\(\mathcal{X}\)</span>为<span class="math inline">\(n\)</span>维实数空间的子集，输入实例<span class="math inline">\(x\)</span>为<span class="math inline">\(n\)</span>维特征向量。</p><p><strong>输出</strong><br>  由于我们只考虑二分类问题，因此实例点的类别只有正类与负类两种。</p><ul><li><strong>输出空间:</strong> <span class="math inline">\(\mathcal{Y} =\{ 0,1 \}\)</span><br></li><li><strong>输出实例:</strong> <span class="math inline">\(y \in\mathcal{Y}\)</span>.</li></ul><p>  其中，输出空间<span class="math inline">\(\mathcal{Y}\)</span>为只包含1，0两个元素的集合，1与0分别代表二分类问题中的正类与负类。输出实例<span class="math inline">\(y\)</span>代表相对应的输出实例<span class="math inline">\(x\)</span>的类别。</p><p><strong>数据集</strong><br>  逻辑回归的训练数据集<span class="math inline">\(T_{train}\)</span>为：</p><p><span class="math display">\[T_{train}=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N) \}\]</span></p><p>  其中，<span class="math inline">\(x_i \in\mathcal{X}=\mathbb{R}^{n},y_i \in\mathcal{Y}=\{0,1\},i=1,2,\dots,N\)</span>，<span class="math inline">\(N\)</span>表示训练数据的样本容量。</p><p><strong>模型</strong><br>  逻辑回归的模型形式表现为条件概率分布：</p><p><span class="math display">\[\begin{split}    P(Y=1|X) &amp;= \frac{1}{1+e^{-(w^{T}x+b)}} \\    P(Y=0|X) &amp;= \frac{e^{-(w^{T}x+b)}}{1+e^{-(w^{T}x+b)}}\end{split}\]</span></p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>机器学习-4.交叉熵与KL散度</title>
    <link href="/2023/11/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-4-%E4%BA%A4%E5%8F%89%E7%86%B5%E4%B8%8EKL%E6%95%A3%E5%BA%A6/"/>
    <url>/2023/11/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-4-%E4%BA%A4%E5%8F%89%E7%86%B5%E4%B8%8EKL%E6%95%A3%E5%BA%A6/</url>
    
    <content type="html"><![CDATA[<h1 id="交叉熵与kl散度">交叉熵与KL散度</h1><p>  在机器学习中，我们经常使用信息熵、交叉熵、KL散度等概率，例如在决策树中，我们使用基于信息熵的信息增益来构造树形结构；而交叉熵常被用于分类问题的损失函数；KL散度则被用于衡量两个分布之间的差异。本文将会介绍这些常用的概率，以便于我们今后学习相应的机器学习模型。</p><h2 id="信息熵-entropy">信息熵 (entropy)</h2><h3 id="熵">熵</h3><p>  在现实中，我们会接触到各种各样的信息，如何对信息进行量化便成为了一个重要的问题。信息论是应用数学的一个分支，由美国数学家香农提出并发展壮大，主要研究的是对一个事件包含信息的多少进行量化。<br>  信息论的基本思想是一个小概率事件发生了，要比大概率事件发生，提供的信息更多。在信息论中，我们认为事件的信息量具有以下三条性质：</p><ul><li>大概率事件所包含的信息量较小。<br></li><li>小概率事件所包含的信息量较大。</li><li>独立事件的信息量可以进行累加。</li></ul><p>  由以上三条性质，我们定义了某一事件<span class="math inline">\(X=x\)</span>的<strong>自信息量</strong>(self-information)为:</p><p><span class="math display">\[I(x)=-\log{P(x)}\]</span></p><p>  其中<span class="math inline">\(X\)</span>为随机变量，表示某一事件；<span class="math inline">\(x\)</span>为随机变量<span class="math inline">\(X\)</span>的取值。当上式中的<span class="math inline">\(\log\)</span>以2为底数时，<span class="math inline">\(I(x)\)</span>的单位是比特(bit)或者香农(shannons)；当<span class="math inline">\(\log\)</span>以2为底数时，<span class="math inline">\(I(x)\)</span>单位是奈特(nats)。这两个单位之间可以通过对数换底公式相互转换。<br>  自信息量表示单个事件的信息量。若我们已知事件<span class="math inline">\(X\)</span>服从某一概率分布<span class="math inline">\(P(X)\)</span>，我们可以使用<strong>香农熵</strong>(Shannonentropy)来对整个概率分布所包含的信息总量进行量化：</p><p><span class="math display">\[H(X)=\mathbb{E}_{X \simP}[I(x)]\]</span></p><p>  若随机变量<span class="math inline">\(X\)</span>为离散型随机变量，则熵可以写为求和形式：</p><p><span class="math display">\[H(X)=\sum_{i=1}^{N}P(x_i)I(x_i)=-\sum_{i=1}^{N}P(x_i)\log{P(x_i)}\]</span></p><p>  若随机变量<span class="math inline">\(X\)</span>为连续型随机变量，则熵可以写为积分形式：</p><p><span class="math display">\[H(X)=\int_{\mathcal{X}}P(x)I(x)dx=-\int_{\mathcal{X}}P(x)\log{P(x)}dx\]</span></p><h3 id="联合熵">联合熵</h3><p>  若有两个随机变量<span class="math inline">\(X,Y\)</span>，且服从某个联合分布<span class="math inline">\(P(X,Y)\)</span>，我们可以使用联合熵来对联合概率分布所包含的信息量进行量化：</p><p><span class="math display">\[H(X,Y)=-\sum_{x \in \mathcal{X}}\sum_{y\in \mathcal{Y}}P(x,y)\log{P(x,y)}\]</span></p><p>  以上给出的是<span class="math inline">\(X,Y\)</span>为离散型随机变量的联合熵，若<span class="math inline">\(X,Y\)</span>为连续型随机变量，则依照熵的形式，联合熵也可以写成积分形式：</p><p><span class="math display">\[H(X,Y)=-\int_{\mathcal{X}}\int_{\mathcal{Y}}P(x,y)\log{P(x,y)}dydx\]</span></p><h3 id="条件熵">条件熵</h3><p>  在数理统计中，我们还学习了条件概率分布，表示在某个事件发生后，另一个事件所发生的概率，在信息论中，用条件熵来表示条件概率分布所包含的信息。若有两个离散随机变量<span class="math inline">\(X,Y\)</span>，且已知联合概率分布<span class="math inline">\(P(X,Y)\)</span>，条件概率分布<span class="math inline">\(P(Y|X)\)</span>，则该条件分布的条件熵为：</p><p><span class="math display">\[H(Y|X)=-\sum_{x \in \mathcal{X}}\sum_{y\in \mathcal{Y}}P(x,y)\log{P(y|x)}\]</span></p><p>  当<span class="math inline">\(X,Y\)</span>为连续型随机变量时，上式可以写出积分形式：</p><p><span class="math display">\[H(Y|X)=-\int_{\mathcal{X}}\int_{\mathcal{Y}}P(x,y)\log{P(y|x)dydx}\]</span></p><h3 id="最大熵思想">最大熵思想</h3><p>  既然信息熵可以用来表示信息量的大小，人们自然希望找到包含信息量最大的概率分布。当我们的概率分布中存在未知参数时，可以使用<strong>最大化分布的熵</strong>的思想来估计未知参数，这类方法在机器学习中被称为最大熵学习，这里不展开讨论最大熵学习，有兴趣的读者可查阅相关资料进行了解。<br>  通过最大熵思想，我们有一个非常有意思的发现：<strong>若有定义在整个实数轴上的随机变量<span class="math inline">\(X\)</span>，其均值为<span class="math inline">\(\mu\)</span>，方差为<span class="math inline">\(\sigma^{2}\)</span>，当<span class="math inline">\(X \sim N(\mu,\sigma^2)\)</span>时，熵 <span class="math inline">\(H(X)\)</span> 最大.</strong>这个结论的证明放在目录，有兴趣的读者可自行阅读。高斯分布具有最大的信息熵，这也是为什么在现实生活中大量随机事件都服从高斯分布，因为自然界总是偏向于制造最大的不确定性，从而包含最多的信息，即最大的熵。</p><h2 id="交叉熵cross-entropy">交叉熵(cross entropy)</h2><p>  在机器学习中，我们常常需要比较两个分布之间的相似程度，例如当我们用一个估计的分布去近似真实分布时，我们自然希望这两个分布越相似越好。交叉熵便是衡量两个分布之间相似程度的一种度量方法，因此经常被用作机器学习模型的损失函数。<br>  为了理解交叉熵为什么能够度量两个分布的相似程度，我们借助贝叶斯统计的思想来进行解释。根据贝叶斯思想，对于某一个事件<span class="math inline">\(X\)</span>，我们会有一个先验的认知，即<span class="math inline">\(X\)</span>的先验分布<span class="math inline">\(P_0(x)\)</span>，在先验认知下，事件<span class="math inline">\(X\)</span>带给我们的信息量为：</p><p><span class="math display">\[I_{0}(x)=-\log{P_{0}(x)}\]</span></p><p>  假设事件<span class="math inline">\(X\)</span>的真实分布为<span class="math inline">\(P_{1}(x)\)</span>，则<strong>我们在主观认知下，通过客观事实所得到的事件<span class="math inline">\(X\)</span>的概率分布所包含的信息总量</strong>为：</p><p><span class="math display">\[H(P_0,P_1)=-\sum_{i=1}^{N}P_{1}(x)\log{P_{0}(x)}dx\]</span></p><p>  当<span class="math inline">\(X\)</span>为连续型随机变量时，其积分形式为：</p><p><span class="math display">\[H(P_{0},P_{1})=-\int_{\mathcal{X}}P_{1}(x)\log{P_{0}(x)}dx\]</span></p><p>  上式即为分布<span class="math inline">\(P_0,P_1\)</span>的<strong>交叉熵</strong>。若交叉熵较大，说明在已有主观先验认知下，事件<span class="math inline">\(X\)</span>的实际情况带给我们的信息量较大，说明我们的先验认知与实际情况差别较大，即<span class="math inline">\(P_{0}(x)\)</span>与<span class="math inline">\(P_{1}(x)\)</span>的相似度较低；若交叉熵较小，说明在已有主观先验认知下，事件<span class="math inline">\(X\)</span>的实际情况带给我们的信息量较小，说明我们的先验认知与实际情况差别较小，即<span class="math inline">\(P_{0}(x)\)</span>与<span class="math inline">\(P_{1}(x)\)</span>的相似度较高。<br>  在机器学习中，我们希望学习到的概率分布<span class="math inline">\(P_{m}\)</span>与训练数据所估计的真实分布<span class="math inline">\(P_{t}\)</span>足够相似，这时我们通常将分布<span class="math inline">\(P_{m}\)</span>与<span class="math inline">\(P_{t}\)</span>的交叉熵作为损失函数，例如逻辑回归模型，通过最小化交叉熵来调整<span class="math inline">\(P_{m}\)</span>，使得<span class="math inline">\(P_{m}\)</span>与真实分布<span class="math inline">\(P_{t}\)</span>足够相似。<br>  另外，有一个非常有意思的结论，<strong>最小化交叉熵实际上是等价于极大似然估计</strong>，这个结论的证明将会放在附录。</p><h2 id="kl散度kl-divergence">KL散度(KL Divergence)</h2><p>  上文介绍了交叉熵，其可以用来衡量两个分布的相似程度。但交叉熵存在一个问题，即若我们主观的先验认知与客观实际完全一致，即<span class="math inline">\(P_{0}=P_{1}\)</span>，此时我们实际上并没有得到任何信息，但交叉熵的计算结果为<span class="math inline">\(P_{0}\)</span>的信息熵，并不为<span class="math inline">\(0\)</span>。因此，我们可以转而考虑信息的增量，KL散度实际上就时在考虑信息熵的增量，我们先给出两个分布<span class="math inline">\(P_0,P_1\)</span>的KL散度的计算公式：</p><p><span class="math display">\[KL(P_1||P_0)=\mathbb{E}_{X \simP_1}[\log{\frac{P_{1}(x)}{P_{0}(x)}}]\]</span></p><p>  通过将上式进行调整，我们可以将<span class="math inline">\(P_{0}\)</span>与<span class="math inline">\(P_{1}\)</span>的KL散度写成<span class="math inline">\(P_{0}\)</span>与<span class="math inline">\(P_{1}\)</span>的交叉熵<span class="math inline">\(H(P_{0},P_{1})\)</span>与<span class="math inline">\(P_{0}\)</span>的信息熵之差：</p><p><span class="math display">\[KL(P_{1}||P_{0})=H(P_{0},P_{1})-H(P_{0})\]</span></p><p>  通过上式我们可以发现，当<span class="math inline">\(P_1\)</span>与<span class="math inline">\(P_0\)</span>的KL散度较大，说明在已有主观认知下，我们从客观事件获得信息增量较大，说明我们的主观认知与客观现实不一致，即<span class="math inline">\(P_1\)</span>与<span class="math inline">\(P_0\)</span>的相似度较低；当<span class="math inline">\(P_1\)</span>与<span class="math inline">\(P_0\)</span>的KL散度较小，说明在已有主观认知下，我们从客观事件获得信息增量较小，说明我们的主观认知与客观现实较为一致，即<span class="math inline">\(P_1\)</span>与<span class="math inline">\(P_0\)</span>的相似度较大；当<span class="math inline">\(P_1\)</span>与<span class="math inline">\(P_0\)</span>的KL散度等于0时，说明已有主观认知下，我们从客观事件中没有获得额外的信息，说明我们的主观认知与客观现实完全一致，即<span class="math inline">\(P_0=P_1\)</span>.<br>  KL散度衡量了一种信息增益，因此也被称为<strong>相对熵</strong>。在机器学习中我们同样可以使用KL散度作为损失函数。</p><h2 id="附录">附录</h2><h3 id="一-高斯分布具有最大的信息熵">(一) 高斯分布具有最大的信息熵</h3><p><strong>结论:</strong> 已知定义在整个实数轴上的连续型随机变量<span class="math inline">\(X\)</span>的均值为<span class="math inline">\(\mu\)</span>，方差为<span class="math inline">\(\sigma^{2}\)</span>，当<span class="math inline">\(X \sim N(\mu,\sigma^{2})\)</span>，<span class="math inline">\(X\)</span>的信息熵<span class="math inline">\(H(X)\)</span>最大。<br><strong>证明:</strong><br>  设随机变量<span class="math inline">\(X\)</span>的概率密度函数为<span class="math inline">\(p(x)\)</span>，由题意可知：</p><p><span class="math display">\[E(X)=\int_{-\infty}^{+\infty}xp(x)dx=\mu,\quadVar(X)=\int_{-\infty}^{+\infty}(x-\mu)^{2}p(x)dx=\sigma^{2}\]</span></p><p>  根据最大熵思想，可以得到如下一个带约束的优化问题：</p><p><span class="math display">\[\begin{split}    \max_{p(x)} \quad &amp;H(X)=-\int_{-\infty}^{+\infty}p(x)\ln{p(x)}dx  \\    s.t. \quad &amp; \int_{-\infty}^{+\infty}p(x)=1 \\    &amp; \int_{-\infty}^{+\infty}xp(x)dx=\mu  \\    &amp; \int_{-\infty}^{+\infty}(x-\mu)^{2}p(x)dx=\sigma^{2}\end{split}\]</span></p><p>  上述原问题的拉格朗日函数为：</p><p><span class="math display">\[\begin{split}    Q(p(x),\lambda_1,\lambda_2,\lambda_3) =&amp;-\int_{-\infty}^{+\infty}p(x)\ln{p(x)}dx+\lambda_1 \left(\int_{-\infty}^{+\infty}p(x)-1 \right) \\    &amp;+ \lambda_2 \left( \int_{-\infty}^{+\infty}xp(x)dx-\mu\right)+\lambda_3 \left(\int_{-\infty}^{+\infty}(x-\mu)^{2}p(x)dx-\sigma^{2} \right)\end{split}\]</span></p><p>  令<span class="math inline">\(\lambda=\begin{bmatrix}  \lambda_1,\lambda_2,\lambda_3\end{bmatrix}^{T}\)</span>，则原问题的无约束形式可以写为</p><p><span class="math display">\[\begin{split}     \max_{p(x)}\min_{\lambda} \quad &amp;Q(p(x),\lambda_1,\lambda_2,\lambda_3)  \\     \space s.t. \quad &amp; \lambda \ge 0\end{split}\]</span></p><p>  则原问题的对偶问题为：</p><p><span class="math display">\[\begin{split}     \min_{\lambda}\max_{p(x)} \quad &amp;Q(p(x),\lambda_1,\lambda_2,\lambda_3)  \\     \space s.t. \quad &amp; \lambda \ge 0\end{split}\]</span></p><p>  设<span class="math inline">\(p^{*}(x)\)</span>为原问题最优解，<span class="math inline">\(\lambda^{*}\)</span>为对偶问题最优解，由KKT条件得：</p><p><span class="math display">\[\frac{\partial Q}{\partial p(x)}|_{p(x)=p^{*}(x)}=-[\ln{p^{*}(x)+1}]+\lambda_1^{*}+\lambda_2^{*}x+\lambda_3^{*}(x-\mu)^{2}=0\]</span></p><p><span class="math display">\[\Rightarrowp^{*}(x)=e^{\lambda_1^{*}-1+\lambda_2^{*}x+\lambda_3^{*}(x-\mu)^{2}}\]</span></p><p>  之后对<span class="math inline">\(p^{*}(x)\)</span>做一些变形：</p><p><span class="math display">\[\begin{split}    p^{*}(x) &amp;=e^{\lambda_1^{*}-1+\lambda_2^{*}x+\lambda_3^{*}(x-\mu)^{2}}=e^{\lambda_1^{*}-1}e^{\lambda_3^{*}x^2+(\lambda_2^{*}-2\mu\lambda_3^{*})x+\mu^{2}\lambda_3^{*}}\\    &amp;=Ce^{\lambda_3^{*}[x^2+(\frac{\lambda_2^{*}}{\lambda_3^{*}}-2\mu)x+\mu^2]}= Ce^{\lambda_3^{*}[x-(\mu-\frac{\lambda_2^{*}}{2\lambda_3^{*}})]^{2}}\end{split}\]</span></p><p>  由密度函数<span class="math inline">\(p^{*}(x)\)</span>的非负性以及正则性可知：<span class="math inline">\(C&gt;0,\lambda_3^{*}&lt;0\)</span>；指数部分中的二次函数<span class="math inline">\([x-(\mu-\frac{\lambda_2^{*}}{2\lambda_3^{*}})]^{2}\)</span>表明 <span class="math inline">\(p^{*}(x)\)</span>的对称轴为 <span class="math inline">\(\mu-\frac{\lambda_2^{*}}{2\lambda_3^{*}}\)</span>，由于对称的密度函数，其对称轴一定等于均值可知：<span class="math inline">\(\lambda_2^{*}=0\)</span>。<br>  由于<span class="math inline">\(\lambda_3^{*}&gt;0\)</span>，可设<span class="math inline">\(\lambda_3^{*}=-\beta\)</span>，则<span class="math inline">\(p^{*}(x)\)</span>可化简为：</p><p><span class="math display">\[p^{*}(x)=Ce^{-\beta(x-\mu)^{2}}\]</span></p><p>  将<span class="math inline">\(p^{*}(x)\)</span>代入正则化约束得：</p><p><span class="math display">\[\begin{split}    1 &amp;=\int_{-\infty}^{+\infty}p^{*}(x)dx=C\int_{-\infty}^{+\infty}e^{-\beta(x-\mu)^{2}}dx\\    &amp;=C\int_{-\infty}^{+\infty}e^{-\betay^{2}}dy=\frac{C}{\sqrt{\beta}}\int_{0}^{+\infty}z^{-\frac{1}{2}}e^{-z}dz \\    &amp;=\frac{C}{\sqrt{\beta}}\Gamma \left( \frac{1}{2} \right) =C\sqrt{\frac{\pi}{\beta}}\end{split}\]</span></p><p>  从而得：<span class="math inline">\(C=\sqrt{\frac{\beta}{\pi}}\)</span>，再利用方差约束条件得：</p><p><span class="math display">\[\begin{split}    \sigma^{2} &amp;= \int_{-\infty}^{+\infty}(x-\mu)^2p^{*}(x)dx =C\int_{-\infty}^{+\infty}(x-\mu)^{2}e^{-\beta(x-\mu)^{2}}dx  \\    &amp;=2C\int_{0}^{+\infty}y^{2}e^{-\beta  y^{2}}dx = \frac{C}{\beta\sqrt{\beta}} \int_{0}^{+\infty} z^{\frac{1}{2}}e^{-z}dz \\    &amp;= \frac{C}{\beta \sqrt{\beta}} \Gamma \left( \frac{3}{2}\right) = \sqrt{\frac{\beta}{\pi}} \cdot \frac{1}{\beta \sqrt{\beta}}\cdot \frac{\sqrt{\pi}}{2}= \frac{1}{2\beta}\end{split}\]</span></p><p>  从而得：<span class="math inline">\(\beta =\frac{1}{2\sigma^{2}}\)</span>，联立：</p><p><span class="math display">\[\left \{\begin{array}{l}p^{*}(x)=Ce^{-\beta(x-\mu)^{2}}  \\C=\sqrt{\frac{\beta}{\pi}}  \\\beta = \frac{1}{2\sigma^{2}} \\\end{array} \right.\]</span></p><p>  解得：</p><p><span class="math display">\[p^{*}(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}}\]</span></p><p>  即当<span class="math inline">\(X \simN(\mu,\sigma^{2})\)</span>时，熵<span class="math inline">\(H(X)\)</span>最大，证毕.</p><h3 id="二-最小化交叉熵等价于极大似然估计">(二)最小化交叉熵等价于极大似然估计</h3><p><strong>结论：</strong> 假设我们从训练数据集<span class="math inline">\(T_{train}\)</span>中所得到的数据的经验分布为<span class="math inline">\(P_{t}(x)=\frac{1}{N}\)</span>(最大熵思想)，<span class="math inline">\(N\)</span>为训练数据的样本容量，我们所需要学习的模型为<span class="math inline">\(P_{m}(x;\theta)\)</span>，则有：</p><p><span class="math display">\[\min_{\theta}H(P_{t}(x),P_{m}(x;\theta)) \Leftrightarrow \max_{\theta} \prod_{x \inT} P_{m}(x;\theta)\]</span></p><p><strong>证明:</strong></p><p><span class="math display">\[\begin{split}    \max_{\theta} \prod_{x \in T} P_{m}(x;\theta) &amp; \Leftrightarrow\max_{\theta} \sum_{x \in T} \log{P_{m}(x;\theta)} \Leftrightarrow-\min_{\theta} \sum_{x \in T} \log{P_{m}(x;\theta)} \\    &amp; \Leftrightarrow -\min_{\theta} \sum_{i=1}^{N}\frac{1}{N}\log{P_{m}(x;\theta)} \Leftrightarrow \min_{\theta}\mathbb{E}_{X \sim P_{t}}[-\log{P_{m}(x;\theta)}]  \\    &amp; \Leftrightarrow \min_{\theta} H(P_{t}(x),P_{m}(x;\theta))\end{split}\]</span></p><p>  证毕.</p><h2 id="参考">参考</h2><p><strong>[1] Book:董平,机器学习中的统计思维(Python实现)</strong><br><strong>[2]Blog：知乎,康斯坦丁,一篇文章讲清楚交叉熵和KL散度</strong></p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>深度学习-2-Paper:生成对抗网络GANs——深度学习二十年间最酷的idea!</title>
    <link href="/2023/11/06/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-2-Paper-%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9CGANs%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8D%81%E5%B9%B4%E9%97%B4%E6%9C%80%E9%85%B7%E7%9A%84idea/"/>
    <url>/2023/11/06/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-2-Paper-%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9CGANs%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8D%81%E5%B9%B4%E9%97%B4%E6%9C%80%E9%85%B7%E7%9A%84idea/</url>
    
    <content type="html"><![CDATA[<h1 id="generative-adversarial-nets">Generative Adversarial Nets</h1><p>  生成对抗网络(GANs)是一种深度学习框架，由 lan Goodfellow和他的同事们于2014年提出，论文成果发表于人工智能顶会NIPS(NeuralInformation ProcessingSystems)。GANs被认为是深度学习领域的一项重大突破，其应用涵盖图像生成、图像修复、语音和文本合成、风格迁移和艺术创作、欺骗检测等多个领域。图灵奖得主，被誉为深度学习三巨头之一的Yann LeCunn 称赞GANs为 "the coolest idea in deep learning in the last 20years."。接下来，我们就来一起欣赏一下深度学习这二十年间最酷的idea！</p><p>  论文链接：https://arxiv.org/abs/1406.2661</p><h2 id="摘要">摘要</h2><p>  图1是 lan Goodfellow 在GANs的原始论文中所写的摘要全文：</p><center><img src="https://s2.loli.net/2023/11/06/fzbnJkFKo4cqOug.png" width="80%" height="80%"><div data-align="center">Image1: The Abstract of GANs</div></center><p>  摘要显示这篇论文提出了一种全新的基于对抗过程的生成模型框架。在这个框架中，存在着两个基于神经网络的模型：生成模型<span class="math inline">\(G\)</span>与判别模型<span class="math inline">\(D\)</span>。生成模型<span class="math inline">\(G\)</span>的作用是估计数据的真实分布；判别模型<span class="math inline">\(D\)</span>是用于判断所输入的样本来自于真实数据而非<span class="math inline">\(G\)</span>所生成的概率。生成模型<span class="math inline">\(G\)</span>的训练过程是最大化判别模型<span class="math inline">\(D\)</span>犯错的概率；判别模型<span class="math inline">\(D\)</span>的训练过程是最小化<span class="math inline">\(D\)</span>犯错的概率，整个GANs的训练过程可视为Minimax的二元博弈过程。通过理论分析发现，存在一个唯一的最优解，使得生成模型<span class="math inline">\(G\)</span>能够正确模拟训练数据的分布，同时判别模型所给出的概率几乎处处为<span class="math inline">\(\frac{1}{2}\)</span>，即几乎不能分辨所输入的样本是来自于真实数据分布还是生成模型<span class="math inline">\(G\)</span>。</p><h2 id="背景及主要思想">背景及主要思想</h2><h3 id="背景">背景</h3><p>  深度学习的目标是构建模型来表示在人工智能的应用中遇到的数据的概率分布，例如图像、音频、自然语言的语料等。在2014年之前，判别模型在这方面占据着主导，这些模型通常是利用反向传播算法、Dropout、ReLU等技术，直接学习一个从高维特征空间到实例类别的映射。与判别模型相比，生成模型的发展则有些相形见绌。这一方面是由于在与最大似然估计相关的策略中，有许多难以解决的概率计算问题；另一方面，判别模型在NLP任务中也难以利用分段线性单元的优势。</p><h3 id="主要思想">主要思想</h3><p>  本文的作者表示他们所提出的新的生成模型能够避开这些困难。生成对抗网络的主要思想可以概括为两个字——“对抗”，具体而言，在GANs框架的训练过程中，生成模型<span class="math inline">\(G\)</span>的训练目标是最大化判别模型犯错的概率，即希望由生成模型<span class="math inline">\(G\)</span>所生成的样本能够成功“骗过”判别模型；判别模型<span class="math inline">\(D\)</span>的训练目标是最小化自身犯错的概率，即希望判别模型的“鉴伪”能力越高越好。当我们将生成模型<span class="math inline">\(G\)</span>所生成的样本用于训练判别模型<span class="math inline">\(D\)</span>，并交替着训练这两个模型，便会引发这两个模型之间的“对抗”，它们为了达到自身的训练目标便会在对抗中提升各自的性能。最终，通过若干次训练，我们能得到一个性能非常好的生成模型<span class="math inline">\(G\)</span>，它所生成的样本与真实样本十分接近，以至于判别模型无法在所给的的参数量下分辨二者的区别，即生成模型所生成的样本几乎能够反映真实的数据分布。<br>  在GANs的论文中，lan Goodfellow用一个十分形象的比喻来说明生成对抗网络的基本思想。假设我们的目标是能够制造足够逼真的假钞，我们只需要找来两个队伍，一方是制造假钞的犯罪集团，另一方是警察队伍，犯罪集团的目标是制造假钞并在不被发现的情况下使用假钞；警察队伍的目标是鉴别假钞。这样，我们并不需要做太多的事情，只需要将这两方放在一起，让他们彼此对抗。在对抗的过程中，警察队伍鉴别假钞的能力会越来越强，犯罪集团所制造的假钞也会越来越逼真，最终我们便能得到足够逼真的假钞。</p><h2 id="模型构成">模型构成</h2><p>  在论文中，为了让生成模型<span class="math inline">\(G\)</span>能够学习到真实数据(训练数据)的分布<span class="math inline">\(p_{data}\)</span>，作者首先定义了输入噪声变量的先验分布<span class="math inline">\(p_{z}(z)\)</span>；生成模型<span class="math inline">\(G(z;\theta_{g})\)</span>由参数为<span class="math inline">\(\theta_{g}\)</span>的神经网络定义，其作用是将噪声变量<span class="math inline">\(z\)</span>映射到数据空间<span class="math inline">\(\mathcal{X}\)</span>:</p><p><span class="math display">\[G(z;\theta_{g}): z \rightarrowx\]</span></p><p>  由生成模型<span class="math inline">\(G(z;\theta_{g})\)</span>生成的数据<span class="math inline">\(x\)</span>的概率分布为<span class="math inline">\(p_{g}\)</span>。同时，作者定义了判别模型<span class="math inline">\(D(x;\theta_{d})\)</span>，判别模型<span class="math inline">\(D(x;\theta_{d})\)</span>由参数为<span class="math inline">\(\theta_{d}\)</span>的神经网络构成，其作用是给出输入数据<span class="math inline">\(x\)</span>是来自于真实数据分布<span class="math inline">\(p_{data}\)</span>而非生成模型<span class="math inline">\(G\)</span>的概率<span class="math inline">\(p\)</span>：</p><p><span class="math display">\[D(x;\theta_{d}): x \rightarrowp\]</span></p><p>  根据GANs的基本思想，在模型的训练过程中，生成模型<span class="math inline">\(G\)</span>的训练目标是最大化判别模型<span class="math inline">\(D\)</span>犯错的概率；判别模型<span class="math inline">\(D\)</span>的训练目标是最小化自身犯错的概率。其训练目标构成了一个minimax 的博弈过程。作者定义了训练的目标函数<span class="math inline">\(V(D,G)\)</span>，训练目标可以写为(1)式：</p><p><span class="math display">\[\begin{equation}\min_{G} \max_{D} V(D,G)=\mathbb{E}_{x \simp_{data}(x)}[\log{D(x)}]+\mathbb{E}_{z \sim p_{z}(z)}[\log{(1-D(G(z)))}]\end{equation}\]</span></p><p>  从目标函数可以得出，若固定<span class="math inline">\(G\)</span>，则判别模型<span class="math inline">\(D\)</span>为(2)式：</p><p><span class="math display">\[\begin{equation}    \begin{split}        D = \arg \max_{D} V(D,G) &amp;= \mathbb{E}_{x \simp_{data}(x)}[\log{D(x)}]+\mathbb{E}_{z \sim p_{z}(z)}[\log{(1-D(G(z)))}]\\        &amp;= \mathbb{E}_{x \sim p_{data}(x)}[\log{D(x)}]+\mathbb{E}_{x\sim p_{g}(x)}[\log{(1-D(x))}] \\    \end{split}\end{equation}\]</span></p><p>  根据(2)式，在训练过程中判别模型<span class="math inline">\(D\)</span>会调整参数<span class="math inline">\(\theta_{d}\)</span>，使得(2)式中的<span class="math inline">\(D(x)\)</span>较大，<span class="math inline">\(D(G(z))\)</span>较小，其含义是若判别模型<span class="math inline">\(D\)</span>的输入数据<span class="math inline">\(x\)</span>来自于真实的数据分布<span class="math inline">\(p_{data}\)</span>，则模型的输出概率值较大；若输入数据<span class="math inline">\(x\)</span>来自于生成模型定义的分布<span class="math inline">\(p_{g}\)</span>，则模型的输出概率值较小。(2)式实际上就是二分类问题中的交叉熵目标函数，通过(2)式的优化，可以得到一个分类性能更好的判别模型<span class="math inline">\(D\)</span>。<br>  若固定<span class="math inline">\(D\)</span>，则生成模型<span class="math inline">\(G\)</span>为(3)式：</p><p><span class="math display">\[\begin{equation}    \begin{split}        G &amp;= \arg \min_{G} V(D,G) = \mathbb{E}_{x \simp_{data}(x)}[\log{D(x)}]+\mathbb{E}_{z \sim p_{z}(z)}[\log{(1-D(G(z)))}]\\        &amp; \Leftrightarrow \arg \min_{G} \mathbb{E}_{z \simp_{z}(z)}[\log{(1-D(G(z)))}]=\mathbb{E}_{x \simp_{g}(x)}[\log{(1-D(x))}] \\    \end{split}\end{equation}\]</span></p><p>  根据(3)式，在训练过程中生成模型<span class="math inline">\(G\)</span>会调整参数<span class="math inline">\(\theta_{g}\)</span>，使得(3)式中的<span class="math inline">\(D(G(z))\)</span>较大，其含义是将从生成模型<span class="math inline">\(G\)</span>所定义的分布<span class="math inline">\(p_{g}\)</span>中给出的数据<span class="math inline">\(x\)</span>输入到判别模型<span class="math inline">\(D\)</span>中，判别模型输出的概率值较大，即判别模型误认为数据<span class="math inline">\(x\)</span>来自于真实的数据分布<span class="math inline">\(p_{data}\)</span>，这表明生成数据分布<span class="math inline">\(p_{g}\)</span>与真实数据分布<span class="math inline">\(p_{data}\)</span>足够相似，以至于当前的判别模型<span class="math inline">\(D\)</span>无法分辨这两个分布所产生的数据<span class="math inline">\(x\)</span>。</p><h2 id="优化算法">优化算法</h2><p>  lan Goodfellow 在GANs的原始论文中给出的目标函数的优化算法如下</p><center><img src="https://s2.loli.net/2023/11/07/TNJ4mGcanZewflz.png" width="80%" height="80%"><div data-align="center">Image2: 优化算法</div></center><p>  优化算法的基本思路是利用小批量随机梯度下降算法对目标函数进行优化。对于目标函数中的生成模型与判别模型，每次迭代时固定其中一个模型，利用SGD对另一个模型进行参数更新，彼此循环迭代，直至收敛。优化算法中需要特别注意的有以下几点：</p><ul><li>在刚开始迭代时，应该首先固定生成模型<span class="math inline">\(G\)</span>，对判别模型<span class="math inline">\(D\)</span>进行更新。这是因为在开始训练时，判别模型<span class="math inline">\(D\)</span>的参数是随机初始化的，其不具备对样本进行正确分类的能力，而判别模型分类的结果又会直接影响生成模型<span class="math inline">\(G\)</span>的训练，若首先更新生成模型<span class="math inline">\(G\)</span>，则生成模型一开始便有可能完全“骗过”判别模型，导致训练无法成功进行。<br></li><li>在训练过程中，每更新<span class="math inline">\(k\)</span>次判别模型<span class="math inline">\(D\)</span>,再更新1次生成模型<span class="math inline">\(G\)</span>。这会使得只要生成模型<span class="math inline">\(G\)</span>变化得足够缓慢，判别模型<span class="math inline">\(D\)</span>就会维持再其最优解附近。</li><li>在实际对生成模型<span class="math inline">\(G\)</span>的更新中，并不使用(3)式中的最小化 <span class="math inline">\(\log{(1-D(x))}\)</span>，而是最大化 <span class="math inline">\(\log{D(x)}\)</span>。这是因为在开始训练时，判别模型<span class="math inline">\(D\)</span>要强于生成模型<span class="math inline">\(G\)</span>，使得<span class="math inline">\(D(x)(x\sim p_{g})\)</span>的值较小，此时 <span class="math inline">\(\log{(1-D(x))}\)</span> 对<span class="math inline">\(D(x)\)</span>的梯度很小，训练会非常缓慢；而 <span class="math inline">\(\log{D(x)}\)</span> 在<span class="math inline">\(D(x)\)</span>较小时，梯度较大，更有利于参数更新。</li></ul><h2 id="理论分析">理论分析</h2><p>  lanGoodfellow在论文中对目标函数的理论最优解以及算法收敛性进行了分析，得出了非常有意义的结果。</p><h3 id="总体最优解">总体最优解</h3><p>  <strong>结论</strong>: 当<span class="math inline">\(p_{g}=p_{data}\)</span>时，目标函数达到总体最优。<br>  <strong>证明</strong>:<br>  训练目标：</p><p><span class="math display">\[\min_{G} \max_{D} V(D,G)=\mathbb{E}_{x\sim p_{data}(x)}[\log{D(x)}]+\mathbb{E}_{z \simp_{z}(z)}[\log{(1-D(G(z)))}]\]</span></p><p>  当固定生成模型<span class="math inline">\(G\)</span>时，<span class="math inline">\(D^{*}=\arg \max_{D} V(D,G)\)</span>，其中:</p><p><span class="math display">\[\begin{split}    V(D,G) &amp;= \mathbb{E}_{x \sim p_{data}}[\log{D(x)}]+\mathbb{E}_{x\sim p_{g}}[\log{(1-D(x))}] \\    &amp;= \int_{x} p_{data}(x)\log{D(x)} dx + \int_{x}p_{g}(x)\log{(1-D(x))} dx \\    &amp;= \int_{x} [p_{data}(x)\log{D(x)}+p_{g}(x)\log{(1-D(x))}] dx\end{split}\]</span></p><p><span class="math display">\[\begin{split}    \max_{D} V(D,G) &amp;= \max_{D} \int_{x}[p_{data}(x)\log{D(x)}+p_{g}(x)\log{(1-D(x))}] dx \\    &amp; \Leftrightarrow \max_{D} \spacep_{data}(x)\log{D(x)}+p_{g}(x)\log{(1-D(x))} \triangleq L(D)\end{split} \]</span></p><p><span class="math display">\[\frac{\partial L(D)}{\partialD}=\frac{p_{data}(x)}{D(x)}-\frac{p_{g}(x)}{1-D(x)}=0 \RightarrowD^{*}(x)=\frac{p_{data}(x)}{p_{data}(x)+p_{g}(x)}\]</span></p><p><span class="math display">\[\begin{split}    \max_{D} V(D,G) &amp;= V(D^{*},G) = \mathbb{E}_{x \simp_{data}}[\log{D^{*}(x)}]+\mathbb{E}_{x \sim p_{g}}[\log{(1-D^{*}(x))}]\\    &amp;= \int_x p_{data}(x)\log{\left(\frac{p_{data}(x)}{p_{data}(x)+p_{g}(x)} \right)} dx + \int_{x}p_{g}(x)\log{\left( \frac{p_{g}(x)}{p_{data}(x)+p_{g}(x)} \right)}dx \\    &amp;= -2\log{2}+\int_x p_{data}(x)\log{\left(\frac{p_{data}(x)}{(p_{data}(x)+p_{g}(x))/2} \right)} dx + \int_{x}p_{g}(x)\log{\left( \frac{p_{g}(x)}{(p_{data}(x)+p_{g}(x))/2}\right)}dx  \\    &amp;= -2\log{2}+ KL \left( p_{data}(x) ||\frac{p_{data}(x)+p_{g}(x)}{2} \right) + KL \left( p_{g}(x) ||\frac{p_{data}(x)+p_{g}(x)}{2} \right) \\    &amp;= -2\log{2}+2JS(p_{data}(x)||p_{g}(x))\end{split}\]</span></p><p>  完成对<span class="math inline">\(D\)</span>的最大化后，再对<span class="math inline">\(G\)</span>进行极小化：</p><p><span class="math display">\[\min_{G} \max_{D} V(D,G) \Leftrightarrow\min_{G} \space J(G)=-2\log{2}+2JS(p_{data}(x)||p_{g}(x))\]</span></p><p>  当 <span class="math inline">\(p_{g}(x)=p_{data}(x)\)</span>时，<span class="math inline">\(JS(p_{data}(x)||p_{g}(x))_{min}=0\)</span>，<span class="math inline">\(J(G)\)</span>达到最小，证毕。</p><h3 id="算法收敛性">算法收敛性</h3><p><strong>结论:</strong> 若生成模型<span class="math inline">\(G\)</span>和判别模型<span class="math inline">\(D\)</span>拥有足够的参数量，并且在上文优化算法的每一步中，在给定生成模型<span class="math inline">\(G\)</span>下，判别模型<span class="math inline">\(D\)</span>都达到了其最优解，从而有：</p><p><span class="math display">\[p_{g}= \arg\min_{p_{g}} \mathbb{E}_{x\sim p_{data}}[\log{D^{*}_{G}(x)}]+\mathbb{E}_{x \simp_{g}}[\log{(1-D^{*}_{G}(x))}]\]</span></p><p>利用上式对<span class="math inline">\(p_{g}\)</span>进行迭代更新，最终<span class="math inline">\(p_{g}\)</span>会收敛到<span class="math inline">\(p_{data}\)</span>.</p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>机器学习-3.硬间隔线性支持向量机</title>
    <link href="/2023/10/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-3-%E7%A1%AC%E9%97%B4%E9%9A%94%E7%BA%BF%E6%80%A7%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    <url>/2023/10/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-3-%E7%A1%AC%E9%97%B4%E9%9A%94%E7%BA%BF%E6%80%A7%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</url>
    
    <content type="html"><![CDATA[<h1 id="硬间隔线性支持向量机">硬间隔线性支持向量机</h1><p>  支持向量机(Support Vector Machine, SVM)是 <span class="math inline">\(Cortes\)</span>和<span class="math inline">\(Vapnik\)</span>于1995年在 <span class="math inline">\(Machine \space Learning\)</span>期刊上提出的分类模型，在自然语言处理、计算机视觉以及生物信息中有着重要的应用。支持向量机的模型形式与感知机相同，但有别于感知机模型的是，支持向量机学习通过添加约束条件，能够得到最优的分离超平面。支持向量机学习的目标是找到一个线性超平面，能够最大程度地分离训练数据集中不同类别的数据点，并且在分离过程中最大化间隔，即数据点与决策边界之间的距离最大化。<br>  根据处理的问题不同，支持向量机可大致分为硬间隔线性支持向量机、软间隔线性支持向量机，非线性支持向量机。本节主要介绍二分类问题下的硬间隔线性支持向量机。</p><h2 id="基本思想">基本思想</h2><p>  我们在之前已经介绍了感知机模型，它的模型形式是：</p><p><span class="math display">\[f(x) = sign(w^Tx+b)\]</span></p><p>  其中，<span class="math inline">\(x \in \mathbb{R}^n; w \in\mathbb{R}^n, b \in \mathbb{R}\)</span>为模型参数。通过设置初始参数，利用梯度下降算法，我们可以得到模型参数<span class="math inline">\(\hat{w},\hat{b}\)</span>.当我们设置不同的初始参数时，我们会得到不同的模型参数，即不同的分离超平面。现假设在二维情形下，我们得到了如图1所示的两个分离超平面<span class="math inline">\(S_1,S_2\)</span>。</p><center><img src="https://s2.loli.net/2023/10/24/Xe3R6PnIalTABcb.jpg" width="60%" height="60%"><div data-align="center">Image1: 感知机中分离超平面</div></center><p>  在感知机学习中，超平面<span class="math inline">\(S_1\)</span>与<span class="math inline">\(S_2\)</span>均可以将训练数据集完全正确分类，这两个分离超平面并没有优劣可分，实际上，感知机学习得到的分离超平面为无数个。<strong>现在的问题是：感知机学习得到的众多分离超平面中，哪一个分离超平面是最优的</strong>？为了回答这个问题，我们先来思考一下图1中分离超平面<span class="math inline">\(S_1\)</span>与<span class="math inline">\(S_2\)</span>哪一个更好。相信大多数读者在直觉上都会认为分离超平面<span class="math inline">\(S_2\)</span>要优于<span class="math inline">\(S_1\)</span>，这个结论是正确的。通过进一步地分析，我们可以发现，训练数据集中的实例点到分离超平面<span class="math inline">\(S_1\)</span>的最小距离要大于其到分离超平面<span class="math inline">\(S_2\)</span>的最小距离，这使得分离超平面<span class="math inline">\(S_1\)</span>的泛化能力要弱于分离超平面<span class="math inline">\(S_2\)</span>，一些轻微的噪声扰动便有可能使得实例点越过分离超平面<span class="math inline">\(S_1\)</span>，造成模型分类错误，如图1中的红色实例点所示，然而由于实例点距离分离超平面<span class="math inline">\(S_2\)</span>的距离相对较远，噪声扰动并不容易使得实例点越过超平面<span class="math inline">\(S_2\)</span>。因此，我们认为分离超平面<span class="math inline">\(S_2\)</span>要优于分离超平面<span class="math inline">\(S_1\)</span>。<br>  硬间隔支持向量机的基本思想便来源于以上的思考，其基本思想为：<strong>当训练数据集线性可分时，在特征空间中寻找一个超平面，其能够将训练数据中的实例点完全正确分类，同时最大程度远离训练数据中的示例点。</strong></p><h2 id="模型">模型</h2><p><strong>输入</strong></p><ul><li><strong>输入空间:</strong> <span class="math inline">\(\mathcal{X}\in \mathbb{R}^n\)</span>.<br></li><li><strong>输入实例:</strong> <span class="math inline">\(x =\begin{bmatrix}  x^{(1)},x^{(2)},\dots,x^{(n)} \\ \end{bmatrix}^T \in\mathcal{X}\)</span>.</li></ul><p>  其中，输入空间<span class="math inline">\(\mathcal{X}\)</span>为<span class="math inline">\(n\)</span>维实数空间的子集，输入实例<span class="math inline">\(x\)</span>为<span class="math inline">\(n\)</span>维特征向量。</p><p><strong>输出</strong><br>  由于我们只考虑二分类问题，因此实例点的类别只有正类与负类两种。</p><ul><li><strong>输出空间:</strong> <span class="math inline">\(\mathcal{Y} =\{ -1,+1 \}\)</span><br></li><li><strong>输出实例:</strong> <span class="math inline">\(y \in\mathcal{Y}\)</span>.</li></ul><p>  其中，输出空间<span class="math inline">\(\mathcal{Y}\)</span>为只包含+1，-1两个元素的集合，+1与-1分别代表二分类问题中的正类与负类。输出实例<span class="math inline">\(y\)</span>代表相对应的输出实例<span class="math inline">\(x\)</span>的类别。</p><p><strong>数据集</strong><br>  支持向量机的训练数据集<span class="math inline">\(T_{train}\)</span>为：</p><p><span class="math display">\[T_{train}=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N) \}\]</span></p><p>  其中，<span class="math inline">\(x_i \in\mathcal{X}=\mathbb{R}^{n},y_i \in\mathcal{Y}=\{+1,-1\},i=1,2,\dots,N\)</span>，<span class="math inline">\(N\)</span>表示训练数据的样本容量。<strong>需要注意的是，硬间隔线性支持向量机模型的前提假设为训练数据集<span class="math inline">\(T_{train}\)</span>是线性可分的</strong>。</p><p><strong>模型</strong><br>  支持向量机的模型形式与感知机相同，其模型形式为：</p><p><span class="math display">\[f(x) = sign(w^{T}x+b)\]</span></p><p>  其中，<span class="math inline">\(w\)</span>和<span class="math inline">\(b\)</span>为感知机模型的参数，<span class="math inline">\(w \in \mathbb{R}^{n}\)</span>为权重向量，<span class="math inline">\(b \in \mathbb{R}\)</span>为偏置。<span class="math inline">\(sign(\cdot)\)</span>为符号函数。硬间隔线性支持向量机为线性分类模型，属于判别模型。</p><p><strong>假设空间</strong><br>  硬间隔线性支持向量机模型的假设空间为：</p><p><span class="math display">\[\mathcal{H} = \{f \vert f(x)=w^{T}x+b\}\]</span></p><p>  与感知机模型相同，硬间隔线性支持向量机模型的假设空间实际上是特征空间中所有超平面的集合。</p><p><strong>参数空间</strong><br>  令<span class="math inline">\(\theta=(w,b)\)</span>，则模型的假设空间<span class="math inline">\(\mathcal{H}\)</span>等价于参数空间<span class="math inline">\(\Theta\)</span>：</p><p><span class="math display">\[\Theta=\{ \theta \vert \theta \in\mathbb{R}^{n+1} \}\]</span></p><h2 id="策略">策略</h2><p>  我们的目标是找到所有分离超平面中最优的一个超平面。首先，假设分离超平面为<span class="math inline">\(S=\{x \vert w^T x + b =0\}\)</span>，则实例点<span class="math inline">\(x_i\)</span>到超平面<span class="math inline">\(S\)</span>的距离为：</p><p><span class="math display">\[d(S,x_i)=\frac{|w^Tx_i+b|}{||w||}\]</span></p><p>  由于硬间隔线性支持向量机假设训练数据集是线性可分的，因此分离超平面<span class="math inline">\(S\)</span>能够将实例点完全正确分类。因此，当 <span class="math inline">\(w^Tx_i+b &gt; 0\)</span>时，有<span class="math inline">\(y_i=+1\)</span>；当<span class="math inline">\(w^Tx_i+b&lt;0\)</span>时，有<span class="math inline">\(y_i=-1\)</span>。则<span class="math inline">\(d(S,x_i)\)</span>同样可以表示为：</p><p><span class="math display">\[\gamma_i=\frac{y_i(w^Tx_i+b)}{||w||}\]</span></p><p>  我们称<span class="math inline">\(\gamma_i\)</span>为实例点<span class="math inline">\(x_i\)</span>到超平面<span class="math inline">\(S\)</span>的<strong>几何间隔</strong>。<br>  我们希望能够找到与训练数据集中的实例点距离最大的超平面，记 <span class="math inline">\(margin(T,S)\)</span> 表示数据集<span class="math inline">\(T\)</span>中的实例点到超平面<span class="math inline">\(S\)</span>的最小距离：</p><p><span class="math display">\[margin(T_{train},S)=\min_{i=1,\dots,N}d(S,x_i)=\min_{i=1,\dots,N} \gamma_i=\min_{i=1,\dots,N}\frac{y_i(w^Tx_i+b)}{||w||}\]</span></p><p>  我们使用 <span class="math inline">\(margin(T_{train},S)\)</span>来衡量训练数据集到超平面的距离，这种距离也称为<span class="math inline">\(hard-margin\)</span>，即硬间隔。由于训练数据集<span class="math inline">\(T_{train}\)</span>是给定的，因此 <span class="math inline">\(margin(T_{train},S)\)</span>实际上是由超平面的参数<span class="math inline">\(w,b\)</span>决定的，不同的分离超平面对应着不同的<span class="math inline">\(margin\)</span>距离，我们希望能够在所有分离超平面中找到与训练数据集距离最远的超平面，这个问题可以被描述为：</p><p><span class="math display">\[\max_{w,b}margin(T_{train},S)=\max_{w,b} \min_{i=1,\dots,N}\frac{y_i(w^Tx_i+b)}{||w||}\]</span></p><p>  综上所述，寻找最优分离超平面的优化问题可以描述为<strong>最小几何间隔最大化问题(1)</strong>：</p><p><span class="math display">\[\begin{equation}    \begin{split}    &amp; \max_{w,b} \min_{i=1,2,\dots,N} \quad\frac{1}{||w||}y_i(w^Tx_i+b) \\    &amp; \space s.t. \quad y_i(w^Tx_i+b) &gt; 0, \quad i=1,2,\dots,N\\    \end{split}\end{equation}\]</span></p><p>  其中称<span class="math inline">\(y_i(w^Tx_i+b)\)</span>为实例点<span class="math inline">\(x_i\)</span>到超平面<span class="math inline">\(S\)</span>的<strong>函数间隔</strong>；约束条件<span class="math inline">\(y_i(w^Tx_i+b) &gt;0\)</span>保证了该优化问题的可行解集为能够将训练数据集正确分类的超平面的集合。确定最优分离超平面需要找到最小几何间隔对应的实例点，这些实例点被称为<strong>支持向量</strong>。<br>  下面我们来对该优化问题做一些简化，通过分析我们可以得到：</p><p><span class="math display">\[\exists r &gt; 0, \space s.t. \space\min_{i=1,\dots,N} y_i(w^Tx_i+b)=r\]</span></p><p>  由于我们没有对<span class="math inline">\(w\)</span>的长度进行限制，则同一个超平面可能对应这不同的参数值<span class="math inline">\(w,b\)</span>，例如超平面<span class="math inline">\(S_1=\{x \vert w^Tx+b=0\}\)</span>与超平面<span class="math inline">\(S_2=\{x \vert(2w)^Tx+(2b)=0\}\)</span>在特征空间中表示同一个超平面。这样会造成，即使是同一个分离超平面，不同的参数值<span class="math inline">\(w,b\)</span>会对应不同的<span class="math inline">\(r\)</span>值。我们想要优化问题能够得到唯一一组确定的参数值<span class="math inline">\(w,b\)</span>，我们可以给定<span class="math inline">\(w\)</span>的模长，也可以固定<span class="math inline">\(r\)</span>值，为了简化优化问题，我们固定<span class="math inline">\(r=1\)</span>，此时优化问题(1)的约束条件可以转化为：</p><p><span class="math display">\[\min_{i=1,\dots,N} y_i(w^Tx_i+b)=1\Leftrightarrow y_i(w^Tx_i+b) \ge 1, i=1,2,\dots,N\]</span></p><p>  优化问题(1)中的目标函数可以转化为：</p><p><span class="math display">\[\max_{w,b} \min_{i=1,\dots,N}\frac{1}{||w||}y_i(w^Tx_i+b)=\max_{w,b}\frac{1}{||w||}\min_{i=1,\dots,N} y_i(w^Tx_i+b)=\max_{w,b} \frac{1}{||w||}\]</span></p><p>  再简最大化问题转化为常见的最小化问题：</p><p><span class="math display">\[\max_{w,b} \frac{1}{||w||}\Leftrightarrow \min_{w,b} ||w|| \Leftrightarrow \min_{w,b}\frac{1}{2}w^Tw\]</span></p><p>  综上所述，优化问题(1)可以转化为以下的优化问题(2)：</p><p><span class="math display">\[\begin{equation}    \begin{split}        &amp; \min_{w,b} \quad \frac{1}{2}w^Tw  \\        &amp; \space s.t. \quad 1-y_i(w^Tx_i+b) \leq 0, \quadi=1,2,\dots,N    \end{split}\end{equation}\]</span></p><p>  通过求解优化问题(2)，我们便可以得到最优的分离超平面参数<span class="math inline">\((w^{*},b^{*})\)</span>。可以证明在训练数据集线性可分的条件下，通过最小间隔最大化问题的求解，最优的分离超平面是唯一的。这部分证明将放在附录，有兴趣的读者可自行阅读。</p><center><img src="https://s2.loli.net/2023/10/25/DCZeJ5TOM93vtLS.jpg" width="60%" height="60%"><div data-align="center">Image2: 最优分离超平面</div></center><h2 id="算法">算法</h2><p>  通过前文的推导，我们得到的优化问题为问题(2)：</p><p><span class="math display">\[\begin{align*}        &amp; \min_{w,b} \frac{1}{2}w^Tw  \\        &amp; \space s.t. \space 1-y_i(w^Tx_i+b) \leq 0, \quadi=1,2,\dots,N \\    \end{align*}\]</span></p><p>  该优化问题为<strong>凸二次规划问题</strong>，有<span class="math inline">\(N\)</span>个约束条件。首先写出优化问题(2)的拉格朗日函数：</p><p><span class="math display">\[L(w,b,\lambda)=\frac{1}{2}w^Tw+\sum_{i=1}^{N}\lambda_i\left[ 1-y_i(w^Tx_i+b) \right]\]</span></p><p><span class="math display">\[\lambda = \begin{bmatrix}    \lambda_1,\lambda_2,\dots,\lambda_N\end{bmatrix}^T,\lambda_i \ge 0, i=1,2,\dots,N\]</span></p><p>  则优化问题(2)的无约束形式(3)为：</p><p><span class="math display">\[\begin{equation}    \begin{split}        &amp; \min_{w,b} \max_{\lambda} \quad L(w,b,\lambda)  \\        &amp; \space s.t. \quad \lambda_i \ge 0, \quad i=1,2,\dots,N \\    \end{split}\end{equation}\]</span></p><p>  则优化问题(2)的<strong>对偶问题</strong>(4)为：</p><p><span class="math display">\[\begin{equation}    \begin{split}        &amp; \max_{\lambda} \min_{w,b} \quad L(w,b,\lambda)  \\        &amp; \space s.t. \quad \lambda_i \ge 0, \quad i=1,2,\dots,N    \end{split}\end{equation}\]</span></p><p>  因为原始问题(2)为凸二次规划问题，其满足<strong>强对偶关系</strong>，即：</p><p><span class="math display">\[\min_{w,b} \max_{\lambda}L(w,b,\lambda)=\max_{\lambda} \max_{w,b} L(w,b,\lambda)\]</span></p><p>  首先来解决内部极大化问题：</p><p><span class="math display">\[\max_{w,b}L(w,b,\lambda)=\frac{1}{2}w^Tw+\sum_{i=1}^{N}\lambda_i \left[1-y_i(w^Tx_i+b) \right]\]</span></p><p>  由费马定理可得：</p><p><span class="math display">\[\left \{\begin{array}{lr}    \frac{\partial L(w,b,\lambda)}{\partial w}=0  \Rightarrow w =\sum_{i=1}^{N}\lambda_{i}y_ix_i  \\    \frac{\partial L(w,b,\lambda)}{\partial b}=0  \Rightarrow\sum_{i=1}^{N}\lambda_iy_i=0\end{array} \right.\]</span></p><p>  将其代入拉格朗日函数<span class="math inline">\(L(w,b,\lambda)\)</span>得：</p><p><span class="math display">\[\begin{align*}    L(w,b,\lambda) &amp;= \frac{1}{2} \left(\sum_{i=1}^{N}\lambda_{i}y_ix_i \right)^{T}\left(\sum_{i=1}^{N}\lambda_{i}y_ix_i \right)-\sum_{i=1}^{N}\lambda_iy_i\left( \sum_{i=1}^{N}\lambda_{i}y_ix_i\right)^{T}x_i-b\sum_{i=1}^{N}\lambda_iy_i+\sum_{i=1}^{N}\lambda_i  \\    &amp;=\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\lambda_i \lambda_jy_iy_j (x_i^{T}x_j)-\sum_{i=1}^{N}\sum_{j=1}^{N}\lambda_i \lambda_jy_iy_j (x_i^{T}x_j)+\sum_{i=1}^{N}\lambda_i  \\    &amp;= -\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\lambda_i \lambda_jy_iy_j (x_i^{T}x_j)+\sum_{i=1}^{N}\lambda_i  \\\end{align*}\]</span></p><p>  则对偶问题(4)可以化为以下优化问题(5)：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{\lambda} \quad &amp;\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\lambda_i \lambda_j y_iy_j(x_i^{T}x_j)-\sum_{i=1}^{N}\lambda_i \\        s.t. \quad &amp; \lambda_i \ge 0, \quad i=1,2,\space,N  \\        &amp; \sum_{i=1}^{N}\lambda_i y_i=0    \end{split}\end{equation}\]</span></p><p>  记 <span class="math inline">\(w^{*},b^{*}\)</span>为原始问题(2)的最优解，<span class="math inline">\(\lambda^{*}\)</span>为对偶问题(4)的最优解，其同时也是优化问题(5)的最优解。根据定理：<strong>原始问题及其对偶问题具备强对偶关系是原始问题与其对偶问题满足KKT条件的充要条件可知</strong>：</p><p><span class="math display">\[KKT条件:  \left \{\begin{array}{l}1-y_i(w^{*} \cdot x_i+b^{*}) \leq 0  \\\\\lambda_i^{*}(1-y_i(w^{*} \cdot x_i+b^{*}))=0 \\\\\frac{\partial L(w,b,\lambda^{*})}{\partial w}|_{w=w^{*}}=0  \Rightarrow w^{*}=\sum_{i=1}^{N}\lambda^{*}_{i}y_ix_i \\\\\frac{\partial L(w,b,\lambda^{*})}{\partial b} |_{b=b^{*}}=0 \Rightarrow\sum_{i=1}^{N}\lambda^{*}_{i}y_i=0 \\\\\lambda^{*} \ge 0 \\  \end{array} \right.\]</span></p><p>  其中，<span class="math inline">\(i=1,2,\dots,N\)</span>.以上关于凸二次规划、对偶关系、KKT条件等最优化知识会在附录中介绍，这里不多家阐述。<br>  由<span class="math inline">\(KKT\)</span>条件得到 <span class="math inline">\(w^{*}=\sum_{i=1}^{N}\lambda^{*}_{i}y_ix_i\)</span>，现在来思考<span class="math inline">\(b^{*}\)</span>如何用<span class="math inline">\(\lambda^{*}\)</span>来表示。由于 <span class="math inline">\(\lambda_i^{*}(1-y_i(w^{*} \cdotx_i+b^{*}))=0\)</span>，若 <span class="math inline">\(y_i(w^{*} \cdotx_i+b^{*}) &gt; 1\)</span>，则 <span class="math inline">\(\lambda_i^{*}=0\)</span>，即在图2中位于间隔边界<span class="math inline">\(H_1,H_2\)</span>两侧的实例点<span class="math inline">\(x_i\)</span>对确定最优超平面<span class="math inline">\(S\)</span>的参数<span class="math inline">\(w^{*},b^{*}\)</span>不起作用；若 <span class="math inline">\(y_i(w^{*} \cdot x_i+b^{*}) = 1\)</span>，则<span class="math inline">\(\lambda_i^{*}\)</span>不一定为0，即在图2中位于间隔边界上的实例点<span class="math inline">\(x_i\)</span>，也就是支持向量，对确定最优超平面<span class="math inline">\(S\)</span>的参数<span class="math inline">\(w^{*},b^{*}\)</span>起作用。此时可以解得：</p><p><span class="math display">\[b^{*}=y_j-\sum_{i=1}^{N}\lambda_i^{*}y_i(x_i^{T}\cdot x_j)\]</span></p><p>  其中实例点<span class="math inline">\(x_j\)</span>为某一个支持向量，由于<span class="math inline">\(b^{*}\)</span>的值是固定的，因此代入不同的支持向量，得到的<span class="math inline">\(b^{*}\)</span>的值是相同的，在实际计算中，我们一般在求解优化问题(5)得到的<span class="math inline">\(\lambda^{*}\)</span>中选择一个正值分量<span class="math inline">\(\lambda_j &gt;0\)</span>，使用其所对应的支持向量<span class="math inline">\(x_j\)</span>来求解<span class="math inline">\(b^{*}\)</span>。</p><h3 id="硬间隔支持向量机算法">硬间隔支持向量机算法</h3><p>  输入：训练数据集<span class="math inline">\(T_{train}=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}\)</span>，其中<span class="math inline">\(x_i \in \mathcal{X}=\mathbb{R}^{n}，y \in\mathcal{Y}=\{-1,+1\},i=1,2,\dots,N\)</span>.<br>  输出：最优分离超平面的参数<span class="math inline">\(w^{*},b^{*}\)</span>，以及相应的分类模型<span class="math inline">\(f(x)=sign(w^{*} \cdot x+b^{*})\)</span>.<br>  (1) 基于训练数据集<span class="math inline">\(T_{train}\)</span>构造凸二次规划问题(6)：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{\lambda} \quad &amp;\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\lambda_i \lambda_j y_iy_j(x_i^{T}x_j)-\sum_{i=1}^{N}\lambda_i \\        s.t. \quad &amp; \lambda_i \ge 0, \quad i=1,2,\space,N  \\        &amp; \sum_{i=1}^{N}\lambda_i y_i=0    \end{split}\end{equation}\]</span></p><p>  解得该优化问题的最优解为：<span class="math inline">\(\lambda^{*}=\begin{bmatrix}  \lambda_1^{*},\lambda_2^{*},\dots,\lambda_N^{*}\\ \end{bmatrix}^{T}\)</span></p><p>  (2) 在最优解<span class="math inline">\(\lambda^{*}\)</span>中选择一个正值分量<span class="math inline">\(\lambda_j &gt; 0\)</span>，取出其下标<span class="math inline">\(j\)</span>对应的数据<span class="math inline">\((x_j,y_j)\)</span>，基于<span class="math inline">\(KKT\)</span>条件求解最优超平面的参数<span class="math inline">\(w^{*},b^{*}\)</span>：</p><p><span class="math display">\[w^{*}=\sum_{i=1}^{N}\lambda_i^{*}y_ix_i\quadb^{*}=y_j-\sum_{i=1}^{N}\lambda_i^{*}y_i(x_i^{T} \cdot x_j)\]</span></p><p>  (3) 得到相应的分类模型：</p><p><span class="math display">\[f(x) = sign(w^{*} \cdot x +b^{*})\]</span></p><h2 id="基于python的算法实现">基于Python的算法实现</h2><p>  导入所需要的Python包：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> cvxopt<br><span class="hljs-keyword">from</span> sklearn.utils <span class="hljs-keyword">import</span> shuffle<br><span class="hljs-keyword">from</span> sklearn.svm <span class="hljs-keyword">import</span> LinearSVC<br></code></pre></td></tr></tbody></table></figure><h3 id="基于numpypandascvxopt等的算法实现">基于numpy,pandas,cvxopt等的算法实现</h3><p>  首先我们来生成我们的训练数据集，为了便于进行可视化，我们依然选择在二维情况下构建数据集。由于硬间隔线性支持向量机假设训练数据集是线性可分的，我利用均匀分布在区域<span class="math inline">\(S_{p}=\{ (x_1,x_2) \vert x_1 \in (1,2), x_2 \in(3,4)\}\)</span>产生了30个正类的实例点，在区域<span class="math inline">\(S_{n}= \{ (x_1,x_2) \vert x_1 \in (3,4), x_2 \in(1,2)\}\)</span>产生了20个负类的实例点。以下是数据生成的代码：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># data generation</span><br>np.random.seed(<span class="hljs-number">520</span>)<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">data_generation</span>(<span class="hljs-params">n,basepoint,ylabel</span>):<br>    x1 = np.random.random((n,))+np.array([basepoint[<span class="hljs-number">0</span>]]*n)<br>    x2 = np.random.random((n,))+np.array([basepoint[<span class="hljs-number">1</span>]]*n)<br>    y = np.array([ylabel]*n)<br>    data = pd.DataFrame({<span class="hljs-string">"x1"</span>:x1,<span class="hljs-string">"x2"</span>:x2,<span class="hljs-string">"y"</span>:y})<br>    <span class="hljs-keyword">return</span> data<br><br><span class="hljs-comment"># positive data</span><br>positivedata = data_generation(n=<span class="hljs-number">30</span>,basepoint=(<span class="hljs-number">1</span>,<span class="hljs-number">3</span>),ylabel=<span class="hljs-number">1</span>)<br><span class="hljs-comment"># negative data</span><br>negativedata = data_generation(n=<span class="hljs-number">20</span>,basepoint=(<span class="hljs-number">3</span>,<span class="hljs-number">1</span>),ylabel=-<span class="hljs-number">1</span>)<br><span class="hljs-comment"># train data</span><br>train_data = pd.concat([positivedata,negativedata])<br>train_data = shuffle(train_data)<br>train_data.index = <span class="hljs-built_in">range</span>(train_data.shape[<span class="hljs-number">0</span>])<br>train_data.head(<span class="hljs-number">5</span>)<br></code></pre></td></tr></tbody></table></figure><p>  这50个实例点构成训练数据集，使用以下代码画出训练数据的散点图:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># train data scatter plot</span><br>plt.scatter(x=positivedata[<span class="hljs-string">"x1"</span>],y=positivedata[<span class="hljs-string">"x2"</span>],marker=<span class="hljs-string">"o"</span>,c=<span class="hljs-string">"green"</span>,label=<span class="hljs-string">"positive data"</span>)<br>plt.scatter(x=negativedata[<span class="hljs-string">"x1"</span>],y=negativedata[<span class="hljs-string">"x2"</span>],marker=<span class="hljs-string">"x"</span>,c=<span class="hljs-string">"red"</span>,label=<span class="hljs-string">"negative data"</span>)<br>plt.xlabel(<span class="hljs-string">"x1"</span>)<br>plt.ylabel(<span class="hljs-string">"x2"</span>)<br>plt.xlim((<span class="hljs-number">0</span>,<span class="hljs-number">5</span>))<br>plt.ylim((<span class="hljs-number">0</span>,<span class="hljs-number">5</span>))<br>plt.xticks(np.arange(<span class="hljs-number">0</span>,<span class="hljs-number">5</span>,<span class="hljs-number">0.5</span>))<br>plt.yticks(np.arange(<span class="hljs-number">0</span>,<span class="hljs-number">5</span>,<span class="hljs-number">0.5</span>))<br>plt.legend()<br></code></pre></td></tr></tbody></table></figure><p>  图3为训练数据集的可视化结果：</p><center><img src="https://s2.loli.net/2023/10/31/eNkl2GVYi4mPy1v.png" width="60%" height="60%"><div data-align="center">Image3: 训练数据集</div></center><p>  从图3中可以很容易看出，训练数据集是线性可分的，满足硬间隔线性支持向量机的前提条件。下面我们来利用硬间隔线性支持向量机算法来求解最优超平面的参数<span class="math inline">\(w^{*},b^{*}\)</span>.<br>  为了使用Python的cvxopt包来求救凸二次规划问题(6)，需要将问题(6)转化为矩阵形式，记：</p><p><span class="math display">\[\lambda = \begin{bmatrix}    \lambda_1 \\    \lambda_2 \\    \vdots \\    \lambda_N \\\end{bmatrix},\quad y = \begin{bmatrix}    y_1 \\    y_2 \\    \vdots \\    y_N \\\end{bmatrix},\quad x = \begin{bmatrix}    x_1 \\    x_2 \\    \vdots \\    x_N \\\end{bmatrix}\]</span></p><p>  则：</p><p><span class="math display">\[x \odot y = \begin{bmatrix}    x_1y_1 \\    x_2y_2 \\    \vdots \\    x_Ny_N \\\end{bmatrix},\qquad Gram(x \odot y) = \begin{bmatrix}    (x_1y_1)^{T}(x_1y_1) &amp; (x_1y_1)^{T}(x_2y_2) &amp; \dots &amp;(x_1y_1)^{T}(x_Ny_N)  \\    (x_2y_2)^{T}(x_1y_1) &amp; (x_2y_2)^{T}(x_2y_2) &amp; \dots &amp;(x_2y_2)^{T}(x_Ny_N)  \\    \vdots &amp; \vdots &amp;  &amp; \vdots \\    (x_Ny_N)^{T}(x_1y_1) &amp; (x_Ny_N)^{T}(x_2y_2) &amp; \dots &amp;(x_Ny_N)^{T}(x_Ny_N)  \\\end{bmatrix}\]</span></p><p>  设：</p><p><span class="math display">\[P=Gram(x \odot y), \quadq=-1_{N}=\begin{bmatrix}    -1 \\    -1 \\    \vdots \\    -1\end{bmatrix}_{N \times 1}, \quad G = -I_{N}=\begin{bmatrix}    -1 &amp; 0 &amp; \dots &amp; 0 \\    0 &amp; -1 &amp; \dots &amp; 0 \\    \vdots &amp; \vdots &amp; &amp; \vdots \\    0 &amp; 0 &amp; \dots &amp; -1 \\\end{bmatrix}_{N \times N}\]</span></p><p><span class="math display">\[h = \begin{bmatrix}    0 \\    0 \\    \vdots \\    0\end{bmatrix}_{N \times 1}, \quad A = y^{T}, \quad b=0\]</span></p><p>  则凸二次规划问题(6)的矩阵形式为(7)：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{\lambda} \quad &amp; \frac{1}{2}\lambda^{T}P\lambda+q^{T}\lambda \\        s.t. \quad &amp;  G\lambda \leq h \\        &amp; A\lambda=b  \\    \end{split}\end{equation}\]</span></p><p>  利用硬间隔支持向量机算法来求解最优超平面的参数<span class="math inline">\(w^{*},b^{*}\)</span>，以下是相应的代码：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># data matrix</span><br>train_X = train_data.iloc[:,[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]].values<br>train_y = train_data.iloc[:,<span class="hljs-number">2</span>].values<br><br><span class="hljs-comment"># parameters solving</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">HardMargin_SVM</span>(<span class="hljs-params">X,y</span>):<br>    Y = np.array([y]*X.shape[<span class="hljs-number">1</span>]).T<br>    XdotY = X * Y<br>    n_samples = X.shape[<span class="hljs-number">0</span>]<br>    Gram_Matrix = np.zeros((n_samples,n_samples))<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_samples):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_samples):<br>            Gram_Matrix[i,j] = np.dot(XdotY[i],XdotY[j])<br>    <br>    P = matrix(Gram_Matrix).T<br>    q = matrix(-np.ones(n_samples))<br>    G = matrix(-np.eye(n_samples))<br>    h = matrix(np.zeros(n_samples))<br>    A = matrix(y.astype(<span class="hljs-built_in">float</span>)).T<br>    b = matrix([<span class="hljs-number">0.0</span>])<br><br>    lamda = np.array(solvers.qp(P,q,G,h,A,b)[<span class="hljs-string">'x'</span>])<br>    threshold = <span class="hljs-number">1e-5</span><br>    lamda[lamda &lt; threshold] = <span class="hljs-number">0</span><br>    <br>    w_hat = np.<span class="hljs-built_in">round</span>(np.<span class="hljs-built_in">sum</span>(lamda*XdotY,axis=<span class="hljs-number">0</span>),<span class="hljs-number">4</span>)<br><br>    positive_index = np.where(lamda&gt;<span class="hljs-number">0</span>)[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]<br>    SV_x = X[positive_index]<br>    SV_y = y[positive_index]<br>    GM_xwithSVx = np.zeros(n_samples)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_samples):<br>        GM_xwithSVx[i] = np.dot(X[i],SV_x)<br><br>    lamda = lamda.ravel()<br>    b_hat = <span class="hljs-built_in">round</span>(SV_y - np.dot(lamda,y * GM_xwithSVx),<span class="hljs-number">4</span>)<br><br>    <span class="hljs-keyword">return</span> [w_hat,b_hat],lamda<br><br>theta,lamda = HardMargin_SVM(X=train_X,y=train_y)<br></code></pre></td></tr></tbody></table></figure><p>  解得最优超平面的参数为：</p><p><span class="math display">\[w^{*}=\begin{bmatrix}    -0.8985 \\    0.6889 \\\end{bmatrix}, \quad b^{*}=0.4643\]</span></p><p>  画出最优分离超平面<span class="math inline">\(S^{*}=\{x \vert w^{*}\cdot x + b^{*}=0\}\)</span>：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># draw the Optimal Classification Hyperplane</span><br>w,b = theta[<span class="hljs-number">0</span>],theta[<span class="hljs-number">1</span>]<br>x1 = np.linspace(<span class="hljs-number">0</span>,<span class="hljs-number">5</span>,<span class="hljs-number">1000</span>)<br>x2 = -(w[<span class="hljs-number">0</span>]*x1+b)/w[<span class="hljs-number">1</span>]<br>plt.figure(figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">6</span>))<br>plt.scatter(x=positivedata[<span class="hljs-string">"x1"</span>],y=positivedata[<span class="hljs-string">"x2"</span>],marker=<span class="hljs-string">"o"</span>,c=<span class="hljs-string">"green"</span>,label=<span class="hljs-string">"positive data"</span>)<br>plt.scatter(x=negativedata[<span class="hljs-string">"x1"</span>],y=negativedata[<span class="hljs-string">"x2"</span>],marker=<span class="hljs-string">"x"</span>,c=<span class="hljs-string">"red"</span>,label=<span class="hljs-string">"negative data"</span>)<br>plt.plot(x1,x2,c=<span class="hljs-string">"blue"</span>,label=<span class="hljs-string">"Optimal Classification Hyperplane"</span>)<br>plt.xlabel(<span class="hljs-string">"x1"</span>)<br>plt.ylabel(<span class="hljs-string">"x2"</span>)<br>plt.xlim((<span class="hljs-number">0</span>,<span class="hljs-number">5</span>))<br>plt.ylim((<span class="hljs-number">0</span>,<span class="hljs-number">5</span>))<br>plt.xticks(np.arange(<span class="hljs-number">0</span>,<span class="hljs-number">5</span>,<span class="hljs-number">0.5</span>))<br>plt.yticks(np.arange(<span class="hljs-number">0</span>,<span class="hljs-number">5</span>,<span class="hljs-number">0.5</span>))<br>plt.legend()<br></code></pre></td></tr></tbody></table></figure><center><img src="https://s2.loli.net/2023/10/31/Pgsh97eGjJTlXfS.png" width="60%" height="60%"><div data-align="center">Image4: 最优分离超平面</div></center><p>  分类模型：</p><p><span class="math display">\[f(x)=sign(w^{*} \cdot x +b^{*})\]</span></p><h3 id="基于sklearn的算法实现">基于sklearn的算法实现</h3><p>  下面我们使用<code>sklearn</code>库来完成这个分类任务，我们依然使用前文生成的由30个正类实例点与20个负类实例点构成的训练数据集，使用<code>sklearn.svm</code>中的<code>LinearSVC</code>进行分类的代码为：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">svm = LinearSVC(penalty=<span class="hljs-string">"l2"</span>,C=<span class="hljs-number">1.0</span>,loss=<span class="hljs-string">'hinge'</span>)<br>svm.fit(train_X,train_y)<br>w,b = svm.coef_[<span class="hljs-number">0</span>],svm.intercept_[<span class="hljs-number">0</span>]<br></code></pre></td></tr></tbody></table></figure><p>  <code>LinearSVC</code>的一些主要参数为：</p><ul><li><code>penalty</code>:{'l1','l2'},default='l2'。设定惩罚项，SVC中默认为'l2'惩罚项，'l1'会导致稀疏的<code>coef_</code>向量。<br></li><li><code>loss</code>: {'hinge', 'squared_hinge'},default='squared_hinge'。设定损失函数，hinge是标准的SVM损失(如SVC类使用的)，而squared_hinge是hinge损失的平方。<br></li><li><code>dual</code>: bool,default=True。选择算法来解决对偶或原始优化问题。 当n_samples&gt;n_features时，首选dual = False。<br></li><li><code>tol</code>: float, default=1e-4。设置停止的条件。</li><li><code>C</code>: float, default=1.0。正则化参数。正则化的强度与C成反比。必须严格设置为正的。</li></ul><p>  得到的最优分类超平面的参数为：</p><p><span class="math display">\[w^{*}=\begin{bmatrix}    -0.8240 \\    0.7860 \\\end{bmatrix}, \quad b^{*}=0.0349\]</span></p><p>  画出最优分离超平面<span class="math inline">\(S^{*}=\{x \vert w^{*}\cdot x + b^{*}=0\}\)</span>：</p><center><img src="https://s2.loli.net/2023/11/01/j2cTdOPCIbWKHEV.png" width="60%" height="60%"><div data-align="center">Image4: 最优分离超平面</div></center><h2 id="附录">附录</h2><h3 id="关于凸优化的相关知识">关于凸优化的相关知识</h3><h4 id="基础概念">基础概念</h4><p><strong>仿射集</strong><br>  设<span class="math inline">\(C\)</span>为某一集合，若 <span class="math inline">\(\forall x_1,x_2 \in C, \theta \in \mathbb{R},\theta x_1+(1-\theta)x_2 \in C\)</span>，则称集合<span class="math inline">\(C\)</span>为仿射集。</p><p><strong>仿射函数</strong><br>  设有映射 <span class="math inline">\(f: \mathbb{R}^{n} \rightarrow\mathbb{R}^{m}\)</span>，若 <span class="math inline">\(f(x)=Ax+b, A \in\mathbb{R}^{m \times n}, b \in \mathbb{R}^{m}\)</span>，则称映射<span class="math inline">\(f\)</span>为仿射函数。</p><p><strong>凸集</strong><br>  设<span class="math inline">\(C\)</span>为某一集合，若 <span class="math inline">\(\forall x_1,x_2 \in C, \theta \in [0,1], \thetax_1+(1-\theta)x_2 \in C\)</span>，则称集合<span class="math inline">\(C\)</span>为凸集。</p><p><strong>凸函数</strong><br>  一个函数<span class="math inline">\(f(x)\)</span>被称为凸函数，如果它的定义域 <span class="math inline">\(dom f\)</span> 为凸集，并且对 <span class="math inline">\(\forall x_1,x_2 \in dom f, \alpha \in[0,1]\)</span>，有以下不等式成立：</p><p><span class="math display">\[f[\alpha x_1+(1-\alpha) x_2] \leq \alphaf(x_1)+(1-\alpha)f(x_2)\]</span></p><p>  <strong>凸函数的一阶条件</strong><br>  设函数<span class="math inline">\(f(x)\)</span><strong>一阶可微</strong>，<span class="math inline">\(x \in \mathbb{R}^{n}\)</span>，则<span class="math inline">\(f(x)\)</span>为凸函数的<strong>充要条件</strong>为：<span class="math inline">\(dom \space f\)</span>为凸集，且对<span class="math inline">\(\forall x,y \in dom \spacef\)</span>，有以下不等式成立：</p><p><span class="math display">\[f(y) \ge f(x)+\nablaf^{T}(x)(y-x)\]</span></p><p>  <strong>凸函数的二阶条件</strong><br>  设函数<span class="math inline">\(f(x)\)</span><strong>二阶可微</strong>，<span class="math inline">\(x \in \mathbb{R}^{n}\)</span>，则<span class="math inline">\(f(x)\)</span>为凸函数的<strong>充要条件</strong>为：<span class="math inline">\(dom \space f\)</span>为凸集，且对<span class="math inline">\(\forall x \in dom \space f\)</span>，有<span class="math inline">\(\nabla^{2}f(x) \succeq 0\)</span>，即<span class="math inline">\(Hessain\)</span>矩阵半正定。</p><p><strong>最优化问题</strong><br>  最优化问题的基本形式(7)：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{x} \quad &amp; f(x) \\        s.t. \quad &amp; m_{i}(x) \leq 0,\quad i=1,2,\dots,M \\        &amp; n_{j}(x) = 0,\quad j=1,2,\dots,N    \end{split}\end{equation}\]</span></p><ul><li><span class="math inline">\(x=\begin{bmatrix}  x_1,x_2,\dots,x_n\end{bmatrix}^{T},x \in \mathbb{R}^{n}\)</span>称为<strong>优化变量(Optimization Variable)</strong>.<br></li><li><span class="math inline">\(f: \mathbb{R}^{n} \rightarrow\mathbb{R}\)</span>，称为<strong>目标函数(ObjectiveFunction)</strong>.<br></li><li><span class="math inline">\(m_i: \mathbb{R}^{n} \rightarrow\mathbb{R}\)</span>，称为<strong>不等式约束(InequalityConstraint)</strong>.<br></li><li><span class="math inline">\(n_j: \mathbb{R}^{n} \rightarrow\mathbb{R}\)</span>，称为<strong>等式约束(EqualityConstraint)</strong>.<br></li><li><span class="math inline">\(D = \left( dom \space f \right) \bigcap\{x \vert m_{i}(x) \leq 0,i=1,2,\dots,M\} \bigcap \{x \vert h_{j}(x) =0,j=1,2,\dots,N\}\)</span>，称为最优化问题的<strong>域(Domain)</strong>，<span class="math inline">\(x \in D\)</span> 称为<strong>可行解(FeasibleSolution)</strong>.</li><li><span class="math inline">\(p^{*} = \inf \{ f(x) \vert x \in D\}\)</span>，称为最优化问题的<strong>最优值(Optimum)</strong>.<br></li><li><span class="math inline">\(f(x^{*})=p^{*}\)</span>，称<span class="math inline">\(x^{*}\)</span>为最优化问题的<strong>最优解(OptimalSolution)</strong>.<br></li><li><span class="math inline">\(X_{opt} = \{ x \vert x \in D,f(x)=p^{*}\}\)</span>，称为最优化问题的<strong>最优解集(Optima Set)</strong>.</li></ul><p><strong>凸优化问题</strong><br>  若在优化问题(7)中，目标函数<span class="math inline">\(f(x)\)</span>为凸函数，不等式约束<span class="math inline">\(m_i(x)\)</span>为凸函数，等式约束<span class="math inline">\(n_j(x)\)</span>为仿射函数，则称该优化问题为凸优化问题。</p><h4 id="对偶关系">对偶关系</h4><p><strong>拉格朗日函数</strong><br>  原问题(7)的拉格朗日函数为：</p><p><span class="math display">\[L(x,\lambda,\eta)=f(x)+\sum_{i=1}^{M}\lambda_im_i(x)+\sum_{j=1}^{N}\eta_j n_j(x)\]</span></p><p><span class="math display">\[\lambda_i \ge 0, \quadi=1,2,\dots,M\]</span></p><p><strong>原问题的无约束形式</strong><br>  原问题(7)的无约束形式(8)为：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{x} \max_{\lambda,\eta} \quad &amp; L(x,\lambda,\eta)  \\        s.t. \quad &amp; \lambda_i \ge 0, \quad i=1,2,\dots,M \\    \end{split}\end{equation}\]</span></p><p>  原问题(7)与其无约束形式(8)是等价的，现证明这个结论。<br><strong>证明:</strong><br>  令<span class="math inline">\(h(x) = \max_{\lambda,\eta}L(x,\lambda,\eta)\)</span>,<br>  若优化变量<span class="math inline">\(x\)</span>不满足某个不等式约束<span class="math inline">\(m_i(x)\)</span>，即 <span class="math inline">\(\exists i, m_i(x) &gt; 0\)</span>,则有：</p><p><span class="math display">\[h(x)=\max_{\lambda,\eta}L(x,\lambda,\eta) \rightarrow +\infty\]</span></p><p>  若优化变量<span class="math inline">\(x\)</span>不满足某个等式约束<span class="math inline">\(n_j(x)\)</span>，即 <span class="math inline">\(\exists j,n_j(x) \ne 0\)</span>，则有：</p><p><span class="math display">\[h(x)=\max_{\lambda,\eta}L(x,\lambda,\eta) \rightarrow +\infty\]</span></p><p>  若优化变量<span class="math inline">\(x\)</span>满足所有的不等式约束<span class="math inline">\(m_i(x)\)</span>与等式约束<span class="math inline">\(n_j(x)\)</span>,即 <span class="math inline">\(\forall i,j, m_i(x) \leq 0,n_j(x)=0\)</span>，则有:</p><p><span class="math display">\[h(x) = \max_{\lambda,\eta}L(x,\lambda,\eta) &lt; +\infty\]</span></p><p><span class="math display">\[\lambda_i = 0, \quadi=1,2,\dots,M\]</span></p><p>  设集合<span class="math inline">\(S_1,S_2\)</span>分别为：</p><p><span class="math display">\[S_1 = \{ x \vert \exists i,m_i(x) &gt; 0\} \cup \{ x \vert \exists j, n_j(x) \ne 0 \}\]</span></p><p><span class="math display">\[S_2 = \{ x \vert \forall i,j, m_i(x)\leq 0,n_j(x) = 0 \}\]</span></p><p>  有:</p><p><span class="math display">\[S_1 \cup S_2 = \mathbb{R}^{n}, S_1 \capS_2 = \emptyset\]</span></p><p>  则无约束问题(8)可以写为</p><p><span class="math display">\[\min_{x} \max_{\lambda,\eta}L(x,\lambda,\eta) \Leftrightarrow \min_{x} h(x) = \left \{\begin{array}{l}+\infty, &amp; {x \in S_1}\\c(x)&lt;+\infty,&amp; {x \in S_2}\\\end{array} \right.\]</span></p><p><span class="math display">\[\Rightarrow \min_{x} \max_{\lambda,\eta}L(x,\lambda,\eta) \Leftrightarrow \min_{x \in S_2} h(x)=\min_{x \in S_2}f(x)\]</span></p><p>  故原问题(7)与其无约束形式(8)等价。<br>  证毕.</p><p><strong>对偶问题</strong><br>  原问题(7)的拉格朗日对偶问题(9)为：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \max_{\lambda,\eta} \min_{x} \quad &amp; L(x,\lambda,\eta) \\        s.t. \quad &amp; \lambda_i \ge 0, \quad i = 1,2,\dots,M    \end{split}\end{equation}\]</span></p><p><strong>弱对偶关系</strong><br>  原问题(7)与其对偶问题(9)满足弱对偶关系：</p><p><span class="math display">\[\max_{\lambda,\eta} \min_{x}L(x,\lambda,\eta) \leq \min_{x} \max_{\lambda,\eta}L(x,\lambda,\eta)\]</span></p><p><strong>证明:</strong><br>  令：</p><p><span class="math display">\[A(\lambda,\eta)=\min_{x}L(x,\lambda,\eta),\quad B(x)=\max_{\lambda,\eta}L(x,\lambda,\eta)\]</span></p><p><span class="math display">\[\because \min_{x}L(x,\lambda,\eta) \leqL(x,\lambda,\eta) \leq \max_{\lambda,\eta}L(x,\lambda,\eta)\]</span></p><p><span class="math display">\[\Rightarrow A(\lambda,\eta) \leqB(x),\quad \forall \lambda,\eta, \forall x\]</span></p><p><span class="math display">\[\Rightarrow \max_{\lambda,\eta}A(\lambda,\eta) \leq \min_{x} B(x)\]</span></p><p>  即：</p><p><span class="math display">\[\max_{\lambda,\eta} \min_{x}L(x,\lambda,\eta) \leq \min_{x} \max_{\lambda,\eta}L(x,\lambda,\eta)\]</span></p><p>  证毕.</p><p><strong>强对偶关系</strong><br>  若原问题(7)与其对偶问题(9)满足：</p><p><span class="math display">\[\max_{\lambda,\eta} \min_{x}L(x,\lambda,\eta) = \min_{x} \max_{\lambda,\eta}L(x,\lambda,\eta)\]</span></p><p>  则称原问题(7)与其对偶问题(9)满足强对偶关系。</p><p><strong>强对偶关系的几何理解</strong><br>  设原问题为仅有不等式约束的优化问题(10)：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{x} \quad &amp; f(x) \\        s.t. \quad &amp; m_{i}(x) \leq 0, \quad i=1,2,\dots,M \\    \end{split}\end{equation}\]</span></p><p>  原问题(10)的拉格朗日函数为：</p><p><span class="math display">\[L(x,\lambda)=f(x)+\sum_{i=1}^{M}\lambda_im_i(x)\]</span></p><p>  若原问题与其对偶问题满足强对偶关系，则有：</p><p><span class="math display">\[\min_{x} \max_{\lambda} L(x,\lambda) =\max_{\lambda} \min_{x} L(x,\lambda)\]</span></p><p>  <strong>鞍点的定义:</strong> 若 <span class="math inline">\(\exists(\hat{w},\hat{z})\)</span>，使得：</p><p><span class="math display">\[\sup_{z} \inf_{w} f(w,z)=f(\hat{w},\hat{z})= \inf_{w} \sup_{z} f(w,z)\]</span></p><p>  则称 <span class="math inline">\((\hat{w},\hat{z})\)</span> 为<span class="math inline">\(f(w,z)\)</span> 的鞍点。由于 <span class="math inline">\(f(\hat{w},z)= \inf_{w} f(w,z),f(w,\hat{z})=sup_{z} f(w,z)\)</span>，则以上等式也可以写成：</p><p><span class="math display">\[\sup_{z}f(\hat{w},z)=f(\hat{w},\hat{z})=\inf_{w} f(w,\hat{z})\]</span></p><center><img src="https://s2.loli.net/2023/10/29/lH3OtvE7UN9A15m.jpg" width="60%" height="60%"><div data-align="center">Image: 鞍点</div></center><p>  结合鞍点以及强对偶关系的概率，我们可以得出结论：<strong>原问题的拉格朗日函数存在鞍点是原问题与对偶问题满足强对偶关系的充要条件，且鞍点即为原问题与对偶问题的最优解。</strong></p><h4 id="kkt条件">KKT条件</h4><p>  下面来介绍如何判断原问题与对偶问题是否满足强对偶关系，以及如何求出相应的最优解。首先来介绍<span class="math inline">\(Slater\)</span>条件。<br><strong><span class="math inline">\(Slater\)</span>条件</strong><br>  若原问题(7)是凸问题，同时 <span class="math inline">\(\exists x \inrelint(D)\)</span>，使得约束满足：</p><p><span class="math display">\[\begin{split}    &amp; m_{i}(x) &lt; 0, \quad i=1,2,\dots,M \\    &amp; n_{j}(x) = 0, \quad j=1,2,\dots,N \\\end{split}\]</span></p><p>  则原问题与对偶问题满足强对偶关系。</p><ul><li>注：<span class="math inline">\(relint(D)\)</span>表示原始凸问题的域的相对内部，即域内除了边界点以外的所有点。</li></ul><p>  <span class="math inline">\(Slater\)</span>条件是一个<strong>充分不必要条件</strong>，若满足<span class="math inline">\(Slater\)</span>条件，则强对偶一定成立，不满足<span class="math inline">\(Slater\)</span>条件，强对偶也可能成立。大多数凸优化问题均满足<span class="math inline">\(Slater\)</span>条件，即有强对偶性。<br>  若我们已知原问题与对偶问题满足强对偶关系，如何求解出原问题以及对偶问题的最优解？下面我们来介绍凸优化中一个非常经典的理论——KKT条件。</p><p><strong>KKT条件</strong><br>  设<span class="math inline">\(x^{*}\)</span>为原始问题(7)的最优解，<span class="math inline">\(\lambda^{*},\eta^{*}\)</span>为对偶问题(9)的最优解，且原始问题与对偶问题满足强对偶关系，则有以下四组条件成立：</p><p><span class="math display">\[KKT条件:  \left \{\begin{array}{l}m_{i}(x^{*}) \leq 0, h_{j}(x^{*}) = 0 &amp;&amp;(primal \spacefeasibility)  \\\\\lambda^{*} \ge 0 &amp;&amp;(dual \space feasibility)\\\\\lambda^{*}m_{i}(x^{*})=0 &amp;&amp;(complementary \space slackness)\\\\\frac{\partial L(x,\lambda^{*},\eta^{*})}{\partial x} \vert_{x=x^{*}}=0&amp;&amp;(stationarity)\\\end{array} \right.\]</span></p><p>  原问题、对偶问题的可行性条件，稳定性条件都很好理解，我们主要来推导一下互补松弛条件是如何得到的。<br>  <strong>证明:</strong><br>  记 <span class="math inline">\(p^{*}\)</span>为原问题(7)的最优值，<span class="math inline">\(d^{*}\)</span>为对偶问题(9)的最优值，即：</p><p><span class="math display">\[p^{*}= \min_{x}f(x)=f(x^{*})\]</span></p><p><span class="math display">\[d^{*}=\max_{\lambda,\eta} \min_{x}L(x,\lambda,\eta) \triangleq \max_{\lambda,\eta}g(\lambda,\eta)=g(\lambda^{*},\eta^{*})\]</span></p><p>  通过分析可得：</p><p><span class="math display">\[\begin{align*}    d^{*} &amp;= \max_{\lambda,\eta} \min_{x} L(x,\lambda,\eta) =\min_{x} \max_{\lambda,\eta} L(x,\lambda,\eta) = \min_{x}L(x,\lambda^{*},\eta^{*}) \\    &amp;\leq L(x^{*},\lambda^{*},\eta^{*}) =f(x^{*})+\sum_{i=1}^{M}\lambda_{i}^{*}m_{i}(x^{*}) +\sum_{j=1}^{N}\eta_{j}^{*}n_{j}(x^{*}) \\    &amp;\leq f(x^{*}) = p^{*}\end{align*}\]</span></p><p>  由原问题与对偶问题满足强对偶关系可知：<span class="math inline">\(p^{*}=d^{*}\)</span>，则以上式子中的小于等于号均取等号，故有：</p><p><span class="math display">\[\sum_{i=1}^{M}\lambda_{i}^{*}m_{i}(x^{*})=0\Rightarrow \lambda^{*}m_{i}(x^{*})=0, \forall i\]</span></p><p>  互补松弛条件成立，证毕.</p><p>  对于<strong>一般的原问题</strong>，KKT条件是 <span class="math inline">\(x^{*},\lambda^{*},\eta^{*}\)</span>为最优解的<strong>必要条件</strong>，即只要 <span class="math inline">\(x^{*},\lambda^{*},\eta^{*}\)</span>为原问题及对偶问题的最优解，则一定满足KKT条件。<br>  对于<strong>原问题为凸问题</strong>，KKT条件是 <span class="math inline">\(x^{*},\lambda^{*},\eta^{*}\)</span>为最优解的<strong>充要条件</strong>，即只要 <span class="math inline">\(x^{*},\lambda^{*},\eta^{*}\)</span>满足KKT条件，则其一定为原问题及对偶问题的最优解，反过来，只要 <span class="math inline">\(x^{*},\lambda^{*},\eta^{*}\)</span>为原问题及对偶问题的最优解，则其一定满足KKT条件。</p><h4 id="凸二次规划">凸二次规划</h4><p>  凸二次规划问题的基本形式为：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{x} \quad &amp; \frac{1}{2} x^{T}Px+q^{T}x+r \\        s.t. \quad &amp;  Gx \leq h \\        &amp; Ax=b  \\    \end{split}\end{equation}\]</span></p><p>  其中，<span class="math inline">\(x \in \mathbb{R}^{n}, P \inS_{+}^{n}\)</span>，为对称正定矩阵；<span class="math inline">\(q,r \in\mathbb{R}^{n}; G \in \mathbb{R}^{M \times n}, h \in \mathbb{R}^{M}; A\in \mathbb{R}^{N \times n}, b \in \mathbb{R}^{N}\)</span>.<br>  结论：<strong>凸二次规划问题满足强对偶关系。</strong></p><h3 id="硬间隔线性支持向量机的解是唯一的">硬间隔线性支持向量机的解是唯一的</h3><p><strong>结论:</strong> 如果训练数据集<span class="math inline">\(T_{train}\)</span>完全线性可分，通过最小间隔最大化问题的求解，存在唯一的分离超平面可以将所有样本点完全分开。<br><strong>证明:</strong><br>  (1) 存在性的证明：<br>  由于训练数据集<span class="math inline">\(T_{train}\)</span>完全线性可分，由线性可分的定义可知存在分离超平面<span class="math inline">\(S = \{x \vert w \cdot x + b = 0\}\)</span>可将所有样本点完全正确分类。存在性得证。<br>  (2) 唯一性的证明：<br>  利用反证法来进行证明。假设优化问题(2)存在两个不同的最优解，分别记为<span class="math inline">\(w_{1}^{*},b_{1}^{*}\)</span>和<span class="math inline">\(w_{2}^{*},b_{2}^{*}\)</span>，意味着两个权值向量所对应的模都是最小值，记为<span class="math inline">\(a\)</span>：</p><p><span class="math display">\[||w_{1}^{*}||=||w_{2}^{*}||=a\]</span></p><p>  根据这两组参数可以构造一组新的参数：</p><p><span class="math display">\[w=\frac{w_{1}^{*}+w_{2}^{*}}{2}, \quadb=\frac{b_{1}^{*}+b_{2}^{*}}{2}\]</span></p><p>  新构造的参数<span class="math inline">\(w\)</span>的模长一定满足：</p><p><span class="math display">\[||w||= || \frac{w_{1}^{*}+w_{2}^{*}}{2}|| \ge a\]</span></p><p>  另一方面，由模长的三角不等式可得：</p><p><span class="math display">\[||\frac{w_{1}^{*}+w_{2}^{*}}{2}|| =||\frac{1}{2}w_{1}^{*}+\frac{1}{2}w_{2}^{*}|| \leq\frac{1}{2}||w_{1}^{*}||+\frac{1}{2}||w_{2}^{*}||=a\]</span></p><p>  从而有：</p><p><span class="math display">\[|| \frac{w_{1}^{*}+w_{2}^{*}}{2}||=a\]</span></p><p>  即：</p><p><span class="math display">\[||w||=\frac{1}{2}||w_{1}^{*}||+\frac{1}{2}||w_{2}^{*}||\]</span></p><p>  由此发现，<span class="math inline">\(w_{1}^{*}\)</span>和<span class="math inline">\(w_{2}^{*}\)</span>在同一直线上：</p><p><span class="math display">\[w_{1}^{*}=\pm w_{2}^{*}\]</span></p><p>  当<span class="math inline">\(w_{1}^{*}=w_{2}^{*}\)</span>时，与假设矛盾；当<span class="math inline">\(w_{1}^{*}=-w_{2}^{*}\)</span>时，违背了权重向量时非零向量的前提。唯一性得证。<br>  证毕.</p><h2 id="参考">参考</h2><p><strong>[1] Book: 李航,统计学习方法(第2版)</strong><br><strong>[2] Book: 董平,机器学习中的统计思维(Python实现)</strong><br><strong>[3] Video: bilibili,简博士,支持向量机系列</strong><br><strong>[4] Video: bilibili,shuhuai008,支持向量机系列</strong><br><strong>[5] Video:bilibili,欧拉的费米子,凸优化理论-中科大凌青</strong><br><strong>[6] Blog:知乎,Lauer,[凸优化笔记6]-拉格朗日对偶、KKT条件</strong></p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>机器学习-2.感知机</title>
    <link href="/2023/10/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
    <url>/2023/10/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-%E6%84%9F%E7%9F%A5%E6%9C%BA/</url>
    
    <content type="html"><![CDATA[<h1 id="感知机与线性判别分析">感知机与线性判别分析</h1><h2 id="感知机">感知机</h2><p>  感知机(perception)是一个处理二分类问题的线性分类模型。感知机模型旨在寻找一个合适的超平面，将特征空间中的示例进行正确分类。为了找到这个超平面，需要设定基于误分类的损失函数，并利用梯度下降方法对损失函数进行极小化，求得感知机模型。感知机学习算法具有简单与易于实现的优点，分为原始形式和对偶形式。<br>  感知机模型由<span class="math inline">\(Rosenblatt\)</span>于1957年提出，尽管感知机在处理复杂问题方面存在局限性，但它为神经网络和深度学习的发展铺平了道路。感知机模型的思想和学习算法为后来更复杂的神经网络提供了基础，尤其是多层感知机和其他深度学习模型。感知机的历史地位在机器学习领域被广泛认可，被视为神经网络的早期里程碑。</p><h3 id="基本思想">基本思想</h3><p>  感知机的基本思想为：在<span class="math inline">\(n\)</span>维特征空间中，寻找一个能够使训练数据集中的正实例点与负实例点完全分开的超平面，对于新的实例，利用该超平面进行分类。为了找到这个超平面，感知机设置了以误分类样本数为基础的损失函数，通过梯度下降算法最小化损失函数，更新超平面，最终使得所有的训练实例均被正确分类，此时便找到了分类所需的超平面。感知机的基本思想可以用如下图1来描述：</p><center><img src="https://s2.loli.net/2023/10/08/r9CDXyRLPI5kdN6.jpg" width="60%" height="60%"><div data-align="center">Image1: 感知机的基本思想</div></center><h3 id="模型">模型</h3><p><strong>输入</strong></p><ul><li><strong>输入空间:</strong> <span class="math inline">\(\mathcal{X} =\mathbb{R}^{n}\)</span><br></li><li><strong>输入实例:</strong> <span class="math inline">\(x =\begin{bmatrix}  x^{(1)},x^{(2)},\dots,x^{(n)} \\ \end{bmatrix} \in\mathcal{X}\)</span></li></ul><p>  其中<span class="math inline">\(\mathcal{X}\)</span>代表<span class="math inline">\(n\)</span>维实数空间，输入实例<span class="math inline">\(x\)</span>为<span class="math inline">\(n\)</span>维特征向量。</p><p><strong>输出</strong></p><ul><li><strong>输出空间:</strong> <span class="math inline">\(\mathcal{Y} =\{-1,+1\}\)</span><br></li><li><strong>输出实例:</strong> <span class="math inline">\(y \in\mathcal{Y}\)</span></li></ul><p>  其中输出空间<span class="math inline">\(\mathcal{Y}\)</span>只包含+1和-1的一个集合，+1与-1分别代表二分类问题中的正类与负类。输出实例<span class="math inline">\(y\)</span>代表输出实例<span class="math inline">\(x\)</span>的类别。</p><p><strong>数据集</strong><br>  感知机模型的训练数据集<span class="math inline">\(T_{train}\)</span>为：</p><p><span class="math display">\[T_{train}=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}\]</span></p><p>  其中，<span class="math inline">\(x_i \in\mathcal{X}=\mathbb{R}^{n},y_i \in\mathcal{Y}=\{+1,-1\},i=1,2,\dots,N\)</span>，<span class="math inline">\(N\)</span>表示训练数据的样本容量。</p><p><strong>模型</strong><br>  设输入空间<span class="math inline">\(\mathcal{X}\)</span>到输出空间<span class="math inline">\(\mathcal{Y}\)</span>的映射为：</p><p><span class="math display">\[f(x)=sign(w^{T}x+b)\]</span></p><p>  称模型<span class="math inline">\(f\)</span>为感知机，其中，<span class="math inline">\(w\)</span>和<span class="math inline">\(b\)</span>为感知机模型的参数，<span class="math inline">\(w \in \mathbb{R}^{n}\)</span>为权重向量，<span class="math inline">\(b \in \mathbb{R}\)</span>为偏置。<span class="math inline">\(sign(\cdot)\)</span>为符号函数，即：</p><p><span class="math display">\[sign(x)= \left \{\begin{array}{rcl}+1, &amp; x \ge 0\\-1, &amp; x &lt; 0  \\\end{array} \right.\]</span></p><p>  感知机是一种线性分类模型，属于判别模型。</p><p><strong>假设空间</strong><br>  感知机模型的假设空间<span class="math inline">\(\mathcal{H}\)</span>为：</p><p><span class="math display">\[\mathcal{H}=\{f \vertf(x)=w^{T}x+b\}\]</span></p><p>  感知机模型的假设空间实际上特征空间中超平面的集合。</p><p><strong>参数空间</strong><br>  令<span class="math inline">\(\theta=(w,b)\)</span>，则模型的假设空间<span class="math inline">\(\mathcal{H}\)</span>等价于参数空间<span class="math inline">\(\Theta\)</span>：</p><p><span class="math display">\[\Theta=\{\theta \vert \theta \in\mathbb{R}^{n+1}\}\]</span></p><h3 id="策略">策略</h3><p>  感知机学习的基本假设是训练数据集是线性可分的，这里先给出数据集线性可分性的定义。</p><p><strong>数据集的线性可分性</strong><br>  给定一个数据集<span class="math inline">\(T\)</span>：</p><p><span class="math display">\[T=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}\]</span></p><p>其中，<span class="math inline">\(x_i \in\mathcal{X}=\mathbb{R}^{n},y_i \in\mathcal{Y}=\{+1,-1\},i=1,2,\dots,N\)</span>，如果存在某个超平面<span class="math inline">\(S\)</span>能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，即<span class="math inline">\(\exists \theta=(w,b) \in\mathbb{R}^{n+1}\)</span>，对所有 <span class="math inline">\(y_i=+1\)</span> 的实例<span class="math inline">\(i\)</span>，有 <span class="math inline">\(w^{T}x_i+b &gt; 0\)</span> ；对所有 <span class="math inline">\(y_i=-1\)</span> 的实例<span class="math inline">\(i\)</span>，有 <span class="math inline">\(w^{T}x_i+b &lt; 0\)</span>，则称数据集<span class="math inline">\(T\)</span>为线性可分数据集(linearly separable dataset)；否则，称数据集<span class="math inline">\(T\)</span>线性不可分。</p><p><strong>损失函数</strong>  为了寻找到合适的超平面，需要设置损失函数，一个很自然的想法是将损失函数设置为误分类点的个数，对于误分类数据<span class="math inline">\((x_i,y_i)\)</span>有：</p><p><span class="math display">\[-y_i(w^{T}x_i+b) &gt; 0\]</span></p><p>这是因为误分类的两种情况:<span class="math inline">\(w^{T}x_i+b &gt;0,y_i=-1\)</span>或<span class="math inline">\(w^{T}x_i+b &lt;0,y_i=+1\)</span>均可由上式表示，则损失函数可以表示为：</p><p><span class="math display">\[L(x,y)=\sum_{i=1}^{N}I_{\{-y_i(w^{T}x_i+b)&gt;0\}}\]</span></p><p><span class="math display">\[I_{\{-y_i(w^{T}x_i+b)&gt;0\}}=\left \{\begin{array}{rcl}1, &amp; {-y_i(w^{T}x_i+b)&gt;0}\\0, &amp; {-y_i(w^{T}x_i+b) \leq 0}\\\end{array} \right.\]</span></p><p>  这样设置损失函数的思路非常直观，但是，这样的损失函数不是参数<span class="math inline">\(w,b\)</span>的连续可导函数，不易进行优化，因此我们一般不将损失函数设置为这种形式。<br>  <strong>感知机所采用的损失函数是误分类点到超平面的总距离</strong>，其与误分类点的个数相关，当损失函数降至0时，意味着没有误分类点了，我们也就找到了可以将训练数据集完全正确分类的超平面了。  为此，首先给出输入空间<span class="math inline">\(\mathcal{X}\)</span>中任意一点<span class="math inline">\(x_0\)</span>到超平面<span class="math inline">\(S=\{x \vert w^{T}x+b=0\}\)</span>的距离：</p><p><span class="math display">\[d(x_0,S)=\frac{|w^{T}x_0+b|}{|| w ||_{2}}\]</span></p><p>  关于<span class="math inline">\(n\)</span>维空间中点到超平面的距离公式的证明可参见附录，这里不多做赘述。对于误分类点<span class="math inline">\(x_i\)</span>，其到初始超平面<span class="math inline">\(S\)</span>的距离为：</p><p><span class="math display">\[d(x_i,S)=\frac{-y_i(w^{T}x_i+b)}{||w||_2}\]</span></p><p>  假设在初始超平面<span class="math inline">\(S\)</span>的分类下，误分类点的集合为<span class="math inline">\(M\)</span>，则误分类点到超平面的总距离为：</p><p><span class="math display">\[d(M,S)=-\frac{1}{||w||_2}\sum_{x_i \inM}y_i(w^{T}x_i+b)\]</span></p><p>  在设定损失函数时，我们可以不考虑<span class="math inline">\(\frac{1}{||w||_2}\)</span>，因为其既不影响损失函数的正负，也不影响感知机算法的最终结果。这样，对于线性可分的训练数据集：</p><p><span class="math display">\[T_{train}=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}\]</span></p><p>  感知机学习的损失函数一般设定为：</p><p><span class="math display">\[L(w,b)=-\sum_{x_i \inM}y_i(w^{T}x_i+b)\]</span></p><p>  损失函数<span class="math inline">\(L(w,b)\)</span>是参数<span class="math inline">\(w,b\)</span>的连续可导函数，便于进行优化。当损失函数减小时，误分类点的数量也会减小，当<span class="math inline">\(L(w,b)=0\)</span>时，意味着训练数据集中所有的实例都被正确分类了，此时得到的超平面即是我们要寻找的可以完全正确分类的超平面。</p><h3 id="算法">算法</h3><p>  感知机算法分为原始形似与对偶形式，这里分别来介绍这两种形式。</p><h4 id="原始形式">原始形式</h4><p>  在策略里我们已经确定了感知机学习的损失函数，我们现在要做的便是最小化损失函数，即：</p><p><span class="math display">\[\min_{w,b} -\sum_{x_i \inM}y_i(w^{T}x_i+b)\]</span></p><p>  我们采用随机梯度下降法(stochastic gradientdescent)来优化损失函数。<br>  首先，随机选取一个初始超平面<span class="math inline">\(S_0=\{x \vertw_0^{T}x+b_0=0\}\)</span>，此时误分类点的集合为<span class="math inline">\(M_0\)</span>，则损失函数<span class="math inline">\(L(w,b)\)</span>关于参数<span class="math inline">\(w_0,b_0\)</span>的梯度为：</p><p><span class="math display">\[\nabla_{w_0}L(w_0,b_0)=-\sum_{x_i \inM_0}y_ix_i\]</span></p><p><span class="math display">\[\nabla_{b_0}L(w_0,b_0)=-\sum_{x_i \inM_0}y_i\]</span></p><p>  在<span class="math inline">\(M_0\)</span>随机选取一个误分类点<span class="math inline">\((x_0,y_0)\)</span>，对<span class="math inline">\(w_0,b_0\)</span>进行更新：</p><p><span class="math display">\[w_1 \leftarrow w_0+\etay_ix_i\]</span></p><p><span class="math display">\[b_1 \leftarrow b_0+\eta y_i\]</span></p><p>  其中，<span class="math inline">\(\eta\)</span>为梯度下降的步长，在机器学习中也称为学习率(learningrate)。更新后我们会得到一个新的超平面<span class="math inline">\(S_1=\{x\vert w_1^{T} x +b_1=0\}\)</span>，此时损失函数的值会减小。这样通过迭代可以期待损失函数<span class="math inline">\(L(w,b)\)</span>不断减小，直到为0，此时误分类集合<span class="math inline">\(M=\varnothing\)</span>，我们便找到了可以对训练集完全正确分类的超平面<span class="math inline">\(S\)</span>.<br>  综上所述，我们写出感知机算法的原始形式。</p><h5 id="感知机学习算法的原始形式">感知机学习算法的原始形式</h5><p>  输入：训练数据集<span class="math inline">\(T_{train}=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}\)</span>，其中<span class="math inline">\(x_i \in \mathcal{X}=\mathbb{R}^{n}，y \in\mathcal{Y}=\{-1,+1\},i=1,2,\dots,N\)</span>；学习率<span class="math inline">\(\eta (0 &lt; \eta \leq 1)\)</span>.<br>  输出：<span class="math inline">\(w,b\)</span>：感知机模型 <span class="math inline">\(f(x)=sign(w^{T}x+b)\)</span><br>  (1) 随机选取初始参数<span class="math inline">\(w_0,b_0\)</span>；<br>  (2) 在训练数据集中选取一个数据<span class="math inline">\((x_i,y_i)\)</span>；<br>  (3) <span class="math inline">\(if \space y_i(w_0^{T}x_i+b_0) \leq0:\)</span></p><p><span class="math display">\[w_1 \leftarrow w_0+\etay_ix_i\]</span></p><p><span class="math display">\[b_1 \leftarrow b_0+\eta y_i\]</span></p><p>  (4) 转至 (2)，直至训练集中没有误分类点。</p><p>  这种学习算法直观上有如下解释：<strong>当一个实例点被误分类，即位于分离超平面的错误一侧时，则调整<span class="math inline">\(w,b\)</span>的值，使超平面向该误分类点的一侧移动，以减小误分类点与超平面间的距离，直至超平面越过该误分类点使其被正确分类。</strong></p><h4 id="对偶形式">对偶形式</h4><p>  在感知机算法的原始形式中，如果实例点<span class="math inline">\((x_i,y_i)\)</span>是误分类点，则可以用该点更新参数，即：</p><p><span class="math display">\[w \leftarrow w + \etay_ix_i\]</span></p><p><span class="math display">\[b \leftarrow b + \eta y_i\]</span></p><p>  现在设想一下，如果在参数更新的过程中，点<span class="math inline">\((x_i,y_i)\)</span>被误分类了<span class="math inline">\(n_i\)</span>次，则在从初始参数<span class="math inline">\(w_0,b_0\)</span>到最终参数<span class="math inline">\(w,b\)</span>中，点<span class="math inline">\((x_i,y_i)\)</span>贡献的增量为<span class="math inline">\(n_i \eta y_ix_i\)</span>和<span class="math inline">\(n_i \eta y_i\)</span>.<br>  假设在迭代过程中，训练集中的每一个实例点<span class="math inline">\((x_i,y_i)\)</span>被误分类的次数为<span class="math inline">\(n_i,i=1,2,\dots,N\)</span>，取初始的参数向量为零向量，则通过迭代最终学习到的次数可以表示为：</p><p><span class="math display">\[w=\sum_{i=1}^{N}\alpha_iy_ix_i\]</span></p><p><span class="math display">\[b =\sum_{i=1}^{N}\alpha_iy_i\]</span></p><p>  其中，<span class="math inline">\(\alpha_i=n_i\eta\)</span>，若<span class="math inline">\(\eta=1\)</span>，则<span class="math inline">\(\alpha_i\)</span>表示训练集中的实例点<span class="math inline">\((x_i,y_i)\)</span>由于被误分类而用于更新参数的次数。<br>  在使用对偶算法进行迭代时，若<span class="math inline">\((x_i,y_i)\)</span>被误分类，则我们便在其对应的<span class="math inline">\(\alpha_i\)</span>上加上增量<span class="math inline">\(\eta\)</span>，从而得到新的超平面，循环迭代过程，直至没有误分类点为止。  综上所述，我们可以写出感知机算法的对偶形式。</p><h5 id="感知机学习算法的对偶形式">感知机学习算法的对偶形式</h5><p>  输入：训练数据集<span class="math inline">\(T_{train}=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}\)</span>，其中<span class="math inline">\(x_i \in \mathcal{X}=\mathbb{R}^{n}，y \in\mathcal{Y}=\{-1,+1\},i=1,2,\dots,N\)</span>；学习率<span class="math inline">\(\eta (0 &lt; \eta \leq 1)\)</span>.<br>  输出：<span class="math inline">\(\alpha\)</span>：感知机模型 <span class="math inline">\(f(x)=sign \left( (\sum_{i=1}^{N}\alpha_iy_ix_i)\cdot x+(\sum_{i=1}^{N}\alpha_iy_i) \right),\alpha=\begin{bmatrix}  \alpha_1, \alpha_2, \dots, \alpha_N \\\end{bmatrix}^T\)</span><br>  (1) <span class="math inline">\(\alpha \leftarrow 0\)</span>;<br>  (2) 在训练数据集<span class="math inline">\(T_{train}\)</span>中选取实例点<span class="math inline">\((x_j,y_j)\)</span>;<br>  (3) <span class="math inline">\(if \space y_i \left(\sum_{i=1}^{N}\alpha_iy_ix_i \cdot x_j+\sum_{i=1}^{N}\alpha_iy_i \right)\leq 0：\)</span></p><p><span class="math display">\[\alpha_i \leftarrow \alpha_i +\eta\]</span></p><p>  (4) 转至(2)直到没有误分类数据。</p><p>  值得注意的是，在对偶算法的每次迭代中，训练实例仅以内积的形式出现。为了方便计算，可以预先将训练数据中的实例间的内积计算出来，并以矩阵的形式储存，这个矩阵便是<span class="math inline">\(Gram\)</span>矩阵：</p><p><span class="math display">\[G=[x_i \cdot x_j]_{N \timesN}\]</span></p><p>  可以证明感知机算法是收敛的，即经过有限次迭代可以得到一个能将训练数据集完全正确分类的分离超平面。这部分证明放在附录里，感兴趣的读者可自行阅读。</p><h3 id="感知机算法的实例及python实现">感知机算法的实例及Python实现</h3><p>  首先，我们需要创造一个线性可分的二分类数据。为了便于可视化，设输入实例<span class="math inline">\(x \in \mathcal{X}=\mathbb{R}^2，y \in \mathcal{Y}=\{-1,1\}\)</span>，以下是用于产生数据的Python代码：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Data Generation</span><br>np.random.seed(<span class="hljs-number">1314</span>)<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">data_generation</span>(<span class="hljs-params">n,basepoint,ylabel</span>):<br>    x1 = np.random.random((n,))+np.array([basepoint[<span class="hljs-number">0</span>]]*n)<br>    x2 = np.random.random((n,))+np.array([basepoint[<span class="hljs-number">1</span>]]*n)<br>    y = np.array([ylabel]*n)<br>    data = pd.DataFrame({<span class="hljs-string">"x1"</span>:x1,<span class="hljs-string">"x2"</span>:x2,<span class="hljs-string">"y"</span>:y})<br>    <span class="hljs-keyword">return</span> data<br><br><span class="hljs-comment"># positive data</span><br>positivedata = data_generation(n=<span class="hljs-number">30</span>,basepoint=(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>),ylabel=<span class="hljs-number">1</span>)<br><span class="hljs-comment"># negative data</span><br>negativedata = data_generation(n=<span class="hljs-number">20</span>,basepoint=(<span class="hljs-number">2</span>,<span class="hljs-number">1</span>),ylabel=-<span class="hljs-number">1</span>)<br><span class="hljs-comment"># train data</span><br>train_data = pd.concat([positivedata,negativedata])<br>train_data = shuffle(train_data)<br>train_data.index = <span class="hljs-built_in">range</span>(train_data.shape[<span class="hljs-number">0</span>])<br>train_data.head(<span class="hljs-number">5</span>)<br></code></pre></td></tr></tbody></table></figure><p>  在这段代码里，我利用均匀分布在<span class="math inline">\(\{x \vertx_1 \in (1,2);x_2 \in (2,3)\}\)</span>里产生了30个正类实例，在<span class="math inline">\(\{x \vert x_1 \in (2,3);x_2 \in(1,2)\}\)</span>里产生了20个负类实例。得到的实例数据如下表1：</p><style>.center {  width: auto;  display: table;  margin-left: auto;  margin-right: auto;}</style><p align="center"><font face="黑体" size="2.">table1 训练数据(部分)</font></p><div class="center"><table><thead><tr class="header"><th style="text-align: center;">index</th><th style="text-align: center;">x1</th><th style="text-align: center;">x2</th><th style="text-align: center;">y</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">0</td><td style="text-align: center;">1.749421</td><td style="text-align: center;">2.193847</td><td style="text-align: center;">1</td></tr><tr class="even"><td style="text-align: center;">1</td><td style="text-align: center;">2.057071</td><td style="text-align: center;">1.983485</td><td style="text-align: center;">-1</td></tr><tr class="odd"><td style="text-align: center;">2</td><td style="text-align: center;">2.830457</td><td style="text-align: center;">1.026819</td><td style="text-align: center;">-1</td></tr><tr class="even"><td style="text-align: center;">3</td><td style="text-align: center;">1.437378</td><td style="text-align: center;">2.345612</td><td style="text-align: center;">1</td></tr><tr class="odd"><td style="text-align: center;">4</td><td style="text-align: center;">2.730692</td><td style="text-align: center;">1.493602</td><td style="text-align: center;">-1</td></tr></tbody></table></div><p>  利用以下代码将训练数据进行可视化：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># train data scatter plot</span><br>plt.scatter(x=positivedata[<span class="hljs-string">"x1"</span>],y=positivedata[<span class="hljs-string">"x2"</span>],marker=<span class="hljs-string">"o"</span>,c=<span class="hljs-string">"green"</span>,label=<span class="hljs-string">"positive data"</span>)<br>plt.scatter(x=negativedata[<span class="hljs-string">"x1"</span>],y=negativedata[<span class="hljs-string">"x2"</span>],marker=<span class="hljs-string">"x"</span>,c=<span class="hljs-string">"red"</span>,label=<span class="hljs-string">"negative data"</span>)<br>plt.xlabel(<span class="hljs-string">"x1"</span>)<br>plt.ylabel(<span class="hljs-string">"x2"</span>)<br>plt.xlim((<span class="hljs-number">0</span>,<span class="hljs-number">4</span>))<br>plt.ylim((<span class="hljs-number">0</span>,<span class="hljs-number">4</span>))<br>plt.xticks(np.arange(<span class="hljs-number">0</span>,<span class="hljs-number">4</span>,<span class="hljs-number">0.5</span>))<br>plt.yticks(np.arange(<span class="hljs-number">0</span>,<span class="hljs-number">4</span>,<span class="hljs-number">0.5</span>))<br>plt.legend()<br></code></pre></td></tr></tbody></table></figure><p>  得到的训练数据散点图为下图2：</p><center><img src="https://s2.loli.net/2023/10/12/b3KfhBve5VYl6Sw.png" width="60%" height="60%"><div data-align="center">Image2: 训练数据散点图</div></center><p>  从图中我们可以看出，训练数据是线性可分的，满足感知机学习的假设。<br>  之后我们可以根据感知机学习的原始算法来求解分离超平面的参数<span class="math inline">\(w,b\)</span>，参数求解的代码如下：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">param_solving</span>(<span class="hljs-params">train_data,lr,start_w</span>):<br>    <span class="hljs-comment"># initial parameter</span><br>    w = np.array(start_w) <span class="hljs-comment"># w = [w1,w2,b]</span><br>    x = train_data[[<span class="hljs-string">"x1"</span>,<span class="hljs-string">"x2"</span>]].values<br>    onecolumn = np.ones((train_data.shape[<span class="hljs-number">0</span>],))<br>    X = np.column_stack((x,onecolumn))<br>    y = train_data[<span class="hljs-string">"y"</span>].values<br>    k = <span class="hljs-number">0</span><br>    inter_data = {<span class="hljs-string">"k"</span>:[],<span class="hljs-string">"Parameter"</span>:[],<span class="hljs-string">"Loss Function"</span>:[],<span class="hljs-string">"Misclassifications"</span>:[],<span class="hljs-string">"Point Used for Update"</span>:[]}<br>    <span class="hljs-comment"># stochastic gradient descent</span><br>    <span class="hljs-keyword">while</span> np.<span class="hljs-built_in">any</span>((np.dot(X,w)*y&lt;<span class="hljs-number">0</span>) | (np.dot(X,w)*y==<span class="hljs-number">0</span>)):<br>        k += <span class="hljs-number">1</span><br>        falseindex = np.where((np.dot(X,w)*y&lt;<span class="hljs-number">0</span>) | (np.dot(X,w)*y==<span class="hljs-number">0</span>))<br><br>        <span class="hljs-comment"># storing information about the iterative process</span><br>        <span class="hljs-keyword">if</span> k%<span class="hljs-number">5</span> == <span class="hljs-number">0</span>:<br>            inter_data[<span class="hljs-string">"k"</span>].append(k)<br>            inter_data[<span class="hljs-string">"Parameter"</span>].append(w.<span class="hljs-built_in">round</span>(<span class="hljs-number">4</span>))<br>            falsedata = train_data.iloc[falseindex[<span class="hljs-number">0</span>],:]<br>            x_falseclassified = falsedata[[<span class="hljs-string">"x1"</span>,<span class="hljs-string">"x2"</span>]].values<br>            X_falseclassified = np.column_stack((x_falseclassified,np.ones((falsedata.shape[<span class="hljs-number">0</span>],))))<br>            y_falseclassified = falsedata[<span class="hljs-string">"y"</span>].values<br>            loss = -<span class="hljs-built_in">sum</span>(np.dot(X_falseclassified,w)*y_falseclassified)<br>            inter_data[<span class="hljs-string">"Loss Function"</span>].append(loss.<span class="hljs-built_in">round</span>(<span class="hljs-number">4</span>))<br>            inter_data[<span class="hljs-string">"Misclassifications"</span>].append(falseindex[<span class="hljs-number">0</span>])<br>            inter_data[<span class="hljs-string">"Point Used for Update"</span>].append(falseindex[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>])<br>        <br>        x_forupdate = train_data.iloc[falseindex[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>],[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]].values<br>        y_forupdate = train_data.iloc[falseindex[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>],[<span class="hljs-number">2</span>]].values<br>        delta = np.append(lr*y_forupdate*x_forupdate,lr*y_forupdate)<br>        w = w + delta<br>    inter_information = pd.DataFrame(inter_data)<br><br>    result_data = {}<br>    result_data[<span class="hljs-string">"final parameters"</span>] = w.<span class="hljs-built_in">round</span>(<span class="hljs-number">4</span>)<br>    result_data[<span class="hljs-string">"interative information"</span>] = inter_information<br>    result_data[<span class="hljs-string">"number of iterations"</span>] = k<br> <br>    <span class="hljs-keyword">return</span> result_data<br></code></pre></td></tr></tbody></table></figure><p>  若我们将初始的参数均设置为0，则我们最终得到的分离超平面参数为：</p><p><span class="math display">\[w,b = \begin{bmatrix}    -0.1117,0.1103 \\\end{bmatrix}^T,0.01\]</span></p><p>  总共的迭代次数为<span class="math inline">\(k=85\)</span>，以下表是迭代过程中的一些信息：</p><style>.center {  width: auto;  display: table;  margin-left: auto;  margin-right: auto;}</style><p align="center"><font face="黑体" size="2.">table2 迭代过程信息(部分)</font></p><div class="center"><table style="width:100%;"><thead><tr class="header"><th style="text-align: center;">k</th><th style="text-align: center;">Parameter</th><th style="text-align: center;">Loss Function</th><th style="text-align: center;">Misclassifications</th><th style="text-align: center;">Point Used for Update</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">5</td><td style="text-align: center;">[-0.0062, 0.0042, 0.0]</td><td style="text-align: center;">0.0161</td><td style="text-align: center;">[0, 5, 7, 9, 24, 28, 36, 43, 45,49]</td><td style="text-align: center;">0</td></tr><tr class="even"><td style="text-align: center;">10</td><td style="text-align: center;">[0.0052, 0.0304, 0.01]</td><td style="text-align: center;">1.3912</td><td style="text-align: center;">[ 1, 2, 4, 10, 12, 13, 15, 18, 22, 29,30, 32, 33, 37, 38, 39, 40,42, 44, 46]</td><td style="text-align: center;">1</td></tr><tr class="odd"><td style="text-align: center;">15</td><td style="text-align: center;">[-0.0215, 0.0147, 0.0]</td><td style="text-align: center;">0.0564</td><td style="text-align: center;">[0, 5, 7, 9, 24, 28, 36, 43, 45,49]</td><td style="text-align: center;">0</td></tr><tr class="even"><td style="text-align: center;">20</td><td style="text-align: center;">[-0.0102, 0.0409, 0.01]</td><td style="text-align: center;">0.9020</td><td style="text-align: center;">[ 1, 2, 4, 10, 12, 13, 15, 18, 22, 29,30, 32, 33, 37, 38, 39, 40,42, 44, 46]</td><td style="text-align: center;">1</td></tr><tr class="odd"><td style="text-align: center;">25</td><td style="text-align: center;">[-0.0369, 0.0252, 0.0]</td><td style="text-align: center;">0.0967</td><td style="text-align: center;">[0, 5, 7, 9, 24, 28, 36, 43, 45,49]</td><td style="text-align: center;">0</td></tr></tbody></table></div><p>  表中每一列的含义为：</p><ul><li><strong>k:</strong> 表示第<span class="math inline">\(k\)</span>次更新参数前。<br></li><li><strong>Parameter:</strong> 表示第<span class="math inline">\(k\)</span>次更新参数前参数向量<span class="math inline">\(\hat{w}=(w^T,b)^T\)</span>.<br></li><li><strong>Loss Function:</strong> 表示第<span class="math inline">\(k\)</span>次更新参数前损失函数<span class="math inline">\(L(w,b)\)</span>的值。<br></li><li><strong>Misclassifications:</strong> 表示第<span class="math inline">\(k\)</span>次更新参数前训练数据中被误分类的示例点的索引。<br></li><li><strong>Point Used for Update:</strong> 表示第<span class="math inline">\(k\)</span>次更新所用的误分类示例点的索引。</li></ul><p>  利用以下代码画出分类超平面：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">result = param_solving(train_data=train_data,lr=<span class="hljs-number">0.01</span>,start_w=[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>])<br>w = result[<span class="hljs-string">"final parameters"</span>]<br>x1_line = np.linspace(<span class="hljs-number">0</span>,<span class="hljs-number">4</span>,<span class="hljs-number">1000</span>)<br>x2_line = (-w[<span class="hljs-number">0</span>]*x1_line - w[<span class="hljs-number">2</span>])/w[<span class="hljs-number">1</span>]<br>plt.scatter(x=positivedata[<span class="hljs-string">"x1"</span>],y=positivedata[<span class="hljs-string">"x2"</span>],marker=<span class="hljs-string">"o"</span>,c=<span class="hljs-string">"green"</span>,label=<span class="hljs-string">"positive data"</span>)<br>plt.scatter(x=negativedata[<span class="hljs-string">"x1"</span>],y=negativedata[<span class="hljs-string">"x2"</span>],marker=<span class="hljs-string">"x"</span>,c=<span class="hljs-string">"red"</span>,label=<span class="hljs-string">"negative data"</span>)<br>plt.plot(x1_line,x2_line,c=<span class="hljs-string">"blue"</span>,label=<span class="hljs-string">"Classification Hyperplane"</span>)<br>plt.xlim((<span class="hljs-number">0</span>,<span class="hljs-number">4</span>))<br>plt.ylim((<span class="hljs-number">0</span>,<span class="hljs-number">4</span>))<br>plt.xticks(np.arange(<span class="hljs-number">0</span>,<span class="hljs-number">4</span>,<span class="hljs-number">0.5</span>))<br>plt.yticks(np.arange(<span class="hljs-number">0</span>,<span class="hljs-number">4</span>,<span class="hljs-number">0.5</span>))<br>plt.xlabel(<span class="hljs-string">"x1"</span>)<br>plt.ylabel(<span class="hljs-string">"x2"</span>)<br>plt.legend()<br></code></pre></td></tr></tbody></table></figure><p>  得到的分类超平面如下图所示：</p><center><img src="https://s2.loli.net/2023/10/12/8tjLUPw4aqFr29O.png" width="60%" height="60%"><div data-align="center">Image3: 分类超平面</div></center><p>  如果我们改变初始参数的值，我们可能会得到不同的分类超平面，比如我们将初始参数设置为<span class="math inline">\(w=\begin{bmatrix}  1,1 \\\end{bmatrix}^T,b=1\)</span>，则我们最终得到的分类超平面如下图4所示：</p><center><img src="https://s2.loli.net/2023/10/12/Hmcf6eJ13yoFWwh.png" width="60%" height="60%"><div data-align="center">Image4: 分类超平面</div></center><p>  此时，分类超平面的参数为：</p><p><span class="math display">\[w,b = \begin{bmatrix}    -0.2049,-0.0153 \\\end{bmatrix}^T,0.45\]</span></p><h2 id="附录">附录</h2><h3 id="空间中点到超平面的距离">空间中点到超平面的距离</h3><p><strong>结论</strong>   设<span class="math inline">\(n\)</span>维空间中存在某超平面<span class="math inline">\(S=\{x \vert w^{T}x+b=0;x,w \in \mathbb{R}^n,b \in\mathbb{R}\}\)</span>，已知点<span class="math inline">\(x_0 \in\mathbb{R}^{n},x_0 \notin S\)</span>，则点<span class="math inline">\(x_0\)</span>到超平面<span class="math inline">\(S\)</span>的距离为：</p><p><span class="math display">\[d(x_0,S)=\frac{|w^{T}x_0+b|}{||w||}\]</span></p><p><strong>证明:</strong></p><center><img src="https://s2.loli.net/2023/10/11/mKVMIlyDUipd45k.jpg" width="60%" height="60%"><div data-align="center">Image6: 点到超平面的距离</div></center><p>  设<span class="math inline">\(x_1\)</span>为<span class="math inline">\(x_0\)</span>在超平面<span class="math inline">\(S\)</span>上的投影，<span class="math inline">\(x_2\)</span>为<span class="math inline">\(S\)</span>上另外任意一点，向量<span class="math inline">\(\overrightarrow{x_0x_1}\)</span>与向量<span class="math inline">\(\overrightarrow{x_0x_2}\)</span>的夹角为<span class="math inline">\(\theta\)</span>，则有：</p><p><span class="math display">\[d(x_0,S)=||\overrightarrow{x_0x_1}||=||\overrightarrow{x_0x_2}||\cos{\theta}\]</span></p><p>  由向量夹角公式可知：</p><p><span class="math display">\[\cos{\theta}=\frac{\overrightarrow{x_0x_1} \cdot\overrightarrow{x_0x_2}}{||\overrightarrow{x_0x_1}|| \cdot||\overrightarrow{x_0x_2}||}\]</span></p><p>  将其代入到<span class="math inline">\(d(x_0,S)\)</span>表达式中可得：</p><p><span class="math display">\[d(x_0,S)=\frac{\overrightarrow{x_0x_1}\cdot \overrightarrow{x_0x_2}}{||\overrightarrow{x_0x_1}||}\]</span></p><p>  由于<span class="math inline">\(x_1,x_2\)</span>是超平面<span class="math inline">\(S\)</span>上的点，故有：</p><p><span class="math display">\[w^{T}x_1+b=0\]</span></p><p><span class="math display">\[w^{T}x_2+b=0\]</span></p><p><span class="math display">\[\Rightarrow w^{T}(x_1-x_2)=0 \Rightarroww \cdot \overrightarrow{x_2x_1}=0\]</span></p><p>  由于<span class="math inline">\(\overrightarrow{x_0x_1}\)</span>与<span class="math inline">\(\overrightarrow{x_2x_1}\)</span>垂直，因而有：</p><p><span class="math display">\[\overrightarrow{x_0x_1} \cdot\overrightarrow{x_2x_1}=0\]</span></p><p>  由于法向量<span class="math inline">\(w\)</span>与向量<span class="math inline">\(\overrightarrow{x_0x_1}\)</span>均垂直于超平面<span class="math inline">\(S\)</span>，故可设<span class="math inline">\(\overrightarrow{x_0x_1}=kw,k \in\mathbb{R}\)</span>，将其代入<span class="math inline">\(d(x_0,S)\)</span>可得：</p><p><span class="math display">\[d(x_0,S)=\frac{|k| \cdot |w \cdot\overrightarrow{x_0x_2}|}{|k| \cdot ||w||}=\frac{|w \cdot(x_2-x_0)|}{||w||}\]</span></p><p>  由于<span class="math inline">\(w \cdot x_2 + b=0\)</span>可得<span class="math inline">\(w \cdot x_2 = -b\)</span>，代入上式可得：</p><p><span class="math display">\[d(x_0,S)=\frac{|-b-w \cdotx_0|}{||w||}=\frac{|w^{T}x_0+b|}{||w||}\]</span></p><p>  证毕.</p><h3 id="感知机算法收敛性证明">感知机算法收敛性证明</h3><p>  现在证明，对于线性可分数据集，感知机学习算法原始形式收敛，即经过有限次迭代可以得到一个将训练数据集完全正确划分的分离超平面及感知机模型。<br>  为了便于叙述和推导，将偏置<span class="math inline">\(b\)</span>并入权重向量<span class="math inline">\(w\)</span>，记作<span class="math inline">\(\hat{w}=(w^T,b)^T\)</span>，同样也将输入向量<span class="math inline">\(x\)</span>进行扩充，加进常数1，记作<span class="math inline">\(\hat{x}=(x^T,1)^T\)</span>。这样，<span class="math inline">\(\hat{x} \in \mathbb{R}^{n+1},\hat{w} \in\mathbb{R}^{n+1}\)</span>。显然，<span class="math inline">\(\hat{w}^T\hat{x}=w^{T}x+b\)</span>。</p><h4 id="novikoff定理"><span class="math inline">\(Novikoff\)</span>定理</h4><p><strong>结论</strong><br>  设训练数据集<span class="math inline">\(T_{train}=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}\)</span>是线性可分的，其中<span class="math inline">\(x_i \in \mathcal{X} = \mathbb{R}^{n},y_i \in\mathcal{Y} = \{-1,+1\},i=1,2,\dots,N\)</span>，则有：<br>  (1) 存在超平面<span class="math inline">\(S=\{x \vert\hat{w}_{opt}^Tx=w_{opt}^Tx+b_{opt}=0,||\hat{w}_{opt}||=1\}\)</span>可将训练数据集<span class="math inline">\(T_{train}\)</span>完全正确分类；且<span class="math inline">\(\exists \gamma &gt;0\)</span>，对所有的<span class="math inline">\(x_i,i=1,2,\dots,N\)</span>，有</p><p><span class="math display">\[y_i(\hat{w}_{opt}^{T}\hat{x}_i)=y_i(w_{opt}^{T}x_i+b_{opt})\ge \gamma\]</span></p><p>  (2) 令 <span class="math inline">\(R=\max \{ ||\hat{x_i}|| \space\vert i=1,2,\dots,N\}\)</span>，则感知机学习的原始算法在训练数据集<span class="math inline">\(T_{train}\)</span>上的误分类次数<span class="math inline">\(k\)</span>满足不等式：</p><p><span class="math display">\[k \leq \left( \frac{R}{\gamma}\right)^2\]</span></p><p>  <span class="math inline">\(Novikoff\)</span>定理说明感知机学习算法能够经过有限次迭代得到一个可以将训练数据集完全正确分类的超平面，即算法收敛。下面，给出这个定理的证明。</p><p><strong>证明:</strong><br>  首先来证明(1):<br>  由于训练数据集<span class="math inline">\(T_{train}\)</span>是线性可分的，由数据集线性可分的定义可知，一定存在某个超平面能够将训练数据集完全正确分类，不妨设这个超平面为<span class="math inline">\(S=\{x \vert \hat{w}_{opt}^Tx=w_{opt}^Tx+b_{opt}=0,||\hat{w}_{opt}||=1\}\)</span>，则对于训练数据集中的每个实例，若<span class="math inline">\(y_i = +1\)</span>,则<span class="math inline">\(\hat{w}_{opt}^Tx_i &gt; 0\)</span>；若<span class="math inline">\(y_i=-1\)</span>，则<span class="math inline">\(\hat{w}_{opt}^Tx_i &lt;0\)</span>，因此有以下不等式成立：</p><p><span class="math display">\[y_i(\hat{w}_{opt}^{T}\hat{x}_i)=y_i(w_{opt}^{T}x_i+b_{opt})&gt; 0\]</span></p><p>  故存在：</p><p><span class="math display">\[\gamma = \min_{i}\{y_i(w_{opt}^{T}x_i+b_{opt})\}\]</span></p><p>  使得：</p><p><span class="math display">\[y_i(\hat{w}_{opt}^{T}\hat{x}_i)=y_i(w_{opt}^{T}x_i+b_{opt})\ge \gamma\]</span></p><p>  再来证明(2):</p><p>  不妨设感知机算法的初始权重向量<span class="math inline">\(\hat{w}_{0}=0\)</span>，如果发现一个实例被误分类，则更新一次权重。令<span class="math inline">\(\hat{w}_{k-1}\)</span>是发现的第<span class="math inline">\(i\)</span>个误分类实例之前的权重向量，即：</p><p><span class="math display">\[\hat{w}_{k-1}=(w_{k-1}^{T},b_{k-1})^T\]</span></p><p>  判断训练集中的实例<span class="math inline">\((x_i,y_i)\)</span>被误分类的条件为：</p><p><span class="math display">\[y_i(\hat{w}_{opt}^{T}\hat{x}_i)=y_i(w_{opt}^{T}x_i+b_{opt})\leq 0\]</span></p><p>  若实例<span class="math inline">\((x_i,y_i)\)</span>是被<span class="math inline">\(\hat{w}_{k-1}=(w_{k-1}^{T},b_{k-1})^T\)</span>误分类的数据，则<span class="math inline">\(w\)</span>和<span class="math inline">\(b\)</span>的更新过程为：</p><p><span class="math display">\[w_{k} \leftarrow w_{k-1} + \etay_ix_i\]</span></p><p><span class="math display">\[b_{k} \leftarrow b_{k-1} + \etay_i\]</span></p><p>  则：</p><p><span class="math display">\[\hat{w}_{k}= \hat{w}_{k-1}+\eta y_i\hat{x}_i\]</span></p><p>  由此可得：</p><p><span class="math display">\[\begin{align*}    \hat{w}_{k} \cdot \hat{w}_{opt} &amp;= \hat{w}_{k-1} \cdot\hat{w}_{opt}+\eta (y_i \hat{w}_{opt}^T \hat{x}_i) \\    &amp; \ge \hat{w}_{k-1} \cdot \hat{w}_{opt} + \eta \gamma  \\    &amp; \ge \hat{w}_{k-2} \cdot \hat{w}_{opt} + 2\eta \gamma  \\      &amp; \space \vdots  \\    &amp; \ge k \eta \gamma \\\end{align*}\]</span></p><p>  同时：</p><p><span class="math display">\[\begin{align*}    ||\hat{w}_{k}||^{2} &amp;= ||\hat{w}_{k-1}+\eta y_i \hat{x}_i||^2 \\    &amp;= ||\hat{w}_{k-1}||^2 + 2 \eta (y_i \hat{w}_{k-1}^T\hat{x}_i)+\eta^2 ||\hat{x}_i||^2 \\    &amp; \leq ||\hat{w}_{k-1}||^{2} + \eta^2||\hat{x_i}||^2 \\    &amp; \leq ||\hat{w}_{k-1}||^{2} + \eta^2R^2 \\    &amp; \leq ||\hat{w}_{k-2}||^{2} + 2\eta^2R^2 \\      &amp; \space \vdots \\    &amp; \leq k \eta^2R^2\end{align*}\]</span></p><p>  由内积不等式可知：</p><p><span class="math display">\[k \eta \gamma \leq \hat{w}_k \cdot\hat{w}_{opt} \leq ||\hat{w}_k||\space ||\hat{w}_{opt}|| \leq \sqrt{k}\eta R\]</span></p><p><span class="math display">\[\Rightarrow k \leq \left(\frac{R}{\gamma} \right)^2\]</span></p><p>  证毕.</p><h2 id="参考">参考</h2><p><strong>[1] Book: 李航.(2019).统计学习方法</strong><br><strong>[2] Video: bilibili,简博士,感知机系列</strong><br><strong>[3] Blog: 知乎,夜魔刀,点到超平面距离的公式推导</strong></p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>矩阵分析-5.线性映射</title>
    <link href="/2023/10/04/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-5-%E7%BA%BF%E6%80%A7%E6%98%A0%E5%B0%84/"/>
    <url>/2023/10/04/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-5-%E7%BA%BF%E6%80%A7%E6%98%A0%E5%B0%84/</url>
    
    <content type="html"><![CDATA[<h1 id="线性映射">线性映射</h1><h2 id="线性映射的定义">线性映射的定义</h2><p>  设<span class="math inline">\(V_1,V_2\)</span>是数域<span class="math inline">\(\mathbb{F}\)</span>上的线性空间，有映射 <span class="math inline">\(\sigma: V_1 \rightarrow V_2\)</span>，如果<span class="math inline">\(\sigma\)</span>满足：<br>(1) <strong>加法关系:</strong> 对<span class="math inline">\(\foralle_1,e_2 \in V_1, \sigma(e_1+e_2) = \sigma(e_1)+\sigma(e_2) \inV_2\)</span>.<br>(2) <strong>数乘关系:</strong> d对<span class="math inline">\(\forall e\in V_1, k \in \mathbb{F}, \sigma(ek)=\sigma(e)k\)</span><br>则称映射<span class="math inline">\(\sigma\)</span>是<span class="math inline">\(V_1\)</span>到<span class="math inline">\(V_2\)</span>的<strong>线性映射</strong>。特别地，若有<span class="math inline">\(V_1=V_2=V\)</span>，则称映射<span class="math inline">\(\sigma\)</span>为<span class="math inline">\(V\)</span>上的<strong>线性变换</strong>。</p><p><strong>注:</strong> 若线性映射<span class="math inline">\(\sigma:V_1 \rightarrow V_2\)</span> 是可逆映射(一一映射)，则称<span class="math inline">\(\sigma\)</span>为<strong>线性同构</strong>。</p><h2 id="线性映射的实例">线性映射的实例</h2><h3 id="例1-线性与非线性映射">例1 线性与非线性映射</h3><p><strong>非线性映射的实例</strong>   设线性空间<span class="math inline">\(V_1,V_2=\mathbb{R}^2\)</span>，有映射：</p><p><span class="math display">\[\mathcal{A}: \begin{bmatrix}    x_1 \\    x_2 \\\end{bmatrix} \mapsto \begin{bmatrix}    x_1+x_2 \\    x_1x_2 \\\end{bmatrix}\]</span></p><p>则映射<span class="math inline">\(\mathcal{A}\)</span>为<span class="math inline">\(V_1\)</span>到<span class="math inline">\(V_2\)</span>的非线性映射。<br><strong>证明:</strong><br>  取<span class="math inline">\(e_1,e_2 \in V_1\)</span>，其中:</p><p><span class="math display">\[e_1=e_2=\begin{bmatrix}    1 \\    1 \\\end{bmatrix},e_1+e_2=\begin{bmatrix}    2 \\    2 \\\end{bmatrix}\]</span></p><p><span class="math display">\[\mathcal{A}(e_1+e_2)=\begin{bmatrix}    2+2 \\    2 \times 2 \\\end{bmatrix}=\begin{bmatrix}    4 \\    4 \\\end{bmatrix}\]</span></p><p><span class="math display">\[\mathcal{A}(e_1)+\mathcal{A}(e_1)=\begin{bmatrix}    1+1 \\    1 \times 1 \\\end{bmatrix}+\begin{bmatrix}    1+1 \\    1 \times 1 \\\end{bmatrix}=\begin{bmatrix}    4 \\    2 \\\end{bmatrix}\]</span></p><p>  <span class="math inline">\(\because \mathcal{A}(e_1+e_2) \ne\mathcal{A}(e_1)+\mathcal{A}(e_1)\)</span>，故映射<span class="math inline">\(\mathcal{A}\)</span>不满足加法关系，其不是<span class="math inline">\(V_1\)</span>到<span class="math inline">\(V_2\)</span>上的线性映射。</p><p><strong>线性映射的实例</strong><br>  设线性空间<span class="math inline">\(V_1=\mathbb{R}^3,V_2=\mathbb{R}^2\)</span>，有映射：</p><p><span class="math display">\[\mathcal{B}: \begin{bmatrix}    x_1 \\    x_2 \\    x_3 \\\end{bmatrix} \mapsto \begin{bmatrix}    x_1+x_2-x_3 \\    \frac{1}{2}x_1-3x_2\end{bmatrix}\]</span></p><p>则线性映射<span class="math inline">\(\mathcal{B}\)</span>为<span class="math inline">\(V_1\)</span>到<span class="math inline">\(V_2\)</span>上的线性映射。<br><strong>证明:</strong><br>  (1) 先验证映射<span class="math inline">\(\mathcal{B}\)</span>满足线性映射的加法关系:<br>  设<span class="math inline">\(\forall \alpha,\beta \in V_1, \alpha =\begin{bmatrix}  \alpha_1 ,\alpha_2,\alpha_3 \end{bmatrix}^T,\beta =\begin{bmatrix}  \beta_1,\beta_2,\beta_3 \end{bmatrix}^T\)</span></p><p><span class="math display">\[\mathcal{B}(\alpha)=\begin{bmatrix}    \alpha_1+\alpha_2-\alpha_3 \\    \frac{1}{2}\alpha_1-3\alpha_2 \\\end{bmatrix},\mathcal{B}(\beta)=\begin{bmatrix}    \beta_1+\beta_2-\beta_3 \\    \frac{1}{2}\beta_1-3\beta_2 \\\end{bmatrix}\]</span></p><p><span class="math display">\[\mathcal{B}(\alpha)+\mathcal{B}(\beta)=\begin{bmatrix}    \alpha_1+\beta_1+\alpha_2+\beta_2-(\alpha_3+\beta_3) \\    \frac{1}{2}(\alpha_1+\beta_1)-3(\alpha_2+\beta_2) \\\end{bmatrix}\]</span></p><p><span class="math display">\[\alpha+\beta=\begin{bmatrix}    \alpha_1+\beta_1 \\    \alpha_2+\beta_2 \\    \alpha_3+\beta_3 \\\end{bmatrix},\mathcal{B}(\alpha+\beta)=\begin{bmatrix}    \alpha_1+\beta_1+\alpha_2+\beta_2-(\alpha_3+\beta_3) \\    \frac{1}{2}(\alpha_1+\beta_1)-3(\alpha_2+\beta_2) \\\end{bmatrix}\]</span></p><p><span class="math display">\[\Rightarrow\mathcal{B}(\alpha+\beta)=\mathcal{B}(\alpha)+\mathcal{B}(\beta)\]</span></p><p>  故映射<span class="math inline">\(\mathcal{B}\)</span>满足线性映射的加法关系。<br>  (2) 再验证映射<span class="math inline">\(\mathcal{B}\)</span>满足线性映射的数乘关系:<br>  设<span class="math inline">\(\forall \alpha \in V_1,k \in\mathbb{F},\alpha = \begin{bmatrix}  \alpha_1 ,\alpha_2,\alpha_3 \\\end{bmatrix}^T\)</span>,有:</p><p><span class="math display">\[\alpha k = \begin{bmatrix}    \alpha_1 k \\    \alpha_2 k \\    \alpha_3 k \\\end{bmatrix}, \mathcal{B}(\alpha k)=\begin{bmatrix}    \alpha_1 k+\alpha_2 k-\alpha_3 k \\    \frac{1}{2}\alpha_1 k-3\alpha_2 k \\\end{bmatrix}=\begin{bmatrix}    (\alpha_1+\alpha_2-\alpha_3)k \\    (\frac{1}{2}\alpha_1-3\alpha_2) k \\\end{bmatrix}\]</span></p><p><span class="math display">\[\mathcal{B}(\alpha)=\begin{bmatrix}    \alpha_1+\alpha_2-\alpha_3 \\    \frac{1}{2}\alpha_1-3\alpha_2 \\\end{bmatrix},\mathcal{B}(\alpha)k=\begin{bmatrix}    (\alpha_1+\alpha_2-\alpha_3)k \\    (\frac{1}{2}\alpha_1-3\alpha_2) k \\\end{bmatrix}\]</span></p><p><span class="math display">\[\Rightarrow \mathcal{B}(\alphak)=\mathcal{B}(\alpha)k\]</span></p><p>  故映射<span class="math inline">\(\mathcal{B}\)</span>满足线性映射的数乘关系。<br>  综上所述，映射<span class="math inline">\(\mathcal{B}\)</span>为<span class="math inline">\(V_1\)</span>到<span class="math inline">\(V_2\)</span>上的线性映射。</p><h3 id="例2-矩阵与标准线性空间之间的线性映射的等同性">例2矩阵与标准线性空间之间的线性映射的等同性</h3><p>  给定矩阵 <span class="math inline">\(A \in \mathbb{F}^{m \times n},x \in \mathbb{F}^n\)</span>，则矩阵<span class="math inline">\(A\)</span>可以作为线性映射<span class="math inline">\(\sigma_{A}\)</span>：</p><p><span class="math display">\[\begin{align*}    \sigma_{A}: &amp;\mathbb{F}^{n} \rightarrow \mathbb{F}^{m} \\    &amp;x \mapsto y=Ax\end{align*}\]</span></p><p>  若我们已知有线性映射 <span class="math inline">\(\sigma:\mathbb{F}^{n} \rightarrow\mathbb{F}^{m}\)</span>，如例1中的线性映射<span class="math inline">\(\mathcal{B}\)</span>，能否求得相应的矩阵<span class="math inline">\(A\)</span>?<br><strong>解:</strong><br>  记标准线性空间<span class="math inline">\(\mathbb{F}^n\)</span>的标准基为：<span class="math inline">\(e_1,e_2,\dots,e_n\)</span>，可以构造矩阵：</p><p><span class="math display">\[\sigma(\begin{bmatrix}    e_1,e_2,\dots,e_n \\\end{bmatrix})=\begin{bmatrix}    \sigma(e_1),\sigma(e_2),\dots,\sigma(e_n) \\\end{bmatrix} \triangleq A \in \mathbb{F}^{n \times m}\]</span></p><p>  对<span class="math inline">\(\forall x \in\mathbb{F}^{n}\)</span>，将<span class="math inline">\(x\)</span>沿着标准基展开：</p><p><span class="math display">\[x = \begin{bmatrix}    e_1,e_2,\dots,e_n \\\end{bmatrix}x=e_1x_1+e_2x_2,\dots,e_nx_n\]</span></p><p><span class="math display">\[\begin{align*}    \sigma(x)&amp;=\sigma(e_1x_1+e_2x_2,\dots,e_nx_n)=\sigma(e_1)x_1+\sigma(e_2)x_2+\dots+\sigma(e_n)x_n\\    &amp;= \begin{bmatrix}    \sigma(e_1),\sigma(e_2),\dots,\sigma(e_n) \\\end{bmatrix}x=Ax\end{align*}\]</span></p><p>  因此矩阵<span class="math inline">\(A\)</span>与线性映射<span class="math inline">\(\sigma\)</span>具有等同性.</p><h2 id="线性映射的矩阵表示">线性映射的矩阵表示</h2><h3 id="定义">定义</h3><p>  设有标准线性空间<span class="math inline">\(V=\mathbb{F}^n,W=\mathbb{F}^m\)</span>，给定线性映射：</p><p><span class="math display">\[\begin{align*}    \sigma: &amp;V \rightarrow W \\    &amp; v \mapsto w\end{align*}\]</span></p><p>  选取<span class="math inline">\(V\)</span>的基向量<span class="math inline">\(v_1,v_2,\dots,v_n\)</span>，作为<strong>入口基</strong>，<span class="math inline">\(W\)</span>的基向量<span class="math inline">\(w_1,w_2,\dots,w_m\)</span>作为<strong>出口基</strong>，记<span class="math inline">\(V\)</span>中第<span class="math inline">\(j\)</span>个入口基向量<span class="math inline">\(v_j\)</span>在<span class="math inline">\(W\)</span>中的象<span class="math inline">\(\sigma(v_j)\)</span>在出口基下的坐标表示为<span class="math inline">\(a_j = \begin{bmatrix}  a_{1j},a_{2j},\dots,a_{mj}\\ \end{bmatrix}^T\)</span>，即：</p><p><span class="math display">\[\sigma(v_j)=\begin{bmatrix}    w_1,w_2,\dots,w_n \\\end{bmatrix}\begin{bmatrix}    a_{1j} \\    a_{2j} \\    \vdots \\    a_{mj} \\\end{bmatrix}\]</span></p><p>  将所有入口基的象在出口基下的坐标拼成矩阵<span class="math inline">\(A\)</span>:</p><p><span class="math display">\[A=\begin{bmatrix}    \sigma(v_1),\sigma(v_2),\dots,\sigma(v_n) \\\end{bmatrix}=\begin{bmatrix}    a_{11} &amp; a_{21} &amp; \dots &amp; a_{1n} \\    a_{21} &amp; a_{22} &amp; \dots &amp; a_{2n} \\    \vdots &amp; \vdots &amp;  &amp; \vdots \\    a_{m1} &amp; a_{m2} &amp; \dots &amp; a_{mn} \\\end{bmatrix}\]</span></p><p>  则有：</p><p><span class="math display">\[\sigma(\begin{bmatrix}    v_1,v_2,\dots,v_n\end{bmatrix})=\begin{bmatrix}    w_1,w_2,\dots,w_m\end{bmatrix}A\]</span></p><p>  则称矩阵<span class="math inline">\(A\)</span>为线性映射<span class="math inline">\(\sigma\)</span>在入口基<span class="math inline">\(v_i,i=1,\dots,n\)</span>和出口基<span class="math inline">\(w_j,j=1,\dots,m\)</span>下的矩阵表示。</p><p><strong>注:</strong></p><p><span class="math display">\[\color{green} \begin{bmatrix}    线性 \\    映射 \\\end{bmatrix}\begin{bmatrix}    入口基 \\    矩阵  \\\end{bmatrix}=\begin{bmatrix}    出口基 \\    矩阵  \\\end{bmatrix}\begin{bmatrix}    线性映射 \\    矩阵表示  \\\end{bmatrix}\]</span></p><h3 id="定理线性映射的坐标计算">定理(线性映射的坐标计算)</h3><p>  设有<span class="math inline">\(n\)</span>维线性空间<span class="math inline">\(V\)</span>和<span class="math inline">\(m\)</span>维线性空间<span class="math inline">\(W\)</span>，<span class="math inline">\(v_1,v_2,\dots,v_n\)</span>为<span class="math inline">\(V\)</span>的一组基向量，<span class="math inline">\(w_1,w_2,\dots,w_m\)</span>为<span class="math inline">\(W\)</span>的一组基向量，映射<span class="math inline">\(\sigma\)</span>为<span class="math inline">\(V\)</span>到<span class="math inline">\(W\)</span>上的线性映射，矩阵<span class="math inline">\(A\)</span>为线性映射<span class="math inline">\(\sigma\)</span>在入口基<span class="math inline">\(\{v_i \vert i=1,2,\dots,n\}\)</span>与出口基<span class="math inline">\(\{w_i \verti=1,2,\dots,m\}\)</span>下的矩阵表示。<br>  给定<span class="math inline">\(\forall v \inV\)</span>，其沿着基向量组展开的坐标为<span class="math inline">\(x\)</span>，即：</p><p><span class="math display">\[v=\begin{bmatrix}    v_1,v_2,\dots,v_n \\\end{bmatrix}x=\begin{bmatrix}    v_1,v_2,\dots,v_n \\\end{bmatrix}\begin{bmatrix}    x_1 \\    x_2 \\    \vdots \\    x_m \\\end{bmatrix}\]</span></p><p>则经过线性映射<span class="math inline">\(\sigma\)</span>后，<span class="math inline">\(\sigma(v) \inW\)</span>在沿着基向量组展开的坐标为<span class="math inline">\(Ax\)</span>，即：</p><p><span class="math display">\[\sigma(v) = \begin{bmatrix}    w_1,w_2,\dots,w_m \\\end{bmatrix}Ax\]</span></p><p><strong>证明:</strong><br>  由题意可知：</p><p><span class="math display">\[v=\begin{bmatrix}    v_1,v_2,\dots,v_n \\\end{bmatrix}\begin{bmatrix}    x_1 \\    x_2 \\    \vdots \\    x_m \\\end{bmatrix}=v_1x_1+v_2x_2+\dots+v_nx_n\]</span></p><p>  则有</p><p><span class="math display">\[\begin{align*}    \sigma(v)&amp;=\sigma(v_1x_1+v_2x_2+\dots+v_nx_n)=\sigma(v_1)x_1+\sigma(v_2)x_2+\dots+\sigma(v_n)x_n\\    &amp;=\begin{bmatrix}        \sigma(v_1),\sigma(v_2),\dots,\sigma(v_n) \\    \end{bmatrix}=(\begin{bmatrix}        w_1,w_2,\dots,w_m    \end{bmatrix}A)x \\    &amp;=\begin{bmatrix}        w_1,w_2,\dots,w_m    \end{bmatrix}(Ax)\end{align*}\]</span></p><p>  故<span class="math inline">\(\sigma(v)\)</span>沿着基向量组<span class="math inline">\(\{w_i \verti=1,2,\dots,m\}\)</span>展开后的坐标为<span class="math inline">\(Ax\)</span>.</p><p><span style="color: green;">结论：基向量组将抽象的线性空间映射为标准线性空间，在基向量组的表示下，原本抽象线性空间之间的线性映射可以被表示为具体的矩阵。</span></p><center><img src="https://s2.loli.net/2023/10/05/GTHiFnjpR4v2CQr.jpg    " width="60%" height="60%"><div data-align="center">Image1: 线性映射的矩阵表示</div></center><h2 id="线性映射矩阵表示的实例">线性映射矩阵表示的实例</h2><h3 id="例1-微分算子的矩阵表示">例1 微分算子的矩阵表示</h3><p>  设线性空间<span class="math inline">\(V=\mathbb{R}_{4}[x],W=\mathbb{R}_{3}[x]\)</span>(<span class="math inline">\(\mathbb{R}_{n}[x]\)</span>表示<span class="math inline">\(n\)</span>维多项式函数空间)，微分算子<span class="math inline">\(\sigma\)</span>为<span class="math inline">\(V\)</span>到<span class="math inline">\(W\)</span>上的线性映射，求<span class="math inline">\(\sigma\)</span>在标准基向量组下的矩阵表示.</p><p><strong>解:</strong><br>  <span class="math inline">\(V\)</span>的标准基向量组为：<span class="math inline">\(\{1,x,x^2,x^3\}\)</span>(入口基)，<span class="math inline">\(W\)</span>的标准基向量组为<span class="math inline">\(\{1,x,x^2\}\)</span>，则有：</p><p><span class="math display">\[\begin{align*}    \sigma(\begin{bmatrix}        1,x,x^2,x^3 \\    \end{bmatrix})&amp;=\begin{bmatrix}        \sigma(1),\sigma(x),\sigma(x^2),\sigma(x^3) \\    \end{bmatrix} \\    &amp;=\begin{bmatrix}        0,1,2x,3x^2 \\    \end{bmatrix}=\begin{bmatrix}        1,x,x^2 \\    \end{bmatrix}A\end{align*}\]</span></p><p><span class="math display">\[A=\begin{bmatrix}    0 &amp; 1 &amp; 0 &amp; 0 \\    0 &amp; 0 &amp; 2 &amp; 0 \\    0 &amp; 0 &amp; 0 &amp; 3 \\\end{bmatrix}\]</span></p><p>  故矩阵<span class="math inline">\(A\)</span>即为微分算子<span class="math inline">\(\sigma\)</span>在<span class="math inline">\(V\)</span>与<span class="math inline">\(W\)</span>的标准基向量组下的矩阵表示.</p><p><strong>应用:</strong></p><p>  <span class="math inline">\(v = \frac{1}{2}x^3+5x \inV\)</span>，将<span class="math inline">\(v\)</span>沿着<span class="math inline">\(V\)</span>的标准基向量组<span class="math inline">\(\{1,x,x^2,x^3\}\)</span>展开得其坐标:</p><p><span class="math display">\[v=\frac{1}{2}x^3+5x=\begin{bmatrix}    1,x,x^2,x^3 \\\end{bmatrix}\begin{bmatrix}    0 \\    5 \\    0 \\    \frac{1}{2} \\\end{bmatrix}\]</span></p><p>  则<span class="math inline">\(\sigma(v)\)</span>在<span class="math inline">\(W\)</span>的标准基向量组<span class="math inline">\(\{1,x,x^2\}\)</span>下的坐标表示为:</p><p><span class="math display">\[A\begin{bmatrix}    0 \\    5 \\    0 \\    \frac{1}{2} \\\end{bmatrix}=\begin{bmatrix}    0 &amp; 1 &amp; 0 &amp; 0 \\    0 &amp; 0 &amp; 2 &amp; 0 \\    0 &amp; 0 &amp; 0 &amp; 3 \\\end{bmatrix}\begin{bmatrix}    0 \\    5 \\    0 \\    \frac{1}{2} \\\end{bmatrix}=\begin{bmatrix}    5 \\    0 \\    \frac{3}{2} \\\end{bmatrix}\]</span></p><p>  故对<span class="math inline">\(v\)</span>求微分的结果为：</p><p><span class="math display">\[\sigma(v)=\begin{bmatrix}    1,x,x^2,x^3 \\\end{bmatrix}\begin{bmatrix}    5 \\    0 \\    \frac{3}{2} \\\end{bmatrix}=\frac{3}{2}x^2+5\]</span></p><h3 id="例2-旋转变换的矩阵表示">例2 旋转变换的矩阵表示</h3><p>  欧几里得空间中的某一物体绕固定轴旋转<span class="math inline">\(\theta\)</span>，求该变换的矩阵表示。<br><strong>解:</strong><br>  设<span class="math inline">\(V=W\)</span>为欧几里得空间，映射<span class="math inline">\(\sigma\)</span>为旋转变换，易知<span class="math inline">\(\sigma\)</span>为线性变换.  设欧几里得空间中的一组标准正交基向量为<span class="math inline">\(e_1,e_2,e_3\)</span>，</p><p><span class="math display">\[e_1=\begin{bmatrix}    1 \\    0 \\    0 \\\end{bmatrix}, e_2=\begin{bmatrix}    0 \\    1 \\    0 \\\end{bmatrix}, e_3=\begin{bmatrix}    0 \\    0 \\    1 \\\end{bmatrix}\]</span></p><p>  其中<span class="math inline">\(e_3\)</span>为旋转变换所固定的轴，则<span class="math inline">\(e_1,e_2\)</span>为与旋转轴所垂直的平面的一组正交基，可以<span class="math inline">\(e_1,e_2,e_3\)</span>的方向为<span class="math inline">\(x,y,z\)</span>轴建立坐标系. 向量组<span class="math inline">\(e_1,e_2,e_3\)</span>既为入口基也为出口基.  旋转变换<span class="math inline">\(\sigma\)</span>作用于基向量组<span class="math inline">\(e_1,e_2,e_3\)</span>时，<span class="math inline">\(e_3\)</span>并不会改变，<span class="math inline">\(e_1,e_2\)</span>在其所在的平面上旋转<span class="math inline">\(\theta\)</span>，旋转变换可用图2表示。</p><center><img src="https://s2.loli.net/2023/10/05/hU9ixmNgfXRtWze.jpg    " width="40%" height="40%"><div data-align="center">Image2: 旋转变换</div></center><p>  由几何知识可得：</p><p><span class="math display">\[\sigma(e_1)=\begin{bmatrix}    cos\theta \\    sin\theta \\    0 \\\end{bmatrix},\sigma(e_2)=\begin{bmatrix}    -sin\theta \\    cos\theta \\    0 \\\end{bmatrix},\sigma(e_3)=\begin{bmatrix}    0 \\    0 \\    1 \\\end{bmatrix}\]</span></p><p>  故有:</p><p><span class="math display">\[\begin{align*}    \sigma(\begin{bmatrix}    e_1,e_2,e_3 \\\end{bmatrix})&amp;=\begin{bmatrix}    \sigma(e_1),\sigma(e_2),\sigma(e_3)\end{bmatrix}=\begin{bmatrix}    cos\theta &amp; -sin\theta &amp; 0 \\    sin\theta &amp; cos\theta &amp; 0 \\    0 &amp; 0 &amp; 1 \\\end{bmatrix} \\    &amp;=\begin{bmatrix}        1 &amp; 0 &amp; 0 \\        0 &amp; 1 &amp; 0 \\        0 &amp; 0 &amp; 1 \\    \end{bmatrix}\begin{bmatrix}    cos\theta &amp; -sin\theta &amp; 0 \\    sin\theta &amp; cos\theta &amp; 0 \\    0 &amp; 0 &amp; 1 \\\end{bmatrix}=\begin{bmatrix}    e_1,e_2,e_3 \\\end{bmatrix}A\end{align*}\]</span></p><p><span class="math display">\[A=\begin{bmatrix}    cos\theta &amp; -sin\theta &amp; 0 \\    sin\theta &amp; cos\theta &amp; 0 \\    0 &amp; 0 &amp; 1 \\\end{bmatrix}\]</span></p><p>  矩阵<span class="math inline">\(A\)</span>即为欧几里得空间中旋转变换<span class="math inline">\(\sigma\)</span>在标准正交基下的矩阵表示.</p><h3 id="例3-镜面反射的矩阵表示">例3 镜面反射的矩阵表示</h3><p>  欧几里得空间中的某一物体对固定平面进行镜面反射，求该变换的矩阵表示.<br><strong>解:</strong><br>  设<span class="math inline">\(V=M\)</span>为欧几里得空间，映射<span class="math inline">\(\sigma\)</span>为镜面反射，易知<span class="math inline">\(\sigma\)</span>为线性变换.  设欧几里得空间中的一组标准正交基向量为<span class="math inline">\(e_1,e_2,e_3\)</span>，</p><p><span class="math display">\[e_1=\begin{bmatrix}    1 \\    0 \\    0 \\\end{bmatrix}, e_2=\begin{bmatrix}    0 \\    1 \\    0 \\\end{bmatrix}, e_3=\begin{bmatrix}    0 \\    0 \\    1 \\\end{bmatrix}\]</span></p><p>  其中，<span class="math inline">\(e_1,e_2\)</span>所在的平面即为进行镜面反射所依赖的平面，<span class="math inline">\(e_3\)</span>为垂直于该平面的一个基向量。可以<span class="math inline">\(e_1,e_2,e_3\)</span>的方向为<span class="math inline">\(x,y,z\)</span>轴建立坐标系. 向量组<span class="math inline">\(e_1,e_2,e_3\)</span>既为入口基也为出口基.<br>  镜面反射<span class="math inline">\(\sigma\)</span>作用于基向量组<span class="math inline">\(e_1,e_2,e_3\)</span>时，<span class="math inline">\(e_1,e_2\)</span>并不会改变，<span class="math inline">\(e_3\)</span>变为相反方向，镜面反射可用图3表示。</p><center><img src="https://s2.loli.net/2023/10/05/oSeUqtnj5I74Kyf.jpg    " width="40%" height="40%"><div data-align="center">Image3: 镜面反射</div></center><p>  由几何知识可知：</p><p><span class="math display">\[\sigma(e_1)=\begin{bmatrix}    1 \\    0 \\    0 \\\end{bmatrix},\sigma(e_2)=\begin{bmatrix}    0 \\    1 \\    0 \\\end{bmatrix},\sigma(e_3)=\begin{bmatrix}    0 \\    0 \\    -1 \\\end{bmatrix}\]</span></p><p>  故有:</p><p><span class="math display">\[\begin{align*}    \sigma(\begin{bmatrix}    e_1,e_2,e_3 \\\end{bmatrix})&amp;=\begin{bmatrix}    \sigma(e_1),\sigma(e_2),\sigma(e_3)\end{bmatrix}=\begin{bmatrix}    1 &amp; 0 &amp; 0 \\    0 &amp; 1 &amp; 0 \\    0 &amp; 0 &amp; -1 \\\end{bmatrix} \\    &amp;=\begin{bmatrix}        1 &amp; 0 &amp; 0 \\        0 &amp; 1 &amp; 0 \\        0 &amp; 0 &amp; 1 \\    \end{bmatrix}\begin{bmatrix}    1 &amp; 0 &amp; 0 \\    0 &amp; 1 &amp; 0 \\    0 &amp; 0 &amp; -1 \\\end{bmatrix}=\begin{bmatrix}    e_1,e_2,e_3 \\\end{bmatrix}A\end{align*}\]</span></p><p><span class="math display">\[A=\begin{bmatrix}    1 &amp; 0 &amp; 0 \\    0 &amp; 1 &amp; 0 \\    0 &amp; 0 &amp; -1 \\\end{bmatrix}\]</span></p><p>  矩阵<span class="math inline">\(A\)</span>即为欧几里得空间中镜面反射<span class="math inline">\(\sigma\)</span>在标准正交基下的矩阵表示.</p>]]></content>
    
    
    <categories>
      
      <category>矩阵分析</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>机器学习-1.概论</title>
    <link href="/2023/09/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-1-%E6%A6%82%E8%AE%BA/"/>
    <url>/2023/09/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-1-%E6%A6%82%E8%AE%BA/</url>
    
    <content type="html"><![CDATA[<h1 id="概论">概论</h1><h2 id="机器学习">机器学习</h2><p><strong>定义</strong><br>  机器学习(MachineLearning)是关于计算机基于数据构建模型并运用模型对数据进行预测与分析的一门学科。</p><p><strong>特点</strong><br>  机器学习的主要特点是:<br>  (1)机器学习以计算机及网络为<strong>平台</strong>，是建立在计算机网络上的。<br>  (2)机器学习以数据为<strong>研究对象</strong>，是数据驱动的学科。<br>  (3) 机器学习的<strong>目的</strong>是对数据进行预测与分析。<br>  (4)机器学习以<strong>方法</strong>为中心，机器学习方法构建模型并应用模型进行预测与分析。<br>  (5)机器学习是微积分、线性代数、概率论、统计学、信息论、数值计算、最优化及计算机科学等多个领域的交叉<strong>学科</strong>，并且在发展中逐步形成独自的理论体系与方法论。</p><p><strong>步骤</strong><br>  实现机器学习方法的步骤：<br>  <strong>(1) 得到一个有限的训练数据集合；</strong><br>  <strong>(2)确定包含所有可能的模型的假设空间，即学习模型的集合；</strong><br>  <strong>(3) 确定模型选择的准则，即学习的策略；</strong><br>  <strong>(4) 实现求解最优模型的算法，即学习的算法；</strong><br>  <strong>(5) 通过学习方法选择最优模型；</strong><br>  <strong>(6) 利用学习的最优模型对新数据进行预测与分析。</strong></p><h2 id="机器学习的分类">机器学习的分类</h2><p>  我们可以从多个方面对机器学习方法进行分类。</p><p>  (1) 基本分类</p><ul><li><strong>监督学习(Supervised Learning):</strong>从标注数据中学习预测模型的机器学习问题。标注数据表示输入到输出的对应关系，预测模型对给定的输入产生相应的输出。监督学习的本质是学习输入到输出的映射的统计规律。<ul><li><strong>监督学习的实例:</strong>线性回归、逻辑回归、决策树、支持向量机、神经网络等。<br></li></ul></li><li><strong>无监督学习(Unsupervised Learning):</strong>从无标注数据中学习预测模型的机器学习问题。无标注数据是自然得到的数据，预测模型表示数据的类别、转换或概率。无监督学习的本质是学习数据中的统计规律或潜在结构。<ul><li><strong>无监督学习的实例:</strong>聚类分析、主成分分析、高斯混合模型、自编码等。</li></ul></li><li><strong>强化学习(Reinforcement Learning):</strong>指智能系统在与环境的连续互动中学习最优行为策略的机器学习问题。假设智能系统与环境的互动基于马尔可夫决策过程(Markovdecisionprocess)，智能系统能观测到的是与环境互动得到的数据序列。强化学习的本质是学习最优的序贯决策。<ul><li><strong>强化学习的实例:</strong>Q-学习、深度Q网络、蒙特卡洛树搜索等。</li></ul></li></ul><p>  (2) 按模型分类</p><ul><li><strong>概率模型与非概率模型</strong><ul><li><strong>概率模型:</strong> 模型的形式为条件概率分布<span class="math inline">\(P(y\vert x)\)</span>.<ul><li><strong>概率模型的实例:</strong>决策树、朴素贝叶斯、隐马尔可夫模型、条件随机场、概率图模型等。</li></ul></li><li><strong>非概率模型:</strong> 模型的形式为决策函数<span class="math inline">\(y = f(x)\)</span>.<ul><li><strong>非概率模型的实例:</strong>感知机、支持向量机、AdaBoost、神经网络等。</li></ul></li></ul></li><li><strong>线性模型与非线性模型</strong><ul><li><strong>线性模型:</strong> 非概率模型中，决策函数<span class="math inline">\(y=f(x)\)</span>是线性函数的模型。<ul><li><strong>线性模型的实例:</strong>感知机、线性支持向量机、k近邻、潜在语义分析等。<br></li></ul></li><li><strong>非线性模型:</strong> 非概率模型中，决策函数<span class="math inline">\(y=f(x)\)</span>是非线性函数的模型。<ul><li><strong>非线性模型的实例:</strong>核函数支持向量机、AdaBoost、神经网络等。<br></li></ul></li></ul></li><li><strong>参数化模型与非参数化模型</strong><ul><li><strong>参数化模型:</strong>假设模型参数的维数是固定的，模型可以由有限维参数完全刻画。<ul><li><strong>参数化模型的实例:</strong>感知机、朴素贝叶斯、逻辑回归、高斯混合模型等。</li></ul></li><li><strong>非参数化模型:</strong>假设模型参数的维度不固定或者说是无穷大，随着训练数据量的增加而不断增大。<ul><li><strong>非参数化模型的实例:</strong>决策树、支持向量机、AdaBoost、k近邻等。</li></ul></li></ul></li></ul><p>  (3) 按算法分类</p><ul><li><strong>在线学习(Online Learning):</strong>指每次接受一个样本，进行预测，之后学习模型，并不断重复该操作的机器学习。<ul><li><strong>在线学习的实例:</strong>在线线性回归、在线逻辑回归、在线决策树、在线神经网络等。<br></li></ul></li><li><strong>批量学习(Batch Learning):</strong>一次性接受所有数据，学习模型，之后进行预测。<ul><li><strong>批量学习的实例:</strong>线性回归、逻辑回归、决策树、神经网络、支持向量机等。</li></ul></li></ul><p>  在监督学习方法又可以生成方法与判别方法：</p><ul><li><strong>生成方法:</strong> 原理上由数据学习联合概率分布<span class="math inline">\(P(X,Y)\)</span>，然后再求出条件概率分布<span class="math inline">\(P(Y \vert X)\)</span>作为预测的模型，即生成模型。<ul><li><strong>生成方法的特点:</strong> 生成方法可以还原出联合概率分布<span class="math inline">\(P(X,Y)\)</span>，而判别方法则不能；生成方法的学习收敛速度更快，即当样本容量增加时，学到的模型可以更块地收敛于真实模型；当存在隐变量时，仍可以用生成方法学习，此时判别方法就不能用；生成模型的输入和输出变量均为随机变量；生成方法所需的数据量较大。<br></li><li><strong>生成模型的实例:</strong>朴素贝叶斯模型、隐马尔可夫模型等。<br></li></ul></li><li><strong>判别方法:</strong> 由数据直接学习决策函数<span class="math inline">\(f(X)\)</span>或条件概率分布<span class="math inline">\(P(Y \vert X)\)</span>作为预测模型，即判别模型。<ul><li><strong>判别方法的特点:</strong>判别方法直接学习的是条件概率分布<span class="math inline">\(P(Y \vertX)\)</span>或决策函数<span class="math inline">\(f(X)\)</span>，直接面对预测，往往学习的准确率更高；由于直接学习<span class="math inline">\(P(Y \vert X)\)</span>和<span class="math inline">\(f(X)\)</span>，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题；判别方法不需要输入和输出变量均为随机变量；判别方法所需的样本量少于生成方法。<br></li><li><strong>判别模型的实例:</strong>感知机、Logistic回归模型、支持向量机、条件随机场等。</li></ul></li></ul><h3 id="基本分类的相关概念">基本分类的相关概念</h3><h4 id="监督学习">监督学习</h4><ul><li><strong>输入空间:</strong> 输入的所有可能取值的集合，记为<span class="math inline">\(\mathcal{X}\)</span>.<br></li><li><strong>输出空间:</strong> 输出的所有可能取值的集合，记为<span class="math inline">\(\mathcal{Y}\)</span>.<br></li><li><strong>实例:</strong> 每一个具体的输入，通常由特征向量表示：<br><span class="math display">\[x_i = \begin{bmatrix}  x_{i}^{(1)}, x_{i}^{(2)}, \dots, x_{i}^{(n)}\end{bmatrix}^T\]</span></li></ul><p>  其中<span class="math inline">\(x_{i}^{(j)}\)</span>表示第<span class="math inline">\(i\)</span>个实例的第<span class="math inline">\(j\)</span>个特征。</p><ul><li><strong>特征空间:</strong>所有特征向量存在的空间称为特征空间，记为<span class="math inline">\(\mathcal{F}\)</span>。<br></li><li><strong>训练集:</strong> 模型用于学习的数据集合，样本容量为<span class="math inline">\(N\)</span>的训练集记为：<br><span class="math display">\[T=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}\]</span></li></ul><p>  输入变量<span class="math inline">\(X\)</span>和输出变量<span class="math inline">\(Y\)</span>有不同的类型，可以是连续的，也可以是离散的。人们根据输入输出变量的不同类型，对预测任务给予不同的名称：</p><ul><li><strong>回归问题:</strong>输入变量与输出变量均为连续变量的预测问题。<br></li><li><strong>分类问题:</strong>输出变量为有限个离散变量的预测问题。<br></li><li><strong>标注问题:</strong>输入变量与输出变量均为变量序列的预测问题。</li></ul><p>  对于监督学习，主要就是研究输入到输出之间的统计规律，所以要有关于输入和输出和模型的基本假设。</p><ul><li><p><strong>监督学习的基本假设:</strong> 输入变量<span class="math inline">\(X\)</span>与输出变量<span class="math inline">\(Y\)</span>具有联合概率分布<span class="math inline">\(P(X,Y)\)</span>。训练数据与测试数据被看作是依联合概率分布<span class="math inline">\(P(X,Y)\)</span>独立同分布产生的。<br></p></li><li><p><strong>监督学习的目的:</strong>学习一个输入空间到输出空间的映射，这一映射以模型的形式表示。<br></p></li><li><p><strong>模型形式:</strong>监督学习的模型可以是概率模型或非概率模型，由条件概率分布<span class="math inline">\(P(Y\vert X)\)</span>或决策函数<span class="math inline">\(Y=f(X)\)</span>表示，随具体学习方法而定。对具体的输入实例进行相应的输出预测时写作<span class="math inline">\(P(y \vert x)\)</span>或决策函数<span class="math inline">\(y=f(x)\)</span>.<br></p></li><li><p><strong>假设空间:</strong> 所有可能的模型的集合，记为<span class="math inline">\(\mathcal{H}\)</span>.</p></li><li><p><strong>监督学习的过程:</strong>监督学习分为学习和预测两个过程，由学习系统和预测系统完成。在学习过程中，学习系统利用给定的训练数据集，通过学习(或训练)得到一个模型，表示为条件概率分布<span class="math inline">\(\hat{P}(Y \vert X)\)</span>或决策函数<span class="math inline">\(Y=\hat{f}(X)\)</span>。条件概率分布<span class="math inline">\(\hat{P}(Y \vert X)\)</span>或决策函数<span class="math inline">\(Y=\hat{f}(X)\)</span>描述输入与输出变量之间的映射关系。在预测过程中，预测系统对于给定的测试样本集中的输入<span class="math inline">\(X_{N+1}\)</span>，由模型<span class="math inline">\(\hat{y}_{N+1}=arg \space max \space \hat{P}(y\vert x_{N+1})\)</span>或<span class="math inline">\(\hat{y}_{N+1}=\hat{f}(x_{N+1})\)</span>给出相应的输出<span class="math inline">\(\hat{y}_{N+1}\)</span>。监督学习的过程可以用图1来描述。</p></li></ul><center><img src="https://s2.loli.net/2023/09/25/6FAu9srNYcwDSkh.jpg" width="60%" height="60%"><div data-align="center">Image1: 监督学习的过程</div></center><h4 id="无监督学习">无监督学习</h4><p>  无监督学习中的大部分概率与监督学习中的一致，不同点主要在于以下几个方面：</p><style>.center {  width: auto;  display: table;  margin-left: auto;  margin-right: auto;}</style><p align="center"><font face="黑体" size="2.">表1 监督学习与无监督学习的区别</font></p><div class="center"><table><thead><tr class="header"><th style="text-align: center;"></th><th style="text-align: center;">监督学习</th><th style="text-align: center;">无监督学习</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">数据</td><td style="text-align: center;">标注数据</td><td style="text-align: center;">无标注数据</td></tr><tr class="even"><td style="text-align: center;">输出</td><td style="text-align: center;">输出空间<span class="math inline">\(\mathcal{Y}\)</span></td><td style="text-align: center;">隐式结构空间<span class="math inline">\(\mathcal{Z}\)</span></td></tr><tr class="odd"><td style="text-align: center;">预测模型</td><td style="text-align: center;">对给定输入产生相应的输出</td><td style="text-align: center;">表示数据的类别、转换或概率</td></tr><tr class="even"><td style="text-align: center;">本质</td><td style="text-align: center;">学习输入到输出的映射的统计规律</td><td style="text-align: center;">学习数据中的统计规律或潜在结构</td></tr></tbody></table></div><ul><li><strong>无监督学习的过程:</strong>和监督学习类似，无监督学习由学习系统与预测系统构成。在学习过程中，学习系统从训练数据集学习，得到一个最优模型，表示为<span class="math inline">\(z=\hat{g}(x)\)</span>，条件概率分布<span class="math inline">\(\hat{P}(z\vert x)\)</span>或者条件概率分布<span class="math inline">\(\hat{P}(x \vertz)\)</span>。在预测过程中，预测系统对于给定的输入<span class="math inline">\(x_{N+1}\)</span>，由模型<span class="math inline">\(\hat{z}_{N+1}=\hat{g}(x_{N+1})\)</span>或<span class="math inline">\(\hat{z}_{N+1}=\arg\max\limits_{z} \hat{P}(z \vertx_{N+1})\)</span>给出相应的输出<span class="math inline">\(\hat{z}_{N+1}\)</span>，进行聚类或降维，或者由模型<span class="math inline">\(\hat{P}(x \vertz)\)</span>给出输入的概率<span class="math inline">\(\hat{P}(x_{N+1}\vertz_{N+1})\)</span>，进行概率估计。无监督学习的过程可以用图2来描述。</li></ul><center><img src="https://s2.loli.net/2023/09/27/itEGIlOMS9mWbhQ.jpg" width="60%" height="60%"><div data-align="center">Image2: 无监督学习的过程</div></center><h4 id="强化学习">强化学习</h4><p>  强化学习其实要强调一个互动，互动是指的是智能系统与环境之间的一个连续互动，通过这个互动，学习一个最优的行为策略。</p><ul><li><strong>强化学习的过程:</strong> 在每一步<span class="math inline">\(t\)</span>，智能系统从环境中观测到一个状态<span class="math inline">\((state)s_t\)</span>，与一个奖励<span class="math inline">\((reward)r_t\)</span>，采取一个动作<span class="math inline">\((action)a_t\)</span>。环境根据智能系统选择的动作，决定下一步<span class="math inline">\(t+1\)</span>的状态<span class="math inline">\(s_{t+1}\)</span>与奖励<span class="math inline">\(r_{t+1}\)</span>。要学习的策略表示为给定的状态下采取的动作。智能系统的目标不是短期奖励的最大化，而是长期累积奖励的最大化。强化学习过程中，智能系统不断试错<span class="math inline">\((trial \space\ and \spaceerror)\)</span>，以达到学习最优策略的目的。强化学习的过程可以用图3来描述。</li></ul><center><img src="https://s2.loli.net/2023/09/27/x4InRw56OMVubzt.jpg" width="60%" height="60%"><div data-align="center">Image3: 强化学习的过程</div></center><h2 id="机器学习方法的三要素">机器学习方法的三要素</h2><p>  机器学习方法都是由模型、策略和算法构成，即机器学习方法由三要素构成，可以简单地表示为：<br><span class="math display">\[方法(Method)=模型(Model)+策略(Strategy)+算法(Algorithm)\]</span></p><p>  下面主要介绍监督学习的三要素。</p><h3 id="监督学习的三要素">监督学习的三要素</h3><h4 id="模型">模型</h4><p>  对于监督学习，模型主要可以表示成两种形式，一种是条件概率形式<span class="math inline">\(P(y \vert x)\)</span>，另一种是决策函数的形式<span class="math inline">\(y=f(x)\)</span>。<br>  如果模型是由参数向量<span class="math inline">\(\theta\)</span>决定的，我们称所有可能的参数向量组成的空间为参数空间<span class="math inline">\(\Theta\)</span>，那么这个假设空间<span class="math inline">\(\mathcal{H}\)</span>就应该是由参数空间<span class="math inline">\(\Theta\)</span>决定的了。<br>  (1) 当用决策函数的形式表示模型时，定义：</p><ul><li><strong>决策函数:</strong> <span class="math inline">\(Y =f(X)\)</span></li><li><strong>假设空间:</strong> 决策函数的集合<span class="math inline">\(\mathcal{H} = \{f \vert Y=f(X) \}\)</span>.<br></li><li><strong>函数族:</strong> 假设空间<span class="math inline">\(\mathcal{H}\)</span>是由一个参数向量<span class="math inline">\(\theta\)</span>决定的函数族构成: <span class="math inline">\(\mathcal{H}=\{f \vert Y=f_{\theta}(X),\theta \in\mathbb{R}^{n}\}\)</span>.<br></li><li><strong>参数空间:</strong> <span class="math inline">\(\Theta =\{\theta \vert \theta \in \mathbb{R}^{n} \}\)</span>.</li></ul><p>  以线性回归问题为例，这个问题的模型可以表示为决策函数的形式。</p><ul><li><strong>决策函数:</strong> <span class="math inline">\(f(x) =w^{T}x+b, x \in \mathbb{R}^{n},w \in \mathbb{R}^{n}, b \in\mathbb{R}\)</span>.<br></li><li><strong>假设空间:</strong> <span class="math inline">\(\mathcal{H}=\{f \vert y = f_{\theta}(x),\theta=(w,b), \theta \in \mathbb{R}^{n+1}\}\)</span><br></li><li><strong>参数空间:</strong> <span class="math inline">\(\Theta =\{\theta \vert \theta = (w,b),\theta \in\mathbb{R}^{n+1}\}\)</span>.</li></ul><p>  (2) 当用条件概率的形式表示模型时，定义:</p><ul><li><strong>条件概率分布:</strong> <span class="math inline">\(P(Y \vertX)\)</span>.<br></li><li><strong>假设空间:</strong> 条件概率分布的集合<span class="math inline">\(\mathcal{H}=\{P \vert P(Y \vertX)\}\)</span>.<br></li><li><strong>分布族:</strong> 假设空间<span class="math inline">\(\mathcal{H}\)</span>是由一个参数向量<span class="math inline">\(\theta\)</span>决定的分布族构成: <span class="math inline">\(\mathcal{H}=\{P\vert P_{\theta}(Y\vert X), \theta\in \mathbb{R}^{n}\}\)</span>.<br></li><li><strong>参数空间:</strong> <span class="math inline">\(\Theta =\{\theta \vert \theta \in \mathbb{R}^{n}\}\)</span>.</li></ul><p>  以二项Logistic回归为例，这个问题的模型可以用条件概率分布的形式表示。</p><ul><li><strong>条件概率分布:</strong></li></ul><p><span class="math display">\[P(Y=1 \vert x) =\frac{exp(wx+b)}{1+exp(wx+b)}\]</span></p><p><span class="math display">\[P(Y=0 \vert x) =\frac{1}{1+exp(wx+b)}\]</span></p><p>  其中，<span class="math inline">\(x \in \mathbb{R}^{n}, Y \in\{0,1\}, w \in \mathbb{R}^{n}, b \in \mathbb{R}\)</span>.</p><ul><li><strong>假设空间:</strong> <span class="math inline">\(\mathcal{H} =\{P \vert P_{\theta}(Y\vert X), \theta = (w,b), \theta \in\mathbb{R}^{n+1} \}\)</span>.<br></li><li><strong>参数空间:</strong> <span class="math inline">\(\Theta =\{\theta \vert \theta = (w,b), \theta \in \mathbb{R}^{n+1}\}\)</span>.</li></ul><h4 id="策略">策略</h4><p>  有了模型的假设空间，机器学习接着需要考虑的是按照什么样的准则学习或者选择最优模型。机器学习的目标在于从假设空间中选取最优模型。所谓策略其实就是一种学习准则，用来选择最优模型的。想要选择模型，那么一定要知道如何度量模型的好坏。所以，这里先要引入几个概念。</p><ul><li><strong>损失函数(Loss function):</strong>度量模型一次预测的好坏，记作<span class="math inline">\(L(Y,f(X))\)</span>.几种常用的损失函数如下：</li></ul><p>  (1)<strong>0-1损失函数:</strong> 主要用于分类问题。</p><p><span class="math display">\[L(Y,f(X))= \left \{\begin{array}{rcl}1, &amp; {Y \neq f(X)}\\0,&amp; {Y = f(X)}\\\end{array} \right.\]</span></p><p>  (2)<strong>平方损失函数:</strong> 主要用于回归问题。</p><p><span class="math display">\[L(Y,f(X))=(Y-f(X))^2\]</span></p><p>  (3)<strong>绝对损失函数:</strong> 主要用于回归问题。</p><p><span class="math display">\[L(Y,f(X))= \vert Y-f(X)\vert\]</span></p><p>  (4) <strong>对数损失函数:</strong> 主要针对概率模型。</p><p><span class="math display">\[L(Y,P(Y \vert X))= - \log P(Y \vertX)\]</span></p><p>  损失函数越小说明模型的预测值与真实值越接近，模型的预测效果越好。但仅衡量一次预测的好坏是不够的，我们更想要知道模型在整个联合分布上的表现，这时就需要风险函数。</p><ul><li><strong>期望风险(Expexted risk):</strong>度量平均意义下模型预测的好坏。</li></ul><p><span class="math display">\[R_{exp}(f)=E_{P}[L(Y,f(X))]=\int_{\mathcal{X}\times \mathcal{Y}}L(y,f(x))P(x,y)dxdy\]</span></p><p>  理论上我们需要计算期望风险来评估模型的好坏，但在实际中联合概率分布<span class="math inline">\(P(X,Y)\)</span>一般是未知的，期望风险无法直接计算，因此我们引入经验风险用于估计风险函数.</p><ul><li><strong>经验风险(Empirical risk):</strong>模型关于数据集的平均损失.</li></ul><p><span class="math display">\[R_{emp}(f)=\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))\]</span></p><p>  其中，训练集<span class="math inline">\(T=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}\)</span>.</p><p>  期望风险<span class="math inline">\(R_{exp}(f)\)</span>是模型关于联合分布的期望损失，经验风险<span class="math inline">\(R_{emp}(f)\)</span>是模型关于训练集的平均损失。我们知道训练集中的样本<span class="math inline">\((x_i,y_i)\)</span>都是独立同分布的，其损失函数的期望<span class="math inline">\(L(Y,f(X))\)</span>存在，由大数定律可知，当<span class="math inline">\(n \rightarrow \infty\)</span>时，有：</p><p><span class="math display">\[R_{emp}(f)=\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))\rightarrow R_{exp}(f)=E_{P}[L(Y,f(X))]\]</span></p><p>  可是在现实生活中，样本容量<span class="math inline">\(N\)</span>一般是有限的，有的时候甚至会很小。因此，仅仅用经验风险来估计期望风险效果并不理想，需要对其进行一定的矫正。这里就涉及到监督学习的两个基本策略，一个是经验风险最小化策略，一个是结构风险最小化策略。</p><ul><li><strong>经验风险最小化策略:</strong> 求解使得经验风险<span class="math inline">\(R_{emp}(f)\)</span>最小的模型<span class="math inline">\(f^{*}\)</span>，模型<span class="math inline">\(f^{*}\)</span>即为最优的模型。根据这一策略，求解最优模型的问题就转化为求解最优化问题：</li></ul><p><span class="math display">\[\min_{f \in \mathcal{H}}\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))\]</span></p><p>  当样本容量足够大时，经验风险最小化能保证有很好的学习效果。但是，当样本容量很小时，经验风险最小化学习的效果就未必很好，会产生“过拟合”(over-fitting)现象。<br>  结构风险最小化策略是为了防止过拟合现象而提出的策略。结构风险在经验风险上加上表示模型复杂度的正则化项。</p><ul><li><strong>结构风险(Structural risk):</strong></li></ul><p><span class="math display">\[R_{srm}(f)=\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))+\lambdaJ(f)\]</span></p><p>  其中，正则化项<span class="math inline">\(J(f)\)</span>为模型复杂度，是定义在假设空间<span class="math inline">\(\mathcal{H}\)</span>上的泛函。模型<span class="math inline">\(f\)</span>越复杂，正则化项<span class="math inline">\(J(f)\)</span>就越大，反之，模型<span class="math inline">\(f\)</span>越简单，正则化项<span class="math inline">\(J(f)\)</span>就越小。<span class="math inline">\(\lambda &gt;0\)</span>是用于权衡经验风险与模型复杂度的系数。</p><ul><li><strong>结构风险最小化策略:</strong> 求解使得结构风险<span class="math inline">\(R_{srm}(f)\)</span>最小的模型<span class="math inline">\(f^{*}\)</span>，模型<span class="math inline">\(f^{*}\)</span>即为最优模型。这一策略同样可以转化为求解最优化问题：</li></ul><p><span class="math display">\[\min_{f \in \mathcal{H}}\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))+\lambda J(f)\]</span></p><p>  关于监督学习的策略，追根究底，就是选取一个目标函数，可以是经验风险，或者是结构风险，然后通过优化这个目标函数，达到学习模型的目的。</p><h4 id="算法">算法</h4><p>  在假设空间里面，根据策略去选择最优模型，需要一个具体的操作方案，操作方案也就是算法，是用来求解最优模型的。如果这个最优模型存在显式解析解，那么简单了，直接把这个结果写出来即可。但是往往这个显式解是不存在的，所以需要一定的数值计算方法，比如梯度下降法。</p><h2 id="模型评估与模型选择">模型评估与模型选择</h2><h3 id="模型评估">模型评估</h3><p>  在通过策略训练完模型后，我们需要评估得到的模型在已知数据和未知数据上的预测效果。我们称模型在训练集上的预测误差为训练误差，在测试集上的误差为测试误差。</p><h4 id="训练误差">训练误差</h4><p>  假设我们定义完假设空间<span class="math inline">\(\mathcal{H}\)</span>后，通过策略与算法学习到的模型为:<br><span class="math display">\[Y = \hat{f(X)}\]</span></p><p>  训练数据集为：</p><p><span class="math display">\[T_{train}=\{(x_1,y_1),(x_2,y_2),\dots,(x_{N_{1}}.y_{N_{1}})\}\]</span></p><p>  其中<span class="math inline">\(N_1\)</span>表示训练样本容量。由前文的概念可知，我们可以使用模型在测试集上的经验风险<span class="math inline">\(R_{emp}\)</span>来表示训练误差：</p><p><span class="math display">\[e_{train}(\hat{f})=\frac{1}{N_1}\sum_{i=1}^{N_1}L(y_i,\hat{f}(x_i))\]</span></p><p>  训练误差<span class="math inline">\(e_{train}\)</span>表示模型<span class="math inline">\(\hat{f}\)</span>在训练集上预测效果的好坏，训练误差越小，说明模型在训练集上的预测效果越好。然而，仅仅获得模型对已知数据的预测效果并不足以能够正确评价模型性能，我们还需要评估模型对未知数据的预测效果，这就需要计算模型在测试集上的预测误差，即测试误差。</p><h4 id="测试误差">测试误差</h4><p>  若测试数据集为：</p><p><span class="math display">\[T_{test}=\{(x_1,y_1),(x_2,y_2),\dots,(x_{N_{2}}.y_{N_{2}})\}\]</span></p><p>  其中<span class="math inline">\(N_2\)</span>表示测试样本容量。同样使用模型在测试集上的经验风险<span class="math inline">\(R_{emp}\)</span>来表示测试误差：</p><p><span class="math display">\[e_{test}(\hat{f})=\frac{1}{N_2}\sum_{i=1}^{N_2}L(y_i,\hat{f}(x_i))\]</span></p><p>  测试误差<span class="math inline">\(e_{test}\)</span>表示模型<span class="math inline">\(\hat{f}\)</span>在测试集上的预测效果的好坏，测试误差越小，说明模型在测试集上的预测效果越好。训练误差反映了学习方法对未知的测试数据集的预测能力，是评估模型性能的重要指标。通常，我们将学习方法对未知数据的预测能力称为<strong>泛化能力(Generalizationability)</strong>。</p><h3 id="模型选择">模型选择</h3><p>  在得到模型的训练误差和测试误差后，我们便需要据此评估模型的性能，选择对实际问题性能最出众的模型。最理想的情况是，我们得到的模型在测试集和训练集上的误差都较小，这样我们可以认为模型对于该问题的效果是较好的。但在实际情况中，我们经常会发现模型在训练集上的误差较小，而在测试集上的误差较大，这表明模型的泛化能力交叉，我们称这样的现象为<strong>过拟合(Over-fitting)</strong>。</p><h4 id="过拟合">过拟合</h4><p>  过拟合是指学习时选择的模型所包含的参数过多，模型复杂度过高，以至于出现这一模型对已知数据预测得很好，但对未知数据预测得很差的现象。<br>  模型选择的目的实际上就是避免过拟合现象并提高模型的泛化能力。这里我们通过一个多项式函数拟合的问题来说明过拟合与模型选择。</p><h4 id="关于模型选择的实例多项式拟合问题">关于模型选择的实例——多项式拟合问题</h4><p>  假设输入空间<span class="math inline">\(\mathcal{X}=(0,1)\)</span>到输出空间<span class="math inline">\(\mathcal{Y}=[-1,1]\)</span>的真实映射为:</p><p><span class="math display">\[Y = \sin(2\pi X)\]</span></p><p>  由于误差的影响，抽取的样本为真实值加上一个随机误差，我们假设这个随机误差满足均值为0，方差为0.1的正态分布：</p><p><span class="math display">\[y=\sin(2 \pi x)+\varepsilon, \varepsilon\sim N(0,0.1)\]</span></p><p>  我们的样本数据集为：</p><p><span class="math display">\[T=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}\]</span></p><p>  其中，样本容量<span class="math inline">\(N=30\)</span>.将样本数据集划分为样本容量为20的训练数据集与样本容量为10的测试数据集。以下给出了生成数成以及测试数据的散点图的代码。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> math<br>%matplotlib inline<br><br><span class="hljs-comment"># Data Generation</span><br>np.random.seed(<span class="hljs-number">1314</span>) <span class="hljs-comment"># set random seed</span><br>x = np.random.random((<span class="hljs-number">30</span>,)) <span class="hljs-comment"># x belong to (0,1)</span><br>r = np.random.normal(<span class="hljs-number">0</span>,<span class="hljs-number">0.1</span>,<span class="hljs-number">30</span>) <span class="hljs-comment"># r ~ N(0,0.1)</span><br>y = np.sin(<span class="hljs-number">2</span>*(math.pi)*x)+r<br>data = {<span class="hljs-string">"y"</span>:y,<span class="hljs-string">"x"</span>:x}<br>Data = pd.DataFrame(data,columns=[<span class="hljs-string">"y"</span>,<span class="hljs-string">"x"</span>]) <span class="hljs-comment"># data set</span><br>train_data = Data[:<span class="hljs-number">20</span>] <span class="hljs-comment"># Train data</span><br>test_data = Data[<span class="hljs-number">20</span>:] <span class="hljs-comment"># Test data</span><br><br><span class="hljs-comment"># Tradin data scatter plot</span><br>plt.scatter(train_data[<span class="hljs-string">"x"</span>],train_data[<span class="hljs-string">"y"</span>],marker=<span class="hljs-string">"o"</span>,c=<span class="hljs-string">"black"</span>,label=<span class="hljs-string">"noise"</span>)<br><span class="hljs-comment"># Real line</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">real_func</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> np.sin(<span class="hljs-number">2</span>*np.pi*x)<br>x_points = np.linspace(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1000</span>)<br>plt.plot(x_points,real_func(x_points),label=<span class="hljs-string">"real curve"</span>)<br>plt.xlabel(<span class="hljs-string">"x"</span>)<br>plt.ylabel(<span class="hljs-string">"y"</span>)<br>plt.legend()<br></code></pre></td></tr></tbody></table></figure>  得到的散点图如下图4所示：<br><center><img src="https://s2.loli.net/2023/09/30/TlHEjSPh5BRZOzN.png" width="60%" height="60%"><div data-align="center">Image4: 训练数据散点图</div></center><p>  图4中的黑色散点代表训练数据，蓝色曲线为输入空间到输出空间的真实映射。下面，我们便要建立模型去拟合数据。我们使用多项式函数进行拟合，模型形式为：</p><p><span class="math display">\[f_{M}(x,w)=w_0+w_{1}x+w_{2}x^2+\dots+w_{M}x^{M}=\sum_{j=1}^{M}w_{j}x_{j}= w^{T}x\]</span></p><p>  其中，<span class="math inline">\(x=\begin{bmatrix}  1,x,x^2,\dots,x^M\end{bmatrix}^{T}\)</span>为输入变量的不同次方，且<span class="math inline">\(x \in (0,1)\)</span>，<span class="math inline">\(w=\begin{bmatrix}  w_0,w_1,\dots,w_M\end{bmatrix}^{T}\)</span>为多项式的参数向量。则该问题的假设空间为：</p><p><span class="math display">\[\mathcal{H}=\{f \verty=f_{M}(x,w)\}\]</span></p><p>  由于多项式函数<span class="math inline">\(f_{M}(x,w)\)</span>是由参数向量<span class="math inline">\(w\)</span>决定的，因此假设空间<span class="math inline">\(\mathcal{H}\)</span>等价于想要的参数空间<span class="math inline">\(\Theta\)</span>：</p><p><span class="math display">\[\Theta = \{w \vert w \in\mathbb{R}^{M+1} \}\]</span></p><p>  在拟合的时候，我们采取使经验风险最小化的策略。由于这个问题为回归问题，经验风险中的损失函数可以选用平方损失函数，即预测值与真实值平方差。此时模型的经验风险可以表示为训练误差：</p><p><span class="math display">\[L(w)=\frac{1}{2}\sum_{i=1}^{N_1}(f_{M}(x_i,w)-y_i)^2=\frac{1}{2}\sum_{i=1}^{N_1}(w^{T}x_i-y_i)^2=\frac{1}{2}(Xw-y)^{T}(Xw-y)\]</span></p><p><span class="math display">\[X=\begin{bmatrix}    1 &amp; x_1 &amp; x_1^2 &amp; \dots &amp; x_1^M \\    1 &amp; x_2 &amp; x_2^2 &amp; \dots &amp; x_2^M \\    \vdots &amp; \vdots &amp; \vdots &amp; &amp; \vdots \\    1 &amp; x_{N_1} &amp; x_{N_1}^2 &amp; \dots &amp; x_{N_1}^M \\\end{bmatrix},w = \begin{bmatrix}    w_0 \\    w_1 \\    \vdots \\    w_M \\\end{bmatrix}, y = \begin{bmatrix}    y_1 \\    y_2 \\    \vdots \\    y_{N_1} \\\end{bmatrix}\]</span></p><p>  采取使得经验风险最小化的策略，则从假设空间中求解最优模型的问题就转化成了一个最优化问题：</p><p><span class="math display">\[\min_{w} \frac{1}{2}(Xw-y)^{T}(Xw-y)\]</span></p><p>  要求<span class="math inline">\(L(w)\)</span>的最小值点，我们需要对<span class="math inline">\(w\)</span>求偏导，并令偏导为<span class="math inline">\(0\)</span>：</p><p><span class="math display">\[\frac{\partial L(w)}{\partialw}=X^TXw-X^Ty=0 \]</span></p><p><span class="math display">\[\Rightarrow \hat{w} =(X^TX)^{-1}X^Ty\]</span></p><p>  其中<span class="math inline">\(N_1 = 20\)</span>。<br>  在估计完参数后，我们便得到了该问题的预测模型：</p><p><span class="math display">\[\hat{f}_{M}(x,\hat{w})=\hat{w}^Tx\]</span></p><p>  对于测试集中的实例<span class="math inline">\(x_i\)</span>，可以利用模型计算<span class="math inline">\(y_i\)</span>的预测值：</p><p><span class="math display">\[\hat{y_i} =\hat{f}_M(x_i,\hat{w})=\hat{w}^Tx_i=y^TX(X^TX)^{-1}x_i\]</span></p><p>  要评估模型的性能我们可以计算模型在训练集与测试机上的误差：</p><p><span class="math display">\[e_{train}(\hat{f}_M)=\frac{1}{N_1}\sum_{i=1}^{N_1}L(y_i,\hat{f}_M(x_i,\hat{w}))\]</span></p><p><span class="math display">\[e_{test}(\hat{f}_M)=\frac{1}{N_2}\sum_{i=1}^{N_2}L(y_i,\hat{f}_M(x_i,\hat{w}))\]</span></p><p>  其中，<span class="math inline">\(N_2=10\)</span>，改变多项式的次数<span class="math inline">\(M\)</span>可以得到不同的模型形式，进而得到不同的<span class="math inline">\(e_{train}\)</span>和<span class="math inline">\(e_{test}\)</span>，我们可以通过比较这两个指标选择最优的多项式次数。下面是多项式回归的代码实现。</p><p>  通过以下这段代码中的<code>ployreg()</code>函数，我们可以计算多项式回归的参数向量：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Ploy-regression</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">ployreg</span>(<span class="hljs-params">M</span>):  <br>    <span class="hljs-keyword">global</span> train_data<br>    x = train_data[<span class="hljs-string">"x"</span>].values<br>    X = np.ones((<span class="hljs-built_in">len</span>(x),M+<span class="hljs-number">1</span>))  <br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(M):  <br>        X[:,i+<span class="hljs-number">1</span>]=np.power(x,i+<span class="hljs-number">1</span>)<br>    y = train_data[<span class="hljs-string">"y"</span>].values<br>    X_T = np.transpose(X)<br>    X_TX = np.dot(X_T,X)<br>    w = np.dot(np.linalg.inv(X_TX),np.dot(X_T,y))<br>    <span class="hljs-keyword">return</span> w.<span class="hljs-built_in">round</span>(<span class="hljs-number">4</span>)<br></code></pre></td></tr></tbody></table></figure><p>  当我们设置多项式的次数<span class="math inline">\(M=2\)</span>时，得到的最优多项式函数为：</p><p><span class="math display">\[\hat{f}(x)=0.6325-1.0391x-0.5826x^2\]</span></p><p>  在得到最优模型后，我们可以计算模型在训练集与测试集上的误差。通过设置不同的多项式次数<span class="math inline">\(M\)</span>，我们可以比较不同模型的性能，从而选择最优的模型形式。以下代码中的<code>calerror()</code>函数便是用于计算模型在训练集与测试集上的误差。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># calculate error</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">calerror</span>(<span class="hljs-params">M</span>):<br>    <span class="hljs-keyword">global</span> train_data,test_data<br>    error = {}<br>    w = ployreg(M)<br>    <span class="hljs-comment"># train error</span><br>    x = train_data[<span class="hljs-string">"x"</span>].values<br>    X = np.ones((<span class="hljs-built_in">len</span>(x),M+<span class="hljs-number">1</span>))  <br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(M):  <br>        X[:,i+<span class="hljs-number">1</span>]=np.power(x,i+<span class="hljs-number">1</span>)<br>    y = train_data[<span class="hljs-string">"y"</span>].values<br>    e = np.dot(X,w)-y<br>    train_error = (np.dot(np.transpose(e),e))/<span class="hljs-built_in">len</span>(x)<br>    <span class="hljs-comment"># test error</span><br>    x = test_data[<span class="hljs-string">"x"</span>].values<br>    X = np.ones((<span class="hljs-built_in">len</span>(x),M+<span class="hljs-number">1</span>))  <br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(M):  <br>        X[:,i+<span class="hljs-number">1</span>]=np.power(x,i+<span class="hljs-number">1</span>)<br>    y = test_data[<span class="hljs-string">"y"</span>].values<br>    e = np.dot(X,w)-y<br>    test_error = (np.dot(np.transpose(e),e))/<span class="hljs-built_in">len</span>(x)<br>    error[<span class="hljs-string">"train error"</span>] = train_error.<span class="hljs-built_in">round</span>(<span class="hljs-number">4</span>)<br>    error[<span class="hljs-string">"test error"</span>] = test_error.<span class="hljs-built_in">round</span>(<span class="hljs-number">4</span>)<br>    <span class="hljs-keyword">return</span> error<br></code></pre></td></tr></tbody></table></figure><p>  设置多项式次数为<span class="math inline">\(M \in[0,9]\)</span>，我们可以得到下表2所示的计算结果：</p><style>.center {  width: auto;  display: table;  margin-left: auto;  margin-right: auto;}</style><p align="center"><font face="黑体" size="2.">表2 模型误差</font></p><div class="center"><table><thead><tr class="header"><th style="text-align: center;">M</th><th style="text-align: center;">Train error</th><th style="text-align: center;">Test error</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">0</td><td style="text-align: center;">0.413</td><td style="text-align: center;">0.562</td></tr><tr class="even"><td style="text-align: center;">1</td><td style="text-align: center;">0.1392</td><td style="text-align: center;">0.2668</td></tr><tr class="odd"><td style="text-align: center;">2</td><td style="text-align: center;">0.1372</td><td style="text-align: center;">0.2398</td></tr><tr class="even"><td style="text-align: center;">3</td><td style="text-align: center;">0.0144</td><td style="text-align: center;">0.0182</td></tr><tr class="odd"><td style="text-align: center;">4</td><td style="text-align: center;">0.0137</td><td style="text-align: center;">0.0151</td></tr><tr class="even"><td style="text-align: center;">5</td><td style="text-align: center;">0.0098</td><td style="text-align: center;">0.0065</td></tr><tr class="odd"><td style="text-align: center;">6</td><td style="text-align: center;">0.0098</td><td style="text-align: center;">0.0065</td></tr><tr class="even"><td style="text-align: center;">7</td><td style="text-align: center;">0.0087</td><td style="text-align: center;">0.0074</td></tr><tr class="odd"><td style="text-align: center;">8</td><td style="text-align: center;">0.0084</td><td style="text-align: center;">0.0121</td></tr><tr class="even"><td style="text-align: center;">9</td><td style="text-align: center;">0.0049</td><td style="text-align: center;">0.0388</td></tr></tbody></table></div><p>  为了更直观的观察模型误差的变化，我们可以利用上表的数据绘制误差曲线图。通过以下代码中的<code>errorplot()</code>函数来绘制图像：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># learning curve</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">errorplot</span>():<br>    train_error = []<br>    test_error = []<br>    <span class="hljs-keyword">for</span> M <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>        error = calerror(M)<br>        train_error.append(error[<span class="hljs-string">'train error'</span>])<br>        test_error.append(error[<span class="hljs-string">'test error'</span>])<br>    M = <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>)<br>    <span class="hljs-comment"># train error curve</span><br>    plt.plot(M,train_error,c=<span class="hljs-string">"blue"</span>,label=<span class="hljs-string">"train error"</span>)<br>    <span class="hljs-comment"># test error curve</span><br>    plt.plot(M,test_error,c=<span class="hljs-string">"green"</span>,label=<span class="hljs-string">"test error"</span>)<br>    plt.xlabel(<span class="hljs-string">"M"</span>)<br>    plt.ylabel(<span class="hljs-string">"error"</span>)<br>    plt.legend()<br></code></pre></td></tr></tbody></table></figure><p>  误差图如下图5所示：</p><center><img src="https://s2.loli.net/2023/10/01/yCiIKm39GYcWXFO.png" width="60%" height="60%"><div data-align="center">Image5: 模型误差</div></center><p>  结合表1和图5，我们可以发现随着多项式次数<span class="math inline">\(M\)</span>的增加，训练误差是逐渐减小的，而测试误差会先减小后增大。我们知道模型的复杂度随着<span class="math inline">\(M\)</span>的增加而增加，实际上，当<span class="math inline">\(M\)</span>增加到7之后，虽然训练误差仍在下降，但测试误差不降反升，此时模型已经出现了过拟合现象，模型的泛化能力下降。当<span class="math inline">\(M=5\)</span>或<span class="math inline">\(M=6\)</span>时，模型的训练误差与测试误差均达到较低水平，此时模型达到最优。在同样的误差下，我们应当选择复杂度较小的模型，以降低预测时的计算量，因此我们选择<span class="math inline">\(M=5\)</span>时的模型作为该问题的最优模型：</p><p><span class="math display">\[\hat{f}(x)=0.0320+4.6026x+15.8081x^2-107.3805x^3+142.5954x^4-55.7287x^5\]</span></p><p>  通过以下代码中的<code>regcurve()</code>函数我们可以画出相应的多项式函数的曲线：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># draw regression curve</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">regcurve</span>(<span class="hljs-params">M</span>):<br>    <span class="hljs-keyword">global</span> train_data<br>    <span class="hljs-comment"># Tradin data scatter plot</span><br>    plt.scatter(train_data[<span class="hljs-string">"x"</span>],train_data[<span class="hljs-string">"y"</span>],marker=<span class="hljs-string">"o"</span>,c=<span class="hljs-string">"black"</span>,label=<span class="hljs-string">"train data"</span>)<br>    <span class="hljs-comment"># real line</span><br>    x_points = np.linspace(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1000</span>)<br>    plt.plot(x_points,real_func(x_points),label=<span class="hljs-string">"real curve"</span>)<br>    <span class="hljs-comment"># regression curve</span><br>    w = ployreg(M)<br>    X_points = np.ones((<span class="hljs-built_in">len</span>(x_points),M+<span class="hljs-number">1</span>))<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(M):<br>        X_points[:,i+<span class="hljs-number">1</span>] = np.power(x_points,i+<span class="hljs-number">1</span>)<br>    y_points = np.dot(X_points,w)<br>    plt.plot(x_points,y_points,c=<span class="hljs-string">"red"</span>,label=<span class="hljs-string">"regression curve"</span>)<br>    plt.xlabel(<span class="hljs-string">"x"</span>)<br>    plt.ylabel(<span class="hljs-string">"y"</span>)<br>    plt.title(<span class="hljs-string">f"M=<span class="hljs-subst">{M}</span>"</span>)<br>    plt.legend()<br></code></pre></td></tr></tbody></table></figure><p>  画出<span class="math inline">\(M=0,1,5,9\)</span>时的拟合曲线的图像：</p><center><img src="https://s2.loli.net/2023/10/01/3zGbMkHDVx75t91.png" width="80%" height="80%"><div data-align="center">Image6: 拟合曲线</div></center><p>  从图6可以看出，当<span class="math inline">\(M=0,1\)</span>时，模型过于简单，与真实曲线相差很大，模型误差也可以看出此时模型在训练集与测试集上的误差均较大，我们称模型出现了<strong>欠拟合</strong>现象。当<span class="math inline">\(M=5\)</span>时，拟合曲线与真实曲线几乎一致，且模型在训练集上的误差也很小，模型效果很好。当<span class="math inline">\(M=9\)</span>时，模型几乎能够预测所有的训练数据，模型的训练误差非常小，但拟合曲线与真实曲线相差较大，模型在测试集上的误差较大，泛化能力较差，此时模型出现了<strong>过拟合</strong>现象。</p><h2 id="正则化与交叉验证">正则化与交叉验证</h2><h3 id="正则化">正则化</h3><p>  上文中我们介绍了进行模型选择时主要评估模型在已知数据与未知数据上的预测能力，即训练误差与测试误差。在进行模型选择时，我们需要综合考虑这两个指标，以避免过拟合与欠拟合现象。<br>  为了综合考虑这两个指标，模型选择的一种经典方法是<strong>正则化(regularization)</strong>。正则化是结构风险最小化策略的实现，是在经验风险上加上一个正则化项(regularizer)或罚项(penaltyterm)。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值越大。比如，正则化项可以是模型参数向量的范数。<br>  正则化一般具有如下形式：</p><p><span class="math display">\[\min_{f \in \mathcal{H}}\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))+\lambda J(f)\]</span></p><p>  其中，第1项是经验风险，第2项是正则化项，<span class="math inline">\(\lambda \geq0\)</span>为调整两者关系的系数。<br>  当模型处于欠拟合状态时，虽然模型复杂度较低，正则化值较低，但模型在训练集上的误差会较大，即经验风险较大；当模型处于过拟合状态时，虽然模型在训练集上的误差会较小，即经验风险较小，但模型复杂度会较高，即正则化值较大。正则化的作用是选择经验风险与模型复杂度同时较小的模型，通过优化正则化表达式，能同时减弱模型的欠拟合与过拟合现象，找到最优模型所处的中间状态。<br>  正则化项可以取不同的形式。在回归问题中，损失函数是平方损失，正则化项可以是参数向量的<span class="math inline">\(L_2\)</span>范数，以减轻模型的过拟合现象：</p><p><span class="math display">\[L(w)=\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))+\frac{\lambda}{2}\Vert w \Vert_{2}^{2}\]</span></p><p>  正则化项也可以是参数向量的<span class="math inline">\(L_1\)</span>范数，以得到一个较为稀疏的参数向量：</p><p><span class="math display">\[L(w)=\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))+\lambda \Vert w \Vert_{1}\]</span></p><p><strong>注: 奥卡姆剃刀原理</strong><br>  正则化符合奥卡姆剃刀原理。奥卡姆剃刀原理应用于模型选择时便为以下想法：在所有可能选择的模型中，能够很好解释已知数据并且十分简单的模型才是最好的模型。</p><h3 id="交叉验证">交叉验证</h3><p>  另一种常用的模型选择方法是交叉验证(crossvalidation).如果给定的样本数据充足，进行模型选择的一种简单方法是随机地将数据集切分成三部分，分别为<strong>训练集、验证集、测试集</strong>。训练集用于训练模型，验证集用于模型的选择，而测试集用于最终对学习方法的评估。我们最终选择对验证集有最小预测误差的模型。<br>  但是，在许多实际应用中数据是不充足的，为了选择好的模型，可以采用交叉验证方法。<strong>交叉验证的基本想法是重复地使用数据；把给定的数据进行切分，将切分的数据组合为训练集与测试集，在此基础上反复地进行训练、测试以及模型选择。</strong><br>  简单介绍几种主要的交叉验证的方法：</p><ul><li><strong>简单交叉验证:</strong>首先随机地将已给数据分为两部分，一部分作为训练集，另一部分作为测试集(例如，70%的数据为训练集，30%的数据为测试集)；然后用训练集在各种条件下(例如，不同的参数个数)训练模型，从而得到不同的模型；在测试集上评价各个模型的测试误差，选出测试误差最小的模型。<br></li><li><strong>S折交叉验证:</strong>首先随机地将已给数据切分为S个互不相交、大小相同的子集；然后利用S-1个子集的数据训练模型，利用余下的子集测试模型；将这一过程对可能的S种选择重复进行；最后选出在S次评测中平均测试误差最小的模型。<br></li><li><strong>留一交叉验证:</strong>令S折交叉验证中的S=N，称为留一交叉验证，往往在数据缺乏的情况下使用。这里，N是给定数据集的容量。</li></ul><h2 id="泛化能力">泛化能力</h2><p>  学习方法发泛化能力是指由该方法学习到的模型对未知数据的预测能力。通常，测试数据集用以评价训练所得模型的泛化能力。由于测试数据集包含的样本有限，仅仅通过测试数据集去评价泛化能力，有时并不可靠，此时需要我们从机器学习理论出发，对模型的泛化能力进行评价。</p><h3 id="泛化误差">泛化误差</h3><p>  如果学习到的模型是<span class="math inline">\(\hat{f}\)</span>，那么用这个模型对未知数据预测的期望风险即为泛化误差(generalizationability)：</p><p><span class="math display">\[R_{exp}(\hat{f})=E_{P}[L(Y,\hat{f}(X))]=\int_{\mathcal{X}\times \mathcal{Y}}L(y,\hat{f}(x))P(x,y)dxdy\]</span></p><p>  计算测试误差所使用的样本为测试集，而泛化误差则是基于整个样本空间的。所以泛化误差是度量对未知数据预测能力的理论值，测试误差则是经验值。如果一种学习方法的模型比另一种方法所学习的模型具有更小的泛化误差，那么这种方法就更有效。</p><h3 id="泛化误差上界">泛化误差上界</h3><p>  学习方法的泛化能力分析往往是通过研究泛化误差的概率上界进行的，简称泛化误差上界(generalizationerror bound)。泛化误差上界通常具有以下性质：</p><ul><li><strong>泛化误差上界是样本容量<span class="math inline">\(N\)</span>的函数，当样本容量<span class="math inline">\(N\)</span>增加时，泛化误差上界趋于0</strong>；</li><li><strong>泛化误差上界是假设空间容量(capacity)的函数，假设空间容量越大，模型就越难学，泛化误差上界越大</strong>。</li></ul><p>  第一条性质，可以从泛化误差的概念出发辅助理解。泛化误差定义为一个期望值，那么经验值就是以全样本空间作为测试集计算所得的测试误差，它表示为一个平均值。在平均值中，样本量位于分母的位置，那么随着样本量的增加，平均值则趋于零。<br>  第二条性质，泛化误差上界是假设空间容量的函数，不同的模型，对应不同的泛化误差上界。假设空间是所有可能的模型的集合，那么假设空间容量越大，所有可能的模型类型就会越多，模型也就愈加难以学习，与之对应的泛化误差上界就会越大。<br>  下面给出一个简单的泛化误差上界的例子：二分类问题的泛化误差上界。</p><h4 id="二分类问题的泛化误差上界">二分类问题的泛化误差上界</h4><p>  已知二分类问题的数据集为<span class="math inline">\(T\)</span>：</p><p><span class="math display">\[T=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}\]</span></p><p>  其中，<span class="math inline">\(N\)</span>为样本容量。假设数据集<span class="math inline">\(T\)</span>是从联合概率分布<span class="math inline">\(P(X,Y)\)</span>独立同分布产生的，<span class="math inline">\(X \in \mathbb{R}^N, Y \in\{-1,1\}\)</span>。模型的假设空间<span class="math inline">\(\mathcal{H}\)</span>是函数的集合：</p><p><span class="math display">\[\mathcal{H}=\{f_1,f_2,\dots,f_d\},f_i:\mathbb{R}^N \rightarrow \{-1,1\}\]</span></p><p>  由于是分类问题，我们可以设置损失函数为0-1损失函数:</p><p><span class="math display">\[L(Y,f(X))= \left \{\begin{array}{rcl}1, &amp; {Y \neq f(X)}\\0,&amp; {Y = f(X)}\\\end{array} \right.\]</span></p><p>  设<span class="math inline">\(f\)</span>是从假设空间<span class="math inline">\(\mathcal{H}\)</span>中选取的函数，则<span class="math inline">\(f\)</span>的期望风险<span class="math inline">\(R(f)\)</span>与经验风险<span class="math inline">\(\hat{R}(f)\)</span>分别为：</p><p><span class="math display">\[R(f)=E[L(Y,f(X))]\]</span></p><p><span class="math display">\[\hat{R}(f)=\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))\]</span></p><p>  为了从假设空间<span class="math inline">\(\mathcal{H}\)</span>中选取最优模型，我们可以采取经验风险最小化策略，设得到的最优模型为<span class="math inline">\(f_N\)</span>:</p><p><span class="math display">\[f_N = \arg \min_{f \in \mathcal{H}}\hat{R}(f)\]</span></p><p>  <span class="math inline">\(f_N\)</span>依赖训练数据集的样本容量<span class="math inline">\(N\)</span>，<span class="math inline">\(f_N\)</span>的泛化能力即其在整个样本空间上的期望风险：</p><p><span class="math display">\[R(f_N)=E[L(Y,f_{N}(X))]\]</span></p><p>  下面，首先给出关于假设空间<span class="math inline">\(\mathcal{H}\)</span>中任意函数<span class="math inline">\(f\)</span>的泛化误差上界的定理，再对定理进行证明。</p><p><strong>定理</strong></p><p>  对二分类问题，当假设空间是有限个函数的集合<span class="math inline">\(\mathcal{H}=\{f_1,f_2,\dots,f_d\}\)</span>时，对<span class="math inline">\(\forall f \in\mathcal{H}\)</span>，其泛化误差<span class="math inline">\(R(f)\)</span>有以下不等式成立：</p><p><span class="math display">\[P(R(f) \leq\hat{R}(f)+\varepsilon(d,N,\delta)) \geq 1-\delta, \delta \in(0,1)\]</span></p><p><span class="math display">\[\varepsilon(d,N,\delta)=\sqrt{\frac{1}{2N}(\log{d}+\log{\frac{1}{\delta}})}\]</span></p><p>  该不等式的左端<span class="math inline">\(R(f)\)</span>是泛化误差，右端即为泛化误差的上界。从泛化误差上界的表达时中，我们可以发现：</p><ul><li>当模型<span class="math inline">\(f\)</span>的经验风险，即训练误差<span class="math inline">\(\hat{R}(f)\)</span>越小时，泛化误差上界也越小。<br></li><li>当样本容量<span class="math inline">\(N \rightarrow\infty\)</span>时，<span class="math inline">\(\varepsilon(d,N,\delta)\rightarrow 0\)</span>，泛化误差上界下降。<br></li><li>当假设空间<span class="math inline">\(\mathcal{H}\)</span>中的函数越多，即<span class="math inline">\(d\)</span>越大时，<span class="math inline">\(\varepsilon(d,N,\delta)\)</span>越大，泛化误差上界越大。</li></ul><p>  下面对该定理进行证明，证明中需要用到 <span class="math inline">\(Hoeffiding\)</span> 不等式，这里先给出其结论。</p><p><strong><span class="math inline">\(Hoeffding\)</span>不等式</strong></p><p>  设 <span class="math inline">\(X_1,X_2,\dots,X_N\)</span>是独立随机变量，且 <span class="math inline">\(X_i \in [a_i,b_i], i =1,2,\dots,N\)</span>；<span class="math inline">\(\bar{X}\)</span> 是<span class="math inline">\(X_1,X_2,\dots,X_N\)</span>的均值，即<span class="math inline">\(\bar{X}=\frac{1}{N}\sum_{i=1}^{N}X_i\)</span>，则对<span class="math inline">\(\forall t &gt;0\)</span>，有以下不等式成立：</p><p><span class="math display">\[P \left(  \bar{X}-E(\bar{X})  \ge t\right) \leq exp \left( -\frac{2N^2t^2}{\sum_{i=1}^{N}(b_i-a_i)^2}\right)\]</span></p><p><span class="math display">\[P \left( | \bar{X}-E(\bar{X}) | \ge t\right) \leq 2exp \left( -\frac{2N^2t^2}{\sum_{i=1}^{N}(b_i-a_i)^2}\right)\]</span></p><p>  <span class="math inline">\(Hoeffding\)</span>不等式的证明放在附录中，感兴趣的读者可以自行阅读，下面我们来正式证明关于二分类问题的误差上界的定理。</p><p><strong>证明</strong><br>  令<span class="math inline">\(X_i =L(y_i,f(x_i))\)</span>，由于样本<span class="math inline">\((x_i,y_i)\)</span>是从联合概率分布<span class="math inline">\(P(X,Y)\)</span>中产生的，故有<span class="math inline">\(X_1,X_2,\dotsb,X_N\)</span>独立同分布，且对 <span class="math inline">\(\forall i, [a_i,b_i]=[0,1]\)</span>,</p><p><span class="math display">\[\bar{X}=\frac{1}{N}\sum_{i=1}^{N}X_i=\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))=\hat{R}(f)\]</span></p><p><span class="math display">\[\begin{align*}    E(\bar{X}) &amp;= E \left( \frac{1}{N} \sum_{i=1}^{N}X_i \right)=\frac{1}{N}\sum_{i=1}^{N}E(X_i) =E(X)\\    &amp;= E \left( L(Y,f(X)) \right) = R(f) \\\end{align*}\]</span></p><p>  根据 <span class="math inline">\(Hoeffding\)</span>不等式可知：对<span class="math inline">\(\forall \varepsilon &gt; 0\)</span>，有：</p><p><span class="math display">\[P(\bar{X}-E(\bar{X}) \ge\varepsilon)=P(R(f)-\hat{R}(f) \ge \varepsilon) \leq exp(-2N\varepsilon^2)\]</span></p><p>   <span class="math inline">\(\because\)</span>事件 <span class="math inline">\(\{R{f}-\hat{R}{f} \ge \varepsilon\}\Leftrightarrow\)</span> 事件 <span class="math inline">\(\{ \exists f\in \mathcal{H}: R(f)-\hat{R}(f) \ge \varepsilon \}\)</span>，故有：</p><p><span class="math display">\[\begin{align*}    P(R(f)-\hat{R}(f) \ge \varepsilon) &amp;= P(\exists f \in\mathcal{H}: R(f)-\hat{R}(f) \ge \varepsilon)=P \left( \bigcup_{f \in\mathcal{H}}\{ R(f)-\hat{R}(f) \ge \varepsilon \} \right) \\    &amp;\leq \sum_{f \in \mathcal{H}} P(R(f)-\hat{R}(f) \ge\varepsilon) \leq dexp(-2N\varepsilon^2)\end{align*}\]</span></p><p>   <span class="math inline">\(\because\)</span> 事件<span class="math inline">\(\{ \exists f \in \mathcal{H}: R(f)-\hat{R}(f) \ge\varepsilon \}\)</span>与事件<span class="math inline">\(\{\forall f \in\mathcal{H}: R(f)-\hat{R}(f) &lt; \varepsilon\}\)</span>互补，故对<span class="math inline">\(\forall f \in \mathcal{H}\)</span>有：</p><p><span class="math display">\[P(R(f)-\hat{R}(f) &lt;\varepsilon)=P(R(f)-\hat{R}(f) \leq \varepsilon) \ge1-dexp(-2N\varepsilon^2)\]</span></p><p>  令<span class="math inline">\(\delta = dexp(-2N\varepsilon^2) \in(0,1)\)</span>，即：</p><p><span class="math display">\[P(R(f)-\hat{R}(f) \leq \varepsilon) \ge1-\delta\]</span></p><p><span class="math display">\[\delta = dexp(-2N\varepsilon^2)\Rightarrow \varepsilon =\sqrt{\frac{1}{2N}(\log{d}+\log{\frac{1}{\delta}})}\]</span></p><p>  证毕.</p><h2 id="附录">附录</h2><h3 id="关于-hoeffding-不等式的证明">关于 <span class="math inline">\(Hoeffding\)</span> 不等式的证明</h3><p>  <span class="math inline">\(Hoeffding\)</span> 不等式的证明需要使用<span class="math inline">\(Markov\)</span> 不等式与 <span class="math inline">\(Hoeffding\)</span>不等式引理，这里我们首先证明这两个结论。</p><h4 id="markov-不等式"><span class="math inline">\(Markov\)</span>不等式</h4><p><strong>结论</strong><br>  对任意非负随变量<span class="math inline">\(X\)</span>，<span class="math inline">\(\varepsilon &gt; 0\)</span>，有：</p><p><span class="math display">\[P(X \ge \varepsilon) \leq\frac{E(X)}{\varepsilon}\]</span></p><p><strong>证明:</strong><br>  设随机变量<span class="math inline">\(X\)</span>的概率密度函数为<span class="math inline">\(f(x)\)</span>，则有：</p><p><span class="math display">\[E(X)=\int_{0}^{\infty}xf(x) \leq\int_{\varepsilon}^{\infty}xf(x)dx \leq \varepsilon\int_{\varepsilon}^{\infty}f(x)dx=\varepsilon P(X \ge\varepsilon)\]</span></p><p><span class="math display">\[\Rightarrow P(X \ge \varepsilon) \leq\frac{E(X)}{\varepsilon}\]</span></p><p>  证毕.</p><h4 id="hoeffding-不等式引理"><span class="math inline">\(Hoeffding\)</span> 不等式引理</h4><p><strong>结论</strong><br>  对任意随机变量<span class="math inline">\(X\)</span>，若其满足<span class="math inline">\(P(X \in [a,b])=1\)</span>和<span class="math inline">\(E(X)=0\)</span>，则有以下不等式成立：</p><p><span class="math display">\[E(e^{sX}) \leqe^{\frac{s^2(b-a)^2}{8}}\]</span></p><p><strong>证明:</strong><br>  设<span class="math inline">\(f(x)=e^{sx}\)</span>，且<span class="math inline">\(dom f = [a,b]\)</span>,<br>  <span class="math inline">\(\because f(x)\)</span>二阶可微，<span class="math inline">\(dom f\)</span>为凸集，且<span class="math inline">\(f^{''}(x)=s^2e^{sx} \ge 0\)</span>,<br>  由凸函数的二阶条件可知：<span class="math inline">\(f(x)\)</span>为凸函数,<br>  由凸函数的性质可知：对<span class="math inline">\(\forall X_1,X_2 \in[a,b]\)</span>，有以下不等式成立：</p><p><span class="math display">\[f(\alpha X_1+\beta X_2) \leq \alphaf(X_1)+\beta f(X_2),\space \forall \alpha,\beta &gt; 0,\alpha+\beta=1\]</span></p><p>  令<span class="math inline">\(\alpha=\frac{b-X}{b-a},\beta=\frac{X-a}{b-a}, X_1=a, X_2=b\)</span>，则有：</p><p><span class="math display">\[f(\alpha X_1+\betaX_2)=f(X)=e^{sX}\]</span></p><p><span class="math display">\[\alpha f(X_1)+\betaf(X_2)=\frac{b-X}{b-a}e^{sa}+\frac{X-a}{b-a}e^{sb}\]</span></p><p>  由凸函数的性质的得：</p><p><span class="math display">\[e^{sX} \leq\frac{b-X}{b-a}e^{sa}+\frac{X-a}{b-a}e^{sb}\]</span></p><p>  两边同时对<span class="math inline">\(X\)</span>取期望，由<span class="math inline">\(E(X)=0\)</span>，得：</p><p><span class="math display">\[0 \leq E(e^{sX}) \leq\frac{be^{sa}-ae^{sb}}{b-a}\]</span></p><p>  现要证明以下不等式成立：</p><p><span class="math display">\[\frac{be^{sa}-ae^{sb}}{b-a} \leqe^{\frac{s^2(b-a)^2}{8}}\]</span></p><p>  两边同时取对数，则等价于证明以下不等式成立：</p><p><span class="math display">\[\log \left( \frac{be^{sa}-ae^{sb}}{b-a}\right) \leq \frac{s^2(b-a)^2}{8}\]</span></p><p>  令<span class="math inline">\(t=b-a\)</span>，则<span class="math inline">\(b=t+a\)</span>，则不等式可作如下变换：</p><p><span class="math display">\[\begin{align*}    \log \left( \frac{be^{sa}-ae^{sb}}{b-a} \right) &amp;=\log(be^{sa}-ae^{sb})-\log(b-a) \\    &amp;= \log(te^{sa}+ae^{sa}-ae^{s(t+a)})-\log t  \\    &amp;= sa + \log(t+a-ae^{st}) - \log t \\    &amp;\leq \frac{s^2t^2}{8} \\\end{align*}\]</span></p><p>  令<span class="math inline">\(u=st, g(u)=sa +\log(t+a-ae^{st})-\log t\)</span>，则以上不等式等价于：</p><p><span class="math display">\[g(u) \leq \frac{1}{8}u^2\]</span></p><p>  对<span class="math inline">\(g(u)\)</span>求一阶导数和二阶导数得：</p><p><span class="math display">\[g^{'}(u)=\frac{-ae^u}{t+a-ae^u},\spaceg^{''}(x)=\frac{-ae^u(t+a)}{(t+a-ae^u)^2}\]</span></p><p>  将<span class="math inline">\(g(u)\)</span>在<span class="math inline">\(u=0\)</span>处进行泰勒展开：</p><p><span class="math display">\[g(u)=g(0)+g^{'}(0)u+\frac{1}{2}g^{''}(\varepsilon)u^2=\frac{1}{2}g^{''}(\varepsilon)u^2\]</span></p><p><span class="math display">\[\begin{align*}    g^{''}(\varepsilon) &amp;=\frac{-ae^{\varepsilon}(t+a)}{(t+a-ae^{\varepsilon})^2}=\frac{t+a}{t+a-ae^{\varepsilon}}\cdot \frac{-ae^{\varepsilon}}{t+a-ae^{\varepsilon}}  \\    &amp;= \left( \frac{t+a}{t+a-ae^{\varepsilon}} \right) \cdot \left(1- \frac{t+a}{t+a-ae^{\varepsilon}} \right)  = m(1-m) \leq \frac{1}{4}\\\end{align*}\]</span></p><p>  故可证明以下不等式成立：</p><p><span class="math display">\[g(u) =\frac{1}{2}g^{''}(\varepsilon)u^2 \leq\frac{1}{8}u^2\]</span></p><p>  证毕.</p><p>  有了<span class="math inline">\(Markov\)</span>不等式与 <span class="math inline">\(Hodeffding\)</span>不等式引理作为基础，下面我们开始正式证明 <span class="math inline">\(Hoeffding\)</span> 不等式。</p><p><strong>证明:</strong><br>  取 <span class="math inline">\(s &gt; 0, t &gt; 0\)</span>，由<span class="math inline">\(Markov\)</span>不等式得：</p><p><span class="math display">\[P(\bar{X}-E(\bar{X}) \ge t)=P \left(e^{s(\bar{X}-E(\bar{X}))} \ge e^{st} \right) \leq e^{-st}E \left(e^{s(\bar{X}-E(\bar{X}))} \right)\]</span></p><p>   <span class="math inline">\(\because\bar{X}=\frac{1}{N}\sum_{i=1}^{N}X_i, \spaceE(\bar{X})=\frac{1}{N}\sum_{i=1}^{N}E(X_i)\)</span>，故有：</p><p><span class="math display">\[E \left( e^{s(\bar{X}-E(\bar{X}))}\right) = E \left( e^{s \sum_{i=1}^{N} \frac{X_i-E(X_i)}{N}} \right) =\prod_{i=1}^{N}E \left( e^{s \frac{X_i-E(X_i)}{N}} \right)\]</span></p><p>  令<span class="math inline">\(Y_i =\frac{X_i-E(X_i)}{N}\)</span>，则<span class="math inline">\(Y_i\)</span>相互独立，且有<span class="math inline">\(E(Y_i)=0, Y_i \in[\frac{a_i-E(X_i)}{N},\frac{b_i-E(X_i)}{N}]\)</span>，由 <span class="math inline">\(Hoeffding\)</span> 不等式引理得：</p><p><span class="math display">\[E \left( e^{sY_i} \right) \leq e^{s\frac{(b_i-a_i)^2}{8N^2}}, i=1,2,\dots,N\]</span></p><p>  由以上不等式可得：</p><p><span class="math display">\[E \left( e^{s(\bar{X}-E(\bar{X}))}\right) = \prod_{i=1}^{N}E \left( e^{sY_i} \right) \leq \prod_{i=1}^{N}e^{s^2 \frac{(b_i-a_i)^2}{8N^2}} = e^{s^2 \sum_{i=1}^{N}\frac{(b_i-a_i)^2}{N}}\]</span></p><p>  则有：</p><p><span class="math display">\[P(\bar{X}-E(\bar{X}) \ge t) \leqe^{-st}E \left( e^{s(\bar{X}-E(\bar{X}))} \right) \leq e^{-st+s^2\sum_{i=1}^{N} \frac{(b_i-a_i)^2}{N}}\]</span></p><p>  对 <span class="math inline">\(\forall s &gt; 0\)</span>均有以上不等式成立，故不等式右端对<span class="math inline">\(s\)</span>取最小值，不等式仍然成立：<br>  令<span class="math inline">\(h(s) = -st+s^2 \sum_{i=1}^{N}\frac{(b_i-a_i)^2}{N}\)</span>，对其求一阶导数并令其为零：</p><p><span class="math display">\[h^{'}(s)=-t+s\frac{\sum_{i=1}^{N}(b_i-a_i)^2}{4N^2}=0 \Rightarrows^{'}=\frac{4N^2t}{\sum_{i=1}^{N}(b_i-a_i)^2}\]</span></p><p>  代入<span class="math inline">\(s^{'}\)</span>可以得到<span class="math inline">\(h(s)\)</span>的最小值：</p><p><span class="math display">\[h_{min}(s)=h(s^{'})=\frac{-2N^2t^2}{\sum_{i=1}^{N}(b_i-a_i)^2}\]</span></p><p>  由于<span class="math inline">\(e^x\)</span>在<span class="math inline">\(\mathbb{R}\)</span>上的单调性可知：</p><p><span class="math display">\[e^{-st+s^2 \sum_{i=1}^{N}\frac{(b_i-a_i)^2}{N}}=e^{h(s)} \gee^{h_{min}(s)}=e^{\frac{-2N^2t^2}{\sum_{i=1}^{N}(b_i-a_i)^2}}\]</span></p><p>  故有以下不等式成立：</p><p><span class="math display">\[P(\bar{X}-E(\bar{X}) \ge t) \leq exp\left( \frac{-2N^2t^2}{\sum_{i=1}^{N}(b_i-a_i)^2} \right)\]</span></p><p>  同时：</p><p><span class="math display">\[P(E(\bar{X})-\bar{X} \ge t) =P(-\bar{X}-E(-\bar{X}) \ge t)\]</span></p><p>  令<span class="math inline">\(\bar{Z}=-\bar{X},E(\bar{Z})=E(-\bar{X})\)</span>，同上可证：</p><p><span class="math display">\[P(E(\bar{X})-\bar{X} \ge t) =P(\bar{Z}-E(\bar{Z}) \ge t) \leq exp \left(\frac{-2N^2t^2}{\sum_{i=1}^{N}(b_i-a_i)^2} \right)\]</span></p><p>  综上所述，有以下不等式成立：</p><p><span class="math display">\[P \left( | \bar{X}-E(\bar{X}) | \ge t\right) \leq 2exp \left( -\frac{2N^2t^2}{\sum_{i=1}^{N}(b_i-a_i)^2}\right)\]</span></p><p>  证毕.</p><h2 id="参考">参考</h2><p><strong>[1] Book: 李航.(2019).统计学习方法</strong><br><strong>[2] Video: B站数学up主——简博士</strong><br><strong>[3] Github:Dod-o/Statistical-Learning-Method_Code</strong><br><strong>[4] Blog: 知乎,ziyoufeixiang,Hoeffding 不等式证明</strong><br><strong>[5] Blog:CSDN,吃吃今天努力学习了吗,[Python]多项式曲线拟合(Polynomial CurveFitting)</strong></p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>矩阵分析-4.线性子空间</title>
    <link href="/2023/09/07/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-4-%E7%BA%BF%E6%80%A7%E5%AD%90%E7%A9%BA%E9%97%B4/"/>
    <url>/2023/09/07/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-4-%E7%BA%BF%E6%80%A7%E5%AD%90%E7%A9%BA%E9%97%B4/</url>
    
    <content type="html"><![CDATA[<h1 id="线性子空间">线性子空间</h1><h2 id="线性子空间的定义">线性子空间的定义</h2><p>  设<span class="math inline">\(V\)</span>是数域<span class="math inline">\(\mathbb{F}\)</span>上的线性空间，<span class="math inline">\(W \in V\)</span>是非空子集，若<span class="math inline">\(W\)</span>有以下两个属性:<br>(1) <strong>对加法封闭</strong>：<span class="math inline">\(\forall\alpha,\beta \in W, \alpha+\beta \in W\)</span>.<br>(2) <strong>对数乘封闭</strong>：<span class="math inline">\(\forall k\in \mathbb{F}, \alpha \in W, \alpha k \in W\)</span>.<br>则称集合<span class="math inline">\(W\)</span>是线性空间<span class="math inline">\(V\)</span>的线性子空间.</p><p>注：线性子空间<span class="math inline">\(W\)</span>本身按<span class="math inline">\(V\)</span>中定义的加法与数乘也构成线性空间.</p><h2 id="线性子空间的实例">线性子空间的实例</h2><h3 id="例1-二维空间的线性子空间">例1 二维空间的线性子空间</h3><p>  <span class="math inline">\(\color{green}{结论:二维空间的线性子空间是任意经过原点的直线.}\)</span><br>  <strong>证明:</strong><br>  设<span class="math inline">\(V = \mathbb{R}^{2}\)</span>，<span class="math inline">\(V\)</span>表示二维平面.<br>  二维平面中经过原点的某一直线：<span class="math inline">\(W = \{x\vert \theta^{T} x=0, x \in V\}, \theta \in\mathbb{R}^{2}\)</span>.<br>  验证直线<span class="math inline">\(W\)</span>是二维平面<span class="math inline">\(V\)</span>的线性子空间：<br>  (1) 对<span class="math inline">\(\forall x_1,x_2 \in W,\theta^{T}(x_1+x_2)= \theta^{T}x_1+\theta^{T}x_2=0\)</span>,且<span class="math inline">\(x_1+x_2 \in V,\)</span><br>  <span class="math inline">\(\Rightarrow x_1+x_2 \inW\)</span>，即<span class="math inline">\(W\)</span>对加法封闭.<br>  (2) 对<span class="math inline">\(\forall \beta \in \mathbb{R},\forall x \in W, \theta^{T}(\betax)=\beta(\theta^{T}x)=0\)</span>，且<span class="math inline">\(\beta x\in V,\)</span><br>  <span class="math inline">\(\Rightarrow \beta x \inW\)</span>，即<span class="math inline">\(W\)</span>对数乘封闭.<br>  综上所述：直线<span class="math inline">\(W\)</span>是二维空间<span class="math inline">\(V\)</span>的线性子空间.</p><p>  <strong>注：二维空间中不经过原点的直线不是其线性子空间。</strong><br>  <strong>证明:</strong><br>  设<span class="math inline">\(V=\mathbb{R}^2\)</span>，<span class="math inline">\(V\)</span>表示二维平面.<br>  二维平面中不经过原点的某一直线：<span class="math inline">\(W=\{x\vert \theta^{T}x+b=0,b \neq 0,x \in V\}, \theta \in\mathbb{R}^{2}.\)</span><br>  验证直线<span class="math inline">\(W\)</span>不是二维平面<span class="math inline">\(V\)</span>的线性子空间：<br>  (1) 对<span class="math inline">\(\forall x_1,x_2 \in W, x_1,x_2 \neq0, \theta^{T}(x_1+x_2)+b = (\theta^{T}x_1+b)+\theta^{T}x_2=\theta^{T}x_2\neq 0.\)</span><br>  又<span class="math inline">\(\because x_1+x_2 \in V \Rightarrowx_1+x_2 \notin W \Rightarrow\)</span> 直线<span class="math inline">\(W\)</span>不满足对加法封闭，故其不是二维空间<span class="math inline">\(V\)</span>的线性子空间.</p><p>  对于二维空间的线性子空间，我们也可以借助图像来直观理解：</p><center><img src="https://s2.loli.net/2023/09/17/6eWufsxXb3FpVYh.jpg" width="40%" height="40%"><div data-align="center">Image1: 二维空间的子空间</div></center><p>  从图中我们可以发现，经过原点的直线<span class="math inline">\(W_1\)</span>上任意两个向量<span class="math inline">\(x_1,x_2\)</span>的和仍位于直线<span class="math inline">\(W_1\)</span>上，说明<span class="math inline">\(W_1\)</span>对加法封闭，同时<span class="math inline">\(W_1\)</span>上的向量伸缩后仍位于直线<span class="math inline">\(W_1\)</span>上，说明<span class="math inline">\(W_1\)</span>对数乘封闭。故经过原点的直线<span class="math inline">\(W_1\)</span>是二维空间<span class="math inline">\(V\)</span>的线性子空间.<br>  不经过原点的直线<span class="math inline">\(W_2\)</span>上的任意两个向量<span class="math inline">\(y_1,y_2\)</span>，这两个向量的和<span class="math inline">\(y_3\)</span>不位于直线<span class="math inline">\(W_2\)</span>上，故直线<span class="math inline">\(W_2\)</span>对加法不封闭，其不是二维空间<span class="math inline">\(V\)</span>的线性子空间.</p><h3 id="例2-向量组生成的线性子空间及线性子空间的生成组">例2向量组生成的线性子空间及线性子空间的生成组</h3><p>  设有向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>，定义：<br><span class="math display">\[W = span\{\alpha_1,\alpha_2,\dots,\alpha_p\}=\{\alpha_{1}c_{1}+\alpha_{2}c_{2}+\dots+\alpha_{p}c_{p} \vert c_{i} \in\mathbb{F},i=1,2,\dots,p\} =\{向量组\alpha_i的线性组合的全体\}\]</span></p><p>  称<span class="math inline">\(W\)</span>为向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>所生成的线性子空间,向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>称为线性子空间的生成组.</p><h3 id="例3-矩阵的核与象">例3 矩阵的核与象</h3><p>  设矩阵<span class="math inline">\(A \in \mathbb{F}^{m\timesn}\)</span>，定义集合<span class="math inline">\(\{x \vert x \in\mathbb{F}^{n},Ax=0 \}\)</span>为矩阵<span class="math inline">\(A\)</span>的核，记为<span class="math inline">\(ker A\)</span>，集合<span class="math inline">\(\{y \vert y \in \mathbb{F}^{m}, \exists x \in \mathbb{F}^{n},y=Ax\}\)</span>为矩阵<span class="math inline">\(A\)</span>的象，记为<span class="math inline">\(imA\)</span>. <span class="math inline">\(ker A\)</span>与<span class="math inline">\(im A\)</span>分别是<span class="math inline">\(\mathbb{F}^n\)</span>与<span class="math inline">\(\mathbb{F}^{m}\)</span>的线性子空间.</p><p><strong>证明:</strong><br>  设矩阵<span class="math inline">\(A \in \mathbb{F}^{m \timesn}\)</span>，记矩阵<span class="math inline">\(A\)</span>的核为集合<span class="math inline">\(ker A = \{x \vert x \in \mathbb{F}^{n},Ax=0\}\)</span>，<br>  矩阵<span class="math inline">\(A\)</span>的象为集合<span class="math inline">\(im A = \{ y \vert y \in \mathbb{F}^{m}, \exists x\in \mathbb{F}^{n}, y=Ax\}\)</span>.<br>  (1) 验证<span class="math inline">\(ker A\)</span>是线性空间<span class="math inline">\(\mathbb{F}^{n}\)</span>的线性子空间:<br>  对 <span class="math inline">\(\forall x_1,x_2 \in kerA\)</span>，有<span class="math inline">\(Ax_1=Ax_2=0 \RightarrowA(x_1+x_2)=0\)</span>，故有：<br>  <span class="math inline">\(x_1+x_2 \in ker A\)</span>，即<span class="math inline">\(ker A\)</span>对加法封闭.<br>  对<span class="math inline">\(\forall k \in \mathbb{F}, A(x_1 k) =(Ax_1)k = 0\)</span>，故有<span class="math inline">\(x_1 k \in kerA\)</span>，即<span class="math inline">\(kerA\)</span>对数乘封闭.<br>  棕上所述，<span class="math inline">\(ker A\)</span>是线性空间<span class="math inline">\(\mathbb{F}^{n}\)</span>的线性子空间.<br>  (2) 验证<span class="math inline">\(im A\)</span>是线性空间<span class="math inline">\(\mathbb{F}^{m}\)</span>的线性子空间:<br>   对<span class="math inline">\(\forall y_1,y_2 \in im A, \existsx_1,x_2 \in \mathbb{F}^n\)</span>，使得<span class="math inline">\(y_1 =Ax_1, y_2 = Ax_2\)</span>，<br>  <span class="math inline">\(\Rightarrow y_1+y_2 = Ax_1 + Ax_2 =A(x_1+x_2)\)</span>，令<span class="math inline">\(y_3=y_1+y_2,x_3 =x_1+x_2\)</span>，<br>  <span class="math inline">\(\because y_1,y_2 \in\mathbb{F}^{m}\)</span>，且线性空间<span class="math inline">\(\mathbb{F}^{m}\)</span>对加法封闭，故有<span class="math inline">\(y_3 \in \mathbb{F}^{m}\)</span>，同理可得<span class="math inline">\(x_3 \in \mathbb{F}^{n}\)</span>，<br>  <span class="math inline">\(y_3 = Ax_3 \Rightarrow y_3 \in imA\)</span>，即<span class="math inline">\(im A\)</span>对加法封闭.<br>  对<span class="math inline">\(\forall k \in \mathbb{F}, y_1 k =(Ax_1)k = A(x_1 k)\)</span>，<br>  <span class="math inline">\(\because x_1 \in\mathbb{F}^{n}\)</span>，且线性空间<span class="math inline">\(\mathbb{F}^{n}\)</span>对数乘封闭, 故有<span class="math inline">\(x_1 k \in \mathbb{F}^{n}\)</span>,<br>  则有<span class="math inline">\(y_1 k \in im A\)</span>，即<span class="math inline">\(im A\)</span>对数乘封闭.<br>  综上所述，<span class="math inline">\(im A\)</span>是线性空间<span class="math inline">\(\mathbb{F}^{m}\)</span>的线性子空间.</p><p><strong>注:</strong> <span class="math inline">\(Ax\)</span>为矩阵<span class="math inline">\(A\)</span>的列向量组<span class="math inline">\(\{\alpha_1, \alpha_2, \dots, \alpha_n\}\)</span>以<span class="math inline">\(x\)</span>为系数的线性组合.<span class="math inline">\(im A\)</span>也就是向量组<span class="math inline">\(\{\alpha_1, \alpha_2, \dots, \alpha_n\}\)</span>所张成的线性子空间.</p><h2 id="线性子空间的交与和">线性子空间的交与和</h2><p>  设<span class="math inline">\(U,W\)</span>是<span class="math inline">\(V\)</span>的线性子空间，则有：<br>(1) <span class="math inline">\(U \cap W = \{v \vert v \in U, v \inW\}\)</span>，<span class="math inline">\(U \cap W\)</span>也是<span class="math inline">\(V\)</span>的线性子空间，称为<span class="math inline">\(U,W\)</span>的交.<br>(2) <span class="math inline">\(U+W = span \{U,W\} = \{u+w \vert u \inU, w \in W\}\)</span>，<span class="math inline">\(U+W\)</span>也是<span class="math inline">\(V\)</span>的线性子空间，称为<span class="math inline">\(U,W\)</span>的和.</p><p><strong>证明:</strong><br>  设<span class="math inline">\(V\)</span>是线性空间,<span class="math inline">\(U,W\)</span>是<span class="math inline">\(V\)</span>的线性子空间,<br>  (1) 验证<span class="math inline">\(U \cap W\)</span>是<span class="math inline">\(V\)</span>的线性子空间<br>  对<span class="math inline">\(\forall v_1,v_2 \in U \capW\)</span>，有<span class="math inline">\(v_1,v_2 \in U, v_1,v_2 \inW\)</span><br>  <span class="math inline">\(\because U,W\)</span>是<span class="math inline">\(V\)</span>的线性子空间，故<span class="math inline">\(U,W\)</span>对加法封闭 <span class="math inline">\(\Rightarrow v_1+v_2 \in U, v_1+v_2 \in W\Rightarrow v_1+v_2 \in U \cap W\)</span><br>  即<span class="math inline">\(U \cap W\)</span>对加法封闭.<br>  对<span class="math inline">\(\forall k \in \mathbb{F}, v_1 \in U \capW\)</span>，<span class="math inline">\(U,W\)</span>对数乘封闭，故有:<br>  <span class="math inline">\(v_1k \in U, v_1k \in W \Rightarrow v_1k\in U \cap W\)</span>，即<span class="math inline">\(U \capW\)</span>对数乘封闭.<br>  (2) 验证<span class="math inline">\(U+W\)</span>是<span class="math inline">\(V\)</span>的线性子空间<br>  对<span class="math inline">\(\forall v_1,v_2 \in U+W, \exists u_1,u_2\in U, w_1,w_2 \in W\)</span>，使得<span class="math inline">\(v_1 =u_1+w_1,v_2 = u_2+w_2\)</span><br>  <span class="math inline">\(v_1+v_2=u_1+w_1+u_2+w_2=(u_1+u_2)+(w_1+w_2)\)</span>，<br>  令<span class="math inline">\(v_3=v_1+v_2, u_3 = u_1+u_2, w_3 =w_1+w_2 \Rightarrow v_3=u_3+w_3\)</span>，由<span class="math inline">\(U,W\)</span>对加法的封闭性可知：<br>  <span class="math inline">\(u_3 \in U, w_3 \in W \Rightarrow v_3 \inU+W\)</span>，即<span class="math inline">\(U+W\)</span>对加法封闭.<br>  对<span class="math inline">\(\forall k \in \mathbb{F}, v_1 \in U+W,v_1k = (u_1+w_1)k=u_1k+w_1k\)</span>,<br>  <span class="math inline">\(\because U,W\)</span>对数乘封闭，故有<span class="math inline">\(u_1k \in U, w_1k \in W \Rightarrow v_1 k \inU+W\)</span>，即<span class="math inline">\(U+W\)</span>对数乘封闭.<br>  综上所述，<span class="math inline">\(U \Cap w, U+W\)</span>均为<span class="math inline">\(V\)</span>的线性子空间.</p>]]></content>
    
    
    <categories>
      
      <category>矩阵分析</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>矩阵分析-3.基与坐标</title>
    <link href="/2023/08/27/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-3-%E5%9F%BA%E4%B8%8E%E5%9D%90%E6%A0%87/"/>
    <url>/2023/08/27/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-3-%E5%9F%BA%E4%B8%8E%E5%9D%90%E6%A0%87/</url>
    
    <content type="html"><![CDATA[<h1 id="基与坐标">基与坐标</h1><p>  在前面的章节，我们介绍了线性空间的概念，线性空间是一个抽象的概念，但在实际应用中，出于对向量运算的需求，我们通常更需要标准线性空间中的向量。为了将抽象线性空间中的向量映射到标准线性空间，我们引入了基向量的概念。抽象向量沿着基向量展开后得到坐标向量，我们用坐标向量来表示映射后的抽象向量。</p><h2 id="有限维线性空间基坐标">有限维线性空间基坐标</h2><p>  设<span class="math inline">\(V\)</span>是数域<span class="math inline">\(\mathbb{F}\)</span>上的线性空间，若有正整数<span class="math inline">\(n\)</span>，及<span class="math inline">\(V\)</span>中的向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_n\}\)</span>，使得:</p><ol type="1"><li><strong>线性无关性</strong>: 向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_n \}\)</span>为线性无关向量组.<br></li><li><strong>线性生成性</strong>: <span class="math inline">\(\forall\alpha \in V\)</span>，均可由向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_n \}\)</span>线性表示.</li></ol><p><span class="math display">\[\alpha = \alpha_{1} k_1 + \alpha_{2} k_2+ \dots + \alpha_{n} k_n = \begin{bmatrix}    \alpha_1,\alpha_2,\dots,\alpha_n \\\end{bmatrix} \begin{bmatrix}    k_1 \\    k_2 \\    \vdots \\    k_n \\\end{bmatrix} = Ak\]</span></p><p>  则称<span class="math inline">\(V\)</span>为<span class="math inline">\(n\)</span>维线性空间，向量组<span class="math inline">\(\{ \alpha_1,\alpha_2,\dots,\alpha_n\}\)</span>称为<span class="math inline">\(V\)</span>中的一个基(坐标系)，<span class="math inline">\(k \in \mathbb{F}^n\)</span>称为<span class="math inline">\(\alpha \in V\)</span>，沿着该基的坐标向量.</p><p>注：</p><p><span class="math display">\[\color{green} \begin{bmatrix}    抽 \\    象 \\    向 \\    量 \\\end{bmatrix} = \begin{bmatrix}    基矩阵 \\\end{bmatrix} \begin{bmatrix}    坐 \\    标 \\    向 \\    量 \\\end{bmatrix}\]</span></p><h2 id="关于基向量组的定理">关于基向量组的定理</h2><p><strong>定理1(基向量个数的唯一性)</strong> 设向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_m\}\)</span>及<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_s\}\)</span>分别是线性空间<span class="math inline">\(V\)</span>的两个基，则有<span class="math inline">\(m=s\)</span>.<br><strong>证明:</strong><br>  从线性空间中任意取<span class="math inline">\(p\)</span>个向量组成一个向量组<span class="math inline">\(\{v_1,v_2,\dots,v_p\}\)</span>，要求<span class="math inline">\(m \leq p, s \leq p\)</span>.<br>  由基向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_m\}\)</span>的定义可知：<br>  1. 向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_m\}\)</span>为线性无关向量组.<br>  2. 对<span class="math inline">\(\forall v \in \{v_1,v_2,\dots,v_p\},v\)</span>均可由向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_m\}\)</span>线性表示.<br>  <span class="math inline">\(\Rightarrow\)</span>向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_m\}\)</span>是向量组<span class="math inline">\(\{v_1,v_2,\dots,v_p\}\)</span>的极大线性无关组.<br>  同理可知：向量组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_s\}\)</span>也为向量组<span class="math inline">\(\{v_1,v_2,\dots,v_p\}\)</span>的极大线性无关组.<br>  由向量组的极大线性无关组中向量个数的唯一性可知: <span class="math inline">\(m=s\)</span>.</p><p><strong>定理2</strong> <span class="math inline">\(\color{green}{基实现了抽象线性空间到标准线性空间的一一映射.}\)</span><br><strong>证明:</strong><br>  设<span class="math inline">\(V\)</span>为<span class="math inline">\(n\)</span>维线性空间，向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_n\}\)</span>是<span class="math inline">\(V\)</span>的一个基.<br>  设由基向量组实现的映射为:<br><span class="math display">\[\sigma: V \longrightarrow\mathbb{F}^{n}\]</span></p><p><span class="math display">\[v \longmapsto k = \begin{bmatrix}    k_1 \\    k_2 \\    \vdots \\    k_n \\\end{bmatrix}\]</span></p><p>  <span class="math inline">\(k \in \mathbb{F}^{n}\)</span>是<span class="math inline">\(v\)</span>在基下的坐标向量.<br>  现需要验证映射<span class="math inline">\(\sigma\)</span>满足一一映射的两个条件.<br>  (1) 验证对<span class="math inline">\(\forall k \in \mathbb{F}^n,\exists v \in V\)</span>，使得<span class="math inline">\(v=\alpha_1k_1+\alpha_2k_2+\dots+\alpha_nk_n\)</span>.<br>  <span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_n\} \inV\)</span>. 由线性空间<span class="math inline">\(V\)</span>对加法与数乘封闭的性质可知:<br>  <span class="math inline">\(\exists v \in V\)</span>使得<span class="math inline">\(v =\alpha_1k_1+\alpha_2k_2+\dots+\alpha_nk_n\)</span>.<br>  (2) 验证若<span class="math inline">\(\sigma(v)=\sigma(v_0)=k\)</span>，则有<span class="math inline">\(v=v_0\)</span>.<br>  <span class="math inline">\(\sigma(v)=k \Leftrightarrowv=\alpha_1k_1+\alpha_2k_2+\dots+\alpha_nk_n\)</span>.<br>  <span class="math inline">\(\sigma(v_0)=k \Leftrightarrowv_0=\alpha_1k_1+\alpha_2k_2+\dots+\alpha_nk_n\)</span>.<br>  <span class="math inline">\(\Rightarrow v_0 = v\)</span>.<br>  综上所述映射<span class="math inline">\(\sigma\)</span>为一一映射.</p><p><strong>注：一一映射的定义</strong><br>  设映射<span class="math inline">\(\sigma: S_1 \rightarrowS_2\)</span>满足:<br>  (1)满射：对<span class="math inline">\(\forall s_2 \in S_2, \existss_1 \in S_1, \sigma(s_1)=s_2 .\)</span><br>  (2)单射：若<span class="math inline">\(\sigma(s_1)=\sigma(s^{*}_{1})\)</span>,则<span class="math inline">\(s_1=s^{*}_{1}.\)</span><br>  则称映射<span class="math inline">\(\sigma\)</span>为集合<span class="math inline">\(S_1\)</span>与<span class="math inline">\(S_2\)</span>之间的一一映射.</p><h2 id="标准线性空间mathbbrn的标准基与一般基">标准线性空间<span class="math inline">\(\mathbb{R}^n\)</span>的标准基与一般基</h2><h3 id="标准基">标准基</h3><p>  标准线性空间<span class="math inline">\(\mathbb{R}^n\)</span>的标准基：</p><p><span class="math display">\[e_1=\begin{bmatrix}    1 \\    0 \\    \vdots \\    0 \\\end{bmatrix}, e_2 = \begin{bmatrix}    0 \\    1 \\    \vdots \\    0\end{bmatrix}, \dots, e_n = \begin{bmatrix}    0 \\    0 \\    \vdots \\    1\end{bmatrix}\]</span></p><p><strong>证明:</strong><br>  (1)先证明标准基向量组的线性无关性：<br>  令 <span class="math inline">\(I_n =\begin{bmatrix}  e_1,e_2,\dots,e_n\end{bmatrix}\)</span>，有线性方程组<span class="math inline">\(I_n x =0\)</span>.<br>  该方程组仅有<span class="math inline">\(x=0\)</span>唯一解，故标准基向量组<span class="math inline">\(\{e_1,e_2,\dots,e_n\}\)</span>线性无关.<br>  (2)再证标准基向量组的线性生成性：<br>  对<span class="math inline">\(\forall v \in\mathbb{R}^n\)</span>，判断线性方程组<span class="math inline">\(I_n x =v\)</span>是否有解.<br>  <span class="math inline">\(I_n x = v \Rightarrow x = v\Rightarrow\)</span>方程有解<span class="math inline">\(\Rightarrowv\)</span>可由标准基向量组<span class="math inline">\(\{e_1,e_2,\dots,e_n\}\)</span>线性表示.<br>  综上所述，向量组<span class="math inline">\(\{e_1,e_2,\dots,e_n\}\)</span>可作为标准线性空间<span class="math inline">\(\mathbb{R}^n\)</span>的基向量组.</p><h3 id="一般基">一般基</h3><p>  <span class="math inline">\(\alpha_1,\alpha_2,\dots,\alpha_n \in\mathbb{R}^n\)</span>，向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_n\}\)</span>构成标准线性空间<span class="math inline">\(\mathbb{R}^n\)</span>的一组基的充要条件为：向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_n\}\)</span>的秩为<span class="math inline">\(n\)</span>.</p><p><strong>证明:</strong><br>  向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_n\}\)</span>的秩为<span class="math inline">\(n\)</span> <span class="math inline">\(\Rightarrow\)</span> 向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_n\}\)</span>线性无关<br>  令<span class="math inline">\(A=\begin{bmatrix}  \alpha_1,\alpha_2,\dots,\alpha_n\end{bmatrix}\)</span>，有<span class="math inline">\(rank(A) =n\)</span>.<br>  对<span class="math inline">\(\forall \beta \in\mathbb{R}^n\)</span>，判断矩阵方程<span class="math inline">\(Ax=\beta\)</span>是否有解.<br>  <span class="math inline">\(\becauserank(A)=rank([A,b])=n\)</span>，<span class="math inline">\(\therefore\)</span>方程<span class="math inline">\(Ax=\beta\)</span>有解.</p><p><span class="math inline">\(\color{green}{注：线性方程组 Ax=\beta的几何语言：在n维线性空间中，将向量\beta沿着矩阵A的列向量组所构成的基展开.}\)</span></p><h2 id="多项式函数空间作为线性空间的基">多项式函数空间作为线性空间的基</h2><p>  在第一节我们已经说明函数空间<span class="math inline">\(V=\mathcal{F}(I,\mathbb{F}^n)\)</span>可以作为线性空间。多项式是函数的一种形式，我们可以定义以多项式为元素的线性空间：</p><p><span class="math display">\[\mathbb{F}[x]=\{以x为自变量，以数域\mathbb{F}中的数为系数的多项式\}=\{f=a_0+a_1x+a_2x^2+\dots\vert a_i \in \mathbb{F}, i =1,2,\dots\}\]</span></p><p>  通过分析我们可以得知这是一个无限维的线性空间，这里我们不讨论无限维线性空间的基，通过对<span class="math inline">\(x\)</span>的次数添加限制，我们可以将这个线性空间变为有限维：</p><p><span class="math display">\[\mathbb{F}_n[x]=\{以x为自变量，以数域\mathbb{F}中的数为系数,次数小于n的多项式\}=\{f=a_0+a_1x++\dots+a_{n-1}x^{n-1}\vert a_i \in \mathbb{F}, i =1,2,\dots,n-1\}\]</span></p><p>  <span class="math inline">\(\mathbb{F}_n[x]\)</span>是一个<span class="math inline">\(n\)</span>维线性空间，取<span class="math inline">\(\mathbb{F}=\mathbb{R}\)</span>，接下来我们来讨论<span class="math inline">\(\mathbb{R}_n[x]\)</span>的基向量组。<br>  在高等代数中，我们知道多项式函数空间中的元素与标准线性空间中的元素一一对应，对<span class="math inline">\(\forall f \in \mathbb{R}_n[x]\)</span>，有：</p><p><span class="math display">\[f=a_0+a_1x++\dots+a_{n-1}x^{n-1}=\begin{bmatrix}    1,x,\dots,x^{n-1} \\\end{bmatrix}\begin{bmatrix}    a_0 \\    a_1 \\    \vdots \\    a_{n-1} \\\end{bmatrix}, f \rightarrow \begin{bmatrix}    a_0 \\    a_1 \\    \vdots \\    a_{n-1} \\\end{bmatrix}\]</span></p><p>  可以取<span class="math inline">\(\{1,x,\dots,x^{n-1}\}\)</span>为<span class="math inline">\(\mathbb{R}_n[x]\)</span>的一组基，以下是证明<span class="math inline">\(\{1,x,\dots,x^{n-1}\}\)</span>可以作为<span class="math inline">\(\mathbb{R}_n[x]\)</span>的基.<br><strong>证明:</strong><br>  (1)线性无关性<br>  若<span class="math inline">\(a_0+a_1x+\dots+a_{n-1}x^{n-1}=0\)</span>，带入<span class="math inline">\(x=1,2,\dots,n\)</span>，得：</p><p><span class="math display">\[\begin{bmatrix}    1^{0}&amp;1^{1}&amp;\dots&amp;1^{n-1} \\    2^{0}&amp;2^{1}&amp;\dots&amp;2^{n-1} \\    \vdots&amp;\vdots&amp;&amp;\vdots \\    n^{0}&amp;n^{1}&amp;\dots&amp;n^{n-1} \\\end{bmatrix}\begin{bmatrix}    a_0 \\    a_1 \\    \vdots \\    a_{n-1} \\\end{bmatrix}=\begin{bmatrix}    0 \\    0 \\    \vdots \\    0\end{bmatrix}\]</span></p><p>  令<span class="math inline">\(A=\begin{bmatrix}  1^{0}&amp;1^{1}&amp;\dots&amp;1^{n-1}\\  2^{0}&amp;2^{1}&amp;\dots&amp;2^{n-1}\\  \vdots&amp;\vdots&amp;&amp;\vdots\\  n^{0}&amp;n^{1}&amp;\dots&amp;n^{n-1} \\\end{bmatrix},a=\begin{bmatrix}  a_0 \\  a_1 \\  \vdots \\  a_{n-1} \\\end{bmatrix}\)</span>，<span class="math inline">\(det(A)\)</span>为范德蒙行列式,且<span class="math inline">\(det(A)\neq 0\)</span>，故<span class="math inline">\(rank(A)=n\)</span>.<br>  <span class="math inline">\(rank(A)=n \Rightarrow\)</span> 方程<span class="math inline">\(Aa=0\)</span>只有零解,即 <span class="math inline">\(a_i=0, i=1,2,\dots,n-1\)</span>.<br>  故<span class="math inline">\(\{1,x,\dots,x^{n-1}\}\)</span>线性无关.<br>  (2)线性生成性<br>  由多项式的定义可知，<span class="math inline">\(\mathbb{R}_n[x]\)</span>中的元素均可由<span class="math inline">\(\{1,x,\dots,x^{n-1}\}\)</span>线性表示.<br>  综上所述：<span class="math inline">\(\{1,x,\dots,x^{n-1}\}\)</span>可以作为<span class="math inline">\(\mathbb{R}_n[x]\)</span>的一组基.</p><p>注：<span class="math inline">\(\{1,x,\dots,x^{n-1}\}\)</span>对应于<span class="math inline">\(\mathbb{R}^n\)</span>中的标准基<br>  设<span class="math inline">\(\sigma\)</span>为以<span class="math inline">\(\{1,x,\dots,x^{n-1}\}\)</span>为基时，从<span class="math inline">\(\mathbb{R}_n[x]\)</span>到<span class="math inline">\(\mathbb{R}^n\)</span>的映射，有：</p><p><span class="math display">\[1=\begin{bmatrix}    1,x,\dots,x^{n-1} \\\end{bmatrix}\begin{bmatrix}    1 \\    0 \\    \vdots \\    0 \\\end{bmatrix}, \sigma: 1 \rightarrow \begin{bmatrix}    1 \\    0 \\    \vdots \\    0 \\\end{bmatrix}\]</span></p><p><span class="math display">\[x=\begin{bmatrix}    1,x,\dots,x^{n-1} \\\end{bmatrix}\begin{bmatrix}    0 \\    1 \\    \vdots \\    0 \\\end{bmatrix}, \sigma: x \rightarrow \begin{bmatrix}    0 \\    1 \\    \vdots \\    0 \\\end{bmatrix}\]</span></p><p><span class="math display">\[\vdots\]</span></p><p><span class="math display">\[x^{n-1}=\begin{bmatrix}    1,x,\dots,x^{n-1} \\\end{bmatrix}\begin{bmatrix}    0 \\    0 \\    \vdots \\    1 \\\end{bmatrix}, \sigma: x^{n-1} \rightarrow \begin{bmatrix}    0 \\    0 \\    \vdots \\    1 \\\end{bmatrix}\]</span></p>]]></content>
    
    
    <categories>
      
      <category>矩阵分析</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>深度学习-1-Code:使用Pytorch搭建深度学习模型的基本框架——以COVID-19 Cases Prediction为例</title>
    <link href="/2023/08/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-1-Code-%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A1%86%E6%9E%B6%E2%80%94%E2%80%94%E4%BB%A5COVID-19%20Cases%20Prediction%E4%B8%BA%E4%BE%8B/"/>
    <url>/2023/08/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-1-Code-%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A1%86%E6%9E%B6%E2%80%94%E2%80%94%E4%BB%A5COVID-19%20Cases%20Prediction%E4%B8%BA%E4%BE%8B/</url>
    
    <content type="html"><![CDATA[<h1>使用Pytorch搭建深度学习模型的基本框架</h1><p>  自2012年Alexnet在Imagenet图像分类竞赛中一鸣惊人，以神经网络算法为主体的深度学习技术在人工智能领域兴起，而后诸如卷积神经网络(CNN)，残差网络(Resnet)，生成式对抗网络(GAN)，自注意力模型(Transformer)等众多性能强大的算法模型被提出，使得人工智能领域的研究与应用进入了一个蓬勃发展的阶段。2016年，DeepMind推出围棋人工智能AlphaGo，其以4:1战胜围棋世界冠军李世石，让世人认识到了深度学习技术的强大。2023年，基于Transformer的LLM模型ChatGPT横空出世，这场来自AI领域的技术革命第一次距离我们如此之近。无人知晓深度学习未来能给人类世界带来多大的变革，它能否最终实现强人工智能，带领我们进入科幻电影中的世界，但至少目前，深度学习仍牢牢占据学术与工业界研究的主流。<br>  在众多用于实现深度学习技术的框架中，Pytorch与TensorFlow目前被使用得最为广泛，而Pyotrch以其简洁的语法与强大的生态颇受学者的青睐，成为目前学术研究最流行的深度学习框架，本节会以COVID-19 Cases为例，使用Pytorch搭建一个深度学习模型。这是一个非常简单的回归任务，在完成这个任务的过程中，我们将会了解使用Pytorch搭建深度学习模型的流程、各种函数的作用以及训练模型的步骤。</p><h2 id="Task-Description">Task Description</h2><ul><li><p><strong>Objectives</strong></p><ul><li>Solve a regression problem with deep neural networks(DNN)</li><li>Understand basic DNN training steps.</li><li>Get familiar with PyTorch.</li></ul></li><li><p><strong>Task</strong><br>  <strong>COVID-19 Cases Prediction:</strong> Given survey results in the past 5 days in a specific states in U.S., then predict the percentage of new tested positive cases in the 5th day.</p></li><li><p><strong>Data</strong></p><ul><li>Training Data: 2699 samples</li><li>Testing Data: 1078 samples</li><li>Feature Infactors(117):<ul><li>States(37, encoded to one-hot vector)</li><li>COVID-like illness(4*5)<ul><li>cli、ili …</li></ul></li><li>Behavior Indicators(8*5)<ul><li>wearing_mask、travel_outside_state …</li></ul></li><li>Mental Health Indicators(3*5)<ul><li>anxious、depressed …</li></ul></li><li>Tested Positive Cases(1*5)<ul><li>tested_positive(<strong>this is what we want to predict</strong>)</li></ul></li></ul></li></ul></li></ul><h2 id="Download-Data">Download Data</h2><p>  本案例的数据集来自于Kaggle，可以访问以下网站下载数据集.</p><ul><li>Data Source: <a href="https://www.kaggle.com/competitions/ml2022spring-hw1/overview">https://www.kaggle.com/competitions/ml2022spring-hw1/overview</a></li></ul><h2 id="Import-Packages">Import Packages</h2><p>  在开始任务前首先导入我吗需要使用到的Python库</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Numerical Operations</span><br><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># Reading/Writing Data</span><br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> csv<br><br><span class="hljs-comment"># For Progress Bar</span><br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><br><span class="hljs-comment"># Pytorch</span><br><span class="hljs-keyword">import</span> torch <br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset, DataLoader, random_split<br><br><span class="hljs-comment"># For plotting learning curve</span><br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br></code></pre></td></tr></tbody></table></figure><p>  一些库的主要功能：</p><ul><li><code>tqdm</code>: 用于在循环中显示进度条，以增强用户对程序运行进度的可视化体验。主要功能有<strong>进度条显示、时间估算、定制输出、支持多种数据结构、并发安全.</strong></li><li><code>torch.nn</code>: 用于构建神经网络模型。它提供了各种用于构建神经网络层、损失函数、优化器等的类和函数，使用户能够方便地创建、训练和部署各种类型的神经网络模型。主要功能有<strong>神经网络层的构建、损失函数的定义、优化器、自定义模型、数据转换层、模型的保存和加载.</strong></li><li><code>torch.utils.data</code>: 用于处理和管理数据加载、预处理以及批量处理等任务。它提供了一组工具和类，帮助用户有效地加载、处理和传输数据到神经网络模型中，从而方便地进行训练、验证和测试。主要功能有<strong>数据加载与管理、数据预处理、数据批处理、并行加载、迭代加载.</strong></li><li><code>torch.utils.tensorboard</code>: 用于在训练过程中可视化模型训练和性能指标，可以帮助深度学习研究人员和工程师实时监视、分析和优化他们的模型训练过程。主要功能有<strong>训练过程的可视化、模型结构的可视化、嵌入向量的可视化、分布的可视化、图像和音频的可视化.</strong></li></ul><h2 id="Set-Random-Seed">Set Random Seed</h2><p>  由于深度学习的训练过程包含一定的随机性，例如网络参数初始化、随机梯度下降、Dropout等，在学术研究中为了使得结果可以复现，我们通常需要事先设置随机数种子以固定模型的随机性，其代码如下:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">same_seed</span>(<span class="hljs-params">seed</span>): <br>    <span class="hljs-comment"># Fixes random number generator seeds for reproducibility.</span><br>    torch.backends.cudnn.deterministic = <span class="hljs-literal">True</span><br>    torch.backends.cudnn.benchmark = <span class="hljs-literal">False</span><br>    np.random.seed(seed)<br>    torch.manual_seed(seed)<br>    <span class="hljs-keyword">if</span> torch.cuda.is_available():<br>        torch.cuda.manual_seed(seed)<br>        torch.cuda.manual_seed_all(seed)<br></code></pre></td></tr></tbody></table></figure><p>  一些主要函数的功能：</p><ul><li><code>torch.bankends.cudnn.deterministic</code>: 用于控制在使用CUDA进行深度学习训练时是否启用确定性计算。在深度学习中，训练过程中使用CUDA可以显著加速计算，但由于浮点数的不精确性和优化算法的随机性，相同的代码在不同的运行环境中可能会产生不同的结果。但需要注意的是，启用确定性计算会带来一定的计算效率损失，因为一些优化策略可能会被禁用，从而降低了性能。</li><li><code>torch.backends.cudnn.benchmark</code>: 用于控制CuDNN（CUDA Deep Neural Network library，NVIDIA深度神经网络库）在使用CUDA加速时是否自动寻找最适合当前硬件环境的优化算法。当设置为True时，将会让程序在开始时花费一点额外时间，为整个网络的每个卷积层搜索最适合它的卷积实现算法，进而实现网络的加速，然而，不同的卷积算法可能在计算精度和数值稳定性方面有微小差异，这可能导致每次前向传播的结果略微不同。当设置为False时，CuDNN不再搜寻最佳算法，而是选择一个固定的卷积算法。这个固定的算法在相同的输入数据和参数情况下产生相同的输出，因为它不受运行时的微小变化影响。</li><li><code>torch.manual_seed()</code>: 用于为CPU设置随机数生成器的种子，从而控制生成的随机数序列。</li><li><code>torch.cuda.is_available()</code>: 用于检查当前系统是否支持CUDA，以及是否安装了可用的GPU设备。</li><li><code>torch.cuda.manual_seed()</code>: 为GPU设置随机数种子，从而控制生成的随机数序列。</li><li><code>torch.cuda.manual_seed_all()</code>: 如果使用的是多GPU模型，其可以设置用于在所有GPU上生成随机数的种子。</li></ul><h2 id="Dataset">Dataset</h2><p>  在PyTorch中，我们需要将原始数据集定义为一个Dataset实例，定义Dataset实例的作用是将数据加载和预处理封装成一个可迭代的对象，以便在训练过程中有效地加载和使用数据。其代码如下：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">COVID19Dataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-string">'''</span><br><span class="hljs-string">    x: Features.</span><br><span class="hljs-string">    y: Targets, if none, do prediction.</span><br><span class="hljs-string">    '''</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, x, y=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-keyword">if</span> y <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            self.y = y<br>        <span class="hljs-keyword">else</span>:<br>            self.y = torch.FloatTensor(y)<br>        self.x = torch.FloatTensor(x)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        <span class="hljs-keyword">if</span> self.y <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">return</span> self.x[idx]<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">return</span> self.x[idx], self.y[idx]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.x)<br></code></pre></td></tr></tbody></table></figure><p>  Dataset类中一些函数的功能：</p><ul><li><code>__init__()</code>: Dataset类的构造函数，用于初始化数据集的属性和参数。</li><li><code>__getitem__()</code>: 用于根据索引获取数据集中的一个样本。在训练过程中，DataLoader会通过迭代访问数据集，调用__getitem__来获取样本。</li><li><code>__len__()</code>: 返回数据集中样本的数量，通常通过获取数据的长度来实现。</li></ul><h2 id="Feature-Selection">Feature Selection</h2><p>  原始数据集包含众多的特征，但如果将所有的特征作为训练数据，可能会造成训练时间过长，模型过拟合等问题。实际上特征之间往往存在多重共线性，对于这个任务，我们并不需要数量非常庞大的特征，这时我们需要进行特征工程方面的工作，这里不展开解释特征工程的方法，有兴趣可以自行查阅相关资料。总得来说，当我们的数据集包含众多特征时，我们需要保有进行特征工程的意识。在本案例为了简单起见，省略掉了特征筛选的过程，以人为设置的特征作为筛选结果。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">select_feat</span>(<span class="hljs-params">train_data, valid_data, test_data, select_all=<span class="hljs-literal">True</span></span>):<br>    <span class="hljs-string">'''Selects useful features to perform regression'''</span><br>    y_train, y_valid = train_data[:,-<span class="hljs-number">1</span>], valid_data[:,-<span class="hljs-number">1</span>]<br>    raw_x_train, raw_x_valid, raw_x_test = train_data[:,:-<span class="hljs-number">1</span>], valid_data[:,:-<span class="hljs-number">1</span>], test_data<br><br>    <span class="hljs-keyword">if</span> select_all:<br>        feat_idx = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(raw_x_train.shape[<span class="hljs-number">1</span>]))<br>    <span class="hljs-keyword">else</span>:<br>        feat_idx = [<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>] <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> Select suitable feature columns.</span><br>        <br>    <span class="hljs-keyword">return</span> raw_x_train[:,feat_idx], raw_x_valid[:,feat_idx], raw_x_test[:,feat_idx], y_train, y_valid<br></code></pre></td></tr></tbody></table></figure><h2 id="Dataloader">Dataloader</h2><p>  在定义了Dataset实例后，我们通常需要将Dataset实例传递给Dataloader类。Dataloader是一个迭代器，用于加载和处理训练数据，其可以将数据集划分成小批量的数据，并可以自动进行数据预处理、洗牌和GPU加速等操作。在定义Dataloader时，我们需要确定Training Data、Testing Data、代码如下：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Set seed for reproducibility</span><br>same_seed(config[<span class="hljs-string">'seed'</span>])<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_valid_split</span>(<span class="hljs-params">data_set, valid_ratio, seed</span>):<br>    <span class="hljs-comment">#Split provided training data into training set and validation set</span><br>    valid_set_size = <span class="hljs-built_in">int</span>(valid_ratio * <span class="hljs-built_in">len</span>(data_set)) <br>    train_set_size = <span class="hljs-built_in">len</span>(data_set) - valid_set_size<br>    train_set, valid_set = random_split(data_set, [train_set_size, valid_set_size], generator=torch.Generator().manual_seed(seed))<br>    <span class="hljs-keyword">return</span> np.array(train_set), np.array(valid_set)<br><br><span class="hljs-comment"># train_data size: 2699 x 118 (id + 37 states + 16 features x 5 days) </span><br><span class="hljs-comment"># test_data size: 1078 x 117 (without last day's positive rate)</span><br>train_data, test_data = pd.read_csv(<span class="hljs-string">'./covid.train.csv'</span>).values, pd.read_csv(<span class="hljs-string">'./covid.test.csv'</span>).values<br>train_data, valid_data = train_valid_split(train_data, config[<span class="hljs-string">'valid_ratio'</span>], config[<span class="hljs-string">'seed'</span>])<br><br><span class="hljs-comment"># Print out the data size.</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f"""train_data size: <span class="hljs-subst">{train_data.shape}</span> </span><br><span class="hljs-string">valid_data size: <span class="hljs-subst">{valid_data.shape}</span> </span><br><span class="hljs-string">test_data size: <span class="hljs-subst">{test_data.shape}</span>"""</span>)<br><br><span class="hljs-comment"># Select features</span><br>x_train, x_valid, x_test, y_train, y_valid = select_feat(train_data, valid_data, test_data, config[<span class="hljs-string">'select_all'</span>])<br><br><span class="hljs-comment"># Print out the number of features.</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f'number of features: <span class="hljs-subst">{x_train.shape[<span class="hljs-number">1</span>]}</span>'</span>)<br><br>train_dataset, valid_dataset, test_dataset = COVID19Dataset(x_train, y_train),COVID19Dataset(x_valid, y_valid), COVID19Dataset(x_test)<br><br><span class="hljs-comment"># Pytorch data loader loads pytorch dataset into batches.</span><br>train_loader = DataLoader(train_dataset, batch_size=config[<span class="hljs-string">'batch_size'</span>], shuffle=<span class="hljs-literal">True</span>, pin_memory=<span class="hljs-literal">True</span>)<br>valid_loader = DataLoader(valid_dataset, batch_size=config[<span class="hljs-string">'batch_size'</span>], shuffle=<span class="hljs-literal">True</span>, pin_memory=<span class="hljs-literal">True</span>)<br>test_loader = DataLoader(test_dataset, batch_size=config[<span class="hljs-string">'batch_size'</span>], shuffle=<span class="hljs-literal">False</span>, pin_memory=<span class="hljs-literal">True</span>)  <br></code></pre></td></tr></tbody></table></figure><p>  在这段代码中，我们首先通常前文定义的<code>same_seed()</code>函数固定随机数种子，使得之后的一系列操作的结果可复现。然后我们定义了<code>train_valid_split()</code>函数，这个函数用于将Training Data进行再次划分，分为真正用于训练的数据与用于检验模型泛化能力的数据，通过<code>valid_ratio</code>参数，我们可以设置train data和valid data的划分比例。定义好函数后，我们读取原始csv文件，得到原始的Training Data和Testing Data，利用<code>train_valid_split()</code>函数得到划分好的train data与valid data。此时，这些数据集仍包含着所有的特征，我们需要通过前文定义的<code>select_feat()</code>函数进行特征筛选，得到正在用于本次任务的数据。最后我们使用这些数据定义Dataset实例，并将定义好的Dataset传递给<code>Dataloader</code>用于之后训练模型。</p><p>  <code>Dataloader</code>的一些主要参数及其作用：</p><ul><li><code>dataset</code>: 指定要加载的Dataset实例，这是DataLoader的必需参数，用于提供数据样本。</li><li><code>batch_size</code>: 指定每个批次中包含的样本数量。将数据划分成小批次可以在训练时提高内存的使用效率。</li><li><code>shuffle</code>: 设置为True时，在每次返回一批次前会对数据进行随机洗牌。这有助于提高模型的泛化能力。默认值为False。</li><li><code>pin_memory</code>：如果在GPU上训练，可以将此参数设置为True，以便在加载数据时将数据置于CUDA固定内存中，从而加速数据传输。</li><li><code>drop_last</code>：如果数据样本数量不能整除批次大小，设置为True时，会丢弃最后一个不完整的批次。默认值为False。</li><li><code>num_workers</code>：指定用于数据加载的并行线程数。通过并行加载数据可以加快速度，特别是在数据集较大时。</li></ul><h2 id="Neural-Network-Model">Neural Network Model</h2><p>  准备好数据后，我们便需要构建本次任务所用的深度学习模型，这里我构建了一个简单的三层前馈神经网络模型，这个神经网络包含一个输入层，一个隐藏层，一个输出层，其代码如下:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">My_Model</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_dim</span>):<br>        <span class="hljs-built_in">super</span>(My_Model, self).__init__()<br>        <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> modify model's structure, be aware of dimensions. </span><br>        self.layers = nn.Sequential(<br>            nn.Linear(input_dim, <span class="hljs-number">16</span>),<br>            nn.ReLU(),<br>            nn.Linear(<span class="hljs-number">16</span>, <span class="hljs-number">8</span>),<br>            nn.ReLU(),<br>            nn.Linear(<span class="hljs-number">8</span>, <span class="hljs-number">1</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.layers(x)<br>        x = x.squeeze(<span class="hljs-number">1</span>) <span class="hljs-comment"># (B, 1) -&gt; (B)</span><br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></tbody></table></figure><p>  一些函数及类的主要功能：</p><ul><li><p><code>nn.Module</code>: 提供了一种组织和管理神经网络组件的方式，使得模型的构建、参数管理和前向传播等过程更加简洁和可控。我们在实际应用中定义的模型<code>My_Model</code>需要继承<code>nn.Module</code>类。</p></li><li><p><code>nn.Sequrntial()</code>: PyTorch中的一个模型容器，用于按顺序组合多个神经网络模块，从而构建一个序列式的神经网络模型。它可以在不需要自定义模型类的情况下，方便地定义简单的神经网络结构。</p></li><li><p><code>nn.Linear()</code>: 用于定义线性变换（全连接层）。它将输入数据与权重矩阵相乘，并添加一个偏置，从而实现线性变换。nn.Linear()主要用于神经网络中的全连接层，将输入特征映射到输出特征。<br>  除了使用<code>nn.Linear()</code>进行线性神经网络层，我们还可以根据任务的不同使用诸如卷积神经网络层<code>nn.convXd</code>、循环神经网络层<code>nn.RNN()</code>等。</p></li><li><p><code>nn.ReLU()</code>: 用于实现激活函数 ReLU（Rectified Linear Activation）。ReLU 是深度学习中常用的激活函数之一，它对输入进行非线性变换，将负值变为零，保持正值不变。ReLU 激活函数在神经网络中引入非线性性质，有助于模型学习复杂的特征和表示。<br>  除了ReLU之外，还有一些常用的激活函数，例如<code>nn.Sigmoid()</code>(Sigmoid函数)、<code>nn.Tanh()</code>(双曲正切激活函数)。我们可以根据训练数据的特点，选择合适的激活函数，或者通过实验确定最佳激活函数。</p></li><li><p><code>forward()</code>: 用于将训练数据通过网络进行前向传播。</p></li></ul><h2 id="Training-Loop">Training Loop</h2><p>  在准备好数据与模型后，下一步就是训练模型，训练模型的主要流程如下：</p><figure class="highlight markdown"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs markdown">设置损失函数<br>定义优化器<br>定义tensorboard<br>总训练轮次、当前训练轮次、当前最佳误差、无效训练次数等参数初始化<br><br>for 每一训练轮次 in 总训练轮次：<br><span class="hljs-code">    设置模型为训练模式</span><br><span class="hljs-code">    定义一个空的训练误差列表</span><br><span class="hljs-code">    将训练数据传递给tqdm</span><br><span class="hljs-code">    for 每一批次特征数据、标签数据 in tqdm(训练数据):</span><br><span class="hljs-code">        初始化优化器梯度值</span><br><span class="hljs-code">        将数据移动到GPU中</span><br><span class="hljs-code">        将特征数据输入模型，通过正向传播得到标签数据的预测值</span><br><span class="hljs-code">        通过标签数据的预测值与真实值得到误差</span><br><span class="hljs-code">        误差反向传播，得到误差对于网络参数的梯度</span><br><span class="hljs-code">        利用梯度下降算法更新网络参数</span><br><span class="hljs-code">        step += 1</span><br><span class="hljs-code">        将本批次的误差添加到训练误差列表中</span><br><span class="hljs-code">        自定义训练进度条内容</span><br><span class="hljs-code">    通过对训练误差列表求平均得到这一轮次的平均训练误差</span><br><span class="hljs-code">    将setp与平均训练误差添加到tensorboard中</span><br><span class="hljs-code">    将模型设置为评估模式</span><br><span class="hljs-code">    定义一个空的评估误差列表</span><br><span class="hljs-code">    for 每一批次特征数据、标签数据 in 评估数据:</span><br><span class="hljs-code">        将数据移动到GPU中</span><br><span class="hljs-code">        with 初始化优化器梯度值:</span><br><span class="hljs-code">            将特征数据输入当前模型，通过正向传播得到标签数据的预测值</span><br><span class="hljs-code">            通过标签数据的预测值与真实值得到误差</span><br><span class="hljs-code">        将本批次的误差添加到评估误差列表</span><br><span class="hljs-code">    通过对评估误差列表求平均得到这一轮次的平均评估误差</span><br><span class="hljs-code">    print(当前训练轮次/平均训练误差/平均评估误差)</span><br><span class="hljs-code">    将setp与平均评估误差添加到tensorboard中</span><br><span class="hljs-code">    if 平均评估误差 &lt; 当前最佳误差:</span><br><span class="hljs-code">        将当前最佳误差设置为平均评估误差</span><br><span class="hljs-code">        保存当前模型</span><br><span class="hljs-code">        初始化无效训练次数</span><br><span class="hljs-code">    else:</span><br><span class="hljs-code">        无效训练次数 += 1</span><br><span class="hljs-code">    if 无效训练次数 &gt; 所设置的无效训练次数上限:</span><br><span class="hljs-code">        停止训练</span><br></code></pre></td></tr></tbody></table></figure><p>  这一流程的代码如下：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">trainer</span>(<span class="hljs-params">train_loader, valid_loader, model, config, device</span>):<br><br>    criterion = nn.MSELoss(reduction=<span class="hljs-string">'mean'</span>) <span class="hljs-comment"># Define your loss function, do not modify this.</span><br><br>    <span class="hljs-comment"># Define your optimization algorithm. </span><br>    optimizer = torch.optim.SGD(model.parameters(), lr=config[<span class="hljs-string">'learning_rate'</span>], momentum=<span class="hljs-number">0.9</span>) <br><br>    writer = SummaryWriter() <span class="hljs-comment"># Writer of tensoboard.</span><br><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.isdir(<span class="hljs-string">'./models'</span>):<br>        os.mkdir(<span class="hljs-string">'./models'</span>) <span class="hljs-comment"># Create directory of saving models.</span><br><br>    n_epochs, best_loss, step, early_stop_count = config[<span class="hljs-string">'n_epochs'</span>], math.inf, <span class="hljs-number">0</span>, <span class="hljs-number">0</span><br><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_epochs):<br>        model.train() <span class="hljs-comment"># Set your model to train mode.</span><br>        loss_record = []<br><br>        <span class="hljs-comment"># tqdm is a package to visualize your training progress.</span><br>        train_pbar = tqdm(train_loader, position=<span class="hljs-number">0</span>, leave=<span class="hljs-literal">True</span>)<br><br>        <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> train_pbar:<br>            optimizer.zero_grad()               <span class="hljs-comment"># Set gradient to zero.</span><br>            x, y = x.to(device), y.to(device)   <span class="hljs-comment"># Move your data to device. </span><br>            pred = model(x)             <br>            loss = criterion(pred, y)<br>            loss.backward()                     <span class="hljs-comment"># Compute gradient(backpropagation).</span><br>            optimizer.step()                    <span class="hljs-comment"># Update parameters.</span><br>            step += <span class="hljs-number">1</span><br>            loss_record.append(loss.detach().item())<br>            <br>            <span class="hljs-comment"># Display current epoch number and loss on tqdm progress bar.</span><br>            train_pbar.set_description(<span class="hljs-string">f'Epoch [<span class="hljs-subst">{epoch+<span class="hljs-number">1</span>}</span>/<span class="hljs-subst">{n_epochs}</span>]'</span>)<br>            train_pbar.set_postfix({<span class="hljs-string">'loss'</span>: loss.detach().item()})<br><br>        mean_train_loss = <span class="hljs-built_in">sum</span>(loss_record)/<span class="hljs-built_in">len</span>(loss_record)<br>        writer.add_scalar(<span class="hljs-string">'Loss/train'</span>, mean_train_loss, step)<br><br>        model.<span class="hljs-built_in">eval</span>() <span class="hljs-comment"># Set your model to evaluation mode.</span><br>        loss_record = []<br>        <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> valid_loader:<br>            x, y = x.to(device), y.to(device)<br>            <span class="hljs-keyword">with</span> torch.no_grad():<br>                pred = model(x)<br>                loss = criterion(pred, y)<br><br>            loss_record.append(loss.item())<br>            <br>        mean_valid_loss = <span class="hljs-built_in">sum</span>(loss_record)/<span class="hljs-built_in">len</span>(loss_record)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f'Epoch [<span class="hljs-subst">{epoch+<span class="hljs-number">1</span>}</span>/<span class="hljs-subst">{n_epochs}</span>]: Train loss: <span class="hljs-subst">{mean_train_loss:<span class="hljs-number">.4</span>f}</span>, Valid loss: <span class="hljs-subst">{mean_valid_loss:<span class="hljs-number">.4</span>f}</span>'</span>)<br>        writer.add_scalar(<span class="hljs-string">'Loss/valid'</span>, mean_valid_loss, step)<br><br>        <span class="hljs-keyword">if</span> mean_valid_loss &lt; best_loss:<br>            best_loss = mean_valid_loss<br>            torch.save(model.state_dict(), config[<span class="hljs-string">'save_path'</span>]) <span class="hljs-comment"># Save your best model</span><br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">'Saving model with loss {:.3f}...'</span>.<span class="hljs-built_in">format</span>(best_loss))<br>            early_stop_count = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">else</span>: <br>            early_stop_count += <span class="hljs-number">1</span><br><br>        <span class="hljs-keyword">if</span> early_stop_count &gt;= config[<span class="hljs-string">'early_stop'</span>]:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">'\nModel is not improving, so we halt the training session.'</span>)<br>            <span class="hljs-keyword">return</span><br></code></pre></td></tr></tbody></table></figure><p>  训练流程中一些函数的作用及参数:</p><ul><li><code>nn.MSELoss()</code>: PyTorch 中的一个损失函数模块，用于计算均方误差损失。<br>  常用的一些损失函数: 交叉熵损失<code>nn.CrossEntropyLoss()</code>，负对数似然损失<code>nn.NLLLoss()</code>，KL散度损失<code>nn.KLDivLoss()</code>等，可以更具任务特点与所用模型选择合适的损失函数，例如均方误差损失多用于回归任务，交叉熵损失多用于分类任务，KL散度损失一般用于GAN模型。</li><li><code>torch.optim.SGD()</code>: 用于实现随机梯度下降（Stochastic Gradient Descent，SGD）优化算法。SGD 是深度学习中最基本和常用的优化算法之一，用于调整模型的参数以最小化损失函数。其重要参数:<ul><li><code>params</code>：这是一个模型参数的可迭代对象，指定了需要进行优化的参数。一般通过<code>model.parameters()</code>来获取模型中的参数列表。</li><li><code>lr</code>：学习率，控制参数更新的步长。它决定了每次参数更新的幅度，过大可能导致不稳定的训练，过小可能导致收敛速度缓慢。</li><li><code>momentum</code>: 动量，用于加速梯度下降过程。设置一个介于 0 到 1 之间的值，代表在更新参数时考虑前一次的动量。较大的动量值可以帮助跳出局部最小值。</li></ul></li></ul><p>  除开基础的SGD优化方法，深度学习中还有Adam<code>torch.optim.Adam()</code>，RMSprop<code>torch.optim.RMSprop()</code>，Adagrad<code>torch.optim.Adagrad()</code>等优化算法，它们各种适应不同的数据特点。想进一步了解深度学习中的优化算法可以查阅相关资料。</p><p>  完成模型训练的流程后，下一步便是设置训练步骤中所需要的超参数。</p><h2 id="Configurations">Configurations</h2><p>  设置我们在整个任务中所需要用到的参数：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">device = <span class="hljs-string">'cuda'</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">'cpu'</span><br>config = {<br>    <span class="hljs-string">'seed'</span>: <span class="hljs-number">5201314</span>,      <span class="hljs-comment"># random seed</span><br>    <span class="hljs-string">'select_all'</span>: <span class="hljs-literal">True</span>,   <span class="hljs-comment"># Whether to use all features.</span><br>    <span class="hljs-string">'valid_ratio'</span>: <span class="hljs-number">0.2</span>,   <span class="hljs-comment"># validation_size = train_size * valid_ratio</span><br>    <span class="hljs-string">'n_epochs'</span>: <span class="hljs-number">3000</span>,     <span class="hljs-comment"># Number of epochs.            </span><br>    <span class="hljs-string">'batch_size'</span>: <span class="hljs-number">256</span>, <br>    <span class="hljs-string">'learning_rate'</span>: <span class="hljs-number">1e-5</span>,              <br>    <span class="hljs-string">'early_stop'</span>: <span class="hljs-number">400</span>,    <span class="hljs-comment"># If model has not improved for this many consecutive epochs, stop training.     </span><br>    <span class="hljs-string">'save_path'</span>: <span class="hljs-string">'./models/model.ckpt'</span>  <span class="hljs-comment"># Your model will be saved here.</span><br>}<br></code></pre></td></tr></tbody></table></figure><h2 id="Start-training">Start training!</h2><p>  万事俱备，开始训练我们的模型吧！</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">model = My_Model(input_dim=x_train.shape[<span class="hljs-number">1</span>]).to(device) <span class="hljs-comment"># put your model and data on the same computation device.</span><br>trainer(train_loader, valid_loader, model, config, device)<br></code></pre></td></tr></tbody></table></figure><p>  由于深度学习模型的参数量一般较大，训练可能会花费一定的时间，待训练完成后我们便得到了已更新好参数的神经网络模型。</p><h2 id="Plot-learning-curves-with-tensorboard">Plot learning curves with tensorboard</h2><p>  通过使用<code>tensorboard</code>，我们可以得到损失曲线与学习曲线，便于我们更好地理解模型训练的过程。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">%reload_ext tensorboard<br>%tensorboard --logdir=./runs/<br></code></pre></td></tr></tbody></table></figure><h2 id="Testing">Testing</h2><p>  最后，我们可以将Testing Data输入到已经更新好参数的模型，得到相应标签数据的预测值，我们可以通过比较真实值与预测值之间的差异评价模型训练的结果。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">test_loader, model, device</span>):<br>    model.<span class="hljs-built_in">eval</span>() <span class="hljs-comment"># Set your model to evaluation mode.</span><br>    preds = []<br>    <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> tqdm(test_loader):<br>        x = x.to(device)                        <br>        <span class="hljs-keyword">with</span> torch.no_grad():                   <br>            pred = model(x)                     <br>            preds.append(pred.detach().cpu())   <br>    preds = torch.cat(preds, dim=<span class="hljs-number">0</span>).numpy()  <br>    <span class="hljs-keyword">return</span> preds<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">save_pred</span>(<span class="hljs-params">preds, file</span>):<br>    <span class="hljs-string">''' Save predictions to specified file '''</span><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file, <span class="hljs-string">'w'</span>) <span class="hljs-keyword">as</span> fp:<br>        writer = csv.writer(fp)<br>        writer.writerow([<span class="hljs-string">'id'</span>, <span class="hljs-string">'tested_positive'</span>])<br>        <span class="hljs-keyword">for</span> i, p <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(preds):<br>            writer.writerow([i, p])<br><br>model = My_Model(input_dim=x_train.shape[<span class="hljs-number">1</span>]).to(device)<br>model.load_state_dict(torch.load(config[<span class="hljs-string">'save_path'</span>]))<br>preds = predict(test_loader, model, device) <br>save_pred(preds, <span class="hljs-string">'pred.csv'</span>)      <br></code></pre></td></tr></tbody></table></figure><p>  预测的结果保存在<code>pred.csv</code>文件中。</p><h2 id="Reference">Reference</h2><p><a href="https://www.bilibili.com/video/BV1Wv411h7kN?p=11&amp;vd_source=234cf2ac075a1558881a6956450ddf89">https://www.bilibili.com/video/BV1Wv411h7kN?p=11&amp;vd_source=234cf2ac075a1558881a6956450ddf89</a></p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>最优化理论-2.仿射集与仿射包</title>
    <link href="/2023/08/08/%E6%9C%80%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA-2-%E4%BB%BF%E5%B0%84%E9%9B%86%E4%B8%8E%E4%BB%BF%E5%B0%84%E5%8C%85/"/>
    <url>/2023/08/08/%E6%9C%80%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA-2-%E4%BB%BF%E5%B0%84%E9%9B%86%E4%B8%8E%E4%BB%BF%E5%B0%84%E5%8C%85/</url>
    
    <content type="html"><![CDATA[<h1 id="仿射集与仿射包">仿射集与仿射包</h1><p>  上节讨论了最优化问题的基本形式，其数学表达为：<br><span class="math display">\[min \space\space f_0(x)\]</span></p><p><span class="math display">\[s.t. \space\space f_i(x) \leq b_i,i=1,\dots,m\]</span></p><p>  其中 <span class="math inline">\(x \in \mathbb{R}^n, x =\begin{bmatrix}  x_1,x_2,\dots,x_n \\ \end{bmatrix}^T\)</span>.<br>  最优化问题可以分为凸优化与非凸优化，本博客会首先从凸优化开始讨论。要准确认识何为"凸"，我们需要学习仿射集、凸集、凸函数等概念，接下来我们先从仿射集的概念开始学习。</p><h2 id="仿射集">仿射集</h2><p>  设<span class="math inline">\(x_1,x_2 \in\mathbb{R}^n\)</span>，且<span class="math inline">\(x_1 \neqx_2\)</span>，<br>  经过<span class="math inline">\(x_1,x_2\)</span>两点的直线<span class="math inline">\(y\)</span>的表达式为：<span class="math inline">\(y=\theta x_1 + (1-\theta)x_2, \theta \in\mathbb{R}.\)</span><br>  经过<span class="math inline">\(x_1,x_2\)</span>两点的线段<span class="math inline">\(y\)</span>的表达式为：<span class="math inline">\(y=\theta x_1 + (1-\theta)x_2, \theta \in[0,1].\)</span></p><h3 id="仿射集的定义与性质">仿射集的定义与性质</h3><p><strong>定义</strong><br>  一个集合<span class="math inline">\(C\)</span>是仿射集，若<span class="math inline">\(\forall x_1,x_2 \in C ,\theta \in \mathbb{R},y=\theta x_1 + (1-\theta)x_2 \in C.\)</span></p><ul><li>注1：从定义可以得知，若经过集合<span class="math inline">\(C\)</span>内任意两点<span class="math inline">\(x_1,x_2\)</span>的直线为在集合<span class="math inline">\(C\)</span>内，则集合<span class="math inline">\(C\)</span>为仿射集。<br></li><li>注2：由仿射集的定义可知：在二维空间中，整个二维空间构成一个仿射集，但二维空间中的某一区域则不是仿射集；二维空间中的直线是一个仿射集，但线段不是仿射集。</li></ul><p><strong>性质1：仿射集对仿射组合封闭.</strong><br>  设<span class="math inline">\(\forall x_1,x_2,\dots,x_k \in C,\theta_1,\theta_2,\dots,\theta_k \in \mathbb{R}, \theta_1 + \theta_2 +\dots + \theta_k = 1\)</span>，定义<strong>仿射组合:</strong><br><span class="math display">\[\theta_1x_1+\theta_2x_2+\dots+\theta_kx_k\]</span></p><p>若集合<span class="math inline">\(C\)</span>为仿射集，则仿射组合<span class="math inline">\(\theta_1x_1+\theta_2x_2+\dots+\theta_kx_k\in C\)</span>.</p><p><strong>证明:</strong><br>  借助定义，使用数学归纳法来证明该性质：<br>  当<span class="math inline">\(k=2\)</span>时，由定义1可知：<br><span class="math display">\[集合C是仿射集 \Rightarrow \forall x_1,x_2\in C,\theta \in \mathbb{R},\theta x_1+(1-\theta)x_2 \in C\]</span></p><p>  当<span class="math inline">\(k=t\)</span>时，假设该结论仍然成立，即：</p><p><span class="math display">\[集合C是仿射集 \Rightarrow \forallx_1,x_2,\dots,x_t \in C,\theta_1,\theta_2,\dots,\theta_t \in \mathbb{R},\sum_{i=1}^{t}\theta_i=1, 有\sum_{i=1}^{t}\theta_ix_i \in C\]</span></p><p>  当<span class="math inline">\(k=t+1\)</span>时，需要证明以下结论成立：</p><p><span class="math display">\[集合C是仿射集 \Rightarrow \forallx_1,x_2,\dots,x_{t+1} \in C,\theta_1,\theta_2,\dots,\theta_{t+1} \in\mathbb{R}, \sum_{i=1}^{t+1}\theta_i=1, 有\sum_{i=1}^{t+1}\theta_ix_i\in C\]</span></p><p>  已知集合<span class="math inline">\(C\)</span>是仿射集，<span class="math inline">\(\forall x_1,x_2,\dots,x_{t+1} \inC,\theta_1,\theta_2,\dots,\theta_{t+1} \in \mathbb{R},\sum_{i=1}^{t+1}\theta_i=1\)</span>,<br>  令 <span class="math inline">\(\beta_i =\theta_i/\sum_{i=1}^{t}\theta_i, i=1,\dots t\)</span>，则有<span class="math inline">\(\sum_{i=1}^{t}\beta_i = 1\)</span>.<br>  由前文的结论可知：<span class="math inline">\(\sum_{i=1}^{t}\beta_ix_i\in C\)</span>.<br>  令<span class="math inline">\(y = \sum_{i=1}^{t}{\beta_ix_i} \in C,x_{t+1} \in C\)</span>，由前文的结论可知: <span class="math inline">\((\sum_{i=1}^{t}\theta_i)y +(1-\sum_{i=1}^{t}\theta_i)x_{t+1} \in C\)</span>.<br>  即: <span class="math inline">\(\sum_{i=1}^{t+1}\theta_ix_i \inC\)</span>，证毕.</p><p><strong>性质2：与仿射集相关的子空间也是仿射集.</strong><br>  设集合<span class="math inline">\(C\)</span>是仿射集，对<span class="math inline">\(\forall x_0 \in C\)</span>，令<span class="math inline">\(V=C-x_0= \{ x-x_0 \vert x \in C\}\)</span>，称集合<span class="math inline">\(V\)</span>为与<span class="math inline">\(C\)</span>相关的子空间.<br>  集合<span class="math inline">\(V\)</span>有以下性质：<br>  (1) <span class="math inline">\(V\)</span>为仿射集.<br>  (2) 记<span class="math inline">\(v_1,v_2 \in V\)</span>，则有<span class="math inline">\(\alpha v_1+\beta v_2 \in V, \alpha,\beta \in\mathbb{R}.\)</span><br>  (3) 集合V一定包含原点<span class="math inline">\(\mathbf{0}\)</span>.</p><p><strong>证明:</strong><br>  记<span class="math inline">\(v_1 ,v_2 \in V, \theta_1,\theta_2 \in\mathbb{R},\theta_1+\theta_2 = 1.\)</span><br>  设<span class="math inline">\(v_1 = x_1 - x_0, v_2 = x_2 - x_0,x_1,x_2 \in C.\)</span><br>  <span class="math inline">\(\theta_1v_1 + \theta_2v_2 =\theta_1(x_1-x_0)+\theta_2(x_2-x_0) = \theta_1x_1 + \theta_2x_2 -x_0\)</span><br>  <span class="math inline">\(\because C\)</span>是仿射集，且<span class="math inline">\(\theta_1+\theta_2=1\)</span>，<span class="math inline">\(\therefore \theta_1x_1+\theta_2x_2 \inC\)</span>，令<span class="math inline">\(x_3 = \theta_1x_1+\theta_2x_2,x_3 \in C\)</span>.<br>  则 <span class="math inline">\(\theta_1v_1+\theta_2v_2 =\theta_1x_1+\theta_2x_2 - x_0 = x_3 - x_0 \in V, \theta_1 + \theta_2 =1.\)</span><br>  故集合<span class="math inline">\(V\)</span>是仿射集，性质(1)证毕.<br>  设<span class="math inline">\(\alpha, \beta \in\mathbb{R},\)</span><br>  <span class="math inline">\(\alpha v_1 + \beta v_2 \in V\Leftrightarrow \alpha v_1 + \beta v_2 +x_0 = x, x \in C \Leftrightarrow\alpha v_1 + \beta v_2 + x_0 \in C.\)</span><br>  <span class="math inline">\(\alpha v_1 + \beta v_2 +x_0 =\alpha(v_1+x_0)+\beta(v_2+x_0)+(1-\alpha-\beta)x_0=\alpha x_1 + \betax_2 + (1-\alpha-\beta)x_0.\)</span><br>  由仿射集的性质1可知: 仿射组合 <span class="math inline">\(\alpha x_1 +\beta x_2 + (1-\alpha-\beta)x_0 \in C.\)</span><br>  故有<span class="math inline">\(\alpha v_1 + \beta v_2 \inV\)</span>，性质(2)证毕.<br>  由集合V的定义易证性质(3)成立.</p><h3 id="仿射集的实例">仿射集的实例</h3><p><strong>例：线性方程组的解集是仿射集</strong></p><p>  设有线性方程组 <span class="math inline">\(Ax=b, A \in\mathbb{R}^{m\times n}, b \in \mathbb{R}^m, x\in \mathbb{R}^n. 其解集C=\{ x \vert Ax=b \}\)</span>，则集合<span class="math inline">\(C\)</span>是仿射集.</p><p><strong>证明:</strong><br>  对<span class="math inline">\(\forall x_1,x_2 \in C\)</span>，有 <span class="math inline">\(Ax_1=b, Ax_2 = b.\)</span><br>  若 <span class="math inline">\(\theta x_1 + (1-\theta)x_2 \in C,\theta \in \mathbb{R}\)</span>，则集合<span class="math inline">\(C\)</span>是仿射集.<br>  <span class="math inline">\(\theta x_1 + (1-\theta)x_2 \in C\Leftrightarrow \theta x_1 + (1-\theta)x_2 = x, x \in C \LeftrightarrowA(\theta x_1 + (1-\theta)x_2) = Ax = b.\)</span><br>  <span class="math inline">\(A(\theta x_1 + (1-\theta)x_2) = \thetaAx_1 + (1-\theta)Ax_2 = \theta b + (1-\theta)b = b\)</span><br>  故有<span class="math inline">\(\theta x_1 + (1-\theta)x_2 \inC\)</span>，解集<span class="math inline">\(C\)</span>是仿射集.</p><ul><li><strong>注：与线性方程组解集C相关的子空间V是系数矩阵A的核</strong></li></ul><p><strong>证明:</strong><br>  设集合 <span class="math inline">\(V= \{ x-x_0 \vert x \in C \},\forall x_0 \in C\)</span>，则集合V是与C相关的子空间.<br>  由<span class="math inline">\(Ax = b \Rightarrow V= \{ x-x_0 \vertAx=b \}\)</span>.<br>  由<span class="math inline">\(Ax_0 = b \Rightarrow V = \{ x-x_0 \vertAx=Ax_0 \} = \{ x-x_0 \vert A(x-x_0)=0 \}.\)</span><br>  令<span class="math inline">\(y = x-x_0， V= \{ y \vert Ay = 0\}\)</span>，即<span class="math inline">\(V= ker \spaceA\)</span>，证毕.</p><h2 id="仿射包">仿射包</h2><h3 id="仿射包的定义">仿射包的定义</h3><p>  对任意集合<span class="math inline">\(C\)</span>，记：</p><p><span class="math display">\[aff \space C = \{\theta_1x_1+\dots+\theta_kx_k \vert \forall x_1,\dots,x_k \in C,\theta_i \in \mathbb{R}, \sum_{i=1}^{k}\theta_i=1 \}\]</span></p><p>则集合<span class="math inline">\(aff \space C\)</span>称为集合<span class="math inline">\(C\)</span>的仿射包.</p><p><font color="green">注：仿射包<span class="math inline">\(aff\spaceC\)</span>是集合<span class="math inline">\(C\)</span>所能构造的最小的仿射集，是集合<span class="math inline">\(C\)</span>中元素的仿射组合的集合.</font></p><h3 id="仿射包的实例">仿射包的实例</h3><p><strong>例：二维空间中的仿射包</strong></p><p>  设集合<span class="math inline">\(V\)</span>为二维平面，<span class="math inline">\(x_1,x_2,x_3\)</span>为二维平面中不在同一条直线上的三个点，<span class="math inline">\(L\)</span>为经过点<span class="math inline">\(x_1,x_2\)</span>的直线，如下图所示：</p><center><img src="https://s2.loli.net/2023/08/13/mh4IpMxoERZ3sy9.png" width="75%"><div data-align="center">Image1: 二维平面及其元素</div></center><p><br></p><p>  设集合<span class="math inline">\(C_1 = \{ x_1,x_2\}\)</span>，则集合<span class="math inline">\(C_1\)</span>的仿射包<span class="math inline">\(aff \space C_1 = \{ \theta_1x_1+\theta_2x_2 \vertx_1,x_2 \in C_1, \theta_1,\theta_2 \in \mathbb{R}, \theta_1+\theta_2=1\}\)</span>，仿射包<span class="math inline">\(aff \spaceC_1\)</span>即为直线<span class="math inline">\(L\)</span>.<br>  设集合<span class="math inline">\(C_2 = \{ x_1,x_2,x_3\}\)</span>，则集合<span class="math inline">\(C_2\)</span>的仿射包<span class="math inline">\(aff \space C_2 = \{\theta_1x_1+\theta_2x_2+\theta_3x_3 \vert x_1,x_2,x_3 \in C_2,\theta_1,\theta_2,\theta_3 \in \mathbb{R}, \theta_1+\theta_2+\theta_3 =1 \}\)</span>，仿射包<span class="math inline">\(aff \spaceC_2\)</span>即为二维平面<span class="math inline">\(V\)</span>.</p>]]></content>
    
    
    <categories>
      
      <category>最优化理论</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>最优化理论-1.最优化问题的基本形式</title>
    <link href="/2023/08/03/%E6%9C%80%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA-1-%E6%9C%80%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%BD%A2%E5%BC%8F/"/>
    <url>/2023/08/03/%E6%9C%80%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA-1-%E6%9C%80%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%BD%A2%E5%BC%8F/</url>
    
    <content type="html"><![CDATA[<h1 id="最优化问题的基本形式">最优化问题的基本形式</h1><h2 id="最优化问题的定义">最优化问题的定义</h2><p>  从一个可行解的集合<span class="math inline">\(S\)</span>中，求出针对某一问题的最优的解<span class="math inline">\(X^*\)</span>。</p><h2 id="最优化问题的基本形式-1">最优化问题的基本形式</h2><p>  最优化问题的基本形式(1)：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{x} \quad &amp; f(x) \\        s.t. \quad &amp; m_{i}(x) \leq 0,\quad i=1,2,\dots,M \\        &amp; n_{j}(x) = 0,\quad j=1,2,\dots,N    \end{split}\end{equation}\]</span></p><ul><li><p><span class="math inline">\(x=\begin{bmatrix}  x_1,x_2,\dots,x_n\end{bmatrix}^{T},x \in \mathbb{R}^{n}\)</span>称为<strong>优化变量(Optimization Variable)</strong>.<br></p></li><li><p><span class="math inline">\(f: \mathbb{R}^{n} \rightarrow\mathbb{R}\)</span>，称为<strong>目标函数(ObjectiveFunction)</strong>.<br></p></li><li><p><span class="math inline">\(m_i: \mathbb{R}^{n} \rightarrow\mathbb{R}\)</span>，称为<strong>不等式约束(InequalityConstraint)</strong>.<br></p></li><li><p><span class="math inline">\(n_j: \mathbb{R}^{n} \rightarrow\mathbb{R}\)</span>，称为<strong>等式约束(EqualityConstraint)</strong>.<br></p></li><li><p><span class="math inline">\(D = \left( dom \space f \right)\bigcap \{x \vert m_{i}(x) \leq 0,i=1,2,\dots,M\} \bigcap \{x \verth_{j}(x) =0,j=1,2,\dots,N\}\)</span>，称为最优化问题的<strong>域(Domain)</strong>，<span class="math inline">\(x \in D\)</span> 称为<strong>可行解(FeasibleSolution)</strong>.</p></li><li><p><span class="math inline">\(p^{*} = \inf \{ f(x) \vert x \in D\}\)</span>，称为最优化问题的<strong>最优值(Optimum)</strong>.<br></p></li><li><p><span class="math inline">\(f(x^{*})=p^{*}\)</span>，称<span class="math inline">\(x^{*}\)</span>为最优化问题的<strong>最优解(OptimalSolution)</strong>.<br></p></li><li><p><span class="math inline">\(X_{opt} = \{ x \vert x \inD,f(x)=p^{*} \}\)</span>，称为最优化问题的<strong>最优解集(OptimaSet)</strong>.</p></li><li><p>注：有时最优解不止一个，最优解的集合称为最优解集。</p></li></ul><h2 id="最优化问题的分类">最优化问题的分类</h2><p>  根据最优化问题的目标函数、可行解的不同，可以将最优化问题分为多种类别，以下是一些比较常见的分类：</p><ul><li><p><strong>线性规划/非线性规划</strong><br>  若目标函数<span class="math inline">\(f_0\)</span>与不等式约束<span class="math inline">\(f_i\)</span>均为线性函数，则该最优化问题称为线性规划问题，否则即为非线性规划问题。<br>  函数<span class="math inline">\(f\)</span>为线性函数的充要条件：<span class="math inline">\(f(\alpha x + \beta y) = \alpha f(x) + \betaf(y)\)</span>.</p></li><li><p><strong>凸优化/非凸优化</strong><br>  若目标函数<span class="math inline">\(f_0\)</span>为凸函数，且可行解<span class="math inline">\(S\)</span>为凸集，则该最优化问题称为凸优化问题。<br>  集合<span class="math inline">\(C\)</span>为凸集的充要条件：<span class="math inline">\(\forall x_1,x_2 \in C, \forall \theta \in\mathbb{R}, \theta x_1 + (1-\theta)x_2 \in C\)</span>.<br>  函数<span class="math inline">\(f\)</span>为凸函数的充要条件: <span class="math inline">\(dom f\)</span>为凸集，且<span class="math inline">\(\forall x_1,x_2 \in dom f, 0 \leq \theta \leq1\)</span>，有以下不等式成立：<br><span class="math display">\[f(\theta x_1 + (1-\theta)x_2) \leq \thetaf(x_1) + (1-\theta)f(x_2)\]</span></p></li><li><p><strong>连续变量优化/离散变量优化</strong><br>  若可行解<span class="math inline">\(S\)</span>是连续的，则该最优化问题为连续变量优化问题；若可行解<span class="math inline">\(S\)</span>是离散的，则该最优化问题称为离散变量优化问题。<br>  离散变量优化问题中比较常见的为可行解均为整数的整数变量优化。</p></li><li><p><strong>单目标优化/多目标优化</strong><br>  若目标函数<span class="math inline">\(f_0\)</span>是唯一的，则为单目标优化问题；若有多个目标函数<span class="math inline">\(f_{0}^{1},f_{0}^{2},\dots,f_{0}^{r}\)</span>，则为多目标优化问题。</p></li><li><p><strong>无约束问题/等式约束问题/不等式约束问题</strong><br>  若最优化问题没有约束条件，只有目标函数，则称为无约束问题；若最优化问题的约束条件是一些等式，则称为等式约束问题；若最优化问题的约束条件是一些不等式，则称为不等式约束问题。</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>最优化理论</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>矩阵分析-2.向量组与线性相关性</title>
    <link href="/2023/08/01/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-2-%E5%90%91%E9%87%8F%E7%BB%84%E4%B8%8E%E7%BA%BF%E6%80%A7%E7%9B%B8%E5%85%B3%E6%80%A7/"/>
    <url>/2023/08/01/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-2-%E5%90%91%E9%87%8F%E7%BB%84%E4%B8%8E%E7%BA%BF%E6%80%A7%E7%9B%B8%E5%85%B3%E6%80%A7/</url>
    
    <content type="html"><![CDATA[<h1 id="向量组与线性相关性">向量组与线性相关性</h1><h2 id="向量组">向量组</h2><h3 id="向量组的定义">向量组的定义</h3><p>  设<span class="math inline">\(V\)</span>是数域<span class="math inline">\(\mathbb{F}\)</span>上的线性空间，<span class="math inline">\(V\)</span>中有限序列<span class="math inline">\(\alpha_1,\alpha_2,\dots,\alpha_n\)</span>称为<span class="math inline">\(V\)</span>中的一个向量组，记为向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_n\}\)</span>。</p><h3 id="向量组所拼成的抽象矩阵">向量组所拼成的抽象矩阵</h3><p>  若矩阵<span class="math inline">\(A\)</span>中的每一列是由向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_n\}\)</span>所构成的，则称矩阵<span class="math inline">\(A\)</span>是由向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_n\}\)</span>所拼成的抽象矩阵，记为：</p><p><span class="math display">\[A = \begin{bmatrix}    \alpha_1,\alpha_2,\dots,\alpha_n \\\end{bmatrix}\]</span></p><h2 id="向量组的线性相关性">向量组的线性相关性</h2><h3 id="线性相关与线性无关的定义">线性相关与线性无关的定义</h3><p>  向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>称为线性相关，如果存在不全为零的<span class="math inline">\(P\)</span>个数<span class="math inline">\(k_i \in\mathbb{F}, i=1,2,\dots,p\)</span>，使得：</p><p><span class="math display">\[\begin{equation}\\alpha_1k_1+\alpha_2k_2+\dots+\alpha_pk_p=0\end{equation}\]</span></p><p>  向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>称为线性无关，如果任意不全为零的<span class="math inline">\(P\)</span>个数<span class="math inline">\(k_i \in\mathbb{F}, i=1,2,\dots,p\)</span>，使得：</p><p><span class="math display">\[\begin{equation}\\alpha_1k_1+\alpha_2k_2+\dots+\alpha_pk_p \ne 0\end{equation}\]</span></p><ul><li><strong>注1</strong> 线性无关的另外一种表述：<br>  向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>称为线性无关，若<span class="math inline">\(\alpha_1k_1+\alpha_2k_2+\dots+\alpha_pk_p =0\)</span> 当且仅当 <span class="math inline">\(k_i=0,i=1,\dots,p\)</span> 时成立。</li></ul><h3 id="线性相关性的矩阵表述">线性相关性的矩阵表述</h3><p>  设向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_n\}\)</span>所拼成的抽象矩阵为<span class="math inline">\(A\)</span>，向量<span class="math inline">\(x =\begin{bmatrix}  x_1,x_2,\dots,x_p\end{bmatrix}^T\)</span>，有齐次线性方程：</p><p><span class="math display">\[\begin{equation}    Ax=\begin{bmatrix}    \alpha_1,\alpha_2,\dots,\alpha_p \\\end{bmatrix}\begin{bmatrix}    x_1 \\    x_2 \\    \vdots \\    x_p\end{bmatrix}=0\end{equation}\]</span></p><p><span style="color: green;">向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>线性相关<span class="math inline">\(\Leftrightarrow\)</span>齐次线性方程(3)有非零解。</span><br><span style="color: green;">向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>线性无关<span class="math inline">\(\Leftrightarrow\)</span>齐次线性方程(3)只有零解。</span></p><h2 id="向量组之间的线性表示">向量组之间的线性表示</h2><h3 id="线性表示的定义">线性表示的定义</h3><p>  设有向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>，<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_q\}\)</span>以及向量<span class="math inline">\(\beta\)</span>。<br>  称向量<span class="math inline">\(\beta\)</span>可由向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>线性表示，如果<span class="math inline">\(\exists k_1,k_2,\dots,k_p \in\mathbb{F}\)</span>，使得：<br><span class="math display">\[\begin{equation}   \beta = \alpha_1k_1+\alpha_2k_2+\dots+\alpha_pk_p\end{equation}\]</span></p><p>  称向量组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_q\}\)</span>可由向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>线性表示，如果每个<span class="math inline">\(\beta_i\)</span>均可由向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>线性表示。</p><h3 id="线性表示的矩阵表述">线性表示的矩阵表述</h3><p>  设向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>所拼成的抽象矩阵为<span class="math inline">\(A\)</span>，向量组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_q\}\)</span>所拼成的抽象矩阵为<span class="math inline">\(B\)</span>,有非齐次线性方程：<br><span class="math display">\[\begin{equation}    Ax=\begin{bmatrix}        \alpha_1,\alpha_2,\dots,\alpha_p    \end{bmatrix}\begin{bmatrix}        x_1 \\        x_2 \\        \vdots \\        x_p    \end{bmatrix} = \beta\end{equation}\]</span></p><p><span class="math display">\[\begin{equation}    AX= \begin{bmatrix}        \alpha_1,\alpha_2,\dots,\alpha_p    \end{bmatrix}\begin{bmatrix}        x_{11} &amp; x_{12} &amp; \dotsb &amp; x_{1q} \\        x_{21} &amp; x_{22} &amp; \dotsb &amp; x_{2q} \\        \vdots &amp; \vdots &amp;        &amp; \vdots \\        x_{p1} &amp; x_{p2} &amp; \dotsb &amp; x_{pq} \\    \end{bmatrix} = \begin{bmatrix}        \beta_1,\beta_2,\dots,\beta_q    \end{bmatrix}=B\end{equation}\]</span></p><p><span style="color: green;">向量<span class="math inline">\(\beta\)</span>可由向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>线性表示<span class="math inline">\(\Leftrightarrow\)</span>非齐次线性方程(5)有解。</span><br><span style="color: green;">向量组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_q\}\)</span>可由向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>线性表示<span class="math inline">\(\Leftrightarrow\)</span>非齐次线性方程组(6)有解。</span></p><h3 id="线性表示的传递性">线性表示的传递性</h3><p>  设有三个向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>、<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_q\}\)</span>、<span class="math inline">\(\{v_1,v_2,\dots,v_t\}\)</span>，其所拼成抽象矩阵分别为<span class="math inline">\(A、B、C\)</span>。<br>  若向量组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_q\}\)</span>可由向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>线性表示，而向量组<span class="math inline">\(\{v_1,v_2,\dots,v_t\}\)</span>可由向量组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_q\}\)</span>线性表示，则向量组<span class="math inline">\(\{v_1,v_2,\dots,v_t\}\)</span>可由向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>线性表示。</p><p><strong>证明：</strong><br>向量组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_q\}\)</span>可由向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>线性表示<span class="math inline">\(\Rightarrow\)</span> 矩阵方程 <span class="math inline">\(AX_{p\times q}=B\)</span> 有解<br>向量组<span class="math inline">\(\{v_1,v_2,\dots,v_t\}\)</span>可由向量组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_q\}\)</span>线性表示<span class="math inline">\(\Rightarrow\)</span> 矩阵方程 <span class="math inline">\(BY_{q\times t}=C\)</span> 有解</p><p><span class="math display">\[\left\{             \begin{array}{lr}             AX_{p\times q}=B \\             BY_{q \times t}=C             \end{array}\right. \Rightarrow AXY=C \Rightarrow 矩阵方程AZ_{p \times t} = C有解，Z_{p \times t}=XY\]</span></p><p>故向量组<span class="math inline">\(\{v_1,v_2,\dots,v_t\}\)</span>可由向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>线性表示。</p><h2 id="向量组的极大线性无关组">向量组的极大线性无关组</h2><h3 id="极大线性无关组的定义">极大线性无关组的定义</h3><p>  设向量组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_s\}\)</span>是向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>的子组，即<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_s\} \subseteq\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>。子组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_s\}\)</span>称为向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>的极大线性无关组，若其满足：<br>1. 子组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_s\}\)</span>线性无关。<br>2. 若向量组<span class="math inline">\(\{v_1,v_2,\dots,v_t\}\)</span>也是向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>的子组，且<span class="math inline">\(s &lt; t\)</span>，则子组<span class="math inline">\(\{v_1,v_2,\dots,v_t\}\)</span>线性相关。</p><ul><li><strong>注：“极大性”的另一种说法：“生成性”。</strong></li></ul><p>  若子组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_s\}\)</span>线性无关，且<span class="math inline">\(\forall \alpha_i \in\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>，<span class="math inline">\(\alpha_i\)</span>都可由子组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_s\}\)</span>线性表示，则子组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_s\}\)</span>是向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>的极大线性无关组。</p><p><span class="math display">\[\color{green} 极大线性无关组\Leftrightarrow 线性无关生成组\]</span></p><h3 id="向量个数的唯一性">向量个数的唯一性</h3><p><strong>定理：</strong>向量组的极大线性无关组所含向量的个数是唯一的。<br><strong>证明：</strong><br>  设有向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>，向量组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_s\}\)</span>与<span class="math inline">\(\{v_1,v_2,\dots,v_t\}\)</span>均是其极大线性无关组，<br>  要证明 $ s=t $，可以利用反证法，假设 <span class="math inline">\(s&lt; t\)</span>，<br>  设 <span class="math inline">\(A =\begin{bmatrix}  \alpha_1,\alpha_2,\dots,\alpha_p\end{bmatrix}\)</span>，<span class="math inline">\(B=\begin{bmatrix}  \beta_1,\beta_2,\dots,\beta_s\end{bmatrix}\)</span>，<span class="math inline">\(C =\begin{bmatrix}  v_1,v_2,\dots,v_t \end{bmatrix}\)</span>，<br>  <span class="math inline">\(\because\)</span> 向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>可由其极大线性无关组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_s\}\)</span>、<span class="math inline">\(\{v_1,v_2,\dots,v_t\}\)</span>线性表示，<br>  <span class="math inline">\(\therefore\)</span> 矩阵方程 <span class="math inline">\(BX=A, AY=C\)</span>均有解，<br>  <span class="math inline">\(\Rightarrow\)</span> 矩阵方程 <span class="math inline">\(BZ=C\)</span> 有解，其中<span class="math inline">\(Z_{s \times t}=XY\)</span>，<br>  $ s &lt; t$ <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(rank(Z) &lt; t\)</span> <span class="math inline">\(\Rightarrow\)</span> 矩阵方程<span class="math inline">\(ZW= \textbf{0}\)</span>有无穷多解 <span class="math inline">\(\Rightarrow\)</span> 矩阵方程<span class="math inline">\(ZW= \textbf{0}\)</span>必有非零解，<br>  设矩阵方程 <span class="math inline">\(ZW= \textbf{0}\)</span>的非零解为<span class="math inline">\(W\)</span>，将矩阵方程 <span class="math inline">\(BZ=C\)</span> 的两边同时乘以<span class="math inline">\(W\)</span>，<br>  有 <span class="math inline">\(B(ZW) = CW, \becauseZW=\textbf{0}\)</span>， <span class="math inline">\(\therefore CW=\textbf{0}\)</span>，<br>  与向量组<span class="math inline">\(\{v_1,v_2,\dots,v_t\}\)</span>线性无关矛盾，<br>  <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(s \nless t\)</span>，同理可证 <span class="math inline">\(s \ngtr t\)</span>，故有 <span class="math inline">\(s = t\)</span></p><h2 id="向量组的秩">向量组的秩</h2><p>  <span style="color: green;">向量组的秩等于向量组的极大线性无关组所含向量的个数。</span></p>]]></content>
    
    
    <categories>
      
      <category>矩阵分析</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>最优化理论-0.引言</title>
    <link href="/2023/07/28/%E6%9C%80%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA-0-%E5%BC%95%E8%A8%80/"/>
    <url>/2023/07/28/%E6%9C%80%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA-0-%E5%BC%95%E8%A8%80/</url>
    
    <content type="html"><![CDATA[<h1 id="引言">引言</h1><p>  最优化理论(OptimizationTheory)是一门研究在给定条件下如何寻找最优解的学科。这个学科涵盖了广泛的数学方法和算法，用于解决机器学习、经济学、工程学等领域的各种实际问题。<br>  如今，人工智能与机器学习领域的研究十分火热，而最优化理论是在其中起着至关重要的作用的一门数学学科。在统计机器学习中，很多问题的实质都能用一个最优化模型表示，例如线性判别分析(LinearDiscriminantAnalysis)的思想是将样本从高维空间投影到低维空间，使得不同类别样本之间的距离尽可能大，同一类别样本之间的距离尽可能小，从而完成样本分类的目标。支持向量机(SupportVectorMachine)的思想是在特征空间中找到一个超平面，使得不同类别的样本尽可能远离该超平面，从而实现分类。这些思想都可以用最优化模型表示，且都是一个凸优化问题，可以利用凸优化的方法得到最优解。在深度学习中，我们要通过最小化损失函数得到神经网络的权重参数，这实际上也是一个最优化问题。由于损失函数大多是非凸函数，我们通常使用梯度下降算法来求得近似最优解。<br>  总得来说，最优化理论在当今是一门非常有用的学科，笔者认为对于统计与数据科学领域的学生来说，学好最优化理论是未来开展研究的基础。</p><h2 id="最优化理论的主要内容">最优化理论的主要内容</h2><p>  最优化理论的主要内容包括以下几个方面：</p><ul><li><strong>最优化问题的表述</strong>:最优化问题通常由目标函数和约束条件构成。目标函数是需要最大化或最小化的函数，而约束条件是问题的限制条件。<br></li><li><strong>最优解的定义</strong>:最优解是指满足约束条件的使得目标函数取得最大值或最小值的变量值。<br></li><li><strong>凸优化</strong>:凸优化是一类特殊的最优化问题，其中目标函数是凸函数，约束条件是凸集。凸优化问题具有良好的性质，可以高效地求解。<br></li><li><strong>等式约束与不等式约束</strong>:最优化问题的约束条件可以是等式约束，也可以是不等式约束。等式约束将变量限制在一个子空间内，而不等式约束则将变量限制在一个半空间内。<br></li><li><strong>无约束优化</strong>:在无约束优化问题中，只需考虑目标函数的最大化或最小化，没有额外的约束条件。<br></li><li><strong>数值优化方法</strong>:为了求解最优化问题，需要使用各种数值优化方法。常见的数值优化方法包括梯度下降法、牛顿法、共轭梯度法、拟牛顿法等。<br></li><li><strong>条件极值与全局极值</strong>:最优化问题可能存在多个极值点，包括局部极值和全局极值。局部极值是在某个特定区域内的最优解，而全局极值是在整个定义域内的最优解。<br></li><li><strong>敏感性分析</strong>:最优化理论还涉及敏感性分析，即研究目标函数和约束条件中参数的微小变化对最优解的影响。<br></li><li><strong>最优化理论在实际问题中的应用</strong>:最优化理论广泛应用于各个领域，如机器学习中的模型训练、工程中的设计优化、经济学中的资源分配问题等。</li></ul><p>  最优化问题可分为凸优化问题与非凸优化问题，本系列首先会以凸优化为主展开讨论，之后再补充非凸优化的相关方法。</p><h2 id="相关学习资料">相关学习资料</h2><p>  本系列主要参考了以下学习资料:</p><ul><li><a href="https://www.bilibili.com/video/BV19M411T7S7/?vd_source=234cf2ac075a1558881a6956450ddf89">Video:《凸优化》- 凌青 中科大</a><br></li><li><a href="https://github.com/SXUSongJH/Book">Book1:《最优化：建模、算法与理论》- 刘浩洋等</a><br></li><li><a href="https://github.com/SXUSongJH/Book">Book2：《ConvexAnalysis》- Stephen Boyd</a></li></ul><p>  本系列首先会参考凌青老师的课程，介绍凸优化的基础知识，然后以另外两本书作为补充，介绍一些课程中没有提及的知识以及非凸优化的方法。</p>]]></content>
    
    
    <categories>
      
      <category>最优化理论</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>矩阵分析-1.线性空间</title>
    <link href="/2023/07/26/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-1-%E7%BA%BF%E6%80%A7%E7%A9%BA%E9%97%B4/"/>
    <url>/2023/07/26/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-1-%E7%BA%BF%E6%80%A7%E7%A9%BA%E9%97%B4/</url>
    
    <content type="html"><![CDATA[<h1 id="线性空间">线性空间</h1><h2 id="加法与数乘">加法与数乘</h2><p><strong>加法的定义</strong><br>  给定非空集合V和数域<span class="math inline">\(\mathbb{F}\)</span>，若存在映射：<br><span class="math display">\[\sigma: V \times V \rightarrowV\]</span></p><p><span class="math display">\[(\alpha,\beta) \mapsto\sigma(\alpha,\beta)\]</span></p><p>即对V中任意元素<span class="math inline">\(\alpha\)</span>，<span class="math inline">\(\beta\)</span>，在集合V中都存在唯一元素<span class="math inline">\(\gamma\)</span>,使得<span class="math inline">\(\gamma=\alpha+\beta \in V\)</span>，则称映射<span class="math inline">\(\sigma\)</span>为集合V上的加法。</p><p><strong>数乘的定义</strong><br>  给定非空集合V和数域<span class="math inline">\(\mathbb{F}\)</span>，若存在映射： <span class="math display">\[\sigma: V \times \mathbb{F} \rightarrowV\]</span></p><p><span class="math display">\[(\alpha,k) \mapsto\sigma(\alpha,k)\]</span></p><p>即对集合V中的任意元素<span class="math inline">\(\alpha\)</span>和数域<span class="math inline">\(\mathbb{F}\)</span>中任意元素k，在集合V中都存在唯一元素<span class="math inline">\(\gamma\)</span>，使得<span class="math inline">\(\gamma=\alpha k \in V\)</span>，则称映射<span class="math inline">\(\sigma\)</span>为集合V上的数乘。</p><ul><li><p><strong>注1：关于映射的符号</strong><br>  设映射<span class="math inline">\(\sigma\)</span>为正弦函数<span class="math inline">\(sin(*)\)</span><br>  “<span class="math inline">\(\rightarrow\)</span>”表示将集合映射到集合。例如<span class="math inline">\(\sigma: R \rightarrow[-1,1]\)</span>，表示将实数集R映射到集合[-1,1]。<br>  “<span class="math inline">\(\mapsto\)</span>”表示将元素映射到元素。例如 <span class="math inline">\(\sigma: \pi \mapsto 0\)</span>，表示将元素<span class="math inline">\(\pi\)</span>映射到元素0。</p></li><li><p><strong>注2：集合的笛卡尔积</strong><br>  集合<span class="math inline">\(S_1\)</span>和<span class="math inline">\(S_2\)</span>的笛卡尔积的数学表示为： <span class="math display">\[S_1 \times S_1 = \{\begin{bmatrix}s_1\\s_2\\\end{bmatrix} \mid s_1 \in S_1,s_1 \in S_2\}\]</span></p></li><li><p><strong>注3：域的定义及常用的域</strong><br>  域的定义：包含加法与乘法的，满足通常运算规则的代数结构称为域。<br>  常用的域：<span class="math inline">\(\mathbb{Q}\)</span>(有理数域)、<span class="math inline">\(\mathbb{R}\)</span>(实数域)、<span class="math inline">\(\mathbb{C}\)</span>(复数域)等。</p></li></ul><h2 id="通常的运算规则">通常的运算规则</h2><ol type="1"><li><strong>加法交换律</strong><br>  已知非空集合V，对 <span class="math inline">\(\forall v_1,v_2 \inV\)</span>，有 <span class="math inline">\(v_1+v_2=v_2+v_1\)</span>。<br></li><li><strong>加法结合律</strong><br>  已知非空集合V，对 <span class="math inline">\(\forall v_1,v_2,v_3 \inV\)</span>，有 <span class="math inline">\((v_1+v_2)+v_3=v_1+(v_2+v_3)\)</span>。<br></li><li><strong>加法零元素</strong><br>  已知非空集合V，对 <span class="math inline">\(\forall v \in V, \existse \in V\)</span>，使得 <span class="math inline">\(e+v=v\)</span>。<br></li><li><strong>加法逆元素</strong><br>  已知非空集合V，对 <span class="math inline">\(\forall v \in V, \existsa \in V\)</span>，使得 <span class="math inline">\(v+a=e\)</span>，记<span class="math inline">\(a=-v\)</span>。<br></li><li><strong>数乘分配律</strong><br>  已知非空集合V和数域<span class="math inline">\(\mathbb{F}\)</span>，对<span class="math inline">\(\forall v_1,v_2 \in V, k \in\mathbb{F}\)</span>，有 <span class="math inline">\((v_1+v_2)k=v_1k+v_2k\)</span>。<br>  已知非空集合V和数域<span class="math inline">\(\mathbb{F}\)</span>，对<span class="math inline">\(\forall v \in V;k_1,k_2 \in\mathbb{F}\)</span>，有 <span class="math inline">\(v(k_1+k_2)=vk_1+vk_2\)</span>。<br></li><li><strong>数乘结合律</strong><br>  已知非空集合V和数域<span class="math inline">\(\mathbb{F}\)</span>，对<span class="math inline">\(\forall v \in V;k,l \in\mathbb{F}\)</span>，有 <span class="math inline">\((vk)l=v(kl)\)</span>。<br></li><li><strong>数乘单位元素</strong><br>  已知非空集合V和数域<span class="math inline">\(\mathbb{F}\)</span>，对<span class="math inline">\(\forall v \in V,\exists 1 \in\mathbb{F}\)</span>，使得 <span class="math inline">\(v1=v\)</span>。</li></ol><h2 id="线性空间的定义">线性空间的定义</h2><p>  <font color="#008000">若集合V满足<b>加法</b>与<b>数乘</b>两种运算，且这两种运算满足<b>通常的运算规则</b>，则称<b>集合V</b>关于此加法和数乘是<b>数域<span class="math inline">\(\mathbb{F}\)</span></b>上的线性空间。</font>一般也把这种线性空间称为向量空间，集合V中的元素称为向量。</p><h2 id="线性空间的具体实例">线性空间的具体实例</h2><h3 id="例1数域mathbbf上的标准线性空间mathbbfn">例1：数域<span class="math inline">\(\mathbb{F}\)</span>上的标准线性空间<span class="math inline">\(\mathbb{F}^n\)</span></h3><p>  已知数域<span class="math inline">\(\mathbb{F}\)</span>,设<br><span class="math display">\[V := \mathbb{F}^n=\mathbb{F} \times\mathbb{F} \times \dots \times \mathbb{F}=\{\begin{bmatrix}  v_1\\  v_2\\  \vdots\\  v_n\\\end{bmatrix} \mid v_i \in \mathbb{F},i=1, \dots,n\}\]</span></p><p>  对 <span class="math inline">\(\forall \alpha,\beta \in V, k \in\mathbb{F}\)</span>，<br>  定义集合V上的加法运算：<br><span class="math display">\[\alpha + \beta=\begin{bmatrix}  \alpha_1\\  \alpha_2\\  \vdots\\  \alpha_n\\\end{bmatrix}+\begin{bmatrix}  \beta_1\\  \beta_2\\  \vdots\\  \beta_n\\\end{bmatrix}=\begin{bmatrix}  \alpha_1+\beta_1\\  \alpha_2+\beta_2\\  \vdots\\  \alpha_n+\beta_n\\\end{bmatrix} \in V\]</span></p><p>  定义集合V上的数乘运算：<br><span class="math display">\[\alpha \cdot k=\begin{bmatrix}  \alpha_1\\  \alpha_2\\  \vdots\\  \alpha_n\\\end{bmatrix} \cdot k = \begin{bmatrix}  \alpha_1 k \\  \alpha_2 k \\  \vdots \\  \alpha_n k \\\end{bmatrix} \in V\]</span></p><p>  易证此加法与数乘满足通常的运算法则，此时称集合V为数域<span class="math inline">\(\mathbb{F}\)</span>上的n维标准线性空间，记为<span class="math inline">\(\mathbb{F}^n\)</span>。<br>  一些常用数域上的n维标准线性空间：<span class="math inline">\(\mathbb{R}^n\)</span>(实数域)、<span class="math inline">\(\mathbb{C}^n\)</span>(复数域)等。</p><h3 id="例2欧几里得空间作为线性空间">例2：欧几里得空间作为线性空间</h3>  令数域<span class="math inline">\(\mathbb{F}=\mathbb{R}\)</span>，集合<span class="math inline">\(V=\{欧几里得空间中的全体有向线段\}\)</span>。(当两条有向线段经过平移能够重叠时，则把这两条线段算做一条线段)<br>  定义集合V上的加法运算：向量运算的平行四边形法则。<br>  定义集合V上的数乘运算：向量同向或反向伸缩。<br><center class="half"><img src="https://s2.loli.net/2023/07/27/GNvpJQHgdrOLMRA.png" width="50%"><img src="https://s2.loli.net/2023/07/27/lFhM6DNEmVkRW7c.png" width="50%"><p>Image 1 向量的平行四边形法则            Image 2 向量的伸缩</p></center><p>  易证此加法与数乘满足通常的运算法则，则集合V是线性空间，说明欧几里得空间可以作为线性空间。</p><h3 id="例3函数空间作为线性空间">例3：函数空间作为线性空间</h3><p>  已知数域<span class="math inline">\(\mathbb{F}\)</span>，集合<span class="math inline">\(V=\mathcal{F}(I,\mathbb{F}^n)\)</span>。集合V为函数空间，V中的元素是以数域<span class="math inline">\(\mathbb{F}\)</span>中的区间<span class="math inline">\(I\)</span>为定义域，具有n个分量的n维向量值函数。例如：<br><span class="math display">\[f=\begin{bmatrix}  f_1(x) \\  f_2(x) \\\end{bmatrix}=\begin{bmatrix}  sin(x) \\  \frac{1}{2}x^3 \\\end{bmatrix}, f \in \mathcal{F}([-1,1],\mathbb{R}^2) \]</span></p><p>  对 <span class="math inline">\(\forall f,g \in\mathcal{F}(I,\mathbb{F}^n),k \in \mathbb{F}\)</span>，<br>  定义集合V上的加法运算：<br><span class="math display">\[f+g = \begin{bmatrix}  f_1(x) \\  f_2(x) \\  \vdots \\  f_n(x) \\\end{bmatrix}+\begin{bmatrix}  g_1(x) \\  g_2(x) \\  \vdots \\  g_n(x) \\\end{bmatrix} = \begin{bmatrix}  f_1(x)+g_1(x) \\  f_2(x)+g_2(x) \\  \vdots \\  f_n(x)+g_n(x) \\\end{bmatrix} \in \mathcal{F}(I,\mathbb{F}^n)\]</span></p><p>  定义集合V上的数乘运算：<br><span class="math display">\[f \cdot k = \begin{bmatrix}  f_1(x) \\  f_2(x) \\  \vdots \\  f_n(x) \\\end{bmatrix} \cdot k = \begin{bmatrix}  kf_1(x) \\  kf_2(x) \\  \vdots \\  kf_n(x) \\\end{bmatrix} \in \mathcal{F}(I,\mathbb{F}^n)\]</span></p><p>  易证此加法与数乘满足通常的运算法则，则集合V是线性空间，说明函数空间可以作为线性空间。</p>]]></content>
    
    
    <categories>
      
      <category>矩阵分析</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>矩阵分析-0.引言</title>
    <link href="/2023/07/16/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-0.%E5%BC%95%E8%A8%80/"/>
    <url>/2023/07/16/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-0.%E5%BC%95%E8%A8%80/</url>
    
    <content type="html"><![CDATA[<h1 id="引言">引言</h1><p>  矩阵分析是笔者开启的第一个博客系列，之所以想把矩阵理论作为起点，是因为在统计与数据科学领域，矩阵是最基本也是最重要的数学工具，当我们在讨论高维空间、随机向量、多元正态分布这些统计领域最基本的概念时，我们都离不开矩阵这个数学工具。掌握矩阵理论，能够帮助我们更好地分析、处理高维空间中数学问题。</p><h2 id="矩阵分析的研究内容">矩阵分析的研究内容</h2><p>  与线性代数偏向于计算不同，矩阵分析的研究内容更加偏向于分析。矩阵分析的研究内容是非常广泛的，很难做到面面俱到，本博客的文章主要是参考一些主流的矩阵理论著作以及学习资料。以下是本系列打算讨论的一些重点内容：<br>1.<strong>线性空间</strong>：在研究问题时，我们通常把对象限定在某一个空间中，而线性空间便是代数中最基本的空间。从线性空间出发，我们会认识向量组、基、子空间等概念。<br>2.<strong>线性映射</strong>：当我们需要将一个线性空间中的对象映射到另一个线性空间时，我们便需要用到线性映射这个方法。基于线性映射，我们会讨论几何中的旋转变换、镜面反射等操作的矩阵表示。<br>3.<strong>矩阵等价与相似</strong>：矩阵的等价与相似是我们在线性代数中非常熟悉的概念，在矩阵分析中，我们将借助线性映射的概念进一步理解等价与相似的几何意义。另外，通过引入多项式矩阵以及Smith型、Jordan标准型等概念，我们将能够把较为复杂的矩阵相似问题转化为简单的矩阵等价问题。<br>4.<strong>内积</strong>：内积是解析几何中一个非常重要的概念，内积赋予空间向量以度量，使得我们可以定义范数、距离和角度等概念，从而建立内积空间的结构。内积空间是函数空间、向量空间和张量空间的基础。<br>5.<strong>矩阵微分</strong>：矩阵微积分是矩阵理论的重要组成部分。它对矩阵的导数、积分和微分方程进行了系统的研究，为解决矩阵和向量值函数的微分问题提供了理论基础。矩阵微分在机器学习、控制论等领域有广泛应用。<br>6.<strong>矩阵分解</strong>：矩阵分解是将一个矩阵拆解为多个简单矩阵或特殊形式矩阵的过程。矩阵分解在线性方程组求解、特征值计算、数据降维等方面有非常重要的作用。我们将会学习一些常见的矩阵分解技术，如LU分解、QR分解、奇异值分解等。</p><p>  以上是我打算在矩阵分析系列前期讨论的一些内容，当然，矩阵理论博大精深，远非一朝一夕能够掌握理解的，随着本人学习的深入，一些新的内容会陆续补充到这个系列中。</p><h2 id="矩阵分析的应用场景">矩阵分析的应用场景</h2><p>  矩阵理论在物理、控制、计算机等领域有着广泛的应用场景，由于本人是统计与数据科学领域的学生，所以我主要介绍一些矩阵理论在本领域的一些应用场景：</p><ul><li><strong>主成分分析（PCA）</strong>：PCA是一种常用的降维技术，它将高维数据转换为低维空间，同时最大程度保留原始数据的方差。PCA的核心是对数据协方差矩阵进行特征值分解，从而得到主成分。<br></li><li><strong>特征提取</strong>：在机器学习中，矩阵理论常用于特征提取。通过将数据矩阵进行降维、转换或者分解，可以得到更具有表征能力的特征，从而提高模型的性能。<br></li><li><strong>多元正态分布</strong>：在多元统计分析中，多元正态分布是一个重要的概率分布模型，用于描述多维随机变量的联合分布。矩阵理论提供了对多元正态分布的理论和应用支持，包括协方差矩阵、特征值分解、条件分布等。<br></li><li><strong>线性回归和广义线性模型</strong>：线性回归和广义线性模型是数据科学中常用的建模方法。它们使用矩阵来描述变量之间的线性关系，并通过矩阵求解技术来拟合模型和估计参数。<br></li><li><strong>时间序列分析</strong>：时间序列分析中，矩阵理论被用于处理多维时间序列数据，如协方差矩阵估计、谱分析等。<br></li><li><strong>神经网络</strong>：深度学习中的神经网络可以用矩阵表示网络的权重和输入输出。矩阵运算在神经网络的前向传播和反向传播过程中发挥着关键作用，实现模型的训练和优化。</li></ul><h2 id="愿景">愿景</h2><p>  矩阵分析是我开始写的第一个博客系列，我希望这个博客作为我学习笔记的同时，也能为读者解决遇到的问题。在我的想法中，我希望这个博客能一直处于更新状态，每当自己在矩阵理论方面有新的收获，便能把它记录在这里，愿自己能够一直坚持下来！</p><h2 id="相关学习资料">相关学习资料</h2><p>  矩阵理论有非常多的著作与学习资料，本博客主要参考的资料有：</p><ul><li><a href="https://www.bilibili.com/video/BV1Em4y1r7ss/?spm_id_from=333.337.search-card.all.click&amp;vd_source=234cf2ac075a1558881a6956450ddf89">Video1：《矩阵论》-严质彬 哈工大</a><br></li><li><a href="https://www.bilibili.com/video/BV1b4411j7V3/?spm_id_from=333.337.search-card.all.click&amp;vd_source=234cf2ac075a1558881a6956450ddf89">Video2：《数据分析、信号处理和机器学习中的矩阵方法》-Gilbert Strang MIT</a><br></li><li><a href="https://github.com/SXUSongJH/Book">Book1：《矩阵分析与应用》(第二版)-张贤达</a><br></li><li><a href="https://github.com/SXUSongJH/Book">Book2：《MatrixAnalysis》(Sencond Edition) - Roger A.Horn</a></li></ul><p>  在本系列的前期，将主要参考严质彬老师的课程。对于课程中没有涉及的内容，后期将借助其它几个资料进行补充。</p>]]></content>
    
    
    <categories>
      
      <category>矩阵分析</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>这才是猛男该弹的曲子！！！</title>
    <link href="/2023/07/15/%E8%BF%99%E6%89%8D%E6%98%AF%E7%8C%9B%E7%94%B7%E8%AF%A5%E5%90%AC%E7%9A%84%E6%9B%B2%E5%AD%90/"/>
    <url>/2023/07/15/%E8%BF%99%E6%89%8D%E6%98%AF%E7%8C%9B%E7%94%B7%E8%AF%A5%E5%90%AC%E7%9A%84%E6%9B%B2%E5%AD%90/</url>
    
    <content type="html"><![CDATA[<p>  无语，最近对于睫毛弯弯这个曲子特着迷，吉他老师在教我光辉岁月时，我满脑子都想的是睫毛弯弯<span class="github-emoji"><span>😂</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f602.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>小时候咋没发现这个旋律这么好听。不行，我早晚得出一期这首歌的弹唱<span class="github-emoji"><span>🎶</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f3b6.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></p><center><iframe src="//player.bilibili.com/player.html?aid=384390831&amp;bvid=BV1fZ4y1t7db&amp;cid=731251859&amp;page=1" width="300" height="200" title="【吉他独奏】睫毛弯弯(王心凌)" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"></iframe></center>]]></content>
    
    
    <categories>
      
      <category>Daily</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
