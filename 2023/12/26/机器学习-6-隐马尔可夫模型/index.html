

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="https://s2.loli.net/2023/07/15/AhaZCquL51QoPpk.png">
  <link rel="icon" href="https://s2.loli.net/2023/07/15/AhaZCquL51QoPpk.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="喵老师">
  <meta name="keywords" content="">
  
    <meta name="description" content="本节主要介绍隐马尔可夫模型的主要作用及理论推导。">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习-6.隐马尔可夫模型">
<meta property="og:url" content="http://example.com/2023/12/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-6-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/index.html">
<meta property="og:site_name" content="喵老师&#39;s Blog">
<meta property="og:description" content="本节主要介绍隐马尔可夫模型的主要作用及理论推导。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2023/07/26/mnOuBDrWQgCHMw3.jpg">
<meta property="article:published_time" content="2023-12-26T04:06:49.000Z">
<meta property="article:modified_time" content="2024-02-02T17:44:13.460Z">
<meta property="article:author" content="喵老师">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://s2.loli.net/2023/07/26/mnOuBDrWQgCHMw3.jpg">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>机器学习-6.隐马尔可夫模型 - 喵老师&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.5","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="喵老师's Blog" type="application/atom+xml">

<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>喵老师&#39;s Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/Daily/" target="_self">
                <i class="iconfont icon-music"></i>
                <span>日常</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('https://s2.loli.net/2023/09/24/EWhbw1DSslpnm4i.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="机器学习-6.隐马尔可夫模型"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-12-26 12:06" pubdate>
          2023年12月26日 中午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          17k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          144 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">机器学习-6.隐马尔可夫模型</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="隐马尔可夫模型">隐马尔可夫模型</h1>
<p>  隐马尔可夫模型(hidden Markov
model,HMM)是一种用于对时许数据建模的概率图模型。它主要应用于对观察序列的概率分布进行建模，这些观察序列背后存在一个不可见的状态序列。HMM的主要思想可以总结如下：</p>
<ul>
<li><strong>状态和观察:</strong>
HMM包含两种类型的变量，即隐藏的状态序列和可见的观察序列。状态序列表示系统内部的状态，而观察序列是我们可以观察的外部现象。<br>
</li>
<li><strong>马尔可夫性质:</strong>
HMM假设状态序列是一个马尔可夫链，即系统的未来状态只依赖于当前状态，而与过去的状态无关。这意味着在给定当前当前状态下，未来状态与过去状态的信息是独立的。</li>
<li><strong>状态转移概率:</strong>
HMM用状态转移概率描述系统从一个状态转移到另一个状态的可能性。这些概率被组织成状态转移矩阵，矩阵的元素表示从一个状态转移到另一个状态的概率。</li>
<li><strong>观察概率:</strong>
对于每个状态，HMM定义了生成每个观察值的概率分布。这些概率被组织成观察概率矩阵。</li>
<li><strong>初始概率:</strong>
HMM还需要定义系统在初始时刻处于每个状态的概率，这些概率称为初始概率。</li>
<li><strong>前向算法和后向算法:</strong>
HMM使用前向算法和后向算法来计算给定观测序列的概率。<br>
</li>
<li><strong>Baum-Welch算法:</strong>
用于无监督学习的算法，通过观察序列来调整模型参数，使其更好地匹配观察数据。Baum-Welch算法本质上就是EM算法。</li>
</ul>
<p>  隐马尔可夫模型在各个领域都具有重要的应用，包括<strong>时序数据建模，语音识别，自然语言处理，生物信息学等</strong>。总体而言，HMM在多个领域中都发挥着关键的作用，为时序数据建模和分析提供了灵活而强大的工具。</p>
<h2 id="基本概念">基本概念</h2>
<h3 id="马尔可夫链mc">马尔可夫链(MC)</h3>
<p>  设有随机序列 <span class="math inline">\(S =\{
S_1,S_2,\dots,S_t,S_{t+1},\dots \}\)</span>，若 <span class="math inline">\(S_{t+1}\)</span> 只依赖于前一时刻 <span class="math inline">\(S_t\)</span>，不依赖于 <span class="math inline">\(S_1,S_2,\dots,S_{t-1}\)</span>，即：</p>
<p><span class="math display">\[P(S_{t+1} | S_1,S_2,\dots,S_t)=P(S_{t+1}
| S_t)\]</span></p>
<p>  则称随机序列<span class="math inline">\(S\)</span>为马尔可夫链(Markov Chain)。</p>
<h3 id="隐马尔可夫模型的定义">隐马尔可夫模型的定义</h3>
<p>  <strong>定义1(隐马尔可夫模型)</strong>
隐马尔可夫模型是关于时序数据的概率模型，描述由一个隐藏的马尔可夫链随机生成不可观测的状态序列，再由各个状态生成一个观测从而产生观测序列的过程。隐藏的马尔可夫链随机生成的状态的序列，称为状态序列(state
sequence)；每个状态生成一个观测，而由此产生的观测的随机序列，称为观测序列(observation
sequence)。序列的每一个位置又可以看作是一个时刻。</p>
<p>  HMM模型可以用如下的概率图表示：</p>
<center>
<img src="https://s2.loli.net/2023/11/26/w4LvBDr9m7oXkf3.jpg" srcset="/img/loading.gif" lazyload width="80%" height="60%">
<div data-align="center">
Image1: HMM模型的概率图
</div>
</center>
<h3 id="模型参数">模型参数</h3>
<p>  隐马尔可夫模型由<strong>初始概率分布</strong>、<strong>状态转移概率分布</strong>、以及<strong>观测概率分布</strong>确定。下面我们来介绍这些模型参数的含义，在这之前先做一些符号定义。</p>
<p><strong>状态序列</strong><br>
  设 <span class="math inline">\(I\)</span>为状态序列，<span class="math inline">\(Q\)</span> 是所有可能状态的集合，记为：</p>
<p><span class="math display">\[Q=\{q_1,q_2,\dots,q_N\},\quad
I=\{i_1,i_2,\dots,i_{T}\}, \forall i \in Q\]</span></p>
<p>其中，<span class="math inline">\(N\)</span>是可能的状态数。</p>
<p><strong>观测序列</strong><br>
  设 <span class="math inline">\(O\)</span> 是 状态序列 <span class="math inline">\(I\)</span> 所对应的观测序列，<span class="math inline">\(V\)</span> 是所有可能的观测的集合，记为：</p>
<p><span class="math display">\[V = \{v_1,v_2,\dots,v_{M}\},\quad
O=\{o_1,o_2,\dots,o_{T}\},\forall o \in V\]</span></p>
<p><strong>状态转移概率矩阵</strong><br>
  设 <span class="math inline">\(A\)</span> 为状态转移概率矩阵：</p>
<p><span class="math display">\[A = [a_{ij}]_{N \times N}\]</span></p>
<p>其中，</p>
<p><span class="math display">\[a_{ij} = P(i_{t+1}=q_{j} |
i_{t}=q_{i}),\quad i,j=1,2,\dots,N\]</span></p>
<p>即系统在时刻 <span class="math inline">\(t\)</span> 处于状态 <span class="math inline">\(q_{i}\)</span> 的条件下在时刻 <span class="math inline">\(t+1\)</span> 转移到状态 <span class="math inline">\(q_{j}\)</span> 的概率。</p>
<p><strong>观测概率矩阵</strong><br>
  设 <span class="math inline">\(B\)</span> 是观测概率矩阵：</p>
<p><span class="math display">\[B=[b_{j}(k)]_{N \times M}\]</span></p>
<p>其中，</p>
<p><span class="math display">\[b_{j}(k)=P(o_{t}=v_{k} |
i_{t}=q_{j}),\quad k=1,2,\dots,M;\quad j=1,2,\dots,N\]</span></p>
<p>即系统在时刻 <span class="math inline">\(t\)</span> 处于状态 <span class="math inline">\(q_{j}\)</span> 的条件下生成观测 <span class="math inline">\(v_{k}\)</span> 的概率。</p>
<p><strong>初始状态概率向量</strong><br>
  设 <span class="math inline">\(\pi\)</span> 是初始状态概率向量：</p>
<p><span class="math display">\[\pi = \begin{bmatrix}
    \pi_1,\pi_2,\dots,\pi_N
\end{bmatrix}^{T}\]</span></p>
<p>其中，</p>
<p><span class="math display">\[\pi_{i}=P(i_{1}=q_{i}),\quad
i=1,2,\dots,N\]</span></p>
<p>  隐马尔可夫模型由<strong>初始状态概率向量<span class="math inline">\(\pi\)</span></strong>、<strong>状态转移概率矩阵<span class="math inline">\(A\)</span></strong>、<strong>观测概率矩阵<span class="math inline">\(B\)</span></strong>
决定。因此，隐马尔可夫模型的参数<span class="math inline">\(\lambda\)</span>可用三元符号表示，即：</p>
<p><span class="math display">\[\lambda = (A,B,\pi)\]</span></p>
<p>  <span class="math inline">\(A,B,\pi\)</span>
称为隐马尔可夫模型的三要素。</p>
<h2 id="模型假设">模型假设</h2>
<p>  隐马尔可夫模型有两个基本假设：<br>
  (1) 齐次马尔可夫性假设，即假设隐藏的马尔可夫链在任意时刻 <span class="math inline">\(t\)</span>
的状态只依赖于前一时刻的状态，与其他时刻的状态及观测无关，也与时刻 <span class="math inline">\(t\)</span> 无关：</p>
<p><span class="math display">\[P(i_{t} |
i_{t-1},\dots,i_{1};o_{t-1},\dots,o_{1})=P(i_{t} | i_{t-1}),\quad
t=1,2,\dots,T\]</span></p>
<p>  (2)
观测独立性假设，即假设任意时刻的观测只依赖于该时刻的马尔可夫链的状态，与其他观测及状态无关：</p>
<p><span class="math display">\[P(o_{t} |
i_{t},\dots,i_{1};o_{t-1},\dots,o_{1})=P(o_{t} | i_{t})\]</span></p>
<h2 id="基本问题">基本问题</h2>
<p>  隐马尔可夫模型有3个基本问题，包括概率计算问题、学习问题、解码问题。</p>
<h3 id="概率计算问题">(1) 概率计算问题</h3>
<p>  给定模型参数 <span class="math inline">\(\lambda=(A,B,\pi)\)</span>
和观测序列 <span class="math inline">\(O=\{o_1,o_2,\dots,o_{T}\}\)</span>，计算在给定模型参数
<span class="math inline">\(\lambda\)</span> 的条件下观测序列 <span class="math inline">\(O\)</span> 出现的概率 <span class="math inline">\(P(O | \lambda)\)</span>。</p>
<h3 id="学习问题">(2) 学习问题</h3>
<p>  已知观测序列 <span class="math inline">\(O=\{o_1,o_2,\dots,o_{T}\}\)</span>，估计模型参数
<span class="math inline">\(\lambda=(A,B,\pi)\)</span>，使得在该模型下观测序列概率
<span class="math inline">\(P(O | \lambda)\)</span> 最大，即：</p>
<p><span class="math display">\[\hat{\lambda}=\arg\max_{\lambda} P(O |
\lambda)\]</span></p>
<p>即用极大似然估计的方法估计参数。</p>
<h3 id="解码问题">(3) 解码问题</h3>
<p>  已知模型参数 <span class="math inline">\(\lambda=(A,B,\pi)\)</span>
和观测序列 <span class="math inline">\(O=\{o_1,o_2,\dots,o_{T}\}\)</span>，求给定观测序列条件下概率
<span class="math inline">\(P(I | O)\)</span> 最大的状态序列 <span class="math inline">\(I=\{i_1,i_2,\dots,i_{T}\}\)</span>，即：</p>
<p><span class="math display">\[\hat{I} = \arg\max_{I} P(I |
O)\]</span></p>
<p>根据所预测的<span class="math inline">\(I\)</span>的时刻不同，解码问题又可分为预测问题与滤波问题：</p>
<ul>
<li>预测问题：<span class="math inline">\(\hat{i}_{t+1} = \arg\max
P(i_{t+1} | o_1,\dots,o_{t})\)</span><br>
</li>
<li>滤波问题：<span class="math inline">\(\hat{i}_{t} = \arg\max P(i_{t}
| o_1,\dots,o_{t})\)</span></li>
</ul>
<h2 id="概率计算问题-1">概率计算问题</h2>
<p>  已知模型参数 <span class="math inline">\(\lambda=(A,B,\pi)\)</span>
和观测序列 <span class="math inline">\(O=\{o_1,o_2,\dots,o_{T}\}\)</span>，计算 <span class="math inline">\(P(O | \lambda)\)</span>。
概率计算问题主要有<strong>直接计算法、前向计算法、后向计算法</strong>。</p>
<h3 id="直接计算法">直接计算法</h3>
<p>  直接计算法的思路是通过列举所有可能的长度为 <span class="math inline">\(T\)</span> 状态序列 <span class="math inline">\(I=\{i_1,i_2,\dots,i_{T}\}\)</span>，求各个状态序列
<span class="math inline">\(I\)</span> 与观测序列 <span class="math inline">\(O=\{o_1,o_2,\dots,o_{T}\}\)</span> 的联合概率
<span class="math inline">\(P(O,I |
\lambda)\)</span>，然后对所有可能的状态序列求和，得到 <span class="math inline">\(P(O | \lambda)\)</span>。计算过程如下：</p>
<p><span class="math display">\[P(O | \lambda) = \sum_{I}P(I,O |
\lambda)=\sum_{I}P(O | I,\lambda)P(I | \lambda)\]</span></p>
<p><span class="math display">\[\begin{split}
    P(I | \lambda) &amp;= P(i_1,i_2,\dots,i_{T} | \lambda) \\
    &amp;= P(i_{T} | i_1,\dots,i_{T-1};\lambda)P(i_1,\dots,i_{T-1} |
\lambda) \\
    &amp;= P(i_{T} | i_{T-1};\lambda)P(i_1,\dots,i_{T-1} | \lambda) \\
    &amp;= \left( a_{i_{T-1}i_{T}} \right) \left(
\pi_{i_1}a_{i_{1}i_{2}} \dotsb a_{i_{T-2}i_{T-1}} \right) \\
    &amp;= \pi_{i_1}a_{i_{1}i_{2}}a_{i_{2}i_{3}} \dotsb a_{i_{T-1}i_{T}}
\end{split}\]</span></p>
<p><span class="math display">\[\begin{split}
    P(O | I,\lambda) &amp;= P(o_1,o_2,\dots,o_{T} |
i_1,i_2,\dots,i_{T};\lambda) \\
    &amp;= b_{i_1}(o_1)b_{i_2}(o_2) \dotsb b_{i_T}(o_T)
\end{split}\]</span></p>
<p><span class="math display">\[\begin{split}
    P(O | \lambda) &amp;= \sum_{I}P(O | I,\lambda)P(I | \lambda) \\
    &amp;=
\sum_{i_1,i_2,\dots,i_T}\pi_{i1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2)
\dotsb a_{i_{T-1}i_{T}}b_{i_{T}}(o_{T})
\end{split}\]</span></p>
<p>  直接计算法的思路非常直观，容易理解，但缺点是计算量很大，是 <span class="math inline">\(O(TN^{T})\)</span>
阶的，随着时间的推移呈指数型增长，这种算法在实际中是不可取的。实际上，在概率计算问题中，我们更常用的是前向计算法和后向计算法。</p>
<h3 id="前向计算法">前向计算法</h3>
<p>  在导出前向算法之前，我们首先来定义<strong>前向概率:</strong></p>
<p><span class="math display">\[\alpha_{t}(i)=P(o_1,o_2,\dots,o_{t};i_{t}=q_{i} |
\lambda)\]</span></p>
<p>前向概率表示在给定模型参数 <span class="math inline">\(\lambda\)</span> 的条件下，到时刻 <span class="math inline">\(t\)</span> 部分观测序列为 <span class="math inline">\(o_1,o_2,\dots,o_{t}\)</span> 且状态为 <span class="math inline">\(q_{i}\)</span>
的概率。则观测序列概率可以表示为：</p>
<p><span class="math display">\[P(O |
\lambda)=\sum_{i=1}^{N}P(o_1,o_2,\dots,o_{T};i_{T}=q_{k} |
\lambda)=\sum_{i=1}^{N}\alpha_{T}(k)\]</span></p>
<p>  前向计算法的主要思想是递推地求得前向概率 <span class="math inline">\(\alpha_{t}(i)\)</span> 及观测序列概率 <span class="math inline">\(P(O |
\lambda)\)</span>，前向计算法的过程可用下图理解：</p>
<center>
<img src="https://s2.loli.net/2023/11/27/ry1pAg8RZwhTGkV.jpg" srcset="/img/loading.gif" lazyload width="60%" height="50%">
<div data-align="center">
Image2: 前向递推
</div>
</center>
<p>  接下来我们需要找到 <span class="math inline">\(\alpha_{t}(i)\)</span> 和 <span class="math inline">\(\alpha_{t+1}(j)\)</span> 之间的递推关系式：</p>
<p><span class="math display">\[\begin{split}
    \alpha_{t+1}(j) &amp;= P(o_1,\dots,o_{t},o_{t+1};i_{t+1}=q_{j} |
\lambda) \\
    &amp;=
\sum_{i=1}^{N}P(o_1,\dots,o_{t},o_{t+1};i_{t}=q_{i},i_{t+1}=q_{j} |
\lambda) \\
    &amp;= \sum_{i=1}^{N} P(o_{t+1} |
o_1,\dots,o_{t};i_{t}=q_{i},i_{t+1}=q_{j};\lambda)P(o_1,\dots,o_{t};i_{t}=q_{i},i_{t+1}=q_{j}
| \lambda) \\
    &amp;=
\sum_{i=1}^{N}P(o_{t+1}|i_{t+1}=q_{j};\lambda)P(i_{t+1}=q_{j}|o_1,\dots,o_{t};i_{t}=q_{i};\lambda)P(o_1,\dots,o_{t};i_{t}=q_{i}
| \lambda) \\
    &amp;= \sum_{i=1}^{N}P(o_{t+1}|i_{t+1}=q_{j};\lambda)P(i_{t+1}=q_{j}
| i_{t}=q_i;\lambda)P(o_1,\dots,o_{t};i_{t}=q_{i} | \lambda) \\
    &amp;= \sum_{i=1}^{N}b_{j}(o_{t+1})a_{ij}\alpha_{t}(i)
\end{split}\]</span></p>
<p>  当 <span class="math inline">\(t=1\)</span> 时，有：</p>
<p><span class="math display">\[\alpha_{1}(i)=\pi_{i}b_{i}(o_1)\]</span></p>
<p>  综上所述，前向计算法的递推关系式可以总计为：</p>
<p><span class="math display">\[\begin{split}
    \alpha_{1}(i) &amp;= \pi_{i}b_{i}(o_1),\quad i=1,2,\dots,N \\
    \alpha_{t+1}(j) &amp;=
b_{j}(o_{t+1})\sum_{i=1}^{N}a_{ij}\alpha_{t}(i),\quad j=1,2,\dots,N
\end{split}\]</span></p>
<h4 id="前向算法">前向算法</h4>
<p>  <strong>观测序列概率的前向算法</strong><br>
  输入：隐马尔可夫模型的参数 <span class="math inline">\(\lambda=(A,B,\pi)\)</span>，观测序列 O；<br>
  输出：观测序列概率 <span class="math inline">\(P(O |
\lambda)\)</span>。<br>
  (1) 初值</p>
<p><span class="math display">\[\alpha_{1}(i)=\pi_{i}b_{i}(o_1),\quad
i=1,2,\dots,N\]</span></p>
<p>  (2) 递推 <span class="math inline">\(\quad\)</span> 对 <span class="math inline">\(t=1,2,\dots,T-1,\)</span></p>
<p><span class="math display">\[\alpha_{t+1}(j) =
b_{j}(o_{t+1})\sum_{i=1}^{N}a_{ij}\alpha_{t}(i),\quad
j=1,2,\dots,N\]</span></p>
<p>  (3) 终止</p>
<p><span class="math display">\[P(O |
\lambda)=\sum_{i=1}^{N}\alpha_{T}(i)\]</span></p>
<p>  前向算法的计算复杂度为 <span class="math inline">\(O(TN^{2})\)</span>，优于直接计算法。</p>
<h3 id="后向计算法">后向计算法</h3>
<p>  后向计算法与前向计算法大致相同，不同点在于后向计算法是从后向前递推。我们首先来定义<strong>后向概率:</strong></p>
<p><span class="math display">\[\beta_{t}(i) =
P(o_{t+1},o_{t+2},\dots,o_{T} | i_{t}=q_{i};\lambda)\]</span></p>
<p>后向概率表示在给定模型参数 <span class="math inline">\(\lambda\)</span>，系统到时刻 <span class="math inline">\(t\)</span> 状态为 <span class="math inline">\(q_{i}\)</span> 的条件下，从 <span class="math inline">\(t+1\)</span> 到 <span class="math inline">\(T\)</span> 的部分观测序列为 <span class="math inline">\(o_{t+1},o_{t+2},\dots,o_{T}\)</span>
的概率。则观测序列概率可以表示为：</p>
<p><span class="math display">\[\begin{split}
    P(O | \lambda) &amp;= P(o_1,o_2,\dots,o_{T} | \lambda) \\
    &amp;= \sum_{i=1}^{N} P(o_1,o_2,\dots,o_{T};i_{1}=q_{i} | \lambda)
\\
    &amp;= \sum_{i=1}^{N} P(o_1,o_2,\dots,o_{T} |
i_{1}=q_{i};\lambda)P(i_1=q_{i} | \lambda) \\
    &amp;= \sum_{i=1}^{N} P(o_1 |
o_2,\dots,o_{T};i_{1}=q_{i};\lambda)P(o_2,\dots,o_{T} |
i_{1}=q_{i};\lambda)\pi_{i} \\  
    &amp;= \sum_{i=1}^{N}P(o_1 | i_{1}=q_{i};\lambda)\beta_{1}(i)\pi_{i}
\\
    &amp;= \sum_{i=1}^{N} \pi_{i} b_{i}(o_1) \beta_{1}(i)
\end{split}\]</span></p>
<p>  后向计算法的主要思想是递推地求得后向概率 <span class="math inline">\(\beta_{t}(i)\)</span> 及观测序列概率 <span class="math inline">\(P(O |
\lambda)\)</span>，后向计算法的过程可用下图理解：</p>
<center>
<img src="https://s2.loli.net/2023/11/27/9jd7FOnlkuo2rM4.jpg" srcset="/img/loading.gif" lazyload width="60%" height="60%">
<div data-align="center">
Image3: 后向递推
</div>
</center>
<p>  之后我们来导出 <span class="math inline">\(\beta_{t}(i)\)</span> 和
<span class="math inline">\(\beta_{t+1}(j)\)</span>
之间的递推关系式：</p>
<p><span class="math display">\[\begin{split}
    \beta_{t}(i) &amp;= P(o_{t+1},\dots,o_{T} | i_{t}=q_{i};\lambda) \\
    &amp;= \sum_{j=1}^{N}P(o_{t+1},\dots,o_{T};i_{t+1}=q_{j} |
i_{t}=q_{i};\lambda) \\
    &amp;= \sum_{j=1}^{N}P(o_{t+1},\dots,o_{T} |
i_{t+1}=q_{j},i_{t}=q_{i};\lambda)P(i_{t+1}=q_{j} | i_{t}=q_{i};\lambda)
\\
    &amp;= \sum_{j=1}^{N}P(o_{t+1},\dots,o_{T} |
i_{t+1}=q_{j};\lambda)a_{ij} \\
    &amp;= \sum_{j=1}^{N}P(o_{t+1} |
o_{t+2},\dots,o_{T};i_{t+1}=q_{j};\lambda)P(o_{t+2},\dots,o_{T} |
i_{t+1}=q_{j};\lambda)a_{ij} \\
    &amp;= \sum_{j=1}^{N}P(o_{t+1} | i_{t+1}=q_{j};\lambda)
\beta_{t+1}(j) a_{ij}  \\
    &amp;= \sum_{j=1}^{N} b_{j}(o_{t+1})a_{ij} \beta_{t+1}(j)
\end{split}\]</span></p>
<p>  当 <span class="math inline">\(t=T\)</span>
时，给定初始的后向概率：</p>
<p><span class="math display">\[\beta_{T}(i) = 1,\quad
i=1,2,\dots,N\]</span></p>
<p>  综上所述，后向计算法的递推关系式可以总计为：</p>
<p><span class="math display">\[\begin{split}
    \beta_{T}(i) &amp;= 1,\quad i=1,2,\dots,N \\
    \beta_{t}(i) &amp;= \sum_{j=1}^{N} b_{j}(o_{t+1})a_{ij}
\beta_{t+1}(j),\quad t=T-1,\dots,1;i=1,\dots,N
\end{split}\]</span></p>
<h4 id="后向算法">后向算法</h4>
<p><strong>观测序列概率的后向算法</strong><br>
  输入：隐马尔可夫模型参数 <span class="math inline">\(\lambda=(A,B,\pi)\)</span>，观测序列 <span class="math inline">\(O\)</span>;<br>
  输出：观测序列概率 <span class="math inline">\(P(O |
\lambda)\)</span>。<br>
  (1) 初值</p>
<p><span class="math display">\[\beta_{T}(i) = 1,\quad
i=1,2,\dots,N\]</span></p>
<p>  (2) 递推 <span class="math inline">\(\quad\)</span> 对 <span class="math inline">\(t=T-1,T-2,\dots,1\)</span></p>
<p><span class="math display">\[\beta_{t}(i) = \sum_{j=1}^{N}
b_{j}(o_{t+1})a_{ij} \beta_{t+1}(j),\quad i=1,\dots,N\]</span></p>
<p>  (3) 终止</p>
<p><span class="math display">\[P(O | \lambda) = \sum_{i=1}^{N} \pi_{i}
b_{i}(o_1) \beta_{1}(i)\]</span></p>
<p>  后向算法的计算复杂度为 <span class="math inline">\(O(TN^{2})\)</span>，与前向算法相同，优于直接计算法。</p>
<h2 id="学习问题-1">学习问题</h2>
<p>  隐马尔可夫模型的学习，根据训练数据是包括观测序列和对应的状态序列还是只有观测序列，可以分为监督学习与无监督学习。监督学习主要利用极大似然法来估计模型参数，无监督学习则是利用
<span class="math inline">\(Baum-Welch\)</span> 算法，也就是 <span class="math inline">\(EM\)</span> 算法来估计参数。</p>
<h3 id="监督学习方法">监督学习方法</h3>
<p>  假设已给训练数据包含 <span class="math inline">\(S\)</span>
个长度相同的观测序列和对应的状态序列 <span class="math inline">\(\{(O_1,I_1),(O_2,I_2),\dotsb,(O_S,I_S)\}\)</span>，可以利用极大似然估计法来估计隐马尔可夫模型的参数。</p>
<p>  <strong>1.转移概率 <span class="math inline">\(a_{ij}\)</span>
的估计</strong><br>
  设样本中时刻 <span class="math inline">\(t\)</span> 处于状态 <span class="math inline">\(q_i\)</span> 且时刻 <span class="math inline">\(t+1\)</span> 转移到状态 <span class="math inline">\(q_j\)</span> 的频数为 <span class="math inline">\(A_{ij}\)</span>，那么状态转移概率 <span class="math inline">\(a_{ij}\)</span> 的估计为：</p>
<p><span class="math display">\[\hat{a}_{ij}=\frac{A_{ij}}{\sum_{j=1}^{N}A_{ij}},\quad
i=1,2,\dotsb,N;\quad j=1,2,\dotsb,N\]</span></p>
<p>  <strong>2.观测概率 <span class="math inline">\(b_{j}(k)\)</span>
的估计</strong><br>
  设样本中状态为 <span class="math inline">\(q_j\)</span> 并观测为 <span class="math inline">\(v_k\)</span> 的频数是 <span class="math inline">\(B_{jk}\)</span>，那么状态为 <span class="math inline">\(q_j\)</span> 观测为 <span class="math inline">\(v_k\)</span> 的概率 <span class="math inline">\(b_{j}(k)\)</span> 的估计为</p>
<p><span class="math display">\[\hat{b}_{j}(k)=\frac{B_{jk}}{\sum_{k=1}^{M}B_{jk}},\quad
j=1,2,\quad,N;\quad k=1,2,\dotsb,M\]</span></p>
<p>  <strong>3.初始状态概率 <span class="math inline">\(\pi_{i}\)</span>
的估计</strong><br>
  设样本中初始状态为 <span class="math inline">\(q_i\)</span> 的频数为
<span class="math inline">\(Q_i\)</span>，则初始状态概率 <span class="math inline">\(\pi_i\)</span> 的估计为</p>
<p><span class="math display">\[\hat{\pi}_{i}=\frac{Q_{i}}{S},\quad
i=1,2,\dotsb,N\]</span></p>
<h3 id="无监督学习方法">无监督学习方法</h3>
<p>  虽然监督学习的方法操作十分简便，也非常容易理解，但监督学习需要对训练数据进行标注，而人工标注训练数据往往代价很高，因此，有时就会利用无监督学习的方法。无监督学习所使用的算法为
<span class="math inline">\(Baum-Welch\)</span>，实际上为 <span class="math inline">\(EM\)</span> 算法。   假设给定训练数据只包含 <span class="math inline">\(S\)</span> 个长度为 <span class="math inline">\(T\)</span> 的观测序列 <span class="math inline">\(\{O_1,O_2,\dotsb,O_S\}\)</span>
而没有对应的状态序列，目标是学习隐马尔可夫模型 <span class="math inline">\(\lambda=(A,B,\pi)\)</span>
的参数。我们将观测序列数据看作观测数据 <span class="math inline">\(O\)</span>，状态序列数据看作不可观测的隐数据 <span class="math inline">\(I\)</span>，那么隐马尔可夫模型实际上是一个含有隐变量的概率模型</p>
<p><span class="math display">\[P(O | \lambda)=\sum_{I}P(O |
I,\lambda)P(I | \lambda)\]</span></p>
<p>它的参数学习可以由 <span class="math inline">\(EM\)</span>
算法实现。</p>
<p>  <strong>1.确定完全数据的对数似然函数</strong><br>
  所有的观测数据写成 <span class="math inline">\(O=(o_1,o_2,\dotsb,o_{T})\)</span>，所有隐数据写成
<span class="math inline">\(I=(i_1,i_2,\dotsb,i_{T})\)</span>，完全数据为
<span class="math inline">\((O,I)=(o_1,o_2,\dotsb,o_{T};i_1,i_2,\dotsb,i_{T})\)</span>。完全数据的对数似然函数为：</p>
<p><span class="math display">\[L(\lambda) = \log{P(O,I |
\lambda)}\]</span></p>
<p>  <strong>2.EM 算法的 E 步：求 <span class="math inline">\(Q\)</span>
函数 <span class="math inline">\(Q(\lambda,\bar{\lambda})\)</span></strong><br>
  由 <span class="math inline">\(Q\)</span> 函数的定义得</p>
<p><span class="math display">\[\begin{split}
    Q(\lambda,\bar{\lambda}) &amp;= E_{I}[\log{P(O,I|\lambda)} |
O,\bar{\lambda}]  \\
    &amp;=\sum_{I}\frac{\log{P(O,I |
\lambda)}P(O,I|\bar{\lambda})}{P(O|\bar{\lambda})}
\end{split}\]</span></p>
<p>其中，<span class="math inline">\(\bar{\lambda}\)</span>
是隐马尔可夫模型当前的估计值，<span class="math inline">\(\lambda\)</span>
是下一步要极大化的隐马尔可夫模型参数。由于 <span class="math inline">\(P(O | \bar{\lambda})\)</span>
为常数，对优化没有影响，可以舍去；同时由概率计算中的直接计算法可得：</p>
<p><span class="math display">\[P(O,I |
\lambda)=\pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2) \dotsb
a_{i_{T-1}i_{T}}b_{i_{T}}(o_{T})\]</span></p>
<p>于是函数 <span class="math inline">\(Q(\lambda,\bar{\lambda})\)</span> 可以写成：</p>
<p><span class="math display">\[\begin{split}
    Q(\lambda,\bar{\lambda}) &amp;=
\sum_{I}\log{P(O,I|\lambda)P(O,I|\bar{\lambda})} \\
    &amp;= \sum_{I} P(O,I | \bar{\lambda}) \log{
[\pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2) \dotsb
a_{i_{T-1}i_{T}}b_{i_{T}}(o_{T}) ]}  \\
    &amp;= \sum_{I} P(O,I | \bar{\lambda}) \log{\left[
\pi_{i_1}\left(\prod_{t=1}^{T-1}a_{i_{t}i_{t+1}}\right)\left(
\prod_{t=1}^{T}b_{i_t}(o_t) \right) \right]} \\
    &amp;=\sum_{I}P(O,I | \bar{\lambda}) \left[
\log(\pi_{i})+\sum_{t=1}^{T-1}\log(a_{i_{t}i_{t+1}})+\sum_{t=1}^{T}\log(b_{i_t}(o_t))
\right] \\
    &amp;=\sum_{I}\log(\pi_{i})P(O,I | \bar{\lambda})+\sum_{I}\left(
\sum_{t=1}^{T-1}\log(a_{i_{t}i_{t+1}}) \right)P(O,I | \bar{\lambda}) \\
    &amp;+ \sum_{I}\left( \sum_{t=1}^{T}\log(b_{i_t}(o_t)) \right)P(O,I
| \bar{\lambda})
\end{split}\]</span></p>
<p>式中求和都是对所有数据的序列总长度 <span class="math inline">\(T\)</span> 进行的。通过观察 <span class="math inline">\(Q(\lambda,\bar{\lambda})\)</span> 的计算式可以看出
<span class="math inline">\(Q(\lambda,\bar{\lambda})\)</span> 的第一项
<span class="math inline">\(\sum_{I}\log(\pi_{i})P(O,I |
\bar{\lambda})\)</span> 只与参数 <span class="math inline">\(\lambda\)</span> 中的初始概率 <span class="math inline">\(\pi\)</span> 有关；第二项 <span class="math inline">\(\sum_{I}\left(
\sum_{t=1}^{T-1}\log(a_{i_{t}i_{t+1}}) \right)P(O,I |
\bar{\lambda})\)</span> 只与参数 <span class="math inline">\(\lambda\)</span> 中的状态转移概率矩阵 <span class="math inline">\(A\)</span> 有关；第三项 <span class="math inline">\(\sum_{I}\left( \sum_{t=1}^{T}\log(b_{i_t}(o_t))
\right)P(O,I | \bar{\lambda})\)</span> 只与参数 <span class="math inline">\(\lambda\)</span> 中的观测概率矩阵 <span class="math inline">\(B\)</span> 有关。因此，可以令：</p>
<p><span class="math display">\[\begin{split}
    Q_{1}(\pi,\bar{\lambda}) &amp;= \sum_{I}\log(\pi_{i})P(O,I |
\bar{\lambda}) \\
    Q_{2}(A,\bar{\lambda}) &amp;= \sum_{I}\left(
\sum_{t=1}^{T-1}\log(a_{i_{t}i_{t+1}}) \right)P(O,I | \bar{\lambda}) \\
    Q_{3}(B,\bar{\lambda}) &amp;= \sum_{I}\left(
\sum_{t=1}^{T}\log(b_{i_t}(o_t)) \right)P(O,I | \bar{\lambda})
\end{split}\]</span></p>
<p>则 <span class="math inline">\(Q(\lambda,\bar{\lambda})\)</span>
可以写成：</p>
<p><span class="math display">\[Q(\lambda,\bar{\lambda})=Q_{1}(\pi,\bar{\lambda})+Q_{2}(A,\bar{\lambda})+Q_{3}(B,\bar{\lambda})\]</span></p>
<p>  <strong>3.EM 算法的 M 步：极大化 <span class="math inline">\(Q\)</span> 函数 <span class="math inline">\(Q(\lambda,\bar{\lambda})\)</span> 求模型参数 <span class="math inline">\(A,B,\pi\)</span></strong><br>
  通过极大化 <span class="math inline">\(Q(\lambda,\bar{\lambda})\)</span> 可以得到模型参数
<span class="math inline">\(A,B,\pi\)</span> 的估计值。<br>
  <strong>(1) 估计初始状态概率 <span class="math inline">\(\pi\)</span></strong><br>
  <span class="math inline">\(Q(\lambda,\bar{\lambda})\)</span> 中只有
<span class="math inline">\(Q_{1}(\pi,\bar{\lambda})\)</span>
与初始状态概率 <span class="math inline">\(\pi\)</span>
有关，因此优化问题可以写成：</p>
<p><span class="math display">\[\begin{split}
    &amp; \max_{\pi} \quad Q_{1}(\pi,\bar{\lambda}) \\
    &amp; \space s.t. \quad \sum_{i=1}^{N} \pi_{i}=1 \\
\end{split}\]</span></p>
<p>优化问题的拉格朗日函数：</p>
<p><span class="math display">\[L(\pi,\gamma)=\sum_{I}\log(\pi_{i})P(O,I
| \bar{\lambda})+\gamma \left( \sum_{i=1}^{N}\pi_{i}-1
\right)\]</span></p>
<p>由费马定理得：</p>
<p><span class="math display">\[\frac{\partial{L(\pi,\gamma)}}{\partial{\pi_{i}}}
= \frac{P(O,i_1=i | \bar{\lambda})}{\pi_{i}}+\gamma=0,\quad
i=1,2,\dotsb,N\]</span></p>
<p>得到：</p>
<p><span class="math display">\[\gamma\pi_{i}=-P(O,i_1=i|\bar{\lambda})\]</span></p>
<p>两边同时对 <span class="math inline">\(i\)</span> 求和得：</p>
<p><span class="math display">\[\begin{split}
    &amp; \gamma\sum_{i=1}^{N}\pi_{i} =
-\sum_{i=1}^{N}P(O,i_{1}=i|\bar{\lambda})=P(O | \bar{\lambda}) \\
    &amp; \Rightarrow \gamma = P(O | \bar{\lambda}) \\
\end{split}\]</span></p>
<p>从而得到 <span class="math inline">\(\pi_{i}\)</span>
的估计值为：</p>
<p><span class="math display">\[\hat{\pi}_{i}=\frac{P(O,i_1=i|\bar{\lambda})}{P(O
| \bar{\lambda})}\]</span></p>
<p>  <strong>(2) 估计状态转移矩阵 <span class="math inline">\(A\)</span></strong><br>
  <span class="math inline">\(Q(\lambda,\bar{\lambda})\)</span> 中只有
<span class="math inline">\(Q_{2}(A,\bar{\lambda})\)</span>
与状态转移概率矩阵 <span class="math inline">\(A\)</span>
有关，因此优化问题可以写成：</p>
<p><span class="math display">\[\begin{split}
    &amp; \max_{A} \quad Q_{2}(A,\bar{\lambda}) \\
    &amp; \space s.t. \quad \sum_{j=1}^{N}a_{ij}=1 \\
\end{split}\]</span></p>
<p>优化问题的拉格朗日函数：</p>
<p><span class="math display">\[\begin{split}
    L(A,\gamma) &amp;= \sum_{I}\left(
\sum_{t=1}^{T-1}\log(a_{i_{t}i_{t+1}}) \right)P(O,I |
\bar{\lambda})+\gamma\left( \sum_{j=1}^{N}a_{ij}-1 \right) \\
    &amp;=\sum_{t=1}^{T-1}\sum_{i=1}^{N}\sum_{j=1}^{N}\left(
P(O,i_{t}=i,i_{t+1}=j|\bar{\lambda})\log{a_{ij}} \right)+\gamma\left(
\sum_{j=1}^{N}a_{ij}-1 \right)
\end{split}\]</span></p>
<p>由费马定理得：</p>
<p><span class="math display">\[\frac{\partial{L(A,\gamma)}}{\partial{a_{ij}}}=\frac{\sum_{t=1}^{T-1}P(O,i_{t}=i,i_{t+1}=j|\bar{\lambda})}{a_{ij}}+\gamma=0,\quad
i,j=1,\dotsb,N\]</span></p>
<p>得到：</p>
<p><span class="math display">\[\gamma
a_{ij}=-\sum_{t=1}^{T-1}P(O,i_{t}=i,i_{t+1}=j|\bar{\lambda})\]</span></p>
<p>两边同时对 <span class="math inline">\(j\)</span> 求和：</p>
<p><span class="math display">\[\begin{split}
    &amp;
\gamma\sum_{j=1}^{N}a_{ij}=-\sum_{t=1}^{T-1}\sum_{j=1}^{N}P(O,i_{t}=i,i_{t+1}=j|\bar{\lambda})
\\
    &amp; \Rightarrow \gamma =
-\sum_{t=1}^{T-1}P(O,i_{t}=i|\bar{\lambda})
\end{split}\]</span></p>
<p>从而得到状态转移概率矩阵元素 <span class="math inline">\(a_{ij}\)</span> 的估计值为：</p>
<p><span class="math display">\[\hat{a}_{ij}=\frac{\sum_{t=1}^{T-1}P(O,i_{t}=i,i_{t+1}=j|\bar{\lambda})}{\sum_{t=1}^{T-1}P(O,i_{t}=i|\bar{\lambda})}\]</span></p>
<p>  <strong>(3) 估计观测概率矩阵 <span class="math inline">\(B\)</span></strong><br>
  <span class="math inline">\(Q(\lambda,\bar{\lambda})\)</span> 中只有
<span class="math inline">\(Q_{3}(B,\bar{\lambda})\)</span>
与观测概率矩阵 <span class="math inline">\(B\)</span>
有关，因此优化问题可以写成：</p>
<p><span class="math display">\[\begin{split}
    &amp; \max_{B} \quad Q_{3}(B,\bar{\lambda}) \\
    &amp; \space s.t. \quad \sum_{k=1}^{M}b_{j}(v_{k})=1 \\
\end{split}\]</span></p>
<p>优化问题的拉格朗日函数：</p>
<p><span class="math display">\[\begin{split}
    L(B,\gamma) &amp;= \sum_{I}\left( \sum_{t=1}^{T}\log(b_{i_t}(o_t))
\right)P(O,I | \bar{\lambda})+\gamma\left( \sum_{k=1}^{M}b_{j}(v_{k})-1
\right) \\
    &amp;=\sum_{t=1}^{T}\sum_{j=1}^{N}\left(
P(O,i_{t}=j|\bar{\lambda})\log{b_{j}(o_{t})} \right)+\gamma\left(
\sum_{k=1}^{M}b_{j}(v_{k})-1 \right)
\end{split}\]</span></p>
<p>由费马定理得：</p>
<p><span class="math display">\[\frac{\partial{L(B,\gamma)}}{\partial{b_{j}(v_{k})}}=\frac{\sum_{t=1}^{T}P(O,i_{t}=j|\bar{\lambda})I(o_{t}=v_{k})}{b_{j}(v_{k})}+\gamma=0,\quad
j=1,\dotsb,N;k=1,\dotsb,M\]</span></p>
<p>得到：</p>
<p><span class="math display">\[\gamma
b_{j}(v_{k})=-\sum_{t=1}^{T}P(O,i_{t}=j|\bar{\lambda})I(o_{t}=v_{k})\]</span></p>
<p>两边同时对 <span class="math inline">\(k\)</span> 求和：</p>
<p><span class="math display">\[\begin{split}
    &amp;
\gamma\sum_{k=1}^{M}b_{j}(v_{k})=-\sum_{k=1}^{M}\sum_{t=1}^{T}P(O,i_{t}=j|\bar{\lambda})I(o_{t}=v_{k})
\\
    &amp; \Rightarrow \gamma = -\sum_{t=1}^{T}P(O,i_{t}=j|\bar{\lambda})
\end{split}\]</span></p>
<p>从而得到观测概率矩阵元素的 <span class="math inline">\(b_{j}(k)\)</span> 的估计值为：</p>
<p><span class="math display">\[\hat{b}_{j}(k)=\frac{\sum_{t=1}^{T}P(O,i_{t}=j|\bar{\lambda})I(o_{t}=v_{k})}{\sum_{t=1}^{T}P(O,i_{t}=j|\bar{\lambda})}\]</span></p>
<h4 id="baum-welch-算法">Baum-Welch 算法</h4>
<p>  通过以上的推导，我们可以总结出无监督学习下隐马尔可夫参数估计的一种算法，其被称为
Baum-Welch 算法，本质上是 EM 算法在隐马尔可夫模型学习中的具体实现。</p>
<p>  <strong>Baum-Welch 算法</strong><br>
  输入：观测数据 <span class="math inline">\(O=(o_1,o_2,\dotsb,o_{T})\)</span>；<br>
  输出：隐马尔可夫模型参数 <span class="math inline">\(\lambda=(A,B,\pi)\)</span>。<br>
  (1) 初始化。对 <span class="math inline">\(n=0\)</span>，选取 <span class="math inline">\(a_{ij}^{(0)},b_{j}^{(0)}(k),\pi_{i}^{(0)}\)</span>，得到模型
<span class="math inline">\(\lambda^{(0)}=(A^{(0)},B^{(0)},\pi^{(0)})\)</span>。<br>
  (2) 递推。对 <span class="math inline">\(n=1,2,\dotsb\)</span>，</p>
<p><span class="math display">\[a_{ij}^{(n+1)}=\frac{\sum_{t=1}^{T-1}P(O,i_{t}=i,i_{t+1}=j|\lambda^{(n)})}{\sum_{t=1}^{T-1}P(O,i_{t}=i|\lambda^{(n)})}\]</span></p>
<p><span class="math display">\[b_{j}^{(n+1)}(k)=\frac{\sum_{t=1}^{T}P(O,i_{t}=j|\lambda^{(n)})I(o_{t}=v_{k})}{\sum_{t=1}^{T}P(O,i_{t}=j|\lambda^{(n)})}\]</span></p>
<p><span class="math display">\[\pi_{i}^{(n+1)}=\frac{P(O,i_1=i|\lambda^{(n)})}{P(O
| \lambda^{(n)})}\]</span></p>
<p>  (3) 终止。得到模型参数 <span class="math inline">\(\lambda^{(n+1)}=(A^{(n+1)},B^{(n+1)},\pi^{(n+1)})\)</span></p>
<h2 id="解码问题-1">解码问题</h2>
<p>  解码问题主要是研究给定观测序列下最有可能出现的状态序列。已知模型参数
<span class="math inline">\(\lambda=(A,B,\pi)\)</span> 和观测序列 <span class="math inline">\(O=(o_1,o_2,\dotsb,o_{T})\)</span>，求某一观测序列
<span class="math inline">\(I=(i_1,i_2,\dotsb,i_{T})\)</span>
使得条件概率 <span class="math inline">\(P(I | O)\)</span>
最大。隐马尔可夫模型中的解码问题主要使用维特比算法。</p>
<h3 id="维特比算法">维特比算法</h3>
<p>  维特比算法实际是用动态规划(dynamic
programming)解隐马尔可夫模型解码问题，即用动态规划求概率最大路径(最优路径)。这时一条路径对应着一个状态序列。维特比算法的思想可以用下图表示：</p>
<center>
<img src="https://s2.loli.net/2023/12/06/E87VbUDGMmWHFJp.jpg" srcset="/img/loading.gif" lazyload width="60%" height="60%">
<div data-align="center">
Image3: 最优路径
</div>
</center>
<p>  根据动态规划的原理，最优路径具有这样的特性：<strong>如果最优路径在
<span class="math inline">\(t\)</span> 时刻通过结点 <span class="math inline">\(i_{t}^{*}\)</span>，那么这一路径从结点 <span class="math inline">\(i_{t}^{*}\)</span> 到终点 <span class="math inline">\(i_{T}^{*}\)</span> 的部分路径，对于从 <span class="math inline">\(i_{t}^{*}\)</span> 到 <span class="math inline">\(i_{T}^{*}\)</span>
的所有可能的部分路径来说，必须是最优的。</strong>
依据这一原理，我们只需要从时刻 <span class="math inline">\(t=1\)</span>
开始，递推地计算在时刻 <span class="math inline">\(t\)</span> 状态为
<span class="math inline">\(i\)</span>
的各条部分路径的最大概率，直至得到时刻 <span class="math inline">\(t=T\)</span> 状态为 <span class="math inline">\(i\)</span> 的各条路径的最大概率。时刻 <span class="math inline">\(t=T\)</span> 的最大概率即为最优路径的概率 <span class="math inline">\(P^{*}\)</span>，最优路径的终结点 <span class="math inline">\(i_{T}^{*}\)</span>
也同时得到。之后，为了找出最优路径的各个结点，从终结点 <span class="math inline">\(i_{T}^{*}\)</span> 开始，由后向前逐步求得结点
<span class="math inline">\(i_{T-1}^{*},\dotsb,i_{1}^{*}\)</span>，得到最优路径
<span class="math inline">\(I^{*}=(i_{1}^{*},i_{2}^{*},\dotsb,i_{T}^{*})\)</span>。这就是维特比算法。</p>
<p>  首先导入两个变量 <span class="math inline">\(\delta\)</span> 和
<span class="math inline">\(\psi\)</span>。定义在时刻 <span class="math inline">\(t\)</span> 状态为 <span class="math inline">\(i\)</span> 的所有单个路径 <span class="math inline">\((i_1,i_2,\dotsb,i_{t})\)</span>
中概率最大值为：</p>
<p><span class="math display">\[\delta_{t}(i)=\max_{i_1,i_2,\dotsb,i_{t-1}}P(i_{t}=i,i_{t-1},\dotsb,i_1;o_{t},\dotsb,o_1
| \lambda),\quad i=1,2,\dotsb,N\]</span></p>
<p>  由定义可得变量 <span class="math inline">\(\delta\)</span>
的递推公式：</p>
<p><span class="math display">\[\begin{split}
    \delta_{t+1}(i) &amp;=
\max_{i_1,i_2,\dotsb,i_{t}}P(i_{t+1}=i,i_{t},\dotsb,i_1;o_{t+1},\dotsb,o_1
| \lambda) \\
    &amp;= \max_{1 \leq j \leq N}[\delta_{t}a_{ji}]b_{i}(o_{t+1}),\quad
i=1,2,\dotsb,N; \space t=1,2,\dotsb,T-1
\end{split}\]</span></p>
<p>  定义在时刻 <span class="math inline">\(t\)</span> 状态为 <span class="math inline">\(i\)</span> 的所有单个路径 <span class="math inline">\((i_1,i_2,\dotsb,i_{t-1},i)\)</span>
中概率最大的路径的第 <span class="math inline">\(t-1\)</span>
个结点为</p>
<p><span class="math display">\[\psi_{t}(i)=\arg\max_{1 \leq j \leq
N}[\delta_{t-1}(j)a_{ji}],\quad i=1,2,\dotsb,N\]</span></p>
<p>  <strong>维特比算法</strong><br>
  输入：模型参数 <span class="math inline">\(\lambda=(A,B,\pi)\)</span>
和观测 <span class="math inline">\(O=(o_1,o_2,\dotsb,o_{T})\)</span>；<br>
  输出：最优路径 <span class="math inline">\(I^{*}=(i_{1}^{*},i_{2}^{*},\dotsb,i_{T}^{*})\)</span>.<br>
  (1) 初始化</p>
<p><span class="math display">\[\delta_{1}(i)=\pi_{i}b_{i}(o_1),\quad
i=1,2,\dotsb,N\]</span></p>
<p><span class="math display">\[\psi_{1}(i)=0,\quad
i=1,2,\dotsb,N\]</span></p>
<p>  (2) 递推。对 <span class="math inline">\(t=2,3,\dotsb,T\)</span></p>
<p><span class="math display">\[\delta_{t}(i)=\max_{1 \leq j \leq
N}[\delta_{t-1}(j)a_{ji}]b_{i}(o_{t}),\quad i=1,2,\dotsb,N\]</span></p>
<p><span class="math display">\[\psi_{t}(i)=\arg\max_{1 \leq j \leq
N}[\delta_{t-1}(j)a_{ji}],\quad i=1,2,\dotsb,N\]</span></p>
<p>  (3) 终止</p>
<p><span class="math display">\[P^{*}=\max_{1 \leq i \leq N}
\delta_{T}(i)\]</span></p>
<p><span class="math display">\[i_{T}^{*}=\arg\max_{1 \leq i \leq
N}[\delta_{T}(i)]\]</span></p>
<p>  (4) 最优路径回溯。对 <span class="math inline">\(t=T-1,T-2,\dotsb,1\)</span></p>
<p><span class="math display">\[i_{t}^{*}=\psi_{t+1}(i_{t+1}^{*})\]</span></p>
<p>求得最优路径 <span class="math inline">\(I^{*}=(i_{1}^{*},i_{2}^{*},\dotsb,i_{T}^{*})\)</span>。</p>
<h2 id="参考">参考</h2>
<p><strong>[1] Book: 李航,统计学习方法(第2版)</strong><br>
<strong>[2] Book: 董平,机器学习中的统计思维(Python实现)</strong><br>
<strong>[3] Video: bilibili,简博士,隐马尔可夫系列</strong><br>
<strong>[4] Video: bilibili,shuhuai008,隐马尔可夫系列</strong></p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="category-chain-item">机器学习</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>机器学习-6.隐马尔可夫模型</div>
      <div>http://example.com/2023/12/26/机器学习-6-隐马尔可夫模型/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>喵老师</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2023年12月26日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2024/02/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-7-%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/" title="机器学习-7-线性判别分析">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">机器学习-7-线性判别分析</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/11/20/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-6-%E7%9F%A9%E9%98%B5%E7%9A%84%E7%AD%89%E4%BB%B7%E4%B8%8E%E7%9B%B8%E4%BC%BC/" title="矩阵分析-6.矩阵的等价与相似">
                        <span class="hidden-mobile">矩阵分析-6.矩阵的等价与相似</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"OLzAG6UdDYLLDlQvyrFhe7ho-gzGzoHsz","appKey":"IuY7HMC7xJw1qmdlkBlrfMRq","path":"window.location.pathname","placeholder":null,"avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    
  
</div>

  
  
    <!-- 备案信息 ICP for China -->
    <div class="beian">
  <span>
    <a href="http://beian.miit.gov.cn/" target="_blank" rel="nofollow noopener">
      京ICP证123456号
    </a>
  </span>
  
    
      <span>
        <a
          href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=12345678"
          rel="nofollow noopener"
          class="beian-police"
          target="_blank"
        >
          
            <span style="visibility: hidden; width: 0">|</span>
            <img src="/img/police_beian.png" srcset="/img/loading.gif" lazyload alt="police-icon"/>
          
          <span>京公网安备12345678号</span>
        </a>
      </span>
    
  
</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
