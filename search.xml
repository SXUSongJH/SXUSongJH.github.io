<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>深度学习-7-生成模型5-扩散模型损失函数的三种等价形式</title>
      <link href="/2024/06/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-7-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B5-%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E4%B8%89%E7%A7%8D%E7%AD%89%E4%BB%B7%E5%BD%A2%E5%BC%8F/"/>
      <url>/2024/06/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-7-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B5-%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E4%B8%89%E7%A7%8D%E7%AD%89%E4%BB%B7%E5%BD%A2%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<h1 id="扩散模型损失函数的三种等价形式">扩散模型损失函数的三种等价形式</h1><p>  在前一个章节，我们推导了扩散模型 ELBO的理论形式，将其作为模型训练的损失函数，然后分析了损失函数中各项的具体计算方法，推导出了<strong>预测原始数据</strong>的损失函数形式。这一节我们将会介绍扩散模型另外两种损失函数的形式，即<strong>预测噪声</strong> 与<strong>分数匹配</strong>。扩散模型的原始论文 DDPM [2]，便是使用的<strong>预测噪声</strong>的损失函数形式，而之后的宋飏等的基于分数的生成模型[3] 则是使用了<strong>分数匹配</strong>的损失函数形式。在这一节，我们将会证明这三种损失函数是等价的。</p><h2 id="预测原始数据的损失函数形式">预测原始数据的损失函数形式</h2><p>  首先，我们还是来回顾一下上一节的结论。在上一篇博客文章中，我们将损失函数分解为了<strong>重构似然损失 <span class="math inline">\(L_{0}\)</span>、去噪匹配损失 <span class="math inline">\(L_{t-1}\)</span>、先验匹配损失 <span class="math inline">\(L_{T}\)</span></strong>。在这三项中，去噪匹配损失<span class="math inline">\(L_{t-1}\)</span>是损失函数中的主要部分，我们通过推导得到了其具体的计算公式：</p><p><span class="math display">\[\begin{align}    D_{KL}(q(x_{t-1}|x_{t},x_{0}) || p_{\theta}(x_{t-1}|x_{t})) =\frac{1}{2\sigma_{q}^{2}(t)}\frac{\bar{\alpha}_{t-1}(1-\alpha_{t})^{2}}{(1-\bar{\alpha}_{t})^{2}}\left[ ||\hat{x}_{\theta}(x_{t-1},t) - x_{0}||_{2}^{2} \right] \tag{1}\end{align}\]</span></p><p>  从(1)式中我们可以看出，优化去噪匹配损失实际上是让模型在每一步尽可能地预测原始数据<span class="math inline">\(x_{0}\)</span>。通过多步的迭代，可以使得模型的输出值<span class="math inline">\(\hat{x}_{\theta}(x_{t-1},t)\)</span>与原始数据 <span class="math inline">\(x_{0}\)</span> 更加相似。</p><h2 id="预测噪声的损失函数形式">预测噪声的损失函数形式</h2><p>  通过上一篇博客的推导(15)，我们可以 <span class="math inline">\(x_{0}\)</span> 与 <span class="math inline">\(x_{t}\)</span> 之间所满足的等式：</p><p><span class="math display">\[\begin{align}    x_{t} = \sqrt{\bar{\alpha}_{t}}x_{0} +\sqrt{1-\bar{\alpha}_{t}}\epsilon_{0} \tag{2}\end{align}\]</span></p><p>  基于 (2) 式，我们可以将 <span class="math inline">\(x_{0}\)</span>用 <span class="math inline">\(x_{t}\)</span> 与 <span class="math inline">\(\epsilon_{0}\)</span> 来表示：</p><p><span class="math display">\[\begin{align}    x_{0} = \frac{x_{t} -\sqrt{1-\bar{\alpha}_{t}}\epsilon_{0}}{\sqrt{\bar{\alpha}_{t}}} \tag{3}\end{align}\]</span></p><p>  通过 (3) 式，我们可以将编码器 <span class="math inline">\(q(x_{t-1}|x_{t},x_{0})\)</span>所满足的高斯分布的均值 <span class="math inline">\(\mu_{q}(x_{t},x_{0})\)</span> 转化为关于原始数据<span class="math inline">\(x_{0}\)</span> 与原始噪声 <span class="math inline">\(\epsilon_{0}\)</span> 的函数：</p><p><span class="math display">\[\begin{align}    \mu_{q}(x_{t},x_{0}) = \frac{1}{\sqrt{\alpha_{t}}}x_{t} -\frac{1-\alpha_{t}}{\sqrt{\alpha_{t}(1-\bar{\alpha}_{t})}}\epsilon_{0}\tag{4} \\\end{align}\]</span></p><p><strong><span class="math inline">\(Proof\)</span></strong></p><p><span class="math display">\[\begin{align}    \mu_{q}(x_{t},x_{0}) &amp;=\frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})x_{t} +\sqrt{\bar{\alpha}_{t-1}}(1-\alpha_{t})x_{0}}{1-\bar{\alpha}_{t}} \notag\\    &amp;= \frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})x_{t} +\sqrt{\bar{\alpha}_{t-1}}(1-\alpha_{t})\frac{x_{t} -\sqrt{1-\bar{\alpha}_{t}}\epsilon_{0}}{\sqrt{\bar{\alpha}_{t}}}}{1-\bar{\alpha}_{t}}\notag \\    &amp;= \frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})x_{t} +(1-\alpha_{t})\frac{x_{t} -\sqrt{1-\bar{\alpha}_{t}}\epsilon_{0}}{\sqrt{\alpha}_{t}}}{1-\bar{\alpha}_{t}}\notag \\    &amp;= \left(\frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_{t}} +\frac{1-\alpha_{t}}{(1-\bar{\alpha}_{t})\sqrt{\alpha_{t}}} \right)x_{t}-\frac{(1-\alpha_{t})\sqrt{1-\bar{\alpha}_{t}}}{(1-\bar{\alpha}_{t})\sqrt{\alpha_{t}}}\epsilon_{0}\notag \\    &amp;= \frac{1}{\sqrt{\alpha_{t}}}x_{t} -\frac{1-\alpha_{t}}{\sqrt{\alpha_{t}(1-\bar{\alpha}_{t})}}\epsilon_{0}\notag \\\end{align}\]</span></p><p>  在前一节中，我们将解码器 <span class="math inline">\(p_{\theta}(x_{t-1}|x_{t})\)</span>同样设置为高斯分布 <span class="math inline">\(N(x_{t-1};\mu_{\theta}(x_{t},t),\Sigma_{q}(t))\)</span>，且高斯分布的均值 <span class="math inline">\(\mu_{\theta}(x_{t},t)\)</span> 具有与编码器的均值<span class="math inline">\(\mu_{q}(x_{t},x_{0})\)</span>相同的形式，故解码器 <span class="math inline">\(p_{\theta}(x_{t-1}|x_{t})\)</span> 同样可以用 (4)式的形式表示，只是在解码过程中，我们没有 <span class="math inline">\(\epsilon_{0}\)</span> 的信息，故神经网络需要根据<span class="math inline">\(x_{t},t\)</span> 的信息预测噪声 <span class="math inline">\(\epsilon_{0}\)</span>。综上所述，我们可以解码器的均值重写为：</p><p><span class="math display">\[\begin{align}    \mu_{\theta}(x_{t},t) = \frac{1}{\sqrt{\alpha_{t}}}x_{t} -\frac{1-\alpha_{t}}{\sqrt{\alpha_{t}(1-\bar{\alpha}_{t})}}\hat{\epsilon}_{\theta}(x_{t},t)\tag{5} \\\end{align}\]</span></p><p>  这样，我们可以将编码器 <span class="math inline">\(q(x_{t-1}|x_{t},x_{0})\)</span> 与解码器 <span class="math inline">\(p_{\theta}(x_{t-1}|x_{t})\)</span> 之间的 KLDivergence (1) 式用噪声的预测误差重新表示：</p><p><span class="math display">\[\begin{align}    D_{KL}(q(x_{t-1}|x_{t},x_{0}) || p_{\theta}(x_{t-1}|x_{t})) =\frac{1}{2\sigma_{q}^{2}(t)}\frac{(1-\alpha_{t})^{2}}{(1-\bar{\alpha}_{t})\alpha_{t}}\left[ ||\hat{\epsilon}_{\theta}(x_{t},t) - \epsilon_{0} ||_{2}^{2} \right]\tag{6} \\\end{align}\]</span></p><p><strong><span class="math inline">\(Proof\)</span></strong></p><p><span class="math display">\[\begin{align}    &amp; D_{KL}(q(x_{t-1}|x_{t},x_{0}) || p_{\theta}(x_{t-1}|x_{t}))\notag \\    &amp;= \frac{1}{2\sigma_{q}^{2}(t)} \left[ ||\mu_{q} - \mu_{\theta}||_{2}^{2} \right] \notag \\    &amp;= \frac{1}{2\sigma_{q}^{2}(t)} \left[||\frac{1}{\sqrt{\alpha_{t}}}x_{t} -\frac{1-\alpha_{t}}{\sqrt{\alpha_{t}(1-\bar{\alpha}_{t})}}\epsilon_{0} -\frac{1}{\sqrt{\alpha_{t}}}x_{t} +\frac{1-\alpha_{t}}{\sqrt{\alpha_{t}(1-\bar{\alpha}_{t})}}\hat{\epsilon}_{\theta}(x_{t},t)||_{2}^{2} \right] \notag \\    &amp;= \frac{1}{2\sigma_{q}^{2}(t)} \left[ ||\frac{1-\alpha_{t}}{\sqrt{\alpha_{t}(1-\bar{\alpha}_{t})}}\hat{\epsilon}_{\theta}(x_{t},t)- \frac{1-\alpha_{t}}{\sqrt{\alpha_{t}(1-\bar{\alpha}_{t})}}\epsilon_{0}||_{2}^{2} \right] \notag \\    &amp;= \frac{1}{2\sigma_{q}^{2}(t)}\frac{(1-\alpha_{t})^{2}}{(1-\bar{\alpha}_{t})\alpha_{t}}\left[ ||\hat{\epsilon}_{\theta}(x_{t},t) - \epsilon_{0} ||_{2}^{2} \right]\notag \\\end{align}\]</span></p><p>  (6)式表明在训练过程中，我们的解码器每一步都需要尽可能地去预测原始噪声 <span class="math inline">\(\epsilon_{0}\)</span>。在前向加噪过程中，我们是将原始噪声<span class="math inline">\(\epsilon_{0}\)</span> 不断地加到原始数据<span class="math inline">\(x_{0}\)</span>中(2)，直至原始数据变为近似高斯噪声，故如果编码器能够根据 <span class="math inline">\(x_{t},t\)</span> 很好地预测噪声 <span class="math inline">\(\epsilon_{0}\)</span>，则在解码过程中，我们可以从高斯噪声开始，每一步逐步减去编码器所预测的原始噪声<span class="math inline">\(\hat{\epsilon}_{\theta}(x_{t},t)\)</span>，从而将高斯噪声还原回原始数据<span class="math inline">\(x_{0}\)</span>。以上过程可以用下图1表示。</p><center><img src="https://s2.loli.net/2024/06/06/li8W17MkhEJOeBp.png" width="80%" height="80%"><div data-align="center">Image1: 预测噪声训练过程</div></center><center><img src="https://s2.loli.net/2024/06/06/zvlHM8wZNYAcueU.png" width="80%" height="80%"><div data-align="center">Image2: DDPM原始论文的训练与采样算法</div></center><p>  在图2右侧的采样过程中，每一步解码，即减去预测的噪声后还加了一个噪声项<span class="math inline">\(\sigma_{t}z\)</span>，这一处理是模仿了前向扩散过程中的随机性，确保每一步生成的样本不是确定的，而是带有一定的随机性，从而可以生成多样化的样本。这是关键的，因为如果每一步都只是简单的去噪而不引入新的随机性，生成的样本将会缺乏多样性。</p><h2 id="分数匹配的损失函数形式">分数匹配的损失函数形式</h2><p>  在 DDPM 之后，Yang Song 等 [3] 在 2021 年提出了基于 VDM 的Score-Based Generative Model (SGM)。在这篇论文中，作者使用 SDEs建立起了扩散模型前向加噪与逆向去噪的一般框架，并利用得分函数作为损失函数来优化模型。关于SGM我们将会在下一篇博客中详细讨论，接下来我们主要来介绍一下基于分数的损失函数。<br>  为了得分函数函数，我们首先来介绍一下概率统计中的 Tweedie 公式。<br>  Tweedie公式表明，给定从指数族分布中抽取的样本，其真实均值可以通过样本的最大似然估计（也称为经验均值）加上一些涉及估计得分的修正项来估计。在只有一个观测样本的情况下，经验均值就是该样本本身。Tweedie公式通常用于减轻样本偏差；如果观测到的样本全部位于真实分布的一端，那么负得分会变大，并将样本的最大似然估计值校正到真实均值。<br>  具体来讲，假设我们从一个指数分布中抽取一些样本，但这些样本偏向分布的一端。单纯地使用这些样本的均值(最大似然估计) 会偏离真实的分布均值。Tweedie公式会通过添加一个修正项来纠正这种偏差。这个修正项会通过样本分布和得分函数来计算。得分函数是统计学中的一个概念，它是指对数似然函数关于参数的导数。如果样本都位于分布的一端，得分函数会变大，修正项会变大，从而将估计的均值向真实均值方向校正。  给定一个高斯随机变量 <span class="math inline">\(z \simN(z;\mu_{z},\Sigma_{z})\)</span>，由Tweedie 可以得到：</p><p><span class="math display">\[\begin{align}    \mathbb{E}[\mu_{z}|z] = z + \Sigma_{z}\nabla_{z}\log{p(z)} \tag{7}\end{align}\]</span></p><p>  在 VDM 中，通过高斯假设，我们推导了前向加噪过程中 <span class="math inline">\(x_{t}\)</span> 所满足的高斯分布：</p><p><span class="math display">\[\begin{align}    q(x_{t}|x_{0}) = N(x_{t}; \sqrt{\bar{\alpha}_{t}}x_{0}, (1 -\bar{\alpha}_{t})\boldsymbol{I}) \tag{8}\end{align}\]</span></p><p>  利用 Tweedie 公式，我们可以给出 <span class="math inline">\(x_{t}\)</span>在给定样本情况下的后验均值的修正估计：</p><p><span class="math display">\[\begin{align}    \mathbb{E}[\mu_{x_{t}}|x_{t}] = x_{t} + (1 -\bar{\alpha}_{t})\nabla_{x_{t}}\log{p(x_{t})} \tag{9}\end{align}\]</span></p><p>  由于在逆向去噪过程中，我们是不知道原始数据 <span class="math inline">\(x_{0}\)</span> 的，我们需要对 <span class="math inline">\(x_{0}\)</span>进行估计，前文中的预测原始数据的损失函数便是在做这件事。这里，通过结合<span class="math inline">\(x_{t}\)</span> 的真实均值 <span class="math inline">\(\sqrt{\bar{\alpha}_{t}}x_{0}\)</span>，我们可以给出后向去噪过程中<span class="math inline">\(x_{0}\)</span> 的估计值：</p><p><span class="math display">\[\begin{align}    x_{0} = \frac{x_{t} + (1 -\bar{\alpha}_{t})\nabla\log{p(x_{t})}}{\sqrt{\bar{\alpha}_{t}}} \tag{10}\\\end{align}\]</span></p><p>  其中，<span class="math inline">\(\nabla\log{p(x_{t})}\)</span> 为<span class="math inline">\(\nabla_{x_{t}}\log{p(x_{t})}\)</span>的简写形式。需要指出的是，在后向去噪过程中，<span class="math inline">\(x_{t}\)</span>所满足的高斯分布的均值是未知的，故解码过程中，(10)式中的得分函数 <span class="math inline">\(\nabla\log{p(x_{t})}\)</span>是未知的。而在前向加噪过程中，基于高斯假设，我们已经得知 <span class="math inline">\(x_{t}\)</span>的所满足的高斯分布(8)，故得分函数是可以计算的。<br>  现在我们需要将前向加噪过程中的 <span class="math inline">\(x_{t-1}\)</span>的均值改写成含有得分函数的形式，将(10)带入 <span class="math inline">\(\mu_{q}(x_{t},x_{0})\)</span>的原始表达式，通过推导可以得到以下等式成立：</p><p><span class="math display">\[\begin{align}    \mu_{q}(x_{t},x_{0}) = \frac{1}{\sqrt{\alpha_{t}}}x_{t} +\frac{1-\alpha_{t}}{\sqrt{\alpha_{t}}}\nabla\log{p(x_{t})} \tag{11} \\\end{align}\]</span></p><p><strong><span class="math inline">\(Proof\)</span></strong></p><p><span class="math display">\[\begin{align}    \mu_{q}(x_{t},x_{0}) &amp;=\frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})x_{t} +\sqrt{\bar{\alpha}_{t-1}}(1-\alpha_{t})x_{0}}{1-\bar{\alpha}_{t}} \notag\\    &amp;= \frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})x_{t} +\sqrt{\bar{\alpha}_{t-1}}(1-\alpha_{t})\frac{x_{t} + (1 -\bar{\alpha}_{t})\nabla\log{p(x_{t})}}{\sqrt{\bar{\alpha}_{t}}}}{1-\bar{\alpha}_{t}}\notag \\    &amp;= \frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})x_{t} +(1-\alpha_{t})\frac{x_{t} + (1 -\bar{\alpha}_{t})\nabla\log{p(x_{t})}}{\sqrt{\alpha}_{t}}}{1-\bar{\alpha}_{t}}\notag \\    &amp;= \left(\frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_{t}} +\frac{1-\alpha_{t}}{(1-\bar{\alpha}_{t})\sqrt{\alpha_{t}}} \right)x_{t}- \frac{1-\alpha_{t}}{\sqrt{\alpha_{t}}}\nabla\log{p(x_{t})} \notag \\    &amp;= \frac{1}{\sqrt{\alpha_{t}}}x_{t} +\frac{1-\alpha_{t}}{\sqrt{\alpha_{t}}}\nabla\log{p(x_{t})} \notag \\\end{align}\]</span></p><p>  因此，我们可以将逆向去噪过程中 <span class="math inline">\(x_{t-1}\)</span>的均值设置成与前向过程相同的形式，只是得分函数需要由神经网络进行估计：</p><p><span class="math display">\[\begin{align}    \mu_{\theta}(x_{t},t) = \frac{1}{\sqrt{\alpha_{t}}}x_{t} +\frac{1-\alpha_{t}}{\sqrt{\alpha_{t}}}s_{\theta}(x_{t},t) \tag{12} \\\end{align}\]</span></p><p>  结合 (11)、(12)式，我们可以得出基于得分函数的 <span class="math inline">\(q(x_{t-1} | x_{t},x_{0})\)</span> 与 <span class="math inline">\(p_{\theta}(x_{t-1}|x_{t})\)</span> 之间的KLDivergence 的表达式：</p><p><span class="math display">\[\begin{align}    D_{KL}(q(x_{t-1}|x_{t},x_{0}) || p_{\theta}(x_{t-1}|x_{t})) =\frac{1}{2\sigma_{q}^{2}(t)} \frac{(1-\alpha_{t})^{2}}{\alpha_{t}}\left[|| s_{\theta}(x_{t},t) - \nabla\log{p(x_{t})} ||_{2}^{2} \right]\tag{13}\end{align}\]</span></p><p><strong><span class="math inline">\(Proof\)</span></strong></p><p><span class="math display">\[\begin{align}    &amp; D_{KL}(q(x_{t-1}|x_{t},x_{0}) || p_{\theta}(x_{t-1}|x_{t}))\notag \\    &amp;= \frac{1}{2\sigma_{q}^{2}(t)} \left[ ||\mu_{q} - \mu_{\theta}||_{2}^{2} \right] \notag \\    &amp;= \frac{1}{2\sigma_{q}^{2}(t)} \left[||\frac{1}{\sqrt{\alpha_{t}}}x_{t} +\frac{1-\alpha_{t}}{\sqrt{\alpha_{t}}}s_{\theta}(x_{t},t) -\frac{1}{\sqrt{\alpha_{t}}}x_{t} -\frac{1-\alpha_{t}}{\sqrt{\alpha_{t}}}\nabla\log{p(x_{t})} ||_{2}^{2}\right] \notag \\    &amp;= \frac{1}{2\sigma_{q}^{2}(t)} \left[ ||\frac{1-\alpha_{t}}{\sqrt{\alpha_{t}}}s_{\theta}(x_{t},t) -\frac{1-\alpha_{t}}{\sqrt{\alpha_{t}}}\nabla\log{p(x_{t})} \right]\notag \\    &amp;= \frac{1}{2\sigma_{q}^{2}(t)}\frac{(1-\alpha_{t})^{2}}{\alpha_{t}}\left[ || s_{\theta}(x_{t},t) -\nabla\log{p(x_{t})} ||_{2}^{2} \right] \notag \\\end{align}\]</span></p><p>  这样我们就可以得到基于得分函数的损失函数，在解码过程中，神经网络需要通过给定的<span class="math inline">\(x_{t}, t\)</span>去预测真实的得分函数。得分函数给出了似然函数的梯度，即使得似然函数最大的方向，去噪过程中数据由高斯噪声，沿着神经网络所预测出的这个方向移动，从而到达最大的重构似然。<br>  实际上，数据在去噪过程移动的方向应该是噪声的反向，即“去噪”，这是符合我们的直觉的。事实也的确如此，联立(3) 式与 (10) 式，我们可以得到得分函数与原始噪声之间的联系：</p><p><span class="math display">\[\begin{align}    \nabla\log{p(x_{t})} =-\frac{1}{\sqrt{1-\bar{\alpha}_{t}}}\epsilon_{0} \tag{14}\end{align}\]</span></p><p>  得分函数衡量了如何在数据空间移动以使得对数似然最大化，由于在前向过程中，我们是将噪声不断地加入到图片中，因此在逆向过程中，我们很自然地应该向反方向移动，即逐渐去噪，以得到更高的对数似然，即与原始图片更加相似。<br>  以上我们推导了扩散模型损失函数的三种等价形式，包括<strong>预测原始数据、预测噪声、得分匹配</strong>。关于得分匹配损失函数，在下一节关于Score-Based Generative Model 中将会有更加详细的解释。</p><h2 id="reference">Reference</h2><p><strong>[1] Paper: Luo C. Understanding diffusion models: A unifiedperspective[J]. arXiv preprint arXiv:2208.11970, 2022.</strong><br><strong>[2] Paper: Ho J, Jain A, Abbeel P. Denoising diffusionprobabilistic models[J]. Advances in neural information processingsystems, 2020, 33: 6840-6851.</strong><br><strong>[3] Paper: Yang Song, Jascha Sohl-Dickstein, et al, "Score-BasedGenerative Modeling through Stochastic Differential Equations," inInternational Conference on Learning Representations,2021.</strong><br><strong>[4] Video: 想不出来昵称又想改, 扩散模型-DiffusionModel【李宏毅2023】, Blibili</strong><br><strong>[5] Blog: 苏剑林, 生成扩散模型漫谈(1-3), 科学空间</strong></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>深度学习-6-生成模型4-变分扩散模型</title>
      <link href="/2024/05/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-6-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B4-%E5%8F%98%E5%88%86%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/"/>
      <url>/2024/05/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-6-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B4-%E5%8F%98%E5%88%86%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="变分扩散模型vdm">变分扩散模型(VDM)</h1><p>  在上一节关于变分自编码的介绍中，我们已经讨论到了具有多层隐变量以及马尔可夫性质的变分自编码模型(MHVAE)，其基本形式与我们今天要介绍的变分扩散模型(VariationalDiffusion Models)已经非常相似，在 MHVAE 的基础上，VDM的主要改进有三个方面：</p><ul><li><strong>隐变量 <span class="math inline">\(z\)</span>的维度:</strong> VDM将隐变量 <span class="math inline">\(z\)</span>的维度设置成与数据 <span class="math inline">\(x\)</span> 一致。<br></li><li><strong>编码器 <span class="math inline">\(q(z|x)\)</span>的分布:</strong> 每个时刻 <span class="math inline">\(t\)</span>的编码器 <span class="math inline">\(q(z_{t}|z_{t-1})\)</span>不再是一个需要学习的分布，而是由前一时刻所输入 <span class="math inline">\(z_{t-1}\)</span> 为中心的高斯分布。<br></li><li><strong>隐变量高斯分布的参数:</strong> 隐变量高斯分布的参数随时间<span class="math inline">\(t\)</span> 改变，经过 <span class="math inline">\(T\)</span> 步后最终变为标准高斯分布。</li></ul><p>  我们来尝试理解一下这些改进的 motivations。在 VAE 中，我们首先将数据<span class="math inline">\(x\)</span> 由数据空间通过编码器 <span class="math inline">\(q_{\phi}(z|x)\)</span> 映射到隐空间中，隐变量<span class="math inline">\(z\)</span> 的维度比 <span class="math inline">\(x\)</span>要小，这一步的目的是希望隐变量能够抽象出数据 <span class="math inline">\(x\)</span>的一般分布特征，而忽略掉特殊细节；隐变量 <span class="math inline">\(z\)</span> 的分布为高斯分布 <span class="math inline">\(N(z;\boldsymbol{\mu}_{\phi}(x),\sigma_{\phi}^{2}(x)\boldsymbol{I})\)</span>，参数由解码器计算出。同时我们希望该高斯分布与标准高斯分布的KL Divergence尽可能小，即尽可能相似，这一方面是因为我们希望提升模型的泛化性能，避免模型学习到的模式过于单一，另一方面是因为在采样时我们需要从标准高斯分布采样出隐变量<span class="math inline">\(z\)</span>，再由解码器生成新的样本 <span class="math inline">\(x'\)</span>，在训练时要求隐变量 <span class="math inline">\(z\)</span>的分布与标准高斯分布尽可能相似也是希望能够与采样过程匹配。但事与愿违，由于训练目标的对抗性，隐变量的分布无法与高斯分布非常相似，另外单个隐变量对于分布特征的抽象能力也十分有限，这造成原始VAE 所生成的图片大多非常模糊，效果不佳。<br>  MHVAE采用了多层次隐变量的架构，通过叠加多个隐变量，使得编码器一步一步地将原始数据的分布特征抽象出来，再通过解码器一步一步对隐变量进行解码，生成新样本。从一步到多步，虽然计算过程变得更复杂，但模型的学习数据分布的能力会变得更强，经过多次编码，隐变量的分布特征变得越发不明显(抽象)，其与标准高斯分布的相似程度也会越高，这样就能更加匹配采样过程。但MHVAE也具有缺陷，它虽然改善了分布不匹配问题，但由于训练目标的对抗性，仍无法彻底解决这个问题。同时，由于存在多个参数化的编码器与解码器，模型参数量较多，模型的训练需要很长时间。  现在我们来讨论 VDM 的想法，既然采样过程是先从标准高斯分布采样出 <span class="math inline">\(z\)</span> ，再经过解码器生成新样本 <span class="math inline">\(x'\)</span>，VAE 与 MHVAE 均是希望应该将数据<span class="math inline">\(x\)</span>编码到与标准高斯分布相似的隐变量分布，以匹配采样过程，主要的困难在于很难学习出能够实现这一过程的编码器<span class="math inline">\(q_{\phi}(z|x)\)</span>。VDM的想法便是，既然编码器很难学，那干脆不学了，人为设定编码器，通过更长的步骤，将数据<span class="math inline">\(x\)</span>逐步编码到近似标准高斯分布(随机噪声)。既然要将数据 <span class="math inline">\(x\)</span>逐步编码为近似噪声，那干脆采用逐步加噪的方法，这种想法最为简洁，即：</p><p><span class="math display">\[x_{t} = \sqrt{\alpha_{t}} x_{t-1} +\sqrt{1-\alpha_{t}}\epsilon,\quad \epsilon \sim N(\epsilon;\boldsymbol{0,I})\]</span></p><p>  其中，<span class="math inline">\(x_{0}\)</span> 表示初始的数据<span class="math inline">\(x\)</span>，<span class="math inline">\(x_{1:T}\)</span>表示编码后的隐变量，对应于MHVAE中的 <span class="math inline">\(z_{1:T}\)</span>，基于这种形式，我们自然需要假设隐变量的维度与数据一致。同时，由于马尔可夫性质，<span class="math inline">\(x_{t}\)</span> 的分布自然是以 <span class="math inline">\(x_{t-1}\)</span>为中心的高斯分布(不考虑系数)。<br>  这样设置的好处是显而易见的，在这种条件下，编码器没有参数要学习，是一个线性过程，速度较快，则可以用更长的加噪步骤使得最终得到的隐变量分布<span class="math inline">\(q(x_{T}|x_{T-1})\)</span>与噪声更加接近。同时，加噪的过程也可以视为将数据 <span class="math inline">\(x\)</span>的分布特征进行抽象，例如一张清晰的猫的图片 <span class="math inline">\(x_{0}\)</span>在经过多次加噪后，只能看见模糊的猫的轮廓了，这也是对猫的图片的分布特征的一种压缩与抽象。解码器则是从随机噪声生成新样本，与编码过程互逆。在正向加噪过程中已经产生了每个步骤加噪前与加噪后的图片对，如果解码器能够训练成编码器的逆过程，即利用正向过程得到的图片对，基于加噪后的图片预测噪声，从而得到加噪前的图片，则可以完成逐步去噪的过程，生成与原始图片相似的新样本。以猫的图片的例子类比，这个过程是从猫的一般特征(模糊)，去生成细节更加丰富的猫的图片(清晰)。这个过程大大改善了以往VAE 所存在的分布不匹配问题。这便是我理解的 VDM 在 MHVAE 基础上的motivations，接下来我们具体来介绍 VDM 的细节。</p><h2 id="概率模型">概率模型</h2><p>  VDM 的概率图与 MHVAE 基本相同，其概率图如下图1所示：</p><center><img src="https://s2.loli.net/2024/05/27/F7RPJpD53LSaEVq.png" width="80%" height="80%"><div data-align="center">Image1: VDM 概率图</div></center><p>  其中 <span class="math inline">\(x_0\)</span> 表示原始数据，<span class="math inline">\(x_{1:T}\)</span> 表示隐变量。由前文的讨论可知，VDM的前向编码过程是一个不需要学习的逐步加噪过程：</p><p><span class="math display">\[\begin{align}    x_{t} = \sqrt{\alpha_{t}} x_{t-1} +\sqrt{1-\alpha_{t}}\epsilon,\quad \epsilon \sim N(\epsilon;\boldsymbol{0,I}) \tag{1}\end{align}\]</span></p><p>  其中 <span class="math inline">\(\alpha_{t}\)</span> 是随层次 <span class="math inline">\(t\)</span> 变化的常数(潜在可学习)。这样第 <span class="math inline">\(t\)</span> 层的编码器便是以 <span class="math inline">\(\sqrt{\alpha_{t}} x_{t-1}\)</span>为均值的高斯分布。同时，与 MHVAE 一样，VDM各层的转移概率分布也满足马尔可夫性质，故有：</p><p><span class="math display">\[\begin{align}    q(x_{1:T}|x_{0}) &amp;= \prod_{t=1}^{T}q(x_{t} | x_{t-1}) \tag{2} \\    q(x_{t} | x_{t-1}) &amp;= N(x_{t}; \sqrt{\alpha_{t}} x_{t-1},(1-\alpha_{t})\boldsymbol{I}) \tag{3}\end{align}\]</span></p><p>  从前文的第三个假设中，我们可以得知，最终层次的隐变量 <span class="math inline">\(x_{T}\)</span> 的先验分布为标准高斯分布。VDM的逆向去噪过程需要通过参数化的解码器 <span class="math inline">\(p_{\theta}(x_{t-1}|x_{t})\)</span>，逐步将图片由高斯噪声<span class="math inline">\(x_{T}\)</span>，还原回原始数据 <span class="math inline">\(x_{0}\)</span>。通过马尔可夫性质，我们可以写出 VDM的联合分布：</p><p><span class="math display">\[\begin{align}    p(x_{0:T}) &amp;= p(x_{T})\prod_{t=1}^{T}p_{\theta}(x_{t-1}|x_{t})\tag{4} \\    p(x_{T}) &amp;= N(x_{T};\boldsymbol{0,I}) \tag{5} \\\end{align}\]</span></p><p>  与 MHVAE 不同的是，在 VDM 中，我们只需要学习解码器的参数 <span class="math inline">\(\boldsymbol{\theta}\)</span>。当训练完成后，采样过程便是先从标准高斯分布<span class="math inline">\(p(x_{T})\)</span> 中采样出高斯噪声 <span class="math inline">\(x_{T}'\)</span>，再通过学习到的各层解码器<span class="math inline">\(p_{\theta}(x_{t-1}|x_{t})\)</span> 经过<span class="math inline">\(T\)</span> 步解码后，生成新的数据 <span class="math inline">\(x_{0}'\)</span>。</p><h2 id="变分下界elbo">变分下界(ELBO)</h2><p>  与 MHVAE 一样，VDM 同样是对似然函数的变分下界进行优化。</p><p><strong>VDM's ELBO</strong></p><p><span class="math display">\[\begin{align}    \log{p(x)} \ge \mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{p(x_{0:T})}{q(x_{1:T}|x_{0})}} \right] \tag{6}\end{align}\]</span></p><p><strong><span class="math inline">\(Proof\)</span></strong></p><p><span class="math display">\[\begin{align}    \log{p(x)} &amp;= \log{p(x)} \int q(x_{1:T}|x_{0})dx_{1:T} \notag \\    &amp;= \int \log{p(x)} q(x_{1:T}|x_{0})dx_{1:T} \notag \\    &amp;= \mathbb{E}_{q(x_{1:T}|x_{0})} \left[ \log{p(x)} \right]\notag \\    &amp;= \mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{p(x_{0:T})q(x_{1:T}|x_{0})}{p(x_{1:T}|x_{0})q(x_{1:T}|x_{0})}}\right] \notag \\    &amp;= \mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{p(x_{0:T})}{q(x_{1:T}|x_{0})}} \right] +\mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{q(x_{1:T}|x_{0})}{p(x_{1:T}|x_{0})}} \right] \notag \\    &amp;= \mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{p(x_{0:T})}{q(x_{1:T}|x_{0})}} \right] +D_{KL}(q(x_{1:T}|x_{0}) || p(x_{1:T}|x_{0})) \tag{7} \\    &amp;\ge \mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{p(x_{0:T})}{q(x_{1:T}|x_{0})}} \right] \notag\end{align}\]</span></p><p>  由以上证明的(7)式我们可以得知，似然函数与ELBO之间的差为 <span class="math inline">\(q(x_{1:T}|x_{0})\)</span> 与 <span class="math inline">\(p(x_{1:T}|x_{0})\)</span> 之间的 KLDivergence，其表示给定原始数据 <span class="math inline">\(x_{0}\)</span>后，编码过程的联合分布与解码过程的联合分布之间的KL距离。最大化 ELBO等价于最小化这个 KLDivergence。这个距离越小，则说明正向加噪与逆向去噪越匹配，模型的生成效果越好。进一步地，利用马尔可夫性质，我们可以将这个KL Divergence 分解成三项：</p><p><span class="math display">\[\begin{align}    &amp; D_{KL}(q(x_{1:T}|x_{0}) || p(x_{1:T}|x_{0})) \tag{8}\\    =&amp;\sum_{t=1}^{T-1}\mathbb{E}_{q(x_{t-1},x_{t+1} | x_{0})} \left[D_{KL}(q(x_{t}|x_{t-1}) || p_{\theta}(x_{t}|x_{t+1})) \right]\tag{consistency term}\\    &amp;+ \mathbb{E}_{q(x_{T-1}|x_{0})}\left[ D_{KL}(q(x_{T}|x_{T-1})|| p(x_{T})) \right] \tag{prior matching term}\\    &amp;- \mathbb{E}_{q(x_{1}|x_{0})}\left[\log{\frac{p_{\theta}(x_{0}|x_{1})}{p(x_{0})}} \right]\tag{reconstruction term}\end{align}\]</span></p><p><strong><span class="math inline">\(Proof\)</span></strong></p><p><span class="math display">\[\begin{align}    &amp; D_{KL}(q(x_{1:T}|x_{0}) || p(x_{1:T}|x_{0})) \notag \\    &amp; = \mathbb{E}_{q(x_{1:T}|x_{0})}\left[\log{\frac{q(x_{1:T}|x_{0})}{p(x_{1:T}|x_{0})}} \right] \notag\\    &amp; = \mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{p(x_{0})\prod_{t=1}^{T}q(x_{t}|x_{t-1})}{p(x_{T})\prod_{t=1}^{T}p_{\theta}(x_{t-1}|x_{t})}}\right] \notag \\    &amp; = \mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{p(x_{0})q(x_{T}|x_{T-1})\prod_{t=1}^{T-1}q(x_{t}|x_{t-1})}{p(x_{T})p_{\theta}(x_{0}|x_{1})\prod_{t=1}^{T-1}p_{\theta}(x_{t}|x_{t+1})}}\right] \notag \\    &amp; =\sum_{t=1}^{T-1}\mathbb{E}_{q(x_{t-1},x_{t},x_{t+1}|x_{0})}\left[\log{\frac{q(x_{t}|x_{t-1})}{p_{\theta}(x_{t}|x_{t+1})}} \right] +\mathbb{E}_{q(x_{T-1},x_{T}|x_{0})}\left[\log{\frac{q(x_{T}|x_{T-1})}{p(x_{T})}} \right] -\mathbb{E}_{q(x_{1}|x_{0})}\left[\log{\frac{p_{\theta}(x_{0}|x_{1})}{p(x_{0})}} \right] \notag \\    &amp; =\sum_{t=1}^{T-1}\underbrace{\mathbb{E}_{q(x_{t-1},x_{t+1}|x_{0})}\left[D_{KL}(q(x_{t}|x_{t-1}) || p_{\theta}(x_{t}|x_{t+1}))\right]}_{consistency \ term} +\underbrace{\mathbb{E}_{q(x_{T-1}|x_{0})}\left[ D_{KL}( q(x_{T}|x_{T-1})|| p(x_{T})) \right]}_{prior \ matching \ term}  -\underbrace{\mathbb{E}_{q(x_{1}|x_{0})}\left[\log{\frac{p_{\theta}(x_{0}|x_{1})}{p(x_{0})}} \right]}_{reconstruction\ term} \notag \\\end{align}\]</span></p><p>  我们对 KL Divergence 进行了分解，得到了 consistency term, priormatching term, reconstruction term 三项。 最小化 KL Divergence等价于最小化这三项，即使得 consistency term, prior matching term尽可能小，reconstruction term尽可能大。我们先不详细解释这三项的含义，接下来我们同样对 VDM 的 ELBO(6)进行分解，我们会发现 ELBO 分解后的结果与 KL Divergence几乎一致，只是符号相反：</p><p><span class="math display">\[\begin{align}   &amp; \mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{p(x_{0:T})}{q(x_{1:T}|x_{0})}} \right] \tag{9} \\   =&amp; \mathbb{E}_{q(x_{1}|x_{0})}[\log{p_{\theta}(x_{0}|x_{1})}]\tag{reconstruction term} \\   -&amp; \mathbb{E}_{q(x_{T-1}|x_{0})}\left[ D_{KL}( q(x_{T}|x_{T-1})|| p(x_{T})) \right] \tag{prior matching term}\\   -&amp; \sum_{t=1}^{T-1}\mathbb{E}_{q(x_{t-1},x_{t+1}|x_{0})}\left[D_{KL}(q(x_{t}|x_{t-1}) || p_{\theta}(x_{t}|x_{t+1})) \right]\tag{consistency term} \\\end{align}\]</span></p><p><strong><span class="math inline">\(Proof\)</span></strong></p><p><span class="math display">\[\begin{align}    &amp; \mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{p(x_{0:T})}{q(x_{1:T}|x_{0})}} \right] \notag \\    &amp; = \mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{p(x_{T})\prod_{t=1}^{T}p_{\theta}(x_{t-1}|x_{t})}{\prod_{t=1}^{T}q(x_{t}|x_{t-1})}}\right] \notag \\    &amp;= \mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{p(x_{T})p_{\theta}(x_{0}|x_{1})\prod_{t=2}^{T}p_{\theta}(x_{t-1}|x_{t})}{q(x_{T}|x_{T-1})\prod_{t=1}^{T-1}q(x_{t}|x_{t-1})}}\right] \notag \\    &amp;= \mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{p(x_{T})p_{\theta}(x_{0}|x_{1})\prod_{t=1}^{T-1}p_{\theta}(x_{t}|x_{t+1})}{q(x_{T}|x_{T-1})\prod_{t=1}^{T-1}q(x_{t}|x_{t-1})}}\right] \notag \\    &amp;= \mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{p(x_{T})p_{\theta}(x_{0}|x_{1})}{q(x_{T}|x_{T-1})}} \right] +\mathbb{E}_{q(x_{1:T}|x_{0})}\left[ \log{\prod_{i=1}^{T-1}}\frac{p_{\theta}(x_{t}|x_{t+1})}{q(x_{t}|x_{t-1})} \right] \notag \\      &amp;= \mathbb{E}_{q(x_{1}|x_{0})}[\log{p_{\theta}(x_{0}|x_{1})}] +\mathbb{E}_{q(x_{T-1},x_{T}|x_{0})} \left[\log{\frac{p(x_{T})}{q(x_{T}|x_{T-1})}} \right] +\sum_{i=1}^{T-1}\mathbb{E}_{q(x_{1:T}|x_{0})}\left[ \log{\frac{p_{\theta}(x_{t}|x_{t+1})}{q(x_{t}|x_{t-1})}} \right] \notag \\    &amp;=\underbrace{\mathbb{E}_{q(x_{1}|x_{0})}[\log{p_{\theta}(x_{0}|x_{1})}]}_{reconstruction\ term} -\underbrace{\mathbb{E}_{q(x_{T-1}|x_{0})}[D_{KL}(q(x_{T}|x_{T-1}) ||p(x_{T}))]}_{prior \ matching \ term} -\sum_{i=1}^{T-1}\underbrace{\mathbb{E}_{q(x_{t-1},x_{t+1}|x_{0})}\left[D_{KL}(q(x_{t}|x_{t-1}) || p_{\theta}(x_{t} || x_{t+1}))\right]}_{consistency \ term} \notag \\\end{align}\]</span></p><p>  由以上的推导我们将 ELBO 分解为与 KL Divergence相似的三项，只是符号相反，这也从另一个方面说明了最大化 ELBO实际上等价于最小化 KL Divergence。最大化 ELBO 的过程是使得reconstruction term 尽可能大，prior matching term, consistency term尽可能小。接下来以 ELBO的分解结果为例，我们尝试理解一下这三项的含义。</p><ul><li><strong>reconstruction term:</strong>重构项。这一项的含义是将原始数据 <span class="math inline">\(x_{0}\)</span> 编码一次后得到隐变量 <span class="math inline">\(x_{1}\)</span> 后再通过解码器还原回 <span class="math inline">\(x_{0}\)</span>，所得到的对数似然。这一项在 VAE中也存在，这一项的值越大，表明数据在编码与解码后与原始数据更相似，即生成的效果更好。<br></li><li><strong>prior matching term:</strong>先验匹配项。这一项的含义是最终隐变量 <span class="math inline">\(x_{T}\)</span> 的后验分布 <span class="math inline">\(q(x_{T}|x_{T-1})\)</span> 与其先验分布 <span class="math inline">\(p(x_{T})\)</span> 之间的 KL Divergence的期望。这一项越小，则先验与后验越匹配，说明编码过程得到的最终隐变量的分布与标准高斯分布越接近，更能匹配采样过程。<br></li><li><strong>consistency term:</strong>一致性项。这一项的含义是从前向和后向两个过程努力使 <span class="math inline">\(x_{t}\)</span>处的分布保持一致。如图2所示，对于每一个中间时间步 <span class="math inline">\(t\)</span>，从噪声图像中得到的去噪图片的分布 <span class="math inline">\(p_{\theta}(x_{t}|x_{t+1})\)</span>应该与从干净图像中得到的相应加噪步骤得到的图片的分布 <span class="math inline">\(q(x_{t}|x_{t-1})\)</span> 相匹配，这在数学上通过KLDivergence得到了体现。这一项越小，说明解码器 <span class="math inline">\(p_{\theta}(x_{t}|x_{t-})\)</span>被训练的越好。</li></ul><center><img src="https://s2.loli.net/2024/05/29/Aq8Eb5kvrWURtNX.png" width="80%" height="80%"><div data-align="center">Image2: 一致性</div></center><p>  通过以上的推导，VDM 的 ELBO均分解成了期望的形式，我们可以使用蒙特卡洛方法来对这些项进行近似，然而，实际上使用我们刚才推导出的(9)式来优化ELBO可能是次优的；因为一致性项在每个时间步<span class="math inline">\(t\)</span> 上都被计算为两个随机变量 <span class="math inline">\(x_{t-1},x_{t+1}\)</span>的期望值，因此其蒙特卡洛估计的方差有可能高于每个时间步仅使用一个随机变量估计的项。由于它是通过对<span class="math inline">\(T-1\)</span>个时间步求和来计算的，因此对于大的T值，ELBO的最终估计值可能具有很高的方差。<br>  为了改善这个问题，我们尝试对 (9)式做一些变形。基于马尔可夫性与贝叶斯公式，我们可以得到以下等式：</p><p><span class="math display">\[\begin{align}    q(x_{t}|x_{t-1}) = q(x_{t}|x_{t-1},x_{0}) =\frac{q(x_{t-1})q(x_{t}|x_{0})}{q(x_{t-1}|x_{0})} \tag{10}\end{align}\]</span></p><p>  通过(10)式，我们可以将 VDM 的 ELBO 分解为如下形式：</p><p><span class="math display">\[\begin{align}   &amp; \mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{p(x_{0:T})}{q(x_{1:T}|x_{0})}} \right] \tag{11} \\   =&amp; \mathbb{E}_{q(x_{1}|x_{0})}[\log{p_{\theta}(x_{0}|x_{1})}]\tag{reconstruction term} \\   -&amp; D_{KL}( q(x_{T}|x_{0}) || p(x_{T}))  \tag{prior matchingterm}\\   -&amp; \sum_{t=2}^{T}\mathbb{E}_{q(x_{t}|x_{0})}\left[D_{KL}(q(x_{t-1}|x_{t},x_{0}) || p_{\theta}(x_{t-1}|x_{t})) \right]\tag{denoising matching term} \\\end{align}\]</span></p><p><strong><span class="math inline">\(Proof\)</span></strong></p><p><span class="math display">\[\begin{align}    &amp; \mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{p(x_{0:T})}{q(x_{1:T}|x_{0})}} \right] \notag \\    &amp;= \mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{p(x_{T})\prod_{t=1}^{T}p_{\theta}(x_{t-1}|x_{t})}{\prod_{t=1}^{T}q(x_{t}|x_{t-1})}}\right] \notag \\    &amp;= \mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{p(x_{T})p_{\theta}(x_{0}|x_{1})\prod_{t=2}^{T}p_{\theta}(x_{t-1}|x_{t})}{q(x_{1}|x_{0})\prod_{t=2}^{T}q(x_{t}|x_{t-1})}}\right] \notag \\    &amp;= \mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{p(x_{T})p_{\theta}(x_{0}|x_{1})}{q(x_{1}|x_{0})}} +\log{\prod_{t=2}^{T}\frac{p_{\theta}(x_{t-1}|x_{t})}{q(x_{t}|x_{t-1},x_{0})}}\right] \notag \\    &amp;= \mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{p(x_{T})p_{\theta}(x_{0}|x_{1})}{q(x_{1}|x_{0})}} +\log{\prod_{t=2}^{T}\frac{p_{\theta}(x_{t-1}|x_{t})}{\frac{q(x_{t-1}|x_{t},x_{0})\cancel{q(x_{t}|x_{0})}}{\cancel{q(x_{t-1}|x_{0})}}}}\right] \notag \\    &amp;= \mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{p(x_{T})p_{\theta}(x_{0}|x_{1})}{\cancel{q(x_{1}|x_{0})}}}+\log{\frac{\cancel{q(x_{1}|x_{0})}}{q(x_{T}|x_{0})}} +\log{\prod_{t=2}^{T}\frac{p_{\theta}(x_{t-1}|x_{t})}{q(x_{t-1}|x_{t},x_{0})}}\right] \notag \\    &amp;= \mathbb{E}_{q(x_{1}|x_{0})}[\log{p_{\theta}(x_{0}|x_{1})}] -\mathbb{E}_{q(x_{T}|x_{0})}\left[ \log{\frac{q(x_{T}|x_{0})}{p(x_{T})}}\right] -\sum_{t=2}^{T} \mathbb{E}_{q(x_{t},x_{t-1}|x_{0})}\left[\log{\frac{q(x_{t-1}|x_{t},x_{0})}{p_{\theta}(x_{t-1}|x_{t})}} \right]\notag \\    &amp;=\underbrace{\mathbb{E}_{q(x_{1}|x_{0})}[\log{p_{\theta}(x_{0}|x_{1})}]}_{reconstruction\ term} - \underbrace{D_{KL}(q(x_{T}|x_{0}) || p(x_{T}))}_{prior \matching \ term} -\sum_{t=2}^{T}\underbrace{\mathbb{E}_{q(x_{t}|x_{0})}[D_{KL}(q(x_{t-1}|x_{t},x_{0})|| p_{\theta}(x_{t-1}|x_{t}))]}_{denoising \ matching \ term} \notag \\\end{align}\]</span></p><p>  reconstruction term 与 prior matching term的含义与(9)基本一致。差别较大的是(9)式中的 consistency term 与(11)式中的 denoising matching term。与 consistency term 相比，denoisingmatching term 中的每一时间步 <span class="math inline">\(t\)</span>，只需要计算一个随机变量 <span class="math inline">\(x_{t}\)</span>的期望，显著改善了蒙特卡洛估计的方差较大的问题。同时，最小化 denoisingmatching term 意味着在每一个时间步 <span class="math inline">\(t\)</span>，通过解码器去噪后的数据的分布 <span class="math inline">\(p_{\theta}(x_{t-1}|x_{t})\)</span>与真实加噪过程中加入噪声前的图片的分布 <span class="math inline">\(q(x_{t-1}|x_{t},x_{0})\)</span> 相匹配，即 KLDivergence尽可能小。这一项越小，说明解码器每一步预测噪声的能力越强，即从一般抽象特征去生成更加细节的特征的能力越强，生成图片与原始图片就会越相似，这一过程如图3所示。</p><center><img src="https://s2.loli.net/2024/05/29/FAxezibV6GnNpv3.png" width="80%" height="80%"><div data-align="center">Image3: 去噪匹配</div></center><h2 id="损失函数">损失函数</h2><p>  在前文中，我们推导了 VDM's ELBO 的理论形式(11)，通过最大化 ELBO来近似最大化对数似然，得到待估参数 <span class="math inline">\(\theta\)</span>。现在我们要利用 VDM的假设条件，根据 ELBO的理论形式，来得到具体用于模型训练的损失函数。<br>  通过前文的分析，我们可以将VDM 的损失函数写作如下的三部分:</p><p><span class="math display">\[\begin{align}    \boldsymbol{L}(\theta) = -ELBO = \mathbb{E}_{q}\left[\underbrace{D_{KL}(q(x_{T}|x_{0}) || p(x_{T}))}_{L_{T}} +\sum_{t=2}^{T}\underbrace{D_{KL}(q(x_{t-1}|x_{t},x_{0}) ||p_{\theta}(x_{t-1}|x_{t}))}_{L_{t-1}} -\underbrace{\log{p_{\theta}(x_{0}|x_{1})}}_{L_{0}} \right] \tag{12}\end{align}\]</span></p><p>  接下来我们来逐个讨论这三项的具体形式。</p><h3 id="先验匹配损失-l_t">先验匹配损失 <span class="math inline">\(L_{T}\)</span></h3><p>  这一项是衡量最终隐变量 <span class="math inline">\(X_{T}\)</span>的先验分布与后验分布的相似程度，其中 <span class="math inline">\(p(x_{T})\)</span> 是标准高斯分布，当给定数据 <span class="math inline">\(x_{0}\)</span> 后， <span class="math inline">\(x_{T}\)</span> 的后验分布 <span class="math inline">\(q(x_{T}|x_{0})\)</span> 可以不依赖于参数 <span class="math inline">\(\theta\)</span> 计算出，故 <span class="math inline">\(L_{T}\)</span>损失函数中相当于常数，可以不考虑。</p><h3 id="去噪匹配损失-l_t-1">去噪匹配损失 <span class="math inline">\(L_{t-1}\)</span></h3><p>  去噪匹配损失 <span class="math inline">\(L_{t-1}\)</span>在损失函数中占主导地位，其衡量了每个时间步 <span class="math inline">\(t\)</span>，编码器的去噪后得到的图片与加噪过程中该时刻的真实图片的相似程度。在<span class="math inline">\(L_{t-1}\)</span> 中我们最主要是需要计算<span class="math inline">\(D_{KL}(q(x_{t-1}|x_{t},x_{0}) ||p_{\theta}(x_{t-1}|x_{t}))\)</span>，对于其中的编码器 <span class="math inline">\(q(x_{t-1}|x_{t},x_{0})\)</span>，由贝叶斯公式以及马尔可夫性质可以得到：</p><p><span class="math display">\[\begin{align}    &amp; q(x_{t-1}|x_{t},x_{0}) =\frac{q(x_{t}|x_{t-1},x_{0})q(x_{t-1}|x_{0})}{q(x_{t}|x_{0})} =\frac{q(x_{t}|x_{t-1})q(x_{t-1}|x_{0})}{q(x_{t}|x_{0})} \tag{13} \\    &amp; q(x_{t}|x_{t-1}) = N(x_{t}; \sqrt{\alpha_{t}}x_{t-1},(1-\alpha_{t})\boldsymbol{I}) \tag{14}\end{align}\]</span></p><p>  在前文中，我们已经得知了正向加噪过程满足递推公式：</p><p><span class="math display">\[x_{t} = \sqrt{\alpha_{t}} x_{t-1} +\sqrt{1-\alpha_{t}}\epsilon,\quad \epsilon \sim N(\epsilon;\boldsymbol{0,I})\]</span></p><p>  利用递推公式，我们可以计算出 <span class="math inline">\(q(x_{t}|x_{0})\)</span> 所满足的高斯分布：</p><p><span class="math display">\[\begin{align}    q(x_{t}|x_{0}) = N(x_{t}; \sqrt{\bar{\alpha}_{t}}x_{0}, (1 -\bar{\alpha}_{t})\boldsymbol{I}),\quad \bar{\alpha}_{t} =\prod_{i=1}^{t}\alpha_{i} \tag{15} \\\end{align}\]</span></p><p><strong><span class="math inline">\(Proof\)</span></strong></p><p><span class="math display">\[\begin{align}    x_{t} &amp;= \sqrt{\alpha_{t}} x_{t-1} +\sqrt{1-\alpha_{t}}\epsilon_{t-1}^{*} \notag \\    &amp;= \sqrt{\alpha_{t}} \left( \sqrt{\alpha_{t-1}} x_{t-2} +\sqrt{1-\alpha_{t-1}}\epsilon_{t-2}^{*} \right) +\sqrt{1-\alpha_{t}}\epsilon_{t-1}^{*} \notag \\    &amp;= \sqrt{\alpha_{t}\alpha_{t-1}} x_{t-2} +\sqrt{\alpha_{t}-\alpha_{t}\alpha_{t-1}}\epsilon_{t-2}^{*} +\sqrt{1-\alpha_{t}}\epsilon_{t-1}^{*} \notag \\    &amp;= \sqrt{\alpha_{t}\alpha_{t-1}} x_{t-2} +\sqrt{\sqrt{\alpha_{t}-\alpha_{t}\alpha_{t-1}}^{2} +\sqrt{1-\alpha_{t}}^{2}}\epsilon_{t-2} \notag \\    &amp;= \sqrt{\alpha_{t}\alpha_{t-1}} x_{t-2} + \sqrt{1 -\alpha_{t}\alpha_{t-1}}\epsilon_{t-2} \notag \\    &amp;= \dotsb \notag \\    &amp;= \sqrt{\prod_{i=1}^{t}\alpha_{i}}x_{0} +\sqrt{1-\prod_{i=1}^{t}\alpha_{i}} \epsilon_{0} \notag \\    &amp;= \sqrt{\bar{\alpha}_{t}}x_{0} +\sqrt{1-\bar{\alpha}_{t}}\epsilon_{0} \sim N(x_{t};\sqrt{\bar{\alpha}_{t}}x_{0}, (1 - \bar{\alpha}_{t})\boldsymbol{I})\notag \\\end{align}\]</span></p><p>  利用 (15) 式，我们可以得到 <span class="math inline">\(x_{t-1}\)</span> 的分布：</p><p><span class="math display">\[\begin{align}    q(x_{t-1}|x_{0}) = N(x_{t-1}; \sqrt{\bar{\alpha}_{t-1}}x_{0}, (1 -\bar{\alpha}_{t-1})\boldsymbol{I}) \tag{16} \\\end{align}\]</span></p><p>  联立(14)、(15)、(16)式，我们可以发现(13)式的分子分母均为高斯分布，由高斯分布的联合分布与边际分布均为高斯分布可知，<span class="math inline">\(q(x_{t-1}|x_{t},x_{0})\)</span>，同样满足高斯分布，现在我们需要利用(13)式来计算其均值与方差，通过计算我们可以得到如下结论：</p><p><span class="math display">\[\begin{align}    q(x_{t-1}|x_{t},x_{0}) = N(x_{t-1}; \mu_{q}(x_{t},x_{0}),\Sigma_{q}(t)) \tag{17} \\\end{align}\]</span></p><p><span class="math display">\[\begin{align}    \mu_{q}(x_{t},x_{0}) &amp;=\frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})x_{t} +\sqrt{\bar{\alpha}_{t-1}}(1-\alpha_{t})x_{0}}{1-\bar{\alpha}_{t}},\quad\Sigma_{q}(t) = \sigma_{q}^{2}(t)\boldsymbol{I} =\frac{1-\alpha_{t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_{t}}\boldsymbol{I}\tag{18} \\\end{align}\]</span></p><p><strong><span class="math inline">\(Proof\)</span></strong></p><p><span class="math display">\[\begin{align}    q(x_{t-1}|x_{t},x_{0}) &amp;=\frac{q(x_{t}|x_{t-1})q(x_{t-1}|x_{0})}{q(x_{t}|x_{0})} \notag \\    &amp;= \frac{N(x_{t}; \sqrt{\alpha_{t}}x_{t-1},(1-\alpha_{t})\boldsymbol{I})N(x_{t-1}; \sqrt{\bar{\alpha}_{t-1}}x_{0},(1 - \bar{\alpha}_{t-1})\boldsymbol{I})}{N(x_{t};\sqrt{\bar{\alpha}_{t}}x_{0}, (1 -\bar{\alpha}_{t})\boldsymbol{I}),\quad \bar{\alpha}_{t}} \notag \\    &amp; \propto \exp \left(-\frac{1}{2}\left[\frac{(x_{t}-\sqrt{\alpha_{t}}x_{t-1})^2}{1-\alpha_{t}} + \frac{(x_{t-1}- \sqrt{\bar{\alpha}_{t-1}}x_{0})^2}{1 - \bar{\alpha}_{t-1}} -\frac{(x_{t} - \sqrt{\bar{\alpha}_{t}}x_{0})^2}{1 - \bar{\alpha}_{t}}\right] \right)  \notag \\    &amp;= \exp\left( -\frac{1}{2}\left[\frac{(-2\sqrt{\alpha_{t}}x_{t}x_{t-1}+\alpha_{t}x_{t-1}^{2})}{1-\alpha_{t}}+ \frac{(x_{t-1}^{2}-2\sqrt{\bar{\alpha}_{t-1}}x_{t-1}x_{0})}{1 -\bar{\alpha}_{t-1}} + C(x_{t},x_{0}) \right] \right)  \notag \\    &amp; \propto \exp\left( -\frac{1}{2}\left[\frac{1-\bar{\alpha}_{t}}{(1-\alpha_{t})(1-\bar{\alpha}_{t-1})}x_{t-1}^{2}- 2\left( \frac{\sqrt{\alpha_{t}}x_{t}}{1-\alpha_{t}} +\frac{\sqrt{\bar{\alpha}_{t-1}}x_{0}}{1-\bar{\alpha}_{t-1}}\right)x_{t-1} \right] \right)   \notag \\    &amp;= \exp\left( -\frac{1}{2}\left(\frac{1-\bar{\alpha}_{t}}{(1-\alpha_{t})(1-\bar{\alpha}_{t-1})}\right)\left[ x_{t-1}^{2}-2\frac{\left(\frac{\sqrt{\alpha_{t}}x_{t}}{1-\alpha_{t}} +\frac{\sqrt{\bar{\alpha}_{t-1}}x_{0}}{1-\bar{\alpha}_{t-1}}\right)(1-\alpha_{t})(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_{t}}x_{t-1}\right] \right)  \notag \\    &amp;= \exp\left( -\frac{1}{2}\left(\frac{1}{\frac{(1-\alpha_{t})(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_{t}}}\right)\left[x_{t-1}^{2}-2\frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})x_{t} +\sqrt{\bar{\alpha}_{t-1}}(1-\alpha_{t})x_{0}}{1-\bar{\alpha}_{t}}x_{t-1}\right] \right)  \notag \\    &amp; \propto N(x_{t+1};\underbrace{\frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})x_{t} +\sqrt{\bar{\alpha}_{t-1}}(1-\alpha_{t})x_{0}}{1-\bar{\alpha}_{t}}}_{\mu_{q}(x_{t},x_{0})},\underbrace{\frac{(1-\alpha_{t})(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_{t}}\boldsymbol{I}}_{\Sigma_{q}(t)=\sigma_{q}^{2}(t)\boldsymbol{I}})\notag \\\end{align}\]</span></p><p>  通过以上的推导，我们得到了加噪过程中<span class="math inline">\(x_{t-1}\)</span>所满足的高斯分布，要计算 <span class="math inline">\(L_{t-1}\)</span> 式中的 KLDivergence，我们还需要去噪过程中 <span class="math inline">\(x_{t-1}\)</span> 的分布。<br>  为了使得去噪过程与加噪过程尽可能匹配，我们同样将去噪过程建模为高斯过程，即<span class="math inline">\(p_{\theta}(x_{t-1}|x_{t})\)</span>满足高斯分布。去噪过程的高斯分布的方差与对应的加噪过程的方差一致，而均值是由参数化的神经网络计算得出。对于均值的计算，VDM将<span class="math inline">\(p_{\theta}(x_{t-1}|x_{t})\)</span>所满足的高斯分布的均值设定为与 <span class="math inline">\(q(x_{t-1}|x_{t},x_{0})\)</span> 具有相同的形式，即<span class="math inline">\(\mu_{q}(x_{t},x_{0})\)</span>，但在去噪过程中是没有给定<span class="math inline">\(x_{0}\)</span>的，故神经网络在均值计算中的实际作用是输出 <span class="math inline">\(x_{0}\)</span> 的预测值 <span class="math inline">\(\hat{x}_{\theta}(x_{t-1},t)\)</span> ，从而得到<span class="math inline">\(p_{\theta}(x_{t-1}|x_{t})\)</span>所满足的高斯分布的均值。以上建模过程总结的数学表达式如下：</p><p><span class="math display">\[\begin{align}    p_{\theta}(x_{t-1}|x_{t}) = N(x_{t-1}; \mu_{\theta}(x_{t},t),\Sigma_{p}(t))  \tag{19}\end{align}\]</span></p><p><span class="math display">\[\begin{align}    \mu_{\theta}(x_{t},t) &amp;=\frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})x_{t} +\sqrt{\bar{\alpha}_{t-1}}(1-\alpha_{t})\hat{x}_{\theta}(x_{t-1},t)}{1-\bar{\alpha}_{t}},\quad\Sigma_{p}(t) = \Sigma_{q}(t) =\frac{1-\alpha_{t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_{t}}\boldsymbol{I}\tag{20} \\\end{align}\]</span></p><p>  通过以上的建模，去噪匹配损失 <span class="math inline">\(L_{t-1}\)</span> 中的 KL Divergence实际上是计算两个方差相同的高斯分布的 KLDivergence，这使得问题变得非常简单，因为高斯分布的 KL Divergence是有显式表达式的，其表达式如下所示：</p><p><span class="math display">\[\begin{align}    D_{KL}(N(x;\mu_{x},\Sigma_{x}) || N(y;\mu_{y},\Sigma_{y})) =\frac{1}{2}\left[ \log{\frac{|\Sigma_{y}|}{|\Sigma_{x}|}}-d +tr(\Sigma_{y}^{-1}\Sigma_{x}) + (\mu_{y} -\mu_{x})^{T}\Sigma_{y}^{-1}(\mu_{y}-\mu_{x}) \right] \tag{21}\end{align}\]</span></p><p>  其中，<span class="math inline">\(d\)</span>是高斯分布的维度。结合(17)、(18)、(19)、(20)、(21)式，我们现在可以来计算<span class="math inline">\(L_{t-1}\)</span> 中的 KL Divergence的具体表达式了，其结果如下：</p><p><span class="math display">\[\begin{align}    D_{KL}(q(x_{t-1}|x_{t},x_{0}) || p_{\theta}(x_{t-1}|x_{t})) =\frac{1}{2\sigma_{q}^{2}(t)}\frac{\bar{\alpha}_{t-1}(1-\alpha_{t})^{2}}{(1-\bar{\alpha}_{t})^{2}}\left[ ||\hat{x}_{\theta}(x_{t-1},t) - x_{0}||_{2}^{2} \right] \tag{22}\end{align}\]</span></p><p><strong><span class="math inline">\(Proof\)</span></strong></p><p><span class="math display">\[\begin{align}    &amp; D_{KL}(q(x_{t-1}|x_{t},x_{0}) || p_{\theta}(x_{t-1}|x_{t}))\notag \\    &amp;= D_{KL}(N(x_{t-1}; \mu_{q}, \Sigma_{q}(t)) || N(x_{t-1};\mu_{\theta}, \Sigma_{p}(t)))  \notag \\    &amp;= \frac{1}{2}\left[\log{\frac{|\Sigma_{q}(t)|}{|\Sigma_{q}(t)|}}-d +tr(\Sigma_{q}(t)^{-1}\Sigma_{q}(t)) + (\mu_{q} -\mu_{\theta})^{T}\Sigma_{q}(t)^{-1}(\mu_{q}-\mu_{\theta}) \right] \notag\\    &amp;= \frac{1}{2}\left[ \log{1}-d + d + (\mu_{q} -\mu_{\theta})^{T}\Sigma_{q}(t)^{-1}(\mu_{q}-\mu_{\theta}) \right] \notag\\    &amp;= \frac{1}{2}\left[(\mu_{q} -\mu_{\theta})^{T}(\sigma_{q}^{2}(t)\boldsymbol{I})^{-1}(\mu_{q}-\mu_{\theta})\right] \notag \\    &amp;= \frac{1}{2\sigma_{q}^{2}(t)} \left[ ||\mu_{q} - \mu_{\theta}||_{2}^{2} \right] \notag \\    &amp;=  \frac{1}{2\sigma_{q}^{2}(t)} \left[||\frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})x_{t} +\sqrt{\bar{\alpha}_{t-1}}(1-\alpha_{t})x_{0}}{1-\bar{\alpha}_{t}}-\frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})x_{t} +\sqrt{\bar{\alpha}_{t-1}}(1-\alpha_{t})\hat{x}_{\theta}(x_{t-1},t)}{1-\bar{\alpha}_{t}}||_{2}^{2}\right]\notag\\    &amp;= \frac{1}{2\sigma_{q}^{2}(t)} \left[||\frac{\sqrt{\bar{\alpha}_{t-1}}(1-\alpha_{t})}{1-\bar{\alpha}_{t}}(\hat{x}_{\theta}(x_{t-1},t) - x_{0}) ||_{2}^{2}\right]  \notag \\    &amp;=\frac{1}{2\sigma_{q}^{2}(t)}\frac{\bar{\alpha}_{t-1}(1-\alpha_{t})^{2}}{(1-\bar{\alpha}_{t})^{2}}\left[ ||\hat{x}_{\theta}(x_{t-1},t) - x_{0}||_{2}^{2} \right] \notag \\\end{align}\]</span></p><p>  对于(12)式中的期望，可以在每个训练批次中使用 Monte Carlo estimate方法来估计。通过 (22) 式 我们可以得知，VDM在逆向去噪过程中，每一步的优化目标都是在给定噪声图片 <span class="math inline">\(x_{t-1}\)</span> 的情况下，预测出原始图片 <span class="math inline">\(x_{0}\)</span>。这种损失函数的设定方式可以总结为<strong>预测原始数据</strong>，在之后的章节我们会讨论扩散模型损失函数的另外两种等价形式，分别为<strong>预测噪声</strong>以及<strong>分数匹配</strong>。</p><h3 id="重构似然损失-l_0">重构似然损失 <span class="math inline">\(L_{0}\)</span></h3><p>  对于损失函数中的 <span class="math inline">\(L_{0}\)</span> 项，在DDPM 的原始论文[2]中，采用了一个独立的离散编码器。这是因为在之前的加噪去噪步骤中，我们都将图片数据的取值由<span class="math inline">\(\{ 0,1,\dotsb,255 \}\)</span>的离散数值映射到 <span class="math inline">\([-1,1]\)</span>。这一方面是使数据标准化，便于神经网络处理；另一方面是因为采样过程是从标准高斯分布中进行采样，再由解码器逐步去噪，故需要在训练时的加噪去噪过程对离散数据进行映射后来匹配采样时的数值范围。<br>  具体来讲，由之前步骤训练得到的参数化的神经网络，以及去噪得到的数据<span class="math inline">\(x_{1}\)</span>，我们可以同样可以得到 <span class="math inline">\(p_{\theta}(x_{0}|x_{1})\)</span> 所满足的高斯分布<span class="math inline">\(N(x_{0};\mu_{\theta}(x_{1},1),\sigma_{1}^{2})\)</span>，但在这里我们不能像去噪过程一样，直接使用该高斯分布来计算<span class="math inline">\(x_{0}\)</span> 的对数似然，这是因为原始数据<span class="math inline">\(x_{0}\)</span>是离散数据，而高斯分布本身是连续的，直接使用高斯分布来计算离散对数本事是不够准确的。在DDPM 的原始论文[2]中，作者采用了一个离散边界函数计算高斯分布在离散区间上的积分，从而精确计算离散数据的对数似然。其计算公式如下：</p><p><span class="math display">\[\begin{align}    p_{\theta}(x_{0}|x_{1}) = \prod_{i=1}^{d}\int_{\delta_{-}(x_{0}^{i})}^{\delta_{+}(x_{0}^{i})} N(x_{0};\mu_{\theta}(x_{1},1),\sigma_{1}^{2}) dx \tag{22} \\\end{align}\]</span></p><p><span class="math display">\[\begin{align}    \delta_{+}(x_{0}^{i}) = \left \{\begin{array}{l}\infty &amp; if \ x = 1 \\x + \frac{1}{255} &amp; if \ x &lt; 1 \\\end{array} \right. \quad \delta_{-}(x_{0}^{i}) = \left \{\begin{array}{l}-\infty &amp; if \ x = -1 \\x - \frac{1}{255} &amp; if \ x &gt; -1 \\\end{array} \right.\tag{23}\end{align}\]</span></p><p>  其中，<span class="math inline">\(i\)</span> 表示数据 <span class="math inline">\(x_{0}\)</span> 的每一个维度，<span class="math inline">\(\delta_{+}(x_{0}^{i}),\delta_{-}(x_{0}^{i})\)</span>表示每个数据维度的离散值的上下边界。由于在做数据映射时，是使用 <span class="math inline">\(x' = (2x - 255) / 255\)</span>将数据取值范围由 <span class="math inline">\(\{ 0,1,\dotsb,255\}\)</span> 映射到 <span class="math inline">\([0,1]\)</span>的，故<span class="math inline">\(x' \pm \frac{1}{255} = [2(x \pm 0.5)-255]/255\)</span>，故(23)式设置的离散边界实际上是等价于在离散值上下分别加上0.5后再计算高斯分布的积分。使用这种方法可以较为精确的计算离散数据的对数似然。</p><h2 id="总结">总结</h2><p>  在这篇博客中，我们首先讨论了 VDM 相较于之前的 MHVAE又做了哪些假设，以及做出这些假设的 motivations。之后与 VAE类似，我们讨论了 VDM 的 ELBO的理论表达式，其可以分解为三项，在此基础上，我们计算了 ELBO三个分解项在损失函数中的具体表达式。<br>  但实际上，DDPM在训练中并不是使用<strong>预测原始数据</strong>，即(22)式作为损失函数，而是使用<strong>预测噪声</strong>作训损失函数，在之后的一些研究中，例如基于分数的生成模型，则是使用<strong>分数匹配</strong>作为损失函数，这三种扩散模型的损失函数实际上是等价的，这一点由于篇幅的限制我们不再做过多的讨论。在下一节，我们将重点讨论三种损失函数的等价关系，如何做训练以及采样，以及扩散模型系数该如何设置或学习。</p><h2 id="reference">Reference</h2><p><strong>[1] Paper: Luo C. Understanding diffusion models: A unifiedperspective[J]. arXiv preprint arXiv:2208.11970, 2022.</strong><br><strong>[2] Paper: Ho J, Jain A, Abbeel P. Denoising diffusionprobabilistic models[J]. Advances in neural information processingsystems, 2020, 33: 6840-6851.</strong><br><strong>[3] Video: 想不出来昵称又想改, 扩散模型-DiffusionModel【李宏毅2023】, Blibili</strong><br><strong>[4] Blog: 苏剑林, 生成扩散模型漫谈(1-3), 科学空间</strong></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>深度学习-5-生成模型3-变分自编码</title>
      <link href="/2024/05/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-5-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B3-%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81/"/>
      <url>/2024/05/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-5-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B3-%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81/</url>
      
        <content type="html"><![CDATA[<h1 id="变分自编码vae">变分自编码(VAE)</h1><p>  GMM诞生于上世纪，其本身受制于当时的计算机性能以及数据量的大小，伴随着计算机技术的发展，在2010年之后，深度学习领域崛起，生成模型的研究也迎来了大的变革。很多学者研究了如何将利用神经网络模型拟合非线性的强大性能来改善生成模型的效果。2014年，Kingma.D.P在ICLR上发表了著名的论文《Auto-encoding variationalbayes》[2]，提出了变分自编码模型(VariationalAuto-encoding)，其采用基于神经网络的编码器-解码器架构，利用编码器对数据的隐变量进行建模，再通过一个解码器将隐变量映射到数据空间来生成数据。同时VAE也提出了变分推断的思想，即对似然函数的变分下界来进行优化。<br>  VAE的出现对现代生成模型具有重大影响，其建立的基本框架，直到今天仍然被Diffusion等模型采用，就在今年5月份，ICLR颁发了首届时间检验奖，用于表彰具有长期影响力的论文，VAE的原始论文[3]获得了该奖项，由此便可说明其在生成模型领域的巨大影响。</p><h2 id="概率模型">概率模型</h2><p>  与GMM相同，VAE同样采用了隐变量假设，即认为观测数据 <span class="math inline">\(x\)</span> 受到隐变量 <span class="math inline">\(z\)</span>的影响，但与GMM不同的是，其采用了标准高斯分布作为隐变量的先验分布，同时引入了贝叶斯思想，即后验分布的概念，其概率图如下图3所示：</p><center><img src="https://s2.loli.net/2024/05/20/f79nweONT41RS6D.png" width="80%" height="80%"><div data-align="center">Image3: VAE 概率图</div></center><p>  对于图中的未知的条件概率 <span class="math inline">\(p(x|z),q(z|x)\)</span>，VAE采用了参数化的神经网络模型对其进行逼近，其分别被称为解码器<span class="math inline">\(p_{\theta}(x|z)\)</span>，表示将隐变量解码到数据空间中，即生成新样本；编码器<span class="math inline">\(q_{\phi}(z|x)\)</span>，表示将观测数据编码到隐空间中，编码器设置为高斯分布，其参数由神经网络模型给出。VAE的概率模型可以总结为：</p><p><span class="math display">\[\begin{align}    &amp; Latend \ prior: p(z) = N(z; \boldsymbol{0, I}) \tag{17} \\    &amp; Encoder: q_{\phi}(z|x) = N(z; \boldsymbol{\mu}_{\phi}(x),\sigma_{\phi}^{2}(x)\boldsymbol{I}) \rightarrow q(z|x)  \tag{18}  \\    &amp; Decoder: p_{\theta}(x|z) \rightarrow p(x|z)  \tag{19} \\\end{align}\]</span></p><h2 id="变分下界elbo">变分下界(ELBO)</h2><p>  在估计概率模型的参数时，我们通常会使用“最大似然法”，然而尽管有EM算法等迭代算法，但在面对复杂的概率模型时，求解似然参数仍然是一件困难且耗时的工作，对此VAE提出变分下界的概念，也叫做证据下界(EvidenceLower Bound)，通过最大化变分下界来代替直接对于似然函数的优化。<br>  <strong>(ELBO)</strong> 对于存在隐变量 <span class="math inline">\(z\)</span>的模型，其似然函数具有以下不等式成立：</p><p><span class="math display">\[\begin{align}    \log{p(x)} \ge \mathbb{E}_{q_{\phi}(z|x)} \left[\log\frac{p(x,z)}{q_{\phi}(z|x)} \right] \tag{20}\end{align}\]</span></p><p><strong><span class="math inline">\(proof\)</span></strong></p><p><span class="math display">\[\begin{align}    \log{p(x)} &amp;= \log{p(x)} \int_{z} q_{\phi}(z|x) dz = \int_{z}q_{\phi}(z|x)\log{p(x)} dz \notag \\    &amp;= \mathbb{E}_{q_{\phi}(z|x)}[\log{p(x)}] =\mathbb{E}_{q_{\phi}(z|x)}\left[\log{\frac{p(x,z)q_{\phi}(z|x)}{q(z|x)q_{\phi}(z|x)}} \right] \notag \\    &amp;= \mathbb{E}_{q_{\phi}(z|x)} \left[\log{\frac{p(x,z)}{q_{\phi}(z|x)}} \right] + \mathbb{E}_{q_{\phi}(z|x)}\left[ \log{\frac{q_{\phi}(z|x)}{q(z|x)}} \right] \notag \\    &amp;= \mathbb{E}_{q_{\phi}(z|x)} \left[\log{\frac{p(x,z)}{q_{\phi}(z|x)}} \right] + D_{KL}(q_{\phi}(z|x) ||q(z|x)) \tag{21} \\    &amp; \ge \mathbb{E}_{q_{\phi}(z|x)} \left[\log{\frac{p(x,z)}{q_{\phi}(z|x)}} \right] \tag{22}\end{align}\]</span></p><p>  从式(21)可以看出，<strong>似然函数 <span class="math inline">\(\log{p(x)}\)</span> 实际上等于ELBO加上真实后验<span class="math inline">\(q(z|x)\)</span> 与近似后验 <span class="math inline">\(q_{\phi}(z|x)\)</span> 之间的 KLDivergence，由于似然函数 <span class="math inline">\(\log{p(x)}\)</span>并不依赖于参数 <span class="math inline">\(\phi\)</span>，其在优化过程中可看作一个固定的常数，因此使用ELBO代替似然函数作为最大化的目标函数是有意义的，因为其实际上等价于最小化真实后验<span class="math inline">\(q(z|x)\)</span> 与近似后验 <span class="math inline">\(q_{\phi}(z|x)\)</span> 之间的 KLDivergence.</strong>   通过(17)式与(19)式，我们可以对 ELBO进一步推导：</p><p><span class="math display">\[\begin{align}    \mathbb{E}_{q_{\phi}(z|x)} \left[ \log{\frac{p(x,z)}{q_{\phi}(z|x)}}\right] &amp;= \mathbb{E}_{q_{\phi}(z|x)} \left[\log{\frac{p(z)p_{\theta}(x|z)}{q_{\phi}(z|x)}} \right]  \notag \\    &amp;= \mathbb{E}_{q_{\phi}(z|x)} \left[ \log{p_{\theta}(x|z)}\right] - \mathbb{E}_{q_{\phi}(z|x)} \left[\log{\frac{q_{\phi}(z|x)}{p(z)}} \right]  \notag \\    &amp;= \underbrace{\mathbb{E}_{q_{\phi}(z|x)} \left[\log{p_{\theta}(x|z)} \right]}_{reconstruction \ term} -\underbrace{D_{KL}(q_{\phi}(z|x) || p(z))}_{prior \ matching \ term}\tag{23} \\\end{align}\]</span></p><p>  通过推导，我们发现 VAE 的 ELBO 包括两项，其意义如下：</p><ul><li><strong>reconstruction term:</strong> 该项代表了解码器基于隐变量<span class="math inline">\(z\)</span> 重构输入数据 <span class="math inline">\(x\)</span>的分布的对数似然。通过最大化这个对数似然，模型被训练以更好地从潜在变量重构输入数据，确保潜在变量<span class="math inline">\(z\)</span>能够有效地捕捉到输入数据的关键特征。<br></li><li><strong>prior matching term:</strong>该项了学习到的变分分布与对隐变量的先验分布的相似程度。最小化这个项鼓励编码器学习一个实际的分布，而不是崩溃成狄拉克delta函数。</li></ul><p>  要最大化 ELBO就是要使第一项尽可能大，同时第二项尽可能小，但这两个目标是想抗衡的，若第二项很小，则说明<span class="math inline">\(q_{\phi}(z|x)\)</span>近似于标准高斯分布，则意味着 <span class="math inline">\(z\)</span>没有任何辨识度，并没有学习到 <span class="math inline">\(x\)</span>的分布特征，此时第一项就不可能非常小；反过来，如果第一项较大，即说明<span class="math inline">\(z\)</span> 具有很高的辨识度，<span class="math inline">\(q_{\phi}(z|x)\)</span>会大幅度偏离标准高斯分布，则第二项不会很小。因此类似于 GAN，这两项的优化实际上是一个对抗过程。在变分自编码器中，ELBO是优化目标，它同时包括了对数据重构的质量（第一项）和对模型复杂性的控制（第二项）。最大化ELBO意味着我们尝试达到这两个目标的最佳平衡：即最大化数据的重构质量，同时保持潜在表示的多样性和广泛性。</p><h2 id="模型训练">模型训练</h2><p>  综上所述，VAE的优化函数可以写成：</p><p><span class="math display">\[\begin{align}    (\hat{\phi},\hat{\theta}) = \argmax_{\phi,\theta}\mathbb{E}_{q_{\phi}(z|x)} \left[ \log{p_{\theta}(x|z)} \right] -D_{KL}(q_{\phi}(z|x) || p(z)) \tag{24}\end{align}\]</span></p><p>  我们来将目标函数进行细化，对于第一项，我们可以使用Monte Carloestimate 来进行估计，即：</p><p><span class="math display">\[\begin{align}    \mathbb{E}_{q_{\phi}(z|x)} \left[ \log{p_{\theta}(x|z)} \right]\approx \frac{1}{N}\sum_{i=1}^{N} \log{p_{\theta}(x|z_i)} \tag{25}\end{align}\]</span></p><p>  其中，<span class="math inline">\(N\)</span>为训练集的样本容量，<span class="math inline">\(z_i\)</span>为从隐变量的后验分布 <span class="math inline">\(q_{\phi}(z|x)\)</span>采样出的样本。同时，由于 <span class="math inline">\(q_{\phi}(z|x)\)</span> 与 <span class="math inline">\(p(z)\)</span>均为高斯分布，故第二项很容易就能得到：</p><p><span class="math display">\[\begin{align}    D_{KL}(q_{\phi}(z|x) || p(z)) = \frac{1}{2}\sum_{k=1}^{d}\left(\mu_{\phi,k}^{2}(x) + \sigma_{\phi,k}^{2}(x) -\log{\sigma_{\phi,k}^{2}(x)}-1 \right) \tag{26}\end{align}\]</span></p><p>  因此(24)式可以写为：</p><p><span class="math display">\[\begin{align}     \argmax_{\phi,\theta} \frac{1}{N}\sum_{i=1}^{N}\log{p_{\theta}(x|z_i)} - \frac{1}{2}\sum_{k=1}^{d}\left(\mu_{\phi,k}^{2}(x) + \sigma_{\phi,k}^{2}(x) -\log{\sigma_{\phi,k}^{2}(x)}-1 \right) \tag{27}\end{align}\]</span></p><p>  然而，这里有一个问题：随机采样过程的不可导性。在第一项中，我们需要从<span class="math inline">\(q_{\phi}(z|x)\)</span> 中采样出 <span class="math inline">\(z\)</span>用于计算损失，但在神经网络中，我们需要计算损失函数关于网络参数的梯度以进行反向传播和参数更新。然而，由于采样步骤是一个随机过程，并且具有“跳跃”的性质(从概率分布中直接抽取样本)，它本身不是一个可导的操作，我们无法对<span class="math inline">\(\phi\)</span> 进行更新。<br>  为了解决这个问题，VAE提出了一种重要技术：重参数化技巧(reparameterizationtrick)。重新参数化技巧将随机变量改写为噪声变量的确定性函数；这允许通过梯度下降对非随机项进行优化。具体而言，对于从<span class="math inline">\(N(z; \boldsymbol{\mu}_{\phi}(x),\sigma_{\phi}^{2}(x)\boldsymbol{I})\)</span> 采样出的样本 <span class="math inline">\(z\)</span>，我们可以将其重写为：</p><p><span class="math display">\[\begin{align}    z = \boldsymbol{\mu}_{\phi}(x) + \sigma_{\phi}^{2}(x) \odot\epsilon,\quad \epsilon \sim N(\epsilon; \boldsymbol{0,I}) \tag{28}\end{align}\]</span></p><p>  其中 <span class="math inline">\(\boldsymbol{\mu}_{\phi}(x)\)</span> 与 $_{}^{2}(x)$ 均由神经网络模型计算，通以上处理，我们便可以对参数 <span class="math inline">\(\phi,\theta\)</span>进行求导，进而利用SGD等算法进行参数更新。<br>  对于解码器 <span class="math inline">\(p_{\theta}(x|z)\)</span>的分布，在VAE的原始论文给出了两种候选方案：伯努利分布或高斯分布。综上所述，VAE的训练过程可以用下图4表示：</p><center><img src="https://s2.loli.net/2024/05/20/j5x4IMcqKQFk6ul.png" width="80%" height="80%"><div data-align="center">Image4: VAE 训练示意图</div></center><h2 id="采样">采样</h2><p>  在完成对VAE的训练后，我们便可以使用VAE来进行采样，即生成新的样本<span class="math inline">\(x'\)</span>。首先重隐变量的先验分布<span class="math inline">\(N(z; \boldsymbol{0,I})\)</span>中采样出隐变量 <span class="math inline">\(z\)</span>，再利用解码器<span class="math inline">\(p_{\theta}(x|z)\)</span>将隐变量映射到数据空间得到新样本 <span class="math inline">\(x'\)</span>。</p><h2 id="分层-vae">分层 VAE</h2><p>  在对VAE的研究中，一些学者提出了其更具有拓展性的框架，即分层变分自变量(HierarchicalVariational Auto-encoders, HVAE)，将 VAE由单个隐变量拓展到多个隐变量的层次，下图5即为 HVAE 的概率图：</p><center><img src="https://s2.loli.net/2024/05/20/3AXuqS9jR1lNdPw.png" width="80%" height="80%"><div data-align="center">Image5: HVAE 概率图</div></center><p>  通过设置多个隐变量，模型可以逐步学习到更高层次、更抽象的特征。在具有<span class="math inline">\(T\)</span> 个隐变量的HVAE中，我们来考虑一种特殊的情况，即具有马尔可夫性质的 HVAE(MHVAE)，此时，某个隐变量的分布仅取决于与之相邻的隐变量。这样，我们可以对重写概率模型：</p><p><span class="math display">\[\begin{align}    &amp; p(x, z_{1:T}) =p(z_{T})p_{\theta}(x|z_1)\prod_{t=2}^{T}p_{\theta}(z_{t-1}|z_{t})  \tag{29}\\    &amp; q_{\phi}(z_{1:T}|x) = q_{\phi}(z_{1}|x)\prod_{t=2}^{T}q_{\phi}(z_{t}|z_{t-1}) \tag{30} \\\end{align}\]</span></p><p>  同时我们可以将 MHVAE 的 ELBO 写为：</p><p><span class="math display">\[\begin{align}    \log{p(x)} &amp;= \log{\int_{z_{1:T}} p(x, z_{1:T})} dz_{1:T} \notag\\    &amp;= \log{\int_{z_{1:T}} \frac{p(x,z_{1:T})q_{\phi}(z_{1:T}|x)}{q_{\phi}(z_{1:T}|x)}dz_{1:T}} \notag \\    &amp;= \log{\mathbb{E_{q_{\phi}(z_{1:T}|x)}} \left[ \frac{p(x,z_{1:T})}{q_{\phi}(z_{1:T}|x)} \right]} \notag \\    &amp;\ge \mathbb{E_{q_{\phi}(z_{1:T}|x)}} \left[ \log\frac{p(x,z_{1:T})}{q_{\phi}(z_{1:T}|x)} \right] \tag{31}\end{align}\]</span></p><p>  利用(29)式与(30)式，ELBO可以进一步写为：</p><p><span class="math display">\[\begin{align}    \mathbb{E_{q_{\phi}(z_{1:T}|x)}} \left[ \log\frac{p(x,z_{1:T})}{q_{\phi}(z_{1:T}|x)} \right] =\mathbb{E_{q_{\phi}(z_{1:T}|x)}} \left[\log\frac{p(z_{T})p_{\theta}(x|z_1)\prod_{t=2}^{T}p_{\theta}(z_{t-1}|z_{t})}{q_{\phi}(z_{1}|x)\prod_{t=2}^{T}q_{\phi}(z_{t}|z_{t-1})} \right] \tag{32}\end{align}\]</span></p><p>  想必读者们已经注意到，MHVAE的基本架构和 Diffusion Model已经非常相似，接下来我们将介绍 Diffusion Model 在 MHVAE上又做了哪些工作。</p><h2 id="references">References</h2><p><strong>[1] Paper: Luo C. Understanding diffusion models: A unifiedperspective[J]. arXiv preprint arXiv:2208.11970, 2022.</strong><br><strong>[2] Paper: Kingma, D. P. and Welling, M. (2014). Auto-encodingvariational bayes. In Proceedings of the International Conference onLearning Representations (ICLR).</strong><br><strong>[4] Blog: 苏剑林 变分自编码(1-3), 科学空间</strong></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>深度学习-4-生成模型2-高斯混合模型</title>
      <link href="/2024/05/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-4-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B2-%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B/"/>
      <url>/2024/05/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-4-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B2-%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="高斯混合模型gmm">高斯混合模型(GMM)</h1><p>  高斯混合模型(Gaussian Mixture Model,GMM)是一种常用的概率生成模型，<strong>其主要思想是通过多个高斯分布的加权组合来表示更为复杂的数据分布</strong>。早在19世纪末期，KariPearson便提出了用高斯分布的混合来处理生物统计方面的问题，之后随着数理统计学科的发展，高斯混合模型的理论得到了很大的完善，但如何高效地进行参数估计一直是其面临的主要问题。直到1977年，Dempster,Laird和Rubin提出了EM算法，用于在含有隐变量的概率模型中进行参数估计。EM算法非常适合于GMM参数的估计问题，EM算法的提出大大推动了GMM的发展。</p><h2 id="概率模型">概率模型</h2><p>  我们有一批训练数据：<span class="math inline">\(T_{train} = \{x_1,x_2,\dotsb,x_{N} \}\)</span>，我们希望去估计数据的分布 <span class="math inline">\(p(x)\)</span>，现在我们对数据的分布做如下假设：</p><ul><li><strong>混合高斯:</strong> 数据分布 <span class="math inline">\(p(x)\)</span>可以表示为多个高斯分布的混合(加权平均)。</li><li><strong>隐变量:</strong> 观测数据 <span class="math inline">\(x\)</span> 的生成受隐变量 <span class="math inline">\(z\)</span> 的影响，隐变量 <span class="math inline">\(z\)</span>决定了多个高斯分布是如何进行混合的(加权系数)。</li></ul><center><img src="https://s2.loli.net/2024/05/19/sdJbKu5qxm8YvBz.jpg" width="80%" height="80%"><div data-align="center">Image1: 混合高斯假设</div></center><p>  图1展示了混合高斯模型的几何理解，黑色的点表示训练数据，左图是一维分布的情况，可以发现训练数据存在两个集中区域，我们便用两个高斯分布(绿色曲线)去表示这个两个区域的数据分布，而真实的数据分布(橙色曲线)可以用这两个高斯分布的加权平均表示。右图是二维分布的情况。<br>  我们假设训练数据 <span class="math inline">\(T_{train}\)</span> 的分布<span class="math inline">\(p(x)\)</span> 是由 <span class="math inline">\(M\)</span> 个高斯分布混合而成，则对于隐变量 <span class="math inline">\(z\)</span>，我们假设其先验分布为一个 <span class="math inline">\(M\)</span> 维的离散分布(2)：</p><p><span class="math display">\[\begin{equation}    p(z = c_{k}) = p_{k}, \quad k = 1,\dotsb, M\end{equation}\]</span></p><p>  给定隐变量 <span class="math inline">\(z\)</span>, 观测变量 <span class="math inline">\(x\)</span> 的条件分布为一个高斯分布(3)：</p><p><span class="math display">\[\begin{equation}    p(x | z) = N(x; \mu_{z}, \Sigma_{z})\end{equation}\]</span></p><p>  通过(2)式与(3)式，训练数据 <span class="math inline">\(T_{train}\)</span> 的分布 <span class="math inline">\(p(x)\)</span>可以表示为多个高斯分布的加权平均(4)：</p><p><span class="math display">\[\begin{align}p(x) &amp;= \int_{z} p(x, z)dz = \sum_{z} p(x, z) \tag{4}\\&amp;=\sum_{k=1}^{M} p(x, z = c_{k}) = \sum_{k=1}^{M} p(z = c_{k})p(x |z = c_{k}) \notag\\&amp;=\sum_{k=1}^{M} p_{k} N(x; \mu_{k}, \Sigma_{k}) \notag\\\end{align}\]</span></p><p>  其中，隐变量的分布的概率值 <span class="math inline">\(p_{k}\)</span>即为高斯分布的加权系数。高斯混合模型的概率图可以表示为图2：</p><center><img src="https://s2.loli.net/2024/05/20/rBiV9cLqkzF1DOs.png" width="60%" height="60%"><div data-align="center">Image2: GMM 概率图</div></center><h2 id="参数估计">参数估计</h2><p>  GMM的参数估计使用了EM算法，EM算法的迭代公式如下(5):</p><p><span class="math display">\[\begin{align}    \theta^{(t+1)} = \argmax_{\theta}{E_{p(z|x, \theta^{(t)})} \left[\log{p(x, z|\theta)} \right]} \tag{5}\end{align}\]</span></p><p>  在GMM中，待估参数 <span class="math inline">\(\theta\)</span>可以为：</p><p><span class="math display">\[\theta = \{p_1,p_2,\dotsb,p_{M};(\mu_1,\Sigma_1),\dotsb,(\mu_{M},\Sigma_{M})\}\]</span></p><p>  接下来我们使用EM算法来求解GMM的参数。<br>  <strong>E-step</strong></p><p><span class="math display">\[\begin{align}    Q(\theta, \theta^{(t)}) &amp;= E_{p(z|x, \theta^{(t)})} \left[\log{p(x, z|\theta)} \right] \tag{6} \\    &amp; = \int_{z} p(z|x, \theta^{(t)})\log{p(x,z|\theta)}dz  \notag\\    &amp;= \sum_{z} \left[ \prod_{i=1}^{N}p(z_{i} | x_{i}, \theta^{(t)})\log{\left( \prod_{i=1}^{N}p(x_i, z_i | \theta) \right)} \right]  \notag\\    &amp;= \sum_{z} \left[\sum_{i=1}^{N}\log{p(x_i,z_i|\theta)}\prod_{i=1}^{N}p(z_{i} | x_{i},\theta^{(t)}) \right]  \tag{7} \\\end{align}\]</span></p><p>  这里，我们需要对(7)式做一些简化，我们来看第1项如何处理：</p><p><span class="math display">\[\begin{align}    &amp; \quad \sum_{z} \left[ \log{p(x_1, z_1 | \theta)}\prod_{i=1}^{N}p(z_i | x_{i}, \theta^{(t)}) \right]  \notag \\    &amp;= \sum_{z_1} \left[ \log{p(x_1, z_1 | \theta)}p(z_{1} | x_1,\theta^{(t)} ) \right] \sum_{z_2, \dotsb, z_{N}} \left[\prod_{i=2}^{N}p(z_i | x_{i}, \theta^{(t)}) \right ] \notag \\    &amp;= \sum_{z_1} \left[ \log{p(x_1, z_1 | \theta)}p(z_{1} | x_1,\theta^{(t)} ) \right] \tag{8} \\\end{align}\]</span></p><p>  将(7)式中的每一项用(8)式的形式进行替代，可以得到 <span class="math inline">\(Q(\theta, \theta^{(t)})\)</span>的化简形式(9)：</p><p><span class="math display">\[\begin{align}    Q(\theta, \theta^{(t)}) = \sum_{i=1}^{N}\sum_{z_i} p(z_i | x_i,\theta^{(t)}) \log{p(x_i, z_i | \theta)} \tag{9}\end{align}\]</span></p><p>  由前文(2)式，(3)式可知：</p><p><span class="math display">\[\begin{align}    p(x_i, z_i |\theta) &amp;= p(z_i | \theta)p(x_i | z_i, \theta)\notag \\    &amp;= \boldsymbol{p}_{z} N(x_i; \mu_{z}, \Sigma_{z}) \tag{10} \\\end{align}\]</span></p><p>  将(10)式带入(9)：</p><p><span class="math display">\[\begin{align}    Q(\theta, \theta^{(t)}) &amp;= \sum_{i=1}^{N}\sum_{z_i} p(z_i | x_i,\theta^{(t)}) \log{\boldsymbol{p}_{z} N(x_i; \mu_{z}, \Sigma_{z})}\notag \\    &amp;= \sum_{k=1}^{M}\sum_{i=1}^{N} \left[ \log{p_{k} + \log{N(x_i;\mu_{k}, \Sigma_{k})}} \right] p(z_i = c_k | x_i,\theta^{(t)})  \tag{11} \\\end{align}\]</span></p><p>  在完成了期望的化简后，接下来对期望进行最大化以求解参数。</p><p><strong>M-step</strong><br>  首先对 隐变量 <span class="math inline">\(z\)</span> 的分布的未知参数<span class="math inline">\(p_k\)</span> 进行求解：</p><p><span class="math display">\[\begin{align}    p_{k} =&amp; \argmax_{p_{k}} \sum_{k=1}^{M}\sum_{i=1}^{N}\log{p_{k}} \cdot p(z_i = c_k | x_i, \theta^{(t)}) \tag{12} \\    &amp; s.t. \sum_{k=1}^{M}p_{k} = 1 \notag\end{align}\]</span></p><p>  优化问题(12)的拉格朗日函数：</p><p><span class="math display">\[\begin{align}    \mathcal{L}(\boldsymbol{p}, \lambda) = \sum_{k=1}^{M}\sum_{i=1}^{N}\log{p_{k}} \cdot p(z_i = c_k | x_i, \theta^{(t)}) +\lambda(\sum_{k=1}^{M}p_{k} - 1) \tag{13}\end{align}\]</span></p><p><span class="math display">\[\begin{align}    \frac{\partial{\mathcal{L}(\boldsymbol{p},\lambda)}}{\partial{p_{k}}} = \sum_{i=1}^{N} \frac{p(z_i = c_k | x_i,\theta^{(t)})}{p_{k}} + \lambda := 0 \tag{14}\end{align}\]</span></p><p><span class="math display">\[\begin{split}    &amp; \Rightarrow \sum_{i=1}^{N} p(z_i = c_k | x_i, \theta^{(t)}) +\lambda p_{k} = 0 \\    &amp; \Rightarrow \sum_{i=1}^{N}\sum_{k=1}^{M} p(z_i = c_k | x_i,\theta^{(t)}) + \lambda\sum_{k=1}^{M}p_{k} =0 \\    &amp; \Rightarrow N+\lambda=0,\quad \lambda= -N\end{split}\]</span></p><p>  将 <span class="math inline">\(\lambda = -N\)</span>带入(14)式可以求得 <span class="math inline">\(p_{k}\)</span>的MLE，即更新后的值为：</p><p><span class="math display">\[\begin{align}    p_{k}^{(t+1)} = \frac{1}{N}\sum_{i=1}^{N}p(z_i = c_k | x_i,\theta^{(t)})  \tag{15}\end{align}\]</span></p><p>  由(4)式与(10)式可得：</p><p><span class="math display">\[\begin{align}    p(z_i = c_{k} | x_i, \theta^{(t)}) &amp;= \frac{p(x_i, z_i=c_{k} |\theta^{(t)})}{p(x_i | \theta^{(t)})}  \notag \\    &amp;= \frac{p_{k}^{(t)}N(x_i;\mu_{k}^{(t)},\Sigma_{k}^{(t)})}{\sum_{k=1}^{M}p_{k}^{(t)}N(x_i;\mu_{k}^{(t)},\Sigma_{k}^{(t)})}  \tag{16}\end{align}\]</span></p><p>  将(16)式带入(15)式得：</p><p><span class="math display">\[p_{k}^{(t+1)} =\frac{1}{N}\sum_{i=1}^{N} \frac{p_{k}^{(t)}N(x_i;\mu_{k}^{(t)},\Sigma_{k}^{(t)})}{\sum_{k=1}^{M}p_{k}^{(t)}N(x_i;\mu_{k}^{(t)},\Sigma_{k}^{(t)})},\quad \boldsymbol{p}^{(t+1)} =\begin{bmatrix}    p_{1}^{(t+1)} \\    p_{2}^{(t+1)} \\    \vdots \\    p_{M}^{(t+1)} \\\end{bmatrix}\]</span></p><p>  同理，可以求得 <span class="math inline">\(\boldsymbol{\mu}^{(t+1)},\boldsymbol{\Sigma}^{(t+1)}\)</span>。</p><h2 id="采样">采样</h2><p>  在对GMM的参数进行估计后，我们便可以使用GMM来进行采样，即生成新的样本<span class="math inline">\(x'\)</span>。GMM的生成过程非常简单，首先根据隐变量<span class="math inline">\(z\)</span>的分布，确定进行采样的高斯分布，然后再从相应的高斯分布采样得到 <span class="math inline">\(x'\)</span>。</p><h2 id="gmm的总结与启示">GMM的总结与启示</h2><p>  从今天的角度来看，GMM是一个比较简单的生成模型，其生成的样本的效果也不尽人意，但我依然想首先介绍GMM是因为其对于生成模型的发展起到了不可忽视的启发作用，具体有如下几点：</p><ul><li><strong>概率分布的混合思想:</strong>GMM的基本思想是通过多个高斯分布的加权组合来表示复杂的数据分布。这一思想为现代生成模型提供了起始，即通过组合简单的概率分布来表示复杂的概率分布。<br></li><li><strong>参数估计:</strong>GMM通过EM算法迭代优化似然函数，这一思想也被现代生成模型所采纳。在VAE、DiffusionModel中，参数的求解仍然是使用SGD等迭代算法对似然函数的下界(ELBO)进行优化。<br></li><li><strong>隐变量假设:</strong>GMM的一个重要特点是引入隐变量来解释观测数据，这为后来的生成模型提供了一个重要框架，现代生成模型几乎都采用了隐变量假设。</li></ul><p>  当然，GMM本身也存在很多缺陷，主要有以下几个方面：</p><ul><li><strong>固定的分布假设:</strong> GMM假设隐变量 <span class="math inline">\(z\)</span>的分布是一个简单的离散分布，这种假设不适应复杂的真实数据。<br></li><li><strong>局部最优解:</strong>GMM的参数估计通常使用EM算法，该算法容易收敛到局部最优解，尤其是在数据分布复杂或高维的情况下。<br></li><li><strong>计算性能:</strong>GMM在处理高维数据时，计算成本和参数估计的复杂性显著增加，难以适应大规模数据集。</li></ul><p>  在之后，我们会介绍变分自编码模型(VAE)，其继承了GMM的主要思想，同时一定程度上改善了GMM的部分缺陷。</p><h2 id="references">References</h2><p><strong>[1] Book: 李航,统计学习方法(第2版)</strong><br><strong>[2] Video: bilibili,shuhuai008,高斯混合模型系列.</strong></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>深度学习-3.生成模型1-引言</title>
      <link href="/2024/05/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-3-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B1-%E5%BC%95%E8%A8%80/"/>
      <url>/2024/05/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-3-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B1-%E5%BC%95%E8%A8%80/</url>
      
        <content type="html"><![CDATA[<h1 id="生成模型">生成模型</h1><p>  在过去三年里，人工智能领域发生了翻天覆地的变化，革命性的产品层出不穷，语言生成模型ChatGPT、Llama、Claude;图片生成模型Midjourney、stable diffusion、DALL;视频生成模型Sora。这些人工智能产品带给了世界前所未有的变化，深深影响了人们的生活，英伟达CEO黄仁勋在2023年的GTC大会上直言：“现在是AI的iphone时刻”。在这一轮人工智能狂潮中，以Diffusion Model 为代表的生成模型扮演了至关重要的角色，stablediffusion、DALL、Sora等产品的底层模型便是 Diffusion Model，其对当今 AIGC的发展起到了不可忽视的推动作用。<br>  早在今年的2月份，OpenAI发布Sora的演示视频时，笔者便想创作博客来介绍其底层的Diffusion Model，但思来想去，还是认为自己的理解过于浅显，对于很多地方的motivation并没有比较清晰的认识，生成模型是笔者的研究方向之一，因此我并不想囫囵吞枣，浮于表面，遂埋头继续钻研。直到上个月，我阅读了Google Brain 的 Calvin Luo 创作的介绍 Diffusion Model原理的文章[1]，这篇文章从一个统一的视角介绍了VAE、DiffusionModel、Score-BasedModel，解答了笔者心中的很多疑问，并将我过去半年在生成模型上的认识很好地串联了起来。在完成了对于所学知识的整合后，笔者终于有信心来开始生成模型这个系列。</p><h2 id="数据分布假设">数据分布假设</h2><p>  在生成模型中，我们通常假设数据点 <span class="math inline">\(\boldsymbol{x}\)</span> 来自于一个真实的分布 <span class="math inline">\(p_{true}(x)\)</span>，生成模型的目的就是通过训练数据集<span class="math inline">\(T_{train}\)</span> 去学习 <span class="math inline">\(p_{true}(x)\)</span> 的近似分布 <span class="math inline">\(p_{g}(x)\)</span>，有了这个近似分布后，我们便可以利用这个分布进行采样，从而生成与真实数据点<span class="math inline">\(\boldsymbol{x}\)</span>高度相似的样本。例如，现在我们有一批<span class="math inline">\(256 \times 256\)</span>的猫的图片，我们想要生成一些类似的猫的图片。已有的猫的图片便是我们的训练数据集<span class="math inline">\(T_{cat}\)</span>，每一张图片便是一个数据点 <span class="math inline">\(\boldsymbol{x} \in \mathbb{R}^{256 \times 256\times 3}\)</span>。我们设 "猫的图片(<span class="math inline">\(256\times 256\)</span>)" 是一个统计学上的总体<span class="math inline">\(X_{cat}\)</span>，其分布为 <span class="math inline">\(p_{cat}(x)\)</span>，则总体中的每一个样本，即每一张<span class="math inline">\(256 \times 256\)</span> 的猫的图片便是服从<span class="math inline">\(p_{cat}(x)\)</span> 的一个高维随机向量 <span class="math inline">\(\boldsymbol{x} \in \mathbb{R}^{256 \times 256\times 3}\)</span>。此时，我们的训练数据集 <span class="math inline">\(T_{cat}\)</span> 便可看作是从总体 <span class="math inline">\(X_{cat}\)</span>中抽取的一个样本，其样本分布为<span class="math inline">\(p_{data}(x)\)</span>，由大数定律可知，在大样本条件下，样本分布会趋向于总体分布，故我们可以认为<span class="math inline">\(p_{data} \rightarrowp_{cat}\)</span>，因此如果我们能够学习到 <span class="math inline">\(p_{data}\)</span> 的一个近似分布 <span class="math inline">\(p_{g}\)</span>，则 <span class="math inline">\(p_{g}\)</span> 也是总体 <span class="math inline">\(p_{cat}\)</span> 的一个近似分布，我们便可以利用<span class="math inline">\(p_{g}\)</span>来进行采样，即生成相似的猫的图片。<br>  虽然我们希望能够直接学习数据的分布 <span class="math inline">\(p_{data}\)</span>，但这在实际中是很难实现的，主要有以下两点原因：</p><ul><li><strong>数据分布的复杂性:</strong>真实世界的数据通常具有复杂的高维分布，例如前文提到的猫的图片是便来自于<span class="math inline">\(256 \times 256 \times 3\)</span>维的分布，直接建模这种分布需要非常复杂的函数。<br></li><li><strong>计算复杂度:</strong>直接对高维数据进行概率密度估计需要大量的数据和计算资源，这往往是难以承受的。</li></ul><p>  因此，在实际中，我们通常会引入隐变量假设。<strong>隐变量假设认为存在一些无法直接观测的隐变量<span class="math inline">\(\boldsymbol{z}\)</span>，这些变量可以解释观测数据的统计特性和依赖关系。隐变量与观测变量之间的依赖关系通常通过特定的概率分布来描述，通过引入隐变量，可以将观测数据的复杂分布特征简化为更为简单的分布特征。</strong>在生成模型中，我们所假设的隐变量通常比观测数据的维度要低，通过对隐变量进行建模，我们实际上是进行了数据压缩，对原始的特征空间进行降维，提取出数据的潜在特征，这些特征往往具有较好的可解释性。例如，在VAE中，隐变量可以表示数据的某些关键特征，如图像的形状、颜色等。<br>  还是以猫的图片为例，在观测数据的分布 <span class="math inline">\(p_{data}\)</span>是一个高维分布，我们可以清晰地观察数据 <span class="math inline">\(\boldsymbol{x}\)</span>(猫)的各种细微特征，例如毛发、瞳孔的颜色、牙齿的形状等。而观测数据 <span class="math inline">\(\boldsymbol{x}\)</span> 背后的隐变量 <span class="math inline">\(\boldsymbol{z}\)</span>是对原始数据特征的一种压缩，它的维度比原始数据更低，如果同样将 <span class="math inline">\(\boldsymbol{z}\)</span>看作图片，那么这在这张图片上可能会有猫的大致轮廓、眼睛、耳朵的轮廓、模糊的毛发颜色等，虽然这张图片的细节特征并没有<span class="math inline">\(\boldsymbol{x}\)</span>丰富，但我们仍然可以看出这是一张猫的图片。这张图片 <span class="math inline">\(\boldsymbol{z}\)</span> 便是对观测数据 <span class="math inline">\(\boldsymbol{x}\)</span>的分布特征一种抽象与压缩，它从现实世界中千奇百怪的猫的图片中抽象出猫的一般特征，如果我们的生成模型能够学习到猫的这种一般特征，便能从这种一般特征的基础上去生成各种猫的图片。</p><h2 id="两种学习方式">两种学习方式</h2><p>  在生成模型，对于一些未知的分布，我们通常会采用参数化的概率模型或者神经网络模型是逼近，而对于参数的学习，目前两大主流的方法为：<strong>最大似然法</strong>以及<strong>对抗训练</strong>。最大似然法是统计推断中最经典的参数估计方法，其通过最大化数据分布的似然函数来确定分布中的未知参数，即：</p><p><span class="math display">\[\begin{equation}    \hat{\theta} = \argmax_{\theta} \log{p_{\theta}(x)}\end{equation}\]</span></p><p>  现如今很多生成模型的学习策略便采用了最大似然法，例如高斯混合模型(GMM)、变分自编码(VAE)、扩散模型(DiffusionModel)。<br>   (1) 式的优化并不是一项简单的工作，在 GMM中，由于模型较为简单，我们可以采用 EM 算法进行求解，而在 VAE、DiffusionModel 中，我们则是对似然函数 <span class="math inline">\(\log{p(x)}\)</span> 的证据下界(Evidence LowerBound) 进行优化。然而在 2014 年，lan.Goodfellow提出了一种完全不同的学习策略，可以绕过求解最大似然这个棘手的问题，这便是基于对抗思想的生成对抗网络(GAN)，对于GAN 的主要思想，可以阅读笔者之前的一篇博客文章<a href="https://sxusongjh.github.io/2023/11/06/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-2-Paper-%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9CGANs%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8D%81%E5%B9%B4%E9%97%B4%E6%9C%80%E9%85%B7%E7%9A%84idea/">《生成对抗网络GANs——深度学习二十年间最酷的idea!》</a>，在这篇博客中我介绍了GAN 的原始论文，感兴趣的读者可以自行阅读。<br>  由于本系列博客的初衷是介绍扩散模型，为了视角的统一，笔者会主要介绍几类基于最大似然的生成模型，包括GMM、VAE、DiffusionModel、Score-based Model。</p><h2 id="conference">Conference</h2><p><strong>[1] Paper: Luo C. Understanding diffusion models: A unifiedperspective[J]. arXiv preprint arXiv:2208.11970, 2022.</strong></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Optimal Transport-6.Sinkhorn Algorithm</title>
      <link href="/2024/05/06/Optimal%20Transport-6.Sinkhorn%20Algorithm/"/>
      <url>/2024/05/06/Optimal%20Transport-6.Sinkhorn%20Algorithm/</url>
      
        <content type="html"><![CDATA[<h1 id="sinkhorn-algorithm">Sinkhorn Algorithm</h1><p>  在上一节中我们介绍了带熵正则项的最优传输问题，通过在原始的最优传输问题中引入熵正则项，可以原问题转化为严格的凸问题，从而可以使用更高效的算法来更快地求解最优传输问题，计算效率的提高在最优传输的应用领域至关重要。本节我们将会介绍求解带熵正则项的最优传输问题的经典算法——<strong>Sinkhorn算法</strong>。我们将会从算法推导、收敛性证明、代码实现几个方面来讨论。<br>  Sinkhorn算法是一种用于解决凸优化问题的迭代算法，主要用于解决带有非负矩阵约束的最优传输问题。其主要思想是通过迭代地调整两个非负矩阵的行和列，使它们近似满足指定的边际约束条件。<br>  Sinkhorn算法最早由RichardSinkhorn在1964年提出，用于解决数学物理中的问题。后来，该算法被应用于最优传输问题，并在机器学习、图像处理等领域得到了广泛应用。随着计算机算力的提高和研究的深入，Sinkhorn算法及其变体在处理大规模数据和优化问题上发挥了重要作用。</p><h2 id="algorithm-iteration">Algorithm iteration</h2><p>  首先，我们来回顾一下上一节熵正则的内容。在引入熵正则项后，原始的最优传输问题变成了如下(1)形式：</p><p><span class="math display">\[\begin{equation}    L_{\boldsymbol{C}}^{\varepsilon}(\boldsymbol{a},\boldsymbol{b}) :=\min_{\boldsymbol{P} \in \boldsymbol{U}(\boldsymbol{a},\boldsymbol{b})}\left&lt; \boldsymbol{P},\boldsymbol{C} \right&gt; - \varepsilonH(\boldsymbol{P})\end{equation}\]</span></p><p><span class="math display">\[U(\boldsymbol{\alpha, \beta}):=\{\boldsymbol{P}: \boldsymbol{P1_{m}}=\boldsymbol{a},\boldsymbol{P^{T}1_{n}}=\boldsymbol{b} \}, \quadH(\boldsymbol{P})=-\sum_{i,j}p_{ij}\log{p_{ij}}\]</span></p><p>  为了求导的便利性，我们将熵的表达式做一些小的修改：</p><p><span class="math display">\[H(\boldsymbol{P})=-\sum_{i,j}p_{ij}\log{p_{ij}}+1=-\sum_{i,j}p_{ij}(\log{p_{ij}}-1)\]</span></p><p>  由于只是在原始的优化问题的目标函数上加了一个常数，故此修改并不会对问题(1)的最优解。为了推导Sinkhorn算法的迭代公式，我们首先来写出优化问题(1)的拉格朗日函数：</p><p><span class="math display">\[\mathcal{L}(\boldsymbol{P,f,g})=\left&lt;\boldsymbol{P,C} \right&gt;-\varepsilon H(\boldsymbol{P})-\left&lt;\boldsymbol{f,P1_{m}-a} \right&gt;-\left&lt; \boldsymbol{g,P^{T}1_{n}-b}\right&gt;\]</span></p><p>  由费马定理可知，对于优化问题(1)的最优解，有下式成立：</p><p><span class="math display">\[\frac{\partial\mathcal{L}(\boldsymbol{P,f,g})}{\partialp_{ij}}=c_{ij}+\varepsilon\log{p_{ij}}-f_{i}-g_{j}=0\]</span></p><p><span class="math display">\[\Rightarrowp_{ij}=e^{\frac{f_i+g_j-c_{ij}}{\varepsilon}}=e^{\frac{f_i}{\varepsilon}}e^{\frac{-c_{ij}}{\varepsilon}}e^{\frac{g_j}{\varepsilon}}\]</span></p><p>  由以上的结果可知，优化问题(1)的最优解的元素可以分解成三部分之积，进一步，我们可以将以上结果写成矩阵形式，设：</p><p><span class="math display">\[\boldsymbol{u}=\begin{bmatrix}    e^{\frac{f_1}{\varepsilon}} \\    e^{\frac{f_2}{\varepsilon}} \\    \vdots \\    e^{\frac{f_n}{\varepsilon}} \\\end{bmatrix},\quad \boldsymbol{v}=\begin{bmatrix}    e^{\frac{g_1}{\varepsilon}} \\    e^{\frac{g_2}{\varepsilon}} \\    \vdots \\    e^{\frac{g_m}{\varepsilon}} \\\end{bmatrix},\quad \boldsymbol{K}=\begin{bmatrix}    e^{\frac{-c_{11}}{\varepsilon}} &amp;e^{\frac{-c_{12}}{\varepsilon}} &amp; \dotsb &amp;e^{\frac{-c_{1m}}{\varepsilon}} \\    e^{\frac{-c_{21}}{\varepsilon}} &amp;e^{\frac{-c_{22}}{\varepsilon}} &amp; \dotsb &amp;e^{\frac{-c_{2m}}{\varepsilon}} \\      \vdots &amp; \vdots &amp;  &amp; \vdots \\    e^{\frac{-c_{n1}}{\varepsilon}} &amp;e^{\frac{-c_{n2}}{\varepsilon}} &amp; \dotsb &amp;e^{\frac{-c_{nm}}{\varepsilon}} \\\end{bmatrix}\]</span></p><p>  则优化问题(1)的最优解可以写成：</p><p><span class="math display">\[\boldsymbol{P}=diag(\boldsymbol{u})\boldsymbol{K}diag(\boldsymbol{v})\]</span></p><p>  考虑边际约束条件：</p><p><span class="math display">\[\left \{\begin{array}{l}\boldsymbol{P1_{m}}=diag(\boldsymbol{u})\boldsymbol{K}diag(\boldsymbol{v})\boldsymbol{1_{m}}=\boldsymbol{a}\\\boldsymbol{P^{T}1_{n}}=  diag(\boldsymbol{v})\boldsymbol{K^{T}}diag(\boldsymbol{u})\boldsymbol{1_{n}}=\boldsymbol{b}\end{array} \right.\]</span></p><p><span class="math display">\[\becausediag(\boldsymbol{u})\boldsymbol{1_{n}}=\boldsymbol{u}, \quaddiag(\boldsymbol{v})\boldsymbol{1_{m}}=\boldsymbol{v}\]</span></p><p><span class="math display">\[\Rightarrow \left \{\begin{array}{l}\boldsymbol{P1_{m}}=diag(\boldsymbol{u})(\boldsymbol{K}\boldsymbol{v})=\boldsymbol{u} \odot(\boldsymbol{Kv})=\boldsymbol{a} \\\boldsymbol{P^{T}1_{n}} =diag(\boldsymbol{v})(\boldsymbol{K^{T}}\boldsymbol{u})=\boldsymbol{v}\odot (\boldsymbol{K^{T}u})=\boldsymbol{b} \\\end{array} \right.\]</span></p><p>  在实际求解优化问题(1)时，由于成本矩阵 <span class="math inline">\(\boldsymbol{C}\)</span> 与惩罚系数 <span class="math inline">\(\varepsilon\)</span> 是已知的，故矩阵 <span class="math inline">\(\boldsymbol{K}\)</span>是已知的，因此我们需要求解向量 <span class="math inline">\((\boldsymbol{u,v})\)</span>使得其满足边际条件，则问题(1)的最优解即为：<span class="math inline">\(\boldsymbol{P}=diag(\boldsymbol{u})\boldsymbol{K}diag(\boldsymbol{v})\)</span>.该问题在数值分析领域被称为矩阵缩放问题，一个直观的解决方法是通过迭代求解。<strong>首先固定住<span class="math inline">\(\boldsymbol{v}\)</span>，更新<span class="math inline">\(\boldsymbol{u}\)</span>，使其满足边界条件；然后固定更新后的<span class="math inline">\(\boldsymbol{u}\)</span>，更新<span class="math inline">\(\boldsymbol{v}\)</span>，使其同样满足边界条件。</strong>这便是Sinkhorn算法的基本思想，写成迭代公式如下：</p><p><span class="math display">\[\Rightarrow \left \{\begin{array}{l}\boldsymbol{u}^{(l+1)} =\frac{\boldsymbol{a}}{\boldsymbol{K}\boldsymbol{v}^{(l)}} \\\\\boldsymbol{v}^{(l+1)} =\frac{\boldsymbol{b}}{\boldsymbol{K}\boldsymbol{u}^{(l+1)}} \\\end{array} \right. \quad\boldsymbol{v}^{(0)}=\boldsymbol{1_{m}}\]</span></p><p>  其中，除法运算表示向量之间各个元素对应相除。<span class="math inline">\(\boldsymbol{v}^{(0)}\)</span>可以初始化为任意的正值向量。通过不断的迭代，我们可以得到近似解<span class="math inline">\((\boldsymbol{u^{*},v^{*}})\)</span>，此时，最优运输矩阵及SinkhornDistance则为：</p><p><span class="math display">\[\boldsymbol{P^{*}} =diag(\boldsymbol{u^{*}})\boldsymbol{K}diag(\boldsymbol{v^{*}}), \quadSinkhorn \ Distance = \left&lt; \boldsymbol{P^{*}, C}\right&gt;\]</span></p><p>  矩阵<span class="math inline">\(\boldsymbol{K}\)</span>被称为 Gibbsdistributions，实际上，对于优化问题(1)的最优解<span class="math inline">\(\boldsymbol{P^{*}}\)</span>，其实际上是矩阵<span class="math inline">\(\boldsymbol{K}\)</span>在可行解集<span class="math inline">\(U(\boldsymbol{a,b})\)</span>上的投影，即(2)：</p><p><span class="math display">\[\begin{equation}    \boldsymbol{P^{*}}=Proj_{U(\boldsymbol{a,b})}^{KL}(\boldsymbol{K}):= \argmin_{\boldsymbol{P \in U(\boldsymbol{a,b})}} KL(\boldsymbol{P |K})\end{equation}\]</span></p><p>  问题(2)被称为<strong>静态薛定谔问题</strong>。进一步地，Sinkhorn算法的迭代过程实际上也是一个迭代投影过程，设：</p><p><span class="math display">\[\mathcal{C}_{\boldsymbol{a}}^{1}:=\{\boldsymbol{P}: \boldsymbol{P1_{m}=a} \} \quad and \quad\mathcal{C}_{\boldsymbol{b}}^{2} := \{ \boldsymbol{P}:\boldsymbol{P^{T}1_{n}=b} \}\]</span></p><p>  则有 <span class="math inline">\(U(\boldsymbol{a,b})=\mathcal{C}_{\boldsymbol{a}}^{1}\cap\mathcal{C}_{\boldsymbol{b}}^{2}\)</span>，<strong>Bregman迭代投影</strong>的过程如下：</p><p><span class="math display">\[\boldsymbol{P}^{(l+1)} :=Proj_{\mathcal{C}_{\boldsymbol{a}}^{1}}^{KL}(\boldsymbol{P}^{(l)}) \quadand \quad \boldsymbol{P}^{(l+2)} :=Proj_{\mathcal{C}_{\boldsymbol{b}}^{2}}^{KL}(\boldsymbol{P}^{(l+1)})\]</span></p><p>  其与Sinkhorn迭代过程的对应关系为：</p><p><span class="math display">\[\boldsymbol{P}^{(2l)} :=diag(\boldsymbol{u}^{(l)})\boldsymbol{K}diag(\boldsymbol{v}^{(l)})\]</span></p><p><span class="math display">\[\boldsymbol{P}^{(2l+1)} :=diag(\boldsymbol{u}^{(l+1)})\boldsymbol{K}diag(\boldsymbol{v}^{(l)})\]</span></p><p><span class="math display">\[\boldsymbol{P}^{(2l+2)} :=diag(\boldsymbol{u}^{(l+1)})\boldsymbol{K}diag(\boldsymbol{v}^{(l+1)})\]</span></p><h2 id="algorithm-covergence">Algorithm Covergence</h2><h2 id="algorithm-implementation">Algorithm Implementation</h2><p>  接下来，我们来通过代码实现一下Sinkhorn算法，首先来给定原始分布、目标分布、成本矩阵以及惩罚系数：</p><p><span class="math display">\[\boldsymbol{a} = \begin{bmatrix}    0.2 \\    0.5 \\    0.3 \\\end{bmatrix}, \boldsymbol{b}=\begin{bmatrix}    0.3 \\    0.4 \\    0.3 \\\end{bmatrix}, \quad \boldsymbol{C}=\begin{bmatrix}    0.1 &amp; 0.2 &amp; 0.3 \\    0.4 &amp; 0.5 &amp; 0.6 \\    0.7 &amp; 0.8 &amp; 0.9 \\\end{bmatrix},\quad \varepsilon=0.01\]</span></p><p>  相应的Python代码为：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">a = np.array([<span class="hljs-number">0.2</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.3</span>])<br>b = np.array([<span class="hljs-number">0.3</span>, <span class="hljs-number">0.4</span>, <span class="hljs-number">0.3</span>])<br>C = np.array([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>],<br>              [<span class="hljs-number">0.4</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.6</span>],<br>              [<span class="hljs-number">0.7</span>, <span class="hljs-number">0.8</span>, <span class="hljs-number">0.9</span>]])<br>reg_param = <span class="hljs-number">0.01</span><br></code></pre></td></tr></tbody></table></figure><p>  按照前文推导出的Sinkhorn算法的迭代公式，我们可以写出其实现代码：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sinkhorn</span>(<span class="hljs-params">a, b, C, reg_param, max_iters=<span class="hljs-number">100</span></span>):<br><br>    n = <span class="hljs-built_in">len</span>(a)<br>    m = <span class="hljs-built_in">len</span>(b)<br>    u = np.ones(n)<br>    v = np.ones(m)<br>    K = np.exp(-C / reg_param)<br><br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_iters):<br>        u = a / (K.dot(v))<br>        v = b / (K.T.dot(u))<br><br>    transport_matrix = np.diag(u).dot(K).dot(np.diag(v))<br>    sinkhorn_distance = np.<span class="hljs-built_in">sum</span>(transport_matrix * C)<br><br>    <span class="hljs-keyword">return</span> transport_matrix, sinkhorn_distance<br><br>transport_matrix, sinkhorn_distance = sinkhorn(a, b, C, reg_param)<br>```  <br>&amp;emsp;&amp;emsp;我们同样可以使用Python中专门用于解决最优传输领域问题的库POT来进行求解，其代码如下：  <br><br>```python<br><span class="hljs-keyword">import</span> ot<br><br>transport_matrix = ot.sinkhorn(a, b, C, reg_param)<br>sinkhorn_distance = ot.sinkhorn2(a, b, C, reg_param)<br></code></pre></td></tr></tbody></table></figure><p>  这两种求解方法得到最优运输矩阵及Sinkhorn距离相同，均为：</p><p><span class="math display">\[\boldsymbol{P^{*}}=\begin{bmatrix}    0.06 &amp; 0.08 &amp; 0.06 \\    0.15 &amp; 0.2 &amp; 0.15 \\    0.09 &amp; 0.12 &amp; 0.09 \\\end{bmatrix}, \quad Sinkhorn \ Distance = 0.53\]</span></p><h2 id="reference">Reference</h2><ul><li><strong>[1] Book: Peyré G, Cuturi M. Computational optimaltransport[J]. Center for Research in Economics and Statistics WorkingPapers, 2017 (2017-86).</strong></li></ul>]]></content>
      
      
      <categories>
          
          <category> 最优传输理论 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Optimal Transport-5.Entropy Regularization</title>
      <link href="/2024/04/20/Optimal%20Transport-5.Entropy%20Regulazation/"/>
      <url>/2024/04/20/Optimal%20Transport-5.Entropy%20Regulazation/</url>
      
        <content type="html"><![CDATA[<h1 id="entropy-regularization">Entropy Regularization</h1><p>  在之前的文章中，我们从概率视角描述了最优传输问题。设 <span class="math inline">\(X,Y\)</span> 是服从分布 <span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta}\)</span>的两个随机变量，运输矩阵为 <span class="math inline">\(\boldsymbol{P}\)</span>，成本矩阵为 <span class="math inline">\(\boldsymbol{C}\)</span>，则分布 <span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta}\)</span>之间的最优传输问题可以被定义为(1)式：</p><p><span class="math display">\[\begin{equation}    L_{\boldsymbol{C}}(\boldsymbol{\alpha},\boldsymbol{\beta}) :=\min_{\boldsymbol{P} \in\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta})} \left&lt;\boldsymbol{P},\boldsymbol{C} \right&gt; = \min_{(X,Y)} \{\mathbb{E}_{(X,Y)}(c(X,Y)): X \sim \boldsymbol{\alpha}, Y \sim\boldsymbol{\beta} \}\end{equation}\]</span></p><p>  我们在之前讨论过，运输矩阵 <span class="math inline">\(\boldsymbol{P}\)</span> 可以看作随机变量 <span class="math inline">\(X,Y\)</span>的联合分布。从Monge的前向运输法到Kantorovich的局部前向运输，最优传输问题的解通常是满足边界条件下，"耦合度"最低的运输方式，即最优运输矩阵<span class="math inline">\(\boldsymbol{P}\)</span>是一个很稀疏的矩阵，其非零元素主要集中在对角线附近。如果我们用概率论与信息论的视角来分析，这意味原始最优传输问题的解是<strong>满足边际分布条件下，熵最小的联合分布。</strong><br>  熵正则的主要思想对联合分布<span class="math inline">\(\boldsymbol{P}\)</span>的信息熵进行惩罚，以期望求得的联合分布<span class="math inline">\(\boldsymbol{P^{*}}\)</span>的熵更大。熵正则的数学表达式如下(2)式：</p><p><span class="math display">\[\begin{equation}    L_{\boldsymbol{C}}^{\varepsilon}(\boldsymbol{\alpha},\boldsymbol{\beta}):= \min_{\boldsymbol{P} \in\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta})} \left&lt;\boldsymbol{P},\boldsymbol{C} \right&gt; - \varepsilon H(\boldsymbol{P})\end{equation}\]</span></p><p>  其中，常数 <span class="math inline">\(\varepsilon\)</span>为惩罚系数，用于控制对联合分布的熵的惩罚力度，<span class="math inline">\(\varepsilon\)</span> 越大，则我们解得的联合分布<span class="math inline">\(\boldsymbol{P^{*}}\)</span>的熵越大。除了这种形式，有一些文献也将熵正则的定义为如下形式(3)式：</p><p><span class="math display">\[\begin{equation}    L_{\boldsymbol{C}}^{\gamma}(\boldsymbol{\alpha},\boldsymbol{\beta}):= \min_{\boldsymbol{P} \in\boldsymbol{U_{\gamma}}(\boldsymbol{\alpha},\boldsymbol{\beta})}\left&lt; \boldsymbol{P},\boldsymbol{C} \right&gt;\end{equation}\]</span></p><p><span class="math display">\[\boldsymbol{U_{\gamma}}(\boldsymbol{\alpha},\boldsymbol{\beta})= \{ \boldsymbol{P} \in\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta}) |KL(\boldsymbol{P} || \boldsymbol{\alpha \beta^{T}}) \leq \gamma \} = \{\boldsymbol{P} \in\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta}) |h(\boldsymbol{P}) \ge h(\boldsymbol{\alpha}) + h(\boldsymbol{\beta}) -\gamma  \} \subset\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta})\]</span></p><p>  (2)和(3)两种熵正则的形式实际上是等价的，熵正则在最优传输理论中起着十分重要的作用，<strong>在本节我们将会讨论熵正则的作用；信息论以及几何视角下的熵正则内涵，以及证明其求解结果能够作为分布之间距离的度量。</strong></p><h2 id="why-entropy-regularzation">Why Entropy Regularzation ?</h2><p>  对于熵正则的作用，我主要会从两个方面来讨论，<strong>数值计算以及最大熵思想。</strong></p><h3 id="lightspeed-computation-of-optimal-transport">LightspeedComputation of Optimal Transport</h3><p>  在很多对于最优传输问题中熵正则项的作用，大部分文章都会提到引入正则项能够提高最优传输问题的计算效率，这当然是其最突出的优点。<br>  原始的最优传输问题是一个线性规划问题，对于线性规划问题，通常的求解方法为单纯形法以及内点法，这些算法的计算复杂度为<span class="math inline">\(O(d^{3}\log(d))\)</span>，<span class="math inline">\(d\)</span>为随机变量的维度。这些算法能够处理一些简单的最优传输问题，但随着维度的增加，巨大的计算成本使得最优传输很难应用到高维数据中。而在机器学习领域中，很多数据都是高维数据，例如长宽均为256的一张图片，其一个数据样本便是<span class="math inline">\(\mathbb{R}^{256 \times 256 \times3}\)</span>的空间中的一个向量。如果想在这种数据集上应用最优传输理论来做一些工作，例如图像生成、图像匹配、风格变化等，庞大的计算成本便是一个很难解决的问题。<br>  <strong>通过在原始的最优传输问题中引入一个熵正则项，可以使得该优化问题变成一个严格的凸问题。</strong>最优传输问题本身在没有正则化时可能不是严格凸的，特别是在离散设置中，可能存在多个最优解，这使得我们求解的计算成本大大增加，例如在单纯形算法中，如果我们需要找到所有的最优解，我们可能需要遍历多面体所有极点，这在高维问题中意味着庞大的计算开销，因为此时多面体可能存在很多极点。而严格的凸性意味着该优化问题只存在唯一的最优解，此时我们只要找到一个最优解，则这个最优解一定是唯一的，算法便可以终止了。对于严格的凸问题我们已经有很多高效的数值求解方法。<br>  <strong>引入熵正则项后的最优传输问题可以通过特定的算法，如Sinkhorn迭代算法来高效求解。</strong>这些算法通常基于矩阵缩放技术，即通过迭代调整矩阵的行和列以使得传输矩阵逼近目标边缘分布。这种方法简单、高效，并且易于实现。Franklin和Lorenz(1989) 以及 Knight (2008)的研究表明，Sinkhorn算法具有线性收敛性。这意味着每次迭代后，误差以固定比率减少，确保了算法在实际应用中的快速收敛。对于Sinkhorn算法，我们将在下一篇博客中详细讨论，这里不作过多的展开。<br>  <strong>Sinkhorn算法能够依靠矩阵乘法的运算来进行迭代，这使得我们能够使用GPU强大的并行计算能力来快速求解高维空间中的最优传输问题。</strong>近二十年来，计算机硬件厂商对GPU不断地进行升级迭代，使得电脑GPU的性能不断提升，而GPU的多核结构十分适合用于矩阵运算，计算性能的提示是得以往一些难以取得进展的领域得到了突破，深度学习便是一个很好的例子，而深度学习的兴起又反过来促使硬件厂商不断提升GPU性能，GPU的算力得到了高速发展。Sinkhorn算法可以使用矩阵乘法进行迭代运算，这使得最优传输问题的求解能够享受到GPU性能不断提升的红利，这大大促进了最优传输理论在机器学习领域的应用。</p><h3 id="maximum-entropy-principle">Maximum Entropy Principle</h3><p>  除开耳熟能详的计算优势外，熵正则所蕴含的最大熵原理的思想是另外一个我想重点讨论的点。熵这个概念最早可以追溯到19世纪R.J.E.Clausius对于热力学的研究，他引入熵这个概念来描述系统的无序程度或者信息缺失的状态。热力学第二定律指出，<strong>一个孤立的系统的总熵不会自发减少，即熵总是趋于增加的。</strong>最大熵思想最初由物理学家 J.W.Gibbs在《物理学原理》这本著作中提出。Gibbs讨论了如何从统计力学的角度来理解熵，以及如何通过概率分布来计算系统的熵，特别是在平衡状态下。Gibbs的工作强调了<strong>在已知部分信息的情况下，选择熵最大的分布可以带来最大的“不确定性”或“混乱度”，这种分布认为每一种可能状态出现的机会均等，直到有额外信息来提供更多的偏好。</strong><br>  我们用一个例子来说明一下系统的这种熵增现象。Albert Einstein在1905年发表了一篇划时代的论文《关于运动着的水分子所引起的、需用显微镜观察的悬浮粒子的运动》，在这篇论文中，他发展了数学模型来描述微观粒子在流体中的随机运动，这种运动后来被称为布朗运动，也称随机扩散过程。我们便用粒子的随机扩散过程体会熵增的过程。<br>  假设现在有一条“又长又细的水管”，我们可以将这个水管看作实数轴，我们在这个实数轴的原点<span class="math inline">\(O\)</span>处滴入一滴粒子数量为<span class="math inline">\(C\)</span>的墨水，设 <span class="math inline">\(\tau\)</span> 代表扩散过程的时间间隔，<span class="math inline">\(\rho(y)\)</span> 表示在时间<span class="math inline">\(\tau\)</span>内粒子移动距离为<span class="math inline">\(y\)</span>的概率分布，其是一个概率密度函数。<span class="math inline">\(f(x,t)\)</span> 表示<span class="math inline">\(t\)</span>时刻在实数轴<span class="math inline">\(x\)</span>处的粒子的数量，其也是一个概率密度函数，同时是关于时间<span class="math inline">\(t\)</span>的一个随机过程。<br>  假设扩散过程是对称的，即<span class="math inline">\(\rho(y)\)</span>和<span class="math inline">\(f(x,t)\)</span>均为关于原点<span class="math inline">\(O\)</span>的对称分布，此时有:</p><p><span class="math display">\[\rho(y) = \rho(-y) \Leftrightarrow\int_{-\infty}^{+\infty}y\rho(y)=0\]</span></p><p>  由假设我们可以得知这个系统的初始状态 <span class="math inline">\(f(0,0) =C\)</span>，即墨水集中在原点处，扩散过程还没有发生，<strong>此时我们可以将这种初始状态看作一个狄拉克<span class="math inline">\(\delta\)</span>分布，这种分布是熵最小的分布，其熵实际上为零，系统不具有随机性。</strong>我们要来推导<span class="math inline">\(f(x,t)\)</span>的表达式，由粒子质量守恒定律可以得到方程：</p><p><span class="math display">\[f(x,t+\tau) =\int_{-\infty}^{+\infty}f(x-y,t)\rho(y)dy\]</span></p><p>  另外，如果我们把<span class="math inline">\(f(x,t)\)</span>看作关于<span class="math inline">\(t\)</span>的一元函数，我们可以将<span class="math inline">\(f(x,t+\tau)\)</span>在<span class="math inline">\(t\)</span>处进行泰勒展开：</p><p><span class="math display">\[f(x,t+\tau) = f(x,t)+\frac{\partialf}{\partial t}\tau+O(\tau)\]</span></p><p>  同理，我们可以把<span class="math inline">\(f(x-y,t)\)</span>在<span class="math inline">\(x\)</span>处展开：</p><p><span class="math display">\[f(x-y,t)=f(x,t) - \frac{\partialf}{\partial x}y + \frac{1}{2} \frac{\partial^{2}f}{\partialx^{2}}y^{2}+O(y^{2})\]</span></p><p>  联立以上三个等式得到扩散过程的SDE：</p><p><span class="math display">\[f(x,t)+\frac{\partial f}{\partial t}\tau= \int_{-\infty}^{+\infty} \left[ f(x,t) - \frac{\partial f}{\partialx}y + \frac{1}{2} \frac{\partial^{2}f}{\partial x^{2}}y^{2} \right]\rho(y)dy\]</span></p><p>  通过化简，我们得到了最终的<strong>扩散方程(4):</strong></p><p><span class="math display">\[\begin{equation}    \frac{\partial f}{\partial t} = \frac{D}{2\tau}\frac{\partial^{2}f}{\partial x^{2}}\end{equation}\]</span></p><p>  求解这个偏微分方程，我们发现<strong>随机过程<span class="math inline">\(f(x,t)\)</span>是一个高斯过程(5):</strong></p><p><span class="math display">\[\begin{equation}    f(x,t) = \frac{1}{\sqrt{2\pi Ct}}\exp\left( -\frac{x^{2}}{2Ct}\right)\end{equation}\]</span></p><p>  在之前机器学习系列的《交叉熵与KL散度》这篇博客文章中，我们证明了在给定均值与方差的条件下，高斯分布具有最大的熵，具体来说，<strong>若有定义在整个实数轴上的随机变量<span class="math inline">\(X\)</span>，给定其均值为<span class="math inline">\(\mu\)</span>，方差为<span class="math inline">\(\sigma^{2}\)</span>，则当<span class="math inline">\(X \sim N(\mu,\sigma^{2})\)</span>时，随机变量<span class="math inline">\(X\)</span>的熵<span class="math inline">\(H(X)\)</span>最大，其熵为<span class="math inline">\(H(X)=\frac{1}{2}\log(2\pi e\sigma^{2})\)</span>。而粒子的扩散过程是一个高斯过程，其在每个时间点<span class="math inline">\(t\)</span>是都是一个高斯分布，具有最大的信息熵，同时随着时间<span class="math inline">\(t\)</span>的增大，高斯分布的方差逐渐增大，其熵会进一步增大，当时间<span class="math inline">\(t\)</span>趋于无穷时，高斯分布的熵也趋于无穷。整个粒子的扩散过程从开始熵为零的单点分布，到熵为无穷的高斯分布，系统的扩散总是趋向于熵最大的方式。</strong></p><center><img src="https://s2.loli.net/2024/04/22/k1cszEIoYygA9MU.jpg" width="60%" height="60%"><div data-align="center">Image1: 扩散过程</div></center><p><br></p><p>  举上面这个例子，我是想让读者对熵增的过程有一个直观的感受，最大熵原理便是基于熵增定律，选择熵最大的分布是一个理智的选择，因为这符合自然界的规律。言归正传，最大熵原理和最优传输的熵正则理论有什么联系了？实际上我们只要对(2)式做一个小的等价变形，就可以发现其中的联系了，(2)式实际上也等价于(6)：</p><p><span class="math display">\[\begin{equation}    L_{\boldsymbol{C}}^{\eta}(\boldsymbol{\alpha},\boldsymbol{\beta}) :=\max_{\boldsymbol{P} \in\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta})}  H(\boldsymbol{P})- \eta \left&lt; \boldsymbol{P},\boldsymbol{C} \right&gt;\end{equation}\]</span></p><p>  <strong>我们原先对于熵正则的理解是在考虑熵的约束的条件下使得运输成本最小化，(6)式告诉我们，从最大熵原理的角度看，熵正则也可以理解成在考虑运输成本的情况下，使得联合分布<span class="math inline">\(P(X,Y)\)</span>的熵最大。</strong><br>  实际上，早在1969年，Alan Wilson在将最优传输理论应用到交通运输领域时，便考虑到了最大熵原理。交通系统往往具有复杂性于动态性，其包含了多种变量的随机性。例如，交通流量会受到天气、时间、事故等因素的影响。最优运输问题的传统解决方案往往倾向于找出成本最低的几条路线，这在理论上是有效的，但这些路线可能无法准确地反映这些动态变化。Wilson在最优传输理论中引入熵正则项，这种正则化旨在使运输方案更加现实，通过平滑运输路线的分布，从而更好地模仿现实世界网络中观察到的实际交通流。</p><h2 id="from-the-perspective-of-information-theory-and-geometry">Fromthe Perspective of Information Theory and Geometry</h2><h3 id="entropic-constraints-on-joint-probabilities">EntropicConstraints on Joint Probabilities</h3><p>  在(1)式中，运输矩阵<span class="math inline">\(\boldsymbol{P}\)</span>的可行解集<span class="math inline">\(U(\boldsymbol{\alpha},\boldsymbol{\beta})\)</span>可以看作随机变量<span class="math inline">\(X,Y\)</span>的联合分布。对于联合分布<span class="math inline">\(P(X,Y)\)</span>，有：</p><p><span class="math display">\[\boldsymbol{\hat{P}} =\argmax_{\boldsymbol{P} \in U(\boldsymbol{\alpha,\beta})}H(\boldsymbol{P}) = \boldsymbol{\alpha \beta^{T}}\]</span></p><p>  即当<span class="math inline">\(P(X,Y)=\boldsymbol{\alpha\beta^{T}}\)</span>时，联合分布的熵最大。如果我们想要最优传输问题的解具有较大的熵，则我们可以将最优传输问题的可行解集约束到<span class="math inline">\(\boldsymbol{\hat{P}}\)</span>附近，这样可行使得求解出的联合分布便可以具有具有较大的信息熵。要实现这种约束，可以对解集<span class="math inline">\(U(\boldsymbol{\alpha,\beta})\)</span>中的<span class="math inline">\(\boldsymbol{P}\)</span>与<span class="math inline">\(\boldsymbol{\hat{P}}\)</span>的KL散度进行约束，这样可以使得解集中的<span class="math inline">\(\boldsymbol{P}\)</span>与<span class="math inline">\(\boldsymbol{\hat{P}}\)</span>的分布相似，即约束在<span class="math inline">\(\boldsymbol{\hat{P}}\)</span>附近，这样我们得到的新的可行解集为：</p><p><span class="math display">\[\boldsymbol{U_{\gamma}}(\boldsymbol{\alpha},\boldsymbol{\beta})= \{ \boldsymbol{P} \in\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta}) |KL(\boldsymbol{P} || \boldsymbol{\alpha \beta^{T}}) \leq \gamma\}\]</span></p><p>  其中，<span class="math inline">\(\gamma\)</span>是一个常数，用来控制约束程度。由信息论的知识我们可以有以下结论：</p><p><span class="math display">\[\begin{split}        &amp; \frac{1}{2} \left( H(\boldsymbol{\alpha}) +H(\boldsymbol{\beta}) \right) \leq H(\boldsymbol{P}) \leqH(\boldsymbol{\alpha}) + H(\boldsymbol{\beta}) = H(\boldsymbol{\alpha\beta^{T}}) \\          &amp; KL(\boldsymbol{P} || \boldsymbol{\alpha \beta^{T}}) =H(\boldsymbol{\alpha}) + H(\boldsymbol{\beta}) - H(\boldsymbol{P}) = I(X|| Y)\end{split}\]</span></p><p>  第一个式子确定了联合分布<span class="math inline">\(P(X,Y)\)</span>的上下界，第二个式子说明<span class="math inline">\(\boldsymbol{P}\)</span> 与 <span class="math inline">\(\boldsymbol{\alpha \beta^{T}}\)</span>的KL散度实际上是随机变量<span class="math inline">\(X,Y\)</span>的<strong>互信息量</strong>。互信息量是一种衡量两个随机变量之间相互依赖程度的度量。给定两个随机变量X和Y，它们的互信息量<span class="math inline">\(I(X \parallel Y)\)</span>可以用联合概率分布来计算：</p><p><span class="math display">\[I(X \parallel Y) =\sum_{x}\sum_{y}P(x,y)\log{\frac{P(x,y)}{P(x)P(y)}} = H(X)-H(X |Y)\]</span></p><p>  互信息量的含义为给定<span class="math inline">\(Y\)</span>后，随机变量<span class="math inline">\(X\)</span>所包含的信息的减少量。减少量越大，说明<span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span>的依赖程度越大。也可以理解为联合概率分布<span class="math inline">\(P(X,Y)\)</span>所提供的信息与独立分布<span class="math inline">\(P(X),P(Y)\)</span>所提供的信息之间的信息差，信息差越大，说明<span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span>的依赖关系越强。<br>  因此对<span class="math inline">\(\boldsymbol{P}\)</span>与<span class="math inline">\(\boldsymbol{\alpha \beta^{T}}\)</span>之间的KL散度的约束，也可以理解成对随机变量<span class="math inline">\(X,Y\)</span>之间的互信息量的约束，即依赖程度的约束。我们希望找到随机变量<span class="math inline">\(X,Y\)</span>之间依赖程度足够小的联合分布:</p><p><span class="math display">\[\boldsymbol{U_{\gamma}}(\boldsymbol{\alpha},\boldsymbol{\beta})= \{ \boldsymbol{P} \in\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta}) |KL(\boldsymbol{P} || \boldsymbol{\alpha \beta^{T}}) \leq \gamma \} = \{\boldsymbol{P} \in\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta}) |h(\boldsymbol{P}) \ge h(\boldsymbol{\alpha}) + h(\boldsymbol{\beta}) -\gamma  \} \subset\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta})\]</span></p><p>  此时引入熵正则的最优传输问题便转化为(3)式的形式，如果我们去写(3)式的拉格朗日函数，可以很容易将优化问题(3)转化为优化问题(2)，这里不再说明。<br>  我们来讨论一下极限情况，设<span class="math inline">\(\boldsymbol{P^{*}}\)</span>是原始最优传输问题(1)的最优解，<span class="math inline">\(\boldsymbol{P^{\gamma}}\)</span>是引入熵正则约束的最优传输问题(3)的最优解，很容易可以得到以下结论：</p><ul><li>当 <span class="math inline">\(\gamma \rightarrow \infty\)</span>时，<span class="math inline">\(\boldsymbol{U_{\gamma}}(\boldsymbol{\alpha},\boldsymbol{\beta})\rightarrow\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta})\)</span>，此时<span class="math inline">\(\boldsymbol{P^{\gamma}} \rightarrow\boldsymbol{P^{*}},L_{\boldsymbol{C}}^{\gamma}(\boldsymbol{\alpha},\boldsymbol{\beta})\rightarrowL_{\boldsymbol{C}}(\boldsymbol{\alpha},\boldsymbol{\beta}).\)</span><br></li><li>当 <span class="math inline">\(\gamma \rightarrow 0\)</span>时，<span class="math inline">\(KL(\boldsymbol{P} || \boldsymbol{\alpha\beta^{T}}) \rightarrow 0\)</span>，此时 <span class="math inline">\(\boldsymbol{P^{\gamma}} \rightarrow\boldsymbol{\alpha \beta^{T}},L_{\boldsymbol{C}}^{\gamma}(\boldsymbol{\alpha},\boldsymbol{\beta})\rightarrow \boldsymbol{\alpha^{T}C\beta}\)</span></li></ul><h3 id="geometric-intuition">Geometric Intuition</h3><p>  我们可以从几何的角度来理解熵正则，这里我使用 Marco Cuturi2013年发表在Nips上的论文《Sinkhorn Distances: Lightspeed Computation ofOptimal Transport》中的配图来说明：</p><center><img src="https://s2.loli.net/2024/04/22/fvY9PXokwb4ZHxR.png" width="60%" height="60%"><div data-align="center">Image2: 熵正则的几何示例</div></center><p><br></p><p>  论文中的部分符号与本博客不完全一致，读者需要自行对照理解，这里的论述以配图中的符号为标准。在之前关于最优传输的博客中，我们说明了原始的最优传输问题(1)实际上是一个线性规划问题，其解<span class="math inline">\(\boldsymbol{P}\)</span>被约束在一个<span class="math inline">\(\mathbb{R^{d \timesd}}\)</span>空间中的一个多面体<span class="math inline">\(U(\boldsymbol{r,c})\)</span>中，如图2中的红色多面体所示。在关于线性规划单纯形的讨论中，我们证明了线性规划问题的最优解一定在多面体的极点上，设原始最优传输问题(1)的最优解为<span class="math inline">\(\boldsymbol{P^{*}}\)</span>，即图2中多面体上绿色的极点。多面体的内部存在着某一点<span class="math inline">\(\boldsymbol{rc^{T}}\)</span>，其表示熵最大的联合分布<span class="math inline">\(\boldsymbol{P}\)</span>。在前文的讨论中，我们说明了引入熵正则项的最优传输问题(3)实际上是将可行解集从原来的多面体，约束到熵最大的解<span class="math inline">\(\boldsymbol{rc^{T}}\)</span>的附近，并将问题变成了一个严格的凸问题，这个新的严格凸的可行解集即为图中的蓝色区域。设问题(3)的最优解为<span class="math inline">\(\boldsymbol{P_{\alpha}}\)</span>，即图2中凸区域上的蓝色点。<span class="math inline">\(\boldsymbol{P_{\alpha}}\)</span>关于凸区域的切线是垂直于梯度方向<span class="math inline">\(\boldsymbol{M}\)</span>。当 <span class="math inline">\(\alpha \rightarrow 0\)</span> 时，<span class="math inline">\(\boldsymbol{P_{\alpha}} \rightarrow\boldsymbol{rc^{T}}\)</span>；当 <span class="math inline">\(\alpha\rightarrow \infty\)</span> 时，<span class="math inline">\(\boldsymbol{P_{\alpha}} \rightarrow\boldsymbol{P^{*}}\)</span>，因此随着 <span class="math inline">\(\alpha\)</span> 的增大，问题(3)的最优解<span class="math inline">\(\boldsymbol{P_{\alpha}}\)</span>会在多面体的内部形成一条路径，这条路径从起点<span class="math inline">\(\boldsymbol{rc^{T}}\)</span>逐渐逼近到<span class="math inline">\(\boldsymbol{P^{*}}\)</span>，如图2中红色的曲线所示。由于问题(3)实际上是与问题(2)等价的，给定一个<span class="math inline">\(\alpha\)</span>，就会有一个对应的<span class="math inline">\(\lambda\)</span>，当 <span class="math inline">\(\lambda \rightarrow 0\)</span> 时，<span class="math inline">\(\boldsymbol{P^{\lambda}} \rightarrow\boldsymbol{rc^{T}}\)</span>；当 <span class="math inline">\(\lambda\rightarrow \infty\)</span> 时，<span class="math inline">\(\boldsymbol{P^{\lambda}} \rightarrow\boldsymbol{P^{*}}\)</span>。因此这条路径也可看作是问题(2)的最优解<span class="math inline">\(\boldsymbol{P^{\lambda}}\)</span>形成的。图2中右端放大的图示的含义是，虽然理论上当<span class="math inline">\(\lambda \rightarrow \infty\)</span>时，<span class="math inline">\(\boldsymbol{P^{\lambda}} \rightarrow\boldsymbol{P^{*}}\)</span>，但计算机的数值精度是有限的，不可能真正现实趋于无穷大，因此在实际计算中，硬件所能表示的最大的<span class="math inline">\(\lambda_{max}\)</span>所对应的解<span class="math inline">\(\boldsymbol{P^{\lambda_{max}}}\)</span>只是<span class="math inline">\(\boldsymbol{P^{*}}\)</span>的一个近似值。</p><h2 id="sinkhorn-distance">Sinkhorn Distance</h2><p>  在引入熵正则项后，我们可以定义一个新的距离：</p><p><span class="math display">\[d_{\boldsymbol{C,\gamma}}(\boldsymbol{\alpha,\beta}):=  \min_{\boldsymbol{P} \in\boldsymbol{U_{\gamma}}(\boldsymbol{\alpha},\boldsymbol{\beta})}\left&lt; \boldsymbol{P},\boldsymbol{C} \right&gt;\]</span></p><p>若成本矩阵<span class="math inline">\(\boldsymbol{C}\)</span>为度量矩阵，即：</p><p><span class="math display">\[\boldsymbol{C} \in \{ \boldsymbol{C} \in\mathbb{R_{+}^{d \times d}}: \boldsymbol{C^{T}}=\boldsymbol{C};\foralli,j \leq d, c_{ij}=0 \Leftrightarrow i=j; \forall i,j,k \leq d, c_{ij}\leq c_{ik}+c_{kj} \}\]</span></p><p>则 <span class="math inline">\(d_{\boldsymbol{C,\gamma}}(\boldsymbol{\alpha,\beta})\)</span>被定义为分布 <span class="math inline">\(\boldsymbol{\alpha,\beta}\)</span>之间的<strong>Sinkhorn Distance</strong>。<br>  证明 Sinkhorn Distance 能够作为分布之间的距离类似于之前博客中Wasserstein Distance的证明过程，这里由于篇幅原因不再多加赘述。基于熵正则的最优传输问题所导出的Sinkhorn Distance 具有广泛的应用，以下是几个典型领域：</p><ul><li><strong>图像处理与计算机视觉</strong>：在图像生成、图像配准和图像分类等任务中，可以使用Sinkhorn距离来度量两个图像之间的相似性，尤其是在非刚性配准和变形问题中。<br></li><li><strong>自然语言处理</strong>：在文本生成、文本分类和词嵌入等任务中，Sinkhorn距离可以用于比较两个文本之间的相似性，帮助解决文本对齐和翻译等问题。<br></li><li><strong>机器学习与模式识别</strong>：在模式匹配、聚类和异常检测等任务中，Sinkhorn距离可以用作特征之间相似性的度量，从而提高模型的性能和鲁棒性。<br></li><li><strong>经济学与运筹学</strong>：在经济学中，Sinkhorn距离可以用于衡量两个市场之间的相似性，从而帮助分析市场结构和预测市场变化。在运筹学中，它可以用于解决运输和分配等问题。</li></ul><h2 id="references">References</h2><ul><li><strong>[1] Book: Peyré G, Cuturi M. Computational optimaltransport[J]. Center for Research in Economics and Statistics WorkingPapers, 2017 (2017-86).</strong><br></li><li><strong>[2] Paper: Cuturi M. Sinkhorn distances: Lightspeedcomputation of optimal transport[J]. Advances in neural informationprocessing systems, 2013, 26.</strong><br></li><li><strong>[3] Video: B站《随机过程》张灏UP-我在人间凑数的这几年.</strong></li></ul>]]></content>
      
      
      <categories>
          
          <category> 最优传输理论 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Optimal Transport-4.Simplex Method</title>
      <link href="/2024/04/08/Optimal%20Transport-4.Simplex%20Method/"/>
      <url>/2024/04/08/Optimal%20Transport-4.Simplex%20Method/</url>
      
        <content type="html"><![CDATA[<h1 id="simplex-method">Simplex Method</h1><p>  单纯形方法(Simplex Method)是解决线性规划问题的一种重要算法。它由George Dantzig在1947年发明。线性规划问题涉及到在一系列线性等式或不等式约束条件下，找到一个线性函数的最大值或最小值。<strong>单纯形方法的基本思想是将问题的解空间视为一个几何形状（通常是多维的），这个几何形状的顶点代表可能的解。算法从这个形状的一个顶点开始，沿着边缘移动到相邻的顶点，以此方式逐步改进解，直到找到最优解为止。每一步都保证目标函数的值不会变差。</strong><br>  单纯形方法在实际应用中非常广泛，包括运筹学、工程优化、金融和经济学等领域。尽管存在某些局限性，它仍然是解决线性规划问题的一个强大工具。</p><h2 id="linear-programming-form-of-ot-problem">Linear Programming Formof OT Problem</h2><p>  在之前的章节中，我们已经介绍了 Kantorovich OT Problem的经济含义与数学形式，我们也说明了其能够转化为线性规划的标准形式，这里我们再回顾一下之前的内容。<br><strong>数学形式</strong><br>  Kantorovich OT Problem 的数学形式如下(1)：</p><p><span class="math display">\[\begin{equation}    L_{\boldsymbol{C}}(\boldsymbol{a}, \boldsymbol{b}) \quad\overset{\text{def.}}{=} \quad \min_{\boldsymbol{P} \in\boldsymbol{U}(\boldsymbol{a},\boldsymbol{b})} \left&lt;\boldsymbol{C},\boldsymbol{P} \right&gt;_{F}=\sum_{i,j}p_{ij}c_{ij}\end{equation}\]</span></p><p>其中 <span class="math inline">\(\boldsymbol{a} \in\mathbb{R}^{n},\boldsymbol{b} \in \mathbb{R}^{m},\boldsymbol{P},\boldsymbol{C} \in \mathbb{R}^{n \timesm}_{+}\)</span>，约束条件 <span class="math inline">\(\boldsymbol{U}(\boldsymbol{a},\boldsymbol{b})\)</span>为：</p><p><span class="math display">\[\boldsymbol{U}(\boldsymbol{a},\boldsymbol{b}) =\{ \boldsymbol{P} \in \mathbb{R}^{n \times m}_{+}: \boldsymbol{P}\mathbf{1}_{m}=\boldsymbol{a} \quad and \quad \boldsymbol{P^{T}}\mathbf{1}_{n}=\boldsymbol{b} \}\]</span></p><p><strong>经济含义</strong><br>  考虑一个实际问题，有 n 个生产木材的工厂，生产的木材需要供应给 m个城市，每个工厂每个月的产能是固定的，记为 <span class="math inline">\(\boldsymbol{a}, \boldsymbol{a} \in\mathbb{R}^{n}\)</span>；每个城市每个月木材的需求量也是固定的，记为<span class="math inline">\(\boldsymbol{b}, \boldsymbol{b} \in\mathbb{R}^{m}\)</span>。记 <span class="math inline">\(\boldsymbol{P} =[p_{ij}]_{n \times m} \in \mathbb{R}^{n \times m}_{+}\)</span>表示运输方案，其中 <span class="math inline">\(p_{ij}\)</span> 表示第 i个工厂运输到第 j 个城市的木材量。记 <span class="math inline">\(\boldsymbol{C} = [c_{ij}]_{n \times m} \in\mathbb{R}^{n \times m}_{+}\)</span> 表示运输成本，其中 <span class="math inline">\(c_{ij}\)</span>表示将单位木材从第i个工厂运输到第j个城市的成本。<br>  则 Kantorovich OT Problem实际上是在寻找满足供应与需求条件下，使得总成本最低的运输方案。</p><p><strong>线性规划形式</strong></p><p>  Kantorovich OT Problem实际上是一个线性规划问题，通过一定的变换，我们能够将原始的 KantorovichOT Problem 转化为线性规划形式。<br>  将矩阵 <span class="math inline">\(\boldsymbol{P},\boldsymbol{C}\)</span>写成列向量形式：</p><p><span class="math display">\[\boldsymbol{P}=\begin{bmatrix}    \boldsymbol{p_1}, \boldsymbol{p_2}, \cdots,\boldsymbol{p_m}\end{bmatrix},\quad \boldsymbol{C} = \begin{bmatrix}    \boldsymbol{c_1},\boldsymbol{c_2}, \cdots,\boldsymbol{c_m}\end{bmatrix}\]</span></p><p>  构造向量 <span class="math inline">\(\boldsymbol{p},\boldsymbol{c}\in \mathbb{R}^{nm}, \boldsymbol{d} \in \mathbb{R}^{n+m}\)</span>，矩阵<span class="math inline">\(A \in \mathbb{R}^{(n+m) \timesnm}\)</span>:</p><p><span class="math display">\[\boldsymbol{p} = \begin{bmatrix}    \boldsymbol{p_{1}^{T}}, \boldsymbol{p_{2}^{T}}, \cdots,\boldsymbol{p_{m}^{T}}\end{bmatrix}^{T}, \quad \boldsymbol{c} = \begin{bmatrix}    \boldsymbol{c_{1}^{T}}, \boldsymbol{c_{2}^{T}}, \cdots,\boldsymbol{c_{m}^{T}}\end{bmatrix}^{T}\]</span></p><p><span class="math display">\[\boldsymbol{A} = \begin{bmatrix}    \boldsymbol{1_{n}^{T}} \otimes \boldsymbol{I_{m}} \\    \boldsymbol{I_{n}} \otimes \boldsymbol{1_{m}^{T}} \\\end{bmatrix}, \quad \boldsymbol{d}=\begin{bmatrix}    \boldsymbol{a} \\    \boldsymbol{b} \\\end{bmatrix}\]</span></p><p>其中，符号"<span class="math inline">\(\otimes\)</span>"表示矩阵的kronecker's product。则(1)式可以被转化为：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{\boldsymbol{p} \in \mathbb{R^{nm}}} \quad&amp;\boldsymbol{c^{T}p} \\        s.t. \quad &amp; \boldsymbol{Ap} = \boldsymbol{d} \\        &amp; \boldsymbol{p} \ge \boldsymbol{0} \\    \end{split}\end{equation}\]</span></p><p>  优化问题(2)即为 Kantorovich OT Problem 的线性规划形式。</p><h2 id="theoretical-foundations-of-simplex-method">TheoreticalFoundations of Simplex Method</h2><p>  解决线性规划的单纯形算法的理论基础是<strong>凸多面体分解定理</strong>，在正式介绍单纯形算法之前，我们首先来介绍凸多面体分解定理。要想理解凸多面体分解定理，我们需要从凸多面体、极点、极方向等开始讲起。</p><h3 id="polyhedron">Polyhedron</h3><p><strong>凸包(Convex Hall)</strong><br>  凸包是点集中元素的凸组合所构成的集合。数学形似为：</p><p><span class="math display">\[C \subseteq R^{n},Conv C= \{\sum_{i=1}^{n}\theta_{i}x_{i} | \forall x_i \in C, \forall \theta_{i}\in [0,1], \sum_{i=1}^{n}\theta_i = 1, i = 1,...,n \}\]</span></p><p>  凸包是欧式空间中一组点集的最小凸集。</p><center><img src="https://s2.loli.net/2024/04/08/DfV5GLTQ8ncANRi.jpg" width="60%" height="60%"><div data-align="center">Image1: 凸包</div></center><p><br></p><p><strong>凸多面体(Convex Polyhedron)</strong><br>  凸多面体是有限个线性等式和不等式的解集。其数学形式为：</p><p><span class="math display">\[P = \{ x | a_{i}^{T}x \leq b_i,i=1,...,m; c_{j}^{T}x = d_{j}, j=1,...,p \}\]</span></p><p>  从凸多面体的数学形式上可以多面体是有限个半空间和超平面的交集，因此凸多面体也是凸集。</p><center><img src="https://s2.loli.net/2024/04/08/5yoEIzh3YAZRdX1.jpg" width="60%" height="60%"><div data-align="center">Image2: 凸多面体</div></center><p><br></p><p>  设线性规划问题(2)的可行解集为 <span class="math inline">\(S = \{\boldsymbol{x}| \boldsymbol{x} \in\mathbb{R}^{nm},\boldsymbol{Ax}=\boldsymbol{d},\boldsymbol{x} \ge 0\}\)</span>。由凸多面体的定义可知<strong>可行解集 <span class="math inline">\(S\)</span> 为凸多面体</strong>。</p><h2 id="properties-of-convex-polyhedron">Properties of ConvexPolyhedron</h2><p><strong>极点(Extreme Point)</strong><br>  对于凸集<span class="math inline">\(C\)</span>，若 <span class="math inline">\(x \in C\)</span> 不能表示成 <span class="math inline">\(C\)</span> 中另外两点的凸组合，则称 <span class="math inline">\(x\)</span> 为凸集 <span class="math inline">\(C\)</span> 的极点。极点的数学形式为：</p><p><span class="math display">\[Ep(C) = \{x | x \in C, \forall x_1,x_2\in C, \forall \theta \in [0,1], x \ne \theta x_1 + (1-\theta)x_2\}\]</span></p><p>  通过极点的定义可以得知：<strong>极点不能落在凸集中另外两点组成的线段上。</strong></p><p><strong>方向(Recession Direction)</strong><br>  对于凸集<span class="math inline">\(C\)</span>，若存在非零向量 <span class="math inline">\(d\)</span>，满足：对于任意 <span class="math inline">\(x \in C\)</span>，均有 <span class="math inline">\(x + \lambda d \in C, \forall \lambda &gt;0\)</span>，则称 <span class="math inline">\(d\)</span> 为凸集 <span class="math inline">\(C\)</span> 的一个方向。方向的数学形式为：</p><p><span class="math display">\[d: d \ne 0, \forall x \in C, \forall\lambda &gt;0, x + \lambda d \in C\]</span></p><p>  凸集<span class="math inline">\(C\)</span>中所有方向构成了一个方向锥：</p><p><span class="math display">\[RDcone(C) = \{d | d \ne 0, \forall x \inC, \forall \lambda &gt;0, x + \lambda d \in C \}\]</span></p><p><strong>极方向(Extreme Direction)</strong><br>  对于凸集<span class="math inline">\(C\)</span>，若方向 <span class="math inline">\(d\)</span>不能表示成另外两个方向的正线性组合，则称 <span class="math inline">\(d\)</span> 为凸集 <span class="math inline">\(C\)</span> 的极方向。极方向的数学形式为：</p><p><span class="math display">\[Ed(C) = \{ d | d \in RDcone(C),\foralld_1,d_2 \in RDcone(C), \forall \lambda_1,\lambda_2 &gt; 0,d \ne\lambda_1 d_1 + \lambda_2 d_2 \}\]</span></p><p>  <strong>凸集<span class="math inline">\(C\)</span>中任意一个方向<span class="math inline">\(d\)</span>可以表示成其极方向的线性组合。</strong></p><center><img src="https://s2.loli.net/2024/04/08/aTVmyZB4udYf7gA.jpg" width="60%" height="60%"><div data-align="center">Image3: 极点、方向、极方向</div></center><p><br></p><p><strong>极点与极方向的数学表示</strong><br>  设 <span class="math inline">\(\boldsymbol{x} \in \mathbb{R}^{n},\boldsymbol{A} \in \mathbb{R}^{m \times n}, \boldsymbol{b} \in\mathbb{R}^{m}, r(\boldsymbol{A})=m\)</span>，则有凸多面体：</p><p><span class="math display">\[S = \{\boldsymbol{x} |\boldsymbol{Ax}=\boldsymbol{b},\boldsymbol{x} \ge0 \}\]</span></p><p><strong>极点定理:</strong> <span class="math inline">\(\boldsymbol{x}\in S\)</span> 是凸多面体 <span class="math inline">\(S\)</span>的极点当且仅当存在 <span class="math inline">\(\boldsymbol{A}\)</span>的分块: <span class="math inline">\(\boldsymbol{A}=\begin{bmatrix}  \boldsymbol{B},\boldsymbol{N}\end{bmatrix}\)</span>，使得 <span class="math inline">\(\boldsymbol{x}\)</span> 可以表示为：</p><p><span class="math display">\[\boldsymbol{x} = \begin{bmatrix}    \boldsymbol{B^{-1}b} \\    \boldsymbol{0} \\\end{bmatrix}, \quad \boldsymbol{A}=\begin{bmatrix}    \boldsymbol{B},\boldsymbol{N}\end{bmatrix},\boldsymbol{B} \in \mathbb{R}^{m \times m}\]</span></p><p>其中，<span class="math inline">\(\boldsymbol{B}\)</span> 可逆，且<span class="math inline">\(\boldsymbol{B^{-1}b} \ge\boldsymbol{0}\)</span>。<br>  由极点定理可知，若 <span class="math inline">\(\boldsymbol{x}\)</span>为凸多面体 <span class="math inline">\(S\)</span> 的一个极点，则 <span class="math inline">\(\boldsymbol{x}\)</span> 正分量对应的矩阵 <span class="math inline">\(\boldsymbol{A}\)</span>的列必然线性无关。同时凸多面体的极点必然是有限个的。</p><p><strong>极方向定理:</strong> <span class="math inline">\(\boldsymbol{d} \in \mathbb{R}^{n}\)</span>是凸多面体 <span class="math inline">\(S\)</span> 的极方向当且仅当存在<span class="math inline">\(\boldsymbol{A}\)</span> 的分块: <span class="math inline">\(\boldsymbol{A}=\begin{bmatrix}  \boldsymbol{B},\boldsymbol{N}\end{bmatrix}\)</span>， 使得 <span class="math inline">\(\boldsymbol{d}\)</span> 可以表示为:</p><p><span class="math display">\[\boldsymbol{d} = t\begin{bmatrix}    -\boldsymbol{B^{-1}n_{j}} \\    \boldsymbol{e_{j}}\end{bmatrix},\quad \boldsymbol{A} = \begin{bmatrix}    \boldsymbol{B},\boldsymbol{N}\end{bmatrix},\boldsymbol{B} \in \mathbb{R}^{m \times m}\]</span></p><p>其中，<span class="math inline">\(\boldsymbol{B}\)</span> 可逆，<span class="math inline">\(t &gt; 0, \boldsymbol{B^{-1}n_{j}} \leq0\)</span>，<span class="math inline">\(\boldsymbol{n_{j}}\)</span>为矩阵 <span class="math inline">\(\boldsymbol{N}\)</span>的第j列，<span class="math inline">\(\boldsymbol{e_{j}} \in\mathbb{R}^{n-m}\)</span> 的第j个分量为1，其余为0.</p><h2 id="decomposition-of-convex-polyhedron">Decomposition of ConvexPolyhedron</h2><p>  设凸多面体 <span class="math inline">\(S\)</span> 的极点为 <span class="math inline">\(\boldsymbol{x_1,x_2,\dots,x_{k}}\)</span>，极方向为<span class="math inline">\(\boldsymbol{d_1,d_2,\dots,d_{I}}\)</span>，则<span class="math inline">\(\boldsymbol{x} \in S\)</span> 当且仅当 <span class="math inline">\(\boldsymbol{x}\)</span> 具有如下形式：</p><p><span class="math display">\[\boldsymbol{x} =\sum_{i=1}^{k}\lambda_{i}\boldsymbol{x_{i}} +\sum_{j=1}^{I}\mu_{j}\boldsymbol{d_{j}}\]</span></p><p>其中，<span class="math inline">\(\sum_{i=1}^{k}\lambda_{i}=1,\lambda_{i} \ge 0,i=1,\dots,k; \mu_{j} \ge 0, j = 1,\dots,I\)</span><br>  通过几何分析我们可以，该定理表示的含义为，<strong>以凸多面体的顶点构成的线段上的点为起点，以凸多面体的方向为方向所形成的所有射线的集合即为凸多么体中的点集。</strong></p><center><img src="https://s2.loli.net/2024/04/08/vYnHhyJeDd2PcTq.jpg" width="60%" height="60%"><div data-align="center">Image4: 多面体的分解</div></center><p><br></p><h2 id="simplex-method-of-linear-programming">Simplex Method of LinearProgramming</h2><p>  在介绍了单纯形方法的理论——凸多面体分解定理后，我们现在可以推导出单纯形算法，我们首先基于分解定理，对原始的线性规划问题做一些分析。</p><h3 id="analysis-for-lp-based-on-decomposition-theroem">Analysis for LPBased on Decomposition Theroem</h3><p>  线性规划问题的形式为：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{\boldsymbol{c} \in \mathbb{R^{n}}} \quad&amp;\boldsymbol{c^{T}x} \\        s.t. \quad &amp; \boldsymbol{Ax} = \boldsymbol{b} \\        &amp; \boldsymbol{x} \ge \boldsymbol{0} \\    \end{split}\end{equation}\]</span></p><p>其中，<span class="math inline">\(\boldsymbol{x} \in \mathbb{R}^{n},\boldsymbol{A} \in \mathbb{R}^{m \times n}, \boldsymbol{b} \in\mathbb{R}^{m}, r(\boldsymbol{A})=m\)</span>.<br>  定义可行解集 <span class="math inline">\(S = \{ \boldsymbol{x} |\boldsymbol{Ax}=\boldsymbol{b}\}\)</span>，则线性规划问题(3)也可以写成如下形式：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{\boldsymbol{c} \in \mathbb{R^{n}}} \quad&amp;\boldsymbol{c^{T}x} \\        s.t. \quad &amp; \boldsymbol{x} \in S \\    \end{split}\end{equation}\]</span></p><p>  由于可行解集 <span class="math inline">\(S\)</span>为一个凸多面体，设凸多面体 <span class="math inline">\(S\)</span>的极点为 <span class="math inline">\(\boldsymbol{x_1,x_2,\dots,x_{k}}\)</span>，极方向为<span class="math inline">\(\boldsymbol{d_1,d_2,\dots,d_{I}}\)</span>，则对<span class="math inline">\(\forall x \in S, \exists\lambda_{i},\mu_{j}\)</span>，使得:</p><p><span class="math display">\[\boldsymbol{x} =\sum_{i=1}^{k}\lambda_{i}\boldsymbol{x_{i}} +\sum_{j=1}^{I}\mu_{j}\boldsymbol{d_{j}}\]</span></p><p>其中，<span class="math inline">\(\sum_{i=1}^{k}\lambda_{i}=1,\lambda_{i} &gt; 0,i=1,\dots,k; \mu_{j} \ge 0, j = 1,\dots,I\)</span>.则优化问题(4)可以转化为：</p><p><span class="math display">\[\begin{split}    \min_{\boldsymbol{c} \in \mathbb{R^{n}}} \quad&amp;\boldsymbol{c^{T}} \left(\sum_{i=1}^{k}\lambda_{i}\boldsymbol{x_{i}} +\sum_{j=1}^{I}\mu_{j}\boldsymbol{d_{j}} \right) \\    s.t. \quad &amp; \sum_{i=1}^{k}\lambda_{i}=1,\lambda_{i} \ge 0 \\    \quad &amp; \mu_{j} \ge 0\end{split}\]</span></p><p>即：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{\boldsymbol{c} \in \mathbb{R^{n}}} \quad&amp;\sum_{i=1}^{k}\lambda_{i}\boldsymbol{c^{T}x_{i}} +\sum_{j=1}^{I}\mu_{j}\boldsymbol{c^{T}d_{j}} \\        s.t. \quad &amp; \sum_{i=1}^{k}\lambda_{i}=1,\lambda_{i} \ge 0\\        \quad &amp; \mu_{j} \ge 0    \end{split}\end{equation}\]</span></p><p>  对于优化问题(5)：</p><ul><li>若 <span class="math inline">\(\exists j, \boldsymbol{c^{T}d_{j}}&lt; 0\)</span>，令 <span class="math inline">\(\mu_{j} \rightarrow+\infty\)</span>，则该问题的最小值为负无穷，无意义。<br></li><li>若 <span class="math inline">\(\forall j, \boldsymbol{c^{T}d_{j}}\ge 0\)</span>，则有 <span class="math inline">\(\mu_{j} = 0, \forallj\)</span>，该问题存在最优解。</li></ul><p>  通过以上的分析，优化问题(5)可被化为：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{\boldsymbol{c} \in \mathbb{R^{n}}} \quad&amp;\sum_{i=1}^{k}\lambda_{i}\boldsymbol{c^{T}x_{i}} \\        s.t. \quad &amp; \sum_{i=1}^{k}\lambda_{i}=1,\lambda_{i} \ge 0\\    \end{split}\end{equation}\]</span></p><p>  对于优化问题(6)，我们实际上只需要找到 i 使得 <span class="math inline">\(\boldsymbol{c^{T}x_{i}}\)</span>最小，此时令对应的 <span class="math inline">\(\lambda_{i}=1\)</span>，目标函数即为最小值。故优化问题(6)又可被转化为：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{i} \quad \boldsymbol{c^{T}x_{i}} \\    \end{split}\end{equation}\]</span></p><p>  故实际上，<strong>线性规划问题的最优值一定在其可行解集的某个极点上取到，其最优解也是某一个极点。</strong></p><p><strong>结论</strong><br>  通过以上的分析，我们来总结一下得到的结论。考虑如下线性规划的形式：</p><p><span class="math display">\[\begin{split}    \min_{\boldsymbol{c} \in \mathbb{R^{n}}} \quad&amp;\boldsymbol{c^{T}x} \\    s.t. \quad &amp; \boldsymbol{Ax} = \boldsymbol{b} \\    &amp; \boldsymbol{x} \ge \boldsymbol{0} \\\end{split}\]</span></p><p>  其中，<span class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{m\times n},r(\boldsymbol{A})=m\)</span>，记其可行解集为 <span class="math inline">\(S = \{ \boldsymbol{x} |\boldsymbol{Ax}=\boldsymbol{b},\boldsymbol{x} \ge \boldsymbol{0}\}\)</span>，设 <span class="math inline">\(S\)</span> 的极点为 <span class="math inline">\(\boldsymbol{x_1,x_2,\dots,x_{k}}\)</span>，极方向为<span class="math inline">\(\boldsymbol{d_1,d_2,\dots,d_{I}}\)</span>，则有：</p><ul><li><strong>线性规划有最优解当且仅当 <span class="math inline">\(\forallj, \boldsymbol{c^{T}d_{j}} \ge 0\)</span>.</strong></li><li><strong>若线性规划有最优值，则必然可在某个极点上达到。</strong></li></ul><h2 id="mathematical-analysis-of-the-simplex-method">MathematicalAnalysis of the Simplex Method</h2><p>  有了前文中关于线性规划问题解的结论后，我们便能够比较容易地理解单纯形法的基本思想了。单纯形法的基本思想是<strong>首先找到可行解集的某一个极点，判断其是否为最优解，若是则算法终止，否则寻找一个更有的极点。</strong><br>  这里有两个核心问题，<strong>如何判断某一个极点是否是最优解</strong>？以及<strong>如何找到一个更优的极点</strong>?如果这两个问题解决了，我们便能够依据单纯形的基本思想完成整个算法的设计。接下来，我们便来分析这两个问题。<br><strong>Analysis</strong><br>  设矩阵 <span class="math inline">\(\boldsymbol{A}\)</span>的某种分块为 <span class="math inline">\(\boldsymbol{A} =\begin{bmatrix}  \boldsymbol{B},\boldsymbol{N}\end{bmatrix}\)</span>，<span class="math inline">\(\boldsymbol{B} \in\mathbb{R}^{m}\)</span>，使得 <span class="math inline">\(\boldsymbol{B}\)</span> 可逆，且 <span class="math inline">\(\boldsymbol{B^{-1}b} \ge\boldsymbol{0}\)</span>，令：</p><p><span class="math display">\[\boldsymbol{\bar{x}} = \begin{bmatrix}    \boldsymbol{B^{-1}b} \\    \boldsymbol{0} \\\end{bmatrix}\]</span></p><p>由极点定理可知 <span class="math inline">\(\boldsymbol{\bar{x}}\)</span> 是可行解集 <span class="math inline">\(S\)</span> 的某一个极点。<strong>接下来我们要判断<span class="math inline">\(\boldsymbol{\bar{x}}\)</span>是否是最优解。</strong><br>  取 <span class="math inline">\(\boldsymbol{x} \in S\)</span>，根据矩阵<span class="math inline">\(\boldsymbol{A}\)</span>的分块结果，相应地将向量 <span class="math inline">\(\boldsymbol{x}\)</span> 与 <span class="math inline">\(\boldsymbol{c}\)</span> 进行分块：</p><p><span class="math display">\[\boldsymbol{x} = \begin{bmatrix}    \boldsymbol{x_{B}} \\    \boldsymbol{x_{N}} \\\end{bmatrix}, \quad \boldsymbol{c} = \begin{bmatrix}    \boldsymbol{c_{B}} \\    \boldsymbol{c_{N}} \\\end{bmatrix}\]</span></p><p><span class="math display">\[\boldsymbol{Ax}=\boldsymbol{b} \quad\Rightarrow \quad \begin{bmatrix}    \boldsymbol{B,N}\end{bmatrix}\begin{bmatrix}    \boldsymbol{x_{B}} \\    \boldsymbol{x_{N}} \\\end{bmatrix}=\boldsymbol{Bx_{B}}+\boldsymbol{Nx_{N}}=\boldsymbol{b}\quad \Rightarrow \quad \boldsymbol{x_{B}} =\boldsymbol{B^{-1}b}-\boldsymbol{B^{-1}Nx_{N}}\]</span></p><p>  若对 <span class="math inline">\(\forall x \in S\)</span>，均有<span class="math inline">\(\boldsymbol{c^{T}x}-\boldsymbol{c^{T}\bar{x}} \ge0\)</span>，则 <span class="math inline">\(\boldsymbol{\bar{x}}\)</span>为最优解。对判断条件 <span class="math inline">\(\boldsymbol{c^{T}x}-\boldsymbol{c^{T}\bar{x}} \ge0\)</span> 做一些化简：</p><p><span class="math display">\[\begin{split}    \boldsymbol{c^{T}x}-\boldsymbol{c^{T}\bar{x}} =&amp; \begin{bmatrix}        \boldsymbol{c_{B}^{T},c_{N}^{T}} \\    \end{bmatrix}\begin{bmatrix}        \boldsymbol{x_{B}} \\        \boldsymbol{x_{N}} \\    \end{bmatrix} - \begin{bmatrix}        \boldsymbol{c_{B}^{T},c_{N}^{T}} \\    \end{bmatrix}\begin{bmatrix}        \boldsymbol{B^{-1}b} \\        \boldsymbol{0} \\    \end{bmatrix} =\boldsymbol{c_{B}^{T}x_{B}}+\boldsymbol{c_{N}^{T}x_{N}} -\boldsymbol{c_{B}^{T}B^{-1}b}  \\    &amp;= \boldsymbol{c_{B}^{T}B^{-1}b} -\boldsymbol{c_{B}^{T}B^{-1}Nx_{N}}+\boldsymbol{c_{N}^{T}x_{N}} -\boldsymbol{c_{B}^{T}B^{-1}b} = \boldsymbol{c_{N}^{T}x_{N}} -\boldsymbol{c_{B}^{T}B^{-1}Nx_{N}} \\    &amp;= \left( \boldsymbol{c_{N}^{T}} - \boldsymbol{c_{B}^{T}B^{-1}N}\right)\boldsymbol{x_{N}}\end{split}\]</span></p><p>  <strong>(1)</strong> 若 <span class="math inline">\(\boldsymbol{c_{N}^{T}} -\boldsymbol{c_{B}^{T}B^{-1}N} \ge \boldsymbol{0}\)</span>，则有 <span class="math inline">\(\boldsymbol{c^{T}x}-\boldsymbol{c^{T}\bar{x}} \ge0\)</span>，<span class="math inline">\(\boldsymbol{\bar{x}}\)</span>即为最优解，此时我们已经找到了最优解，算法可以终止了。<br>  <strong>(2)</strong> 若 <span class="math inline">\(\boldsymbol{c_{N}^{T}} -\boldsymbol{c_{B}^{T}B^{-1}N} \ngeq \boldsymbol{0}\)</span>，此时 <span class="math inline">\(\boldsymbol{\bar{x}}\)</span>不是最优解，我们需要<strong>寻找一个更优的极点。</strong><br>  不妨设 <span class="math inline">\(\boldsymbol{c_{N}^{T}} -\boldsymbol{c_{B}^{T}B^{-1}N}\)</span> 中的第j个分量小于零，即 <span class="math inline">\(\boldsymbol{c_{Nj}^{T}} -\boldsymbol{c_{B}^{T}B^{-1}n_{j}} &lt; 0\)</span>，记：</p><p><span class="math display">\[\boldsymbol{d_{j}} = \begin{bmatrix}    -\boldsymbol{B^{-1}n_{j}} \\    \boldsymbol{e_{j}}\end{bmatrix}\]</span></p><p>则：</p><p><span class="math display">\[\boldsymbol{c^{T}d_{j}} =\begin{bmatrix}        \boldsymbol{c_{B}^{T},c_{N}^{T}} \\    \end{bmatrix}\begin{bmatrix}    -\boldsymbol{B^{-1}n_{j}} \\    \boldsymbol{e_{j}}\end{bmatrix} = \boldsymbol{c_{Nj}^{T}} -\boldsymbol{c_{B}^{T}B^{-1}n_{j}} &lt; 0\]</span></p><p>记 <span class="math inline">\(\boldsymbol{r_{j}} =\boldsymbol{B^{-1}n_{j}}\)</span><br>  <strong>1.</strong> 若 <span class="math inline">\(\boldsymbol{r_{j}}\leq \boldsymbol{0}\)</span>，则 <span class="math inline">\(\boldsymbol{d_{j}} \ge0\)</span>，由极方向定理可知: <span class="math inline">\(\boldsymbol{d_{j}}\)</span> 为 <span class="math inline">\(S\)</span> 的一个极方向，则对 <span class="math inline">\(\forall \lambda \ge 0,\boldsymbol{\bar{x}}+\lambda \boldsymbol{d_{j}} \inS\)</span>，故有：</p><p><span class="math display">\[\boldsymbol{c^{T}}\left(\boldsymbol{\bar{x}} + \lambda \boldsymbol{d_{j}} \right) =\boldsymbol{c^{T}\bar{x}}+\lambda \boldsymbol{c^{T}d_{j}} \rightarrow-\infty, \quad \lambda \rightarrow +\infty\]</span></p><p>  <strong>2.</strong> 若 <span class="math inline">\(\boldsymbol{r_{j}} \nleq\boldsymbol{0}\)</span>，则 <span class="math inline">\(\boldsymbol{d_{j}} \ngeq\boldsymbol{0}\)</span>，故 <span class="math inline">\(\boldsymbol{d_{j}}\)</span> 不是极方向，此时：</p><p><span class="math display">\[\boldsymbol{c^{T}}\left(\boldsymbol{\bar{x}} + \lambda \boldsymbol{d_{j}} \right) =\boldsymbol{c^{T}\bar{x}}+\lambda \boldsymbol{c^{T}d_{j}} &lt;\boldsymbol{c^{T}\bar{x}}\]</span></p><p>  由上式可知沿着 <span class="math inline">\(\boldsymbol{d_{j}}\)</span>方向，能够使得目标函数下降，此时我们需要<strong>选取一个合适的步长 <span class="math inline">\(\lambda\)</span>，</strong>使得下降速度尽可能大，但又不会使得新得到的解 <span class="math inline">\(\boldsymbol{\bar{x}} + \lambda\boldsymbol{d_{j}}\)</span> 违反约束条件。<br>  首先来检验等式约束，对于 <span class="math inline">\(\forall \lambda\ge 0\)</span>：</p><p><span class="math display">\[\begin{split}    \boldsymbol{A}\left( \boldsymbol{\bar{x}} + \lambda\boldsymbol{d_{j}} \right) =&amp; \boldsymbol{A\bar{x}} + \lambda\boldsymbol{Ad_{j}} \\    =&amp; \boldsymbol{b} + \lambda \begin{bmatrix}        \boldsymbol{B,N} \\    \end{bmatrix} \begin{bmatrix}        -\boldsymbol{B^{-1}n_{j}} \\        \boldsymbol{e_{j}} \\    \end{bmatrix} \\    =&amp; \boldsymbol{b} + \lambda \left( -\boldsymbol{n_{j}} +\boldsymbol{n_{j}} \right) \\    =&amp; \boldsymbol{b}  \end{split}\]</span></p><p>  再来检验不等式约束：</p><p><span class="math display">\[\boldsymbol{\bar{x}} + \lambda\boldsymbol{d_{j}} = \begin{bmatrix}    \boldsymbol{B^{-1}b} \\    \boldsymbol{0} \\\end{bmatrix} + \lambda \begin{bmatrix}    -\boldsymbol{B^{-1}n_{j}} \\    \boldsymbol{e_{j}}\end{bmatrix} \ge \boldsymbol{0}\]</span></p><p><span class="math display">\[\Rightarrow \lambda \boldsymbol{e_{j}}\ge 0,\quad \boldsymbol{B^{-1}b}-\lambda\boldsymbol{B^{-1}n_{j}}=\boldsymbol{B^{-1}b} - \lambda\boldsymbol{r_{j}} \ge \boldsymbol{0}\]</span></p><p>记 <span class="math inline">\(\boldsymbol{h} = \boldsymbol{B^{-1}b}\ge \boldsymbol{0}\)</span>，则：</p><p><span class="math display">\[\boldsymbol{h}-\lambda\boldsymbol{r_{j}} \ge \boldsymbol{0} \quad \Leftrightarrow \quad\begin{bmatrix}    \boldsymbol{h_{1}} - \lambda \boldsymbol{r_{j1}} \\    \vdots \\    \boldsymbol{h_{m}} - \lambda \boldsymbol{r_{jm}} \\\end{bmatrix} \ge \boldsymbol{0} \quad \Leftrightarrow \quad \forall i,\boldsymbol{h_{i}} - \lambda \boldsymbol{r_{ji}} \ge 0\]</span></p><p><span class="math display">\[\Rightarrow \quad \lambda \leq\frac{\boldsymbol{h_{i}}}{\boldsymbol{r_{ji}}}, \forall i,且\boldsymbol{r_{ji}} &gt; 0 \quad \Rightarrow \quad \lambda^{*} =\min_{i} \{ \frac{\boldsymbol{h_{i}}}{\boldsymbol{r_{ji}}} |\boldsymbol{r_{ji}} &gt; 0 \}\]</span></p><p>  通过以上分析，我们找到了一个最佳的步长 <span class="math inline">\(\lambda^{*}\)</span>，有一个非常重要的结论：<strong>利用<span class="math inline">\(\lambda^{*}\)</span> 构造的新解 <span class="math inline">\(\boldsymbol{\hat{x}}=\boldsymbol{\bar{x}}+\lambda\boldsymbol{d_{j}}\)</span> 为可行解集 <span class="math inline">\(S\)</span> 的一个新的极点。</strong><br>  此时我们只需要重复以上步骤判断 <span class="math inline">\(\boldsymbol{\hat{x}}\)</span>是否是最优解，若是则算法终止，否则再构造下一个极点。</p><h2 id="algorithm-steps-for-simplex-method">Algorithm Steps for SimplexMethod</h2><p>  输入: <span class="math inline">\(\boldsymbol{c} \in\mathbb{R}^{n}, \boldsymbol{A} \in \mathbb{R}^{m \times n},\boldsymbol{b} \in \mathbb{R}^{m}\)</span><br>  输出: <span class="math inline">\(\boldsymbol{x} \in\mathbb{R}^{n}\)</span><br>  算法步骤：<br>  (1) 确定某种 <span class="math inline">\(\boldsymbol{A}\)</span>的分块: <span class="math inline">\(\boldsymbol{A} =\begin{bmatrix}  \boldsymbol{B,N}\end{bmatrix}\)</span>，以及对应的初始极点：</p><p><span class="math display">\[\boldsymbol{x} = \begin{bmatrix}    \boldsymbol{B^{-1}b} \\    \boldsymbol{0} \\\end{bmatrix}\]</span></p><p>  (2) 判断 <span class="math inline">\(\boldsymbol{c_{N}^{T}} -\boldsymbol{c_{B}^{T}B^{-1}N} \ge \boldsymbol{0}?\)</span>若是，则算法终止，输出 <span class="math inline">\(\boldsymbol{x}\)</span>，否则转入下一步(3);<br>  (3) 找到 <span class="math inline">\(\boldsymbol{c_{N}^{T}} -\boldsymbol{c_{B}^{T}B^{-1}N}\)</span> 的负值分量<span class="math inline">\(j\)</span>，检验是否 <span class="math inline">\(\exists j\)</span>，使得 <span class="math inline">\(\boldsymbol{r_{j}}=\boldsymbol{Bn_{j}} \leq\boldsymbol{0}\)</span>，若存在，则该问题是无界的，无最小值，算法终止，否则转入(4);<br>  (4) 挑选一个<span class="math inline">\(j\)</span>，构造一个新的极点<span class="math inline">\(\boldsymbol{\bar{x}}\)</span>:</p><p><span class="math display">\[\boldsymbol{\bar{x}} = \boldsymbol{x} +\lambda^{*} \boldsymbol{d_{j}},\quad \boldsymbol{d_{j}} =\begin{bmatrix}    -\boldsymbol{B^{-1}n_{j}} \\    \boldsymbol{e_{j}}\end{bmatrix},\quad \lambda^{*} = \min_{i} \{\frac{\boldsymbol{h_{i}}}{\boldsymbol{r_{ji}}} | \boldsymbol{r_{ji}}&gt; 0 \}\]</span></p><p>  (5) 根据 <span class="math inline">\(\boldsymbol{\bar{x}}\)</span>，对矩阵<span class="math inline">\(\boldsymbol{A}\)</span>进行重新分块 <span class="math inline">\(\boldsymbol{A} =\begin{bmatrix}  \boldsymbol{\bar{B},\bar{N}}\end{bmatrix}\)</span>，重复(2)~(5)步。</p><h2 id="python-impletation-of-simplex-method">Python Impletation ofSimplex Method</h2><p>  给定算法的输入：</p><p><span class="math display">\[\boldsymbol{c}=\begin{bmatrix}    1 \\    2 \\    3 \\    4 \\    5 \\\end{bmatrix},\quad \boldsymbol{A}= \begin{bmatrix}    1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\    2 &amp; 2 &amp; 3 &amp; 4 &amp; 5 \\\end{bmatrix},\quad \boldsymbol{b}=\begin{bmatrix}    10 \\    20 \\\end{bmatrix}\]</span></p><p>  我们可以调用 Python 的 <code>scipy.optimize</code> 类中的<code>linprog</code>方法来执行单纯形算法，代码如下：</p><div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.optimize <span class="im">import</span> linprog</span><span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span><span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>]</span><span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span><span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>A_eq <span class="op">=</span> [</span><span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>],</span><span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>] </span><span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>]</span><span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>b_eq <span class="op">=</span> [</span><span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="dv">10</span>, </span><span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="dv">20</span>  </span><span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>]</span><span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span><span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> linprog(c, A_eq<span class="op">=</span>A_eq, b_eq<span class="op">=</span>b_eq, method<span class="op">=</span><span class="st">'simplex'</span>)</span><span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span><span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(result[<span class="st">'x'</span>])</span></code></pre></div><p>  使用单纯形算法解得最优解为：</p><p><span class="math display">\[\boldsymbol{x^{*}} = \begin{bmatrix}    10 &amp; 0 &amp; 0 &amp; 0 &amp; 0\end{bmatrix}^{T}\]</span></p><h2 id="reference">Reference</h2><p><strong>[1] Video: 最优化理论与方法-第十一讲-线性规划, superfatseven,B站.</strong><br><strong>[2] Book: Boyd S P, Vandenberghe L. Convex optimization[M].Cambridge university press, 2004.</strong></p>]]></content>
      
      
      <categories>
          
          <category> 最优传输理论 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Optimal Transport-3.Kantorovich-OT Dual Problem</title>
      <link href="/2024/03/16/Optimal%20Transport-3.Dual%20Problem/"/>
      <url>/2024/03/16/Optimal%20Transport-3.Dual%20Problem/</url>
      
        <content type="html"><![CDATA[<h1 id="kantorovich-ot-dual-problem">Kantorovich-OT Dual Problem</h1><p>  在前面的小节，我们介绍了 KantorovichOT问题的基本形式，其基本形式如下：</p><p><span class="math display">\[L_{\boldsymbol{C}}(\boldsymbol{a},\boldsymbol{b}) \quad \overset{\text{def.}}{=} \quad\min_{\boldsymbol{P} \in \boldsymbol{U}(\boldsymbol{a},\boldsymbol{b})}\left&lt; \boldsymbol{C},\boldsymbol{P}\right&gt;_{F}=\sum_{i,j}p_{ij}c_{ij}\]</span></p><p>其中，<span class="math inline">\(\boldsymbol{P} \in\mathbb{R}_{+}^{n \times m}\)</span> 为运输矩阵，<span class="math inline">\(\boldsymbol{C} \in \mathbb{R}_{+}^{n \timesm}\)</span> 为成本矩阵，且约束条件为：</p><p><span class="math display">\[\boldsymbol{U}(\boldsymbol{a},\boldsymbol{b}) =\{ \boldsymbol{P} \in \mathbb{R}^{n \times m}_{+}: \boldsymbol{P}\mathbf{1}_{m}=\boldsymbol{a} \quad and \quad \boldsymbol{P^{T}}\mathbf{1}_{n}=\boldsymbol{b} \}\]</span></p><p>  从今天的视角来看，KantorovichOT问题本质上是一种线性规划问题，然而在 Kantorovich 给出 OT问题的上述形式时，严格的线性规划理论还没有被建立。作为线性规划理论的主要建立人，Kantorovich对于 OT问题的研究，在其建立线性规划理论上起到了重要的推动作用，线性规划中很多与经济资源分配的实际案例，便来源于Kantorovich 对于 OT 问题的研究。Kantorovich在线性规划理论中，提出了著名的<strong>对偶关系</strong>，这成为了最优化学科的核心理论之一。<br>  在本节，我们将首先介绍一些凸优化与对偶关系的基础知识，并给出Kantorovich OT问题的标准线性规划形式，最后我们将讨论其对偶问题的经济内涵。</p><h2 id="convex-optimization-foundations">Convex OptimizationFoundations</h2><h3 id="basic-concept">Basic Concept</h3><p><strong>仿射集</strong><br>  设<span class="math inline">\(C\)</span>为某一集合，若 <span class="math inline">\(\forall x_1,x_2 \in C, \theta \in \mathbb{R},\theta x_1+(1-\theta)x_2 \in C\)</span>，则称集合<span class="math inline">\(C\)</span>为仿射集。</p><p><strong>仿射函数</strong><br>  设有映射 <span class="math inline">\(f: \mathbb{R}^{n} \rightarrow\mathbb{R}^{m}\)</span>，若 <span class="math inline">\(f(x)=Ax+b, A \in\mathbb{R}^{m \times n}, b \in \mathbb{R}^{m}\)</span>，则称映射<span class="math inline">\(f\)</span>为仿射函数。</p><p><strong>凸集</strong><br>  设<span class="math inline">\(C\)</span>为某一集合，若 <span class="math inline">\(\forall x_1,x_2 \in C, \theta \in [0,1], \thetax_1+(1-\theta)x_2 \in C\)</span>，则称集合<span class="math inline">\(C\)</span>为凸集。</p><p><strong>凸函数</strong><br>  一个函数<span class="math inline">\(f(x)\)</span>被称为凸函数，如果它的定义域 <span class="math inline">\(dom f\)</span> 为凸集，并且对 <span class="math inline">\(\forall x_1,x_2 \in dom f, \alpha \in[0,1]\)</span>，有以下不等式成立：</p><p><span class="math display">\[f[\alpha x_1+(1-\alpha) x_2] \leq \alphaf(x_1)+(1-\alpha)f(x_2)\]</span></p><p>  <strong>凸函数的一阶条件</strong><br>  设函数<span class="math inline">\(f(x)\)</span><strong>一阶可微</strong>，<span class="math inline">\(x \in \mathbb{R}^{n}\)</span>，则<span class="math inline">\(f(x)\)</span>为凸函数的<strong>充要条件</strong>为：<span class="math inline">\(dom \space f\)</span>为凸集，且对<span class="math inline">\(\forall x,y \in dom \spacef\)</span>，有以下不等式成立：</p><p><span class="math display">\[f(y) \ge f(x)+\nablaf^{T}(x)(y-x)\]</span></p><p>  <strong>凸函数的二阶条件</strong><br>  设函数<span class="math inline">\(f(x)\)</span><strong>二阶可微</strong>，<span class="math inline">\(x \in \mathbb{R}^{n}\)</span>，则<span class="math inline">\(f(x)\)</span>为凸函数的<strong>充要条件</strong>为：<span class="math inline">\(dom \space f\)</span>为凸集，且对<span class="math inline">\(\forall x \in dom \space f\)</span>，有<span class="math inline">\(\nabla^{2}f(x) \succeq 0\)</span>，即<span class="math inline">\(Hessain\)</span>矩阵半正定。</p><p><strong>最优化问题</strong><br>  最优化问题的基本形式(1)：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{x} \quad &amp; f(x) \\        s.t. \quad &amp; m_{i}(x) \leq 0,\quad i=1,2,\dots,M \\        &amp; n_{j}(x) = 0,\quad j=1,2,\dots,N    \end{split}\end{equation}\]</span></p><ul><li><span class="math inline">\(x=\begin{bmatrix}  x_1,x_2,\dots,x_n\end{bmatrix}^{T},x \in \mathbb{R}^{n}\)</span>称为<strong>优化变量(Optimization Variable)</strong>.<br></li><li><span class="math inline">\(f: \mathbb{R}^{n} \rightarrow\mathbb{R}\)</span>，称为<strong>目标函数(ObjectiveFunction)</strong>.<br></li><li><span class="math inline">\(m_i: \mathbb{R}^{n} \rightarrow\mathbb{R}\)</span>，称为<strong>不等式约束(InequalityConstraint)</strong>.<br></li><li><span class="math inline">\(n_j: \mathbb{R}^{n} \rightarrow\mathbb{R}\)</span>，称为<strong>等式约束(EqualityConstraint)</strong>.<br></li><li><span class="math inline">\(D = \left( dom \space f \right) \bigcap\{x \vert m_{i}(x) \leq 0,i=1,2,\dots,M\} \bigcap \{x \vert h_{j}(x) =0,j=1,2,\dots,N\}\)</span>，称为最优化问题的<strong>域(Domain)</strong>，<span class="math inline">\(x \in D\)</span> 称为<strong>可行解(FeasibleSolution)</strong>.</li><li><span class="math inline">\(p^{*} = \inf \{ f(x) \vert x \in D\}\)</span>，称为最优化问题的<strong>最优值(Optimum)</strong>.<br></li><li><span class="math inline">\(f(x^{*})=p^{*}\)</span>，称<span class="math inline">\(x^{*}\)</span>为最优化问题的<strong>最优解(OptimalSolution)</strong>.<br></li><li><span class="math inline">\(X_{opt} = \{ x \vert x \in D,f(x)=p^{*}\}\)</span>，称为最优化问题的<strong>最优解集(OptimalSet)</strong>.</li></ul><p><strong>凸优化问题</strong><br>  若在优化问题(7)中，目标函数<span class="math inline">\(f(x)\)</span>为凸函数，不等式约束<span class="math inline">\(m_i(x)\)</span>为凸函数，等式约束<span class="math inline">\(n_j(x)\)</span>为仿射函数，则称该优化问题为凸优化问题。</p><h3 id="dual-problem">Dual Problem</h3><p><strong>拉格朗日函数</strong><br>  原问题(1)的拉格朗日函数为：</p><p><span class="math display">\[L(x,\lambda,\eta)=f(x)+\sum_{i=1}^{M}\lambda_im_i(x)+\sum_{j=1}^{N}\eta_j n_j(x)\]</span></p><p><span class="math display">\[\lambda_i \ge 0, \quadi=1,2,\dots,M\]</span></p><p><strong>原问题的无约束形式</strong><br>  原问题(1)的无约束形式(2)为：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{x} \max_{\lambda,\eta} \quad &amp; L(x,\lambda,\eta)  \\        s.t. \quad &amp; \lambda_i \ge 0, \quad i=1,2,\dots,M \\    \end{split}\end{equation}\]</span></p><p>  原问题(1)与其无约束形式(2)是等价的。</p><p><strong>对偶问题</strong><br>  原问题(1)的拉格朗日对偶问题(3)为：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \max_{\lambda,\eta} \min_{x} \quad &amp; L(x,\lambda,\eta) \\        s.t. \quad &amp; \lambda_i \ge 0, \quad i = 1,2,\dots,M    \end{split}\end{equation}\]</span></p><p><strong>弱对偶关系</strong><br>  原问题(1)与其对偶问题(3)满足弱对偶关系：</p><p><span class="math display">\[\max_{\lambda,\eta} \min_{x}L(x,\lambda,\eta) \leq \min_{x} \max_{\lambda,\eta}L(x,\lambda,\eta)\]</span></p><p><strong>强对偶关系</strong><br>  若原问题(1)与其对偶问题(3)满足：</p><p><span class="math display">\[\max_{\lambda,\eta} \min_{x}L(x,\lambda,\eta) = \min_{x} \max_{\lambda,\eta}L(x,\lambda,\eta)\]</span></p><p>  则称原问题(1)与其对偶问题(3)满足强对偶关系。</p><p><strong><span class="math inline">\(Slater\)</span>条件</strong><br>  若原问题(1)是凸问题，同时 <span class="math inline">\(\exists x \inrelint(D)\)</span>，使得约束满足：</p><p><span class="math display">\[\begin{split}    &amp; m_{i}(x) &lt; 0, \quad i=1,2,\dots,M \\    &amp; n_{j}(x) = 0, \quad j=1,2,\dots,N \\\end{split}\]</span></p><p>  则原问题与对偶问题满足强对偶关系。</p><ul><li>注：<span class="math inline">\(relint(D)\)</span>表示原始凸问题的域的相对内部，即域内除了边界点以外的所有点。</li></ul><p>  <span class="math inline">\(Slater\)</span>条件是一个<strong>充分不必要条件</strong>，若满足<span class="math inline">\(Slater\)</span>条件，则强对偶一定成立，不满足<span class="math inline">\(Slater\)</span>条件，强对偶也可能成立。大多数凸优化问题均满足<span class="math inline">\(Slater\)</span>条件，即有强对偶性。</p><h3 id="linear-programming">Linear Programming</h3><p>  线性规划问题的一般形式为(4)：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{x} \quad &amp; c^{T}x + d \\        s.t. \quad &amp;  Gx \leq h \\        &amp; Ax=b  \\    \end{split}\end{equation}\]</span></p><p>其中，<span class="math inline">\(x,c,d \in\mathbb{R}^{n}\)</span>；<span class="math inline">\(G \in \mathbb{R}^{M\times n}, h \in \mathbb{R}^{M}; A \in \mathbb{R}^{N \times n}, b \in\mathbb{R}^{N}\)</span>.<br>  一般的线性规划问题都可以通过变形，转化成线性规划的标准形式(5)：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{x} \quad &amp; c^{T}x  \\        s.t. \quad &amp;  Ax \leq b \\        &amp; x \leq 0    \end{split}\end{equation}\]</span></p><p>  有一个重要的结论是：<strong>线性规划问题是凸问题且满足强对偶关系。</strong></p><h2 id="linear-programming-form-of-kantorovich-ot-problem">LinearProgramming Form of Kantorovich-OT Problem</h2><p>  Kantorovich-OT Problem本质上是一个线性规划问题，我们可以通过变形，将其转化为线性规划形式，下面我们尝试将Kantorovich-OTProblem 转化为与其等价的线性规划问题。</p><h3 id="primal-linear-programming">Primal Linear Programming</h3><p>  原始的Kantorovich-OT Problem的形式如下：</p><p><span class="math display">\[L_{\boldsymbol{C}}(\boldsymbol{a},\boldsymbol{b}) \quad \overset{\text{def.}}{=} \quad\min_{\boldsymbol{P} \in \boldsymbol{U}(\boldsymbol{a},\boldsymbol{b})}\left&lt; \boldsymbol{C},\boldsymbol{P}\right&gt;_{F}=\sum_{i,j}p_{ij}c_{ij}\]</span></p><p>  运输矩阵 <span class="math inline">\(\boldsymbol{P}\)</span> 与成本矩阵 <span class="math inline">\(\boldsymbol{C}\)</span>可以写成列向量形式：</p><p><span class="math display">\[\boldsymbol{P}=\begin{bmatrix}    \boldsymbol{p_1}, \boldsymbol{p_2}, \cdots,\boldsymbol{p_m}\end{bmatrix},\quad \boldsymbol{C} = \begin{bmatrix}    \boldsymbol{c_1},\boldsymbol{c_2}, \cdots,\boldsymbol{c_m}\end{bmatrix}\]</span></p><p>  构造向量 <span class="math inline">\(\boldsymbol{p},\boldsymbol{c}\in \mathbb{R}^{nm}\)</span>，矩阵 <span class="math inline">\(A \in\mathbb{R}^{(n+m) \times nm}\)</span>:</p><p><span class="math display">\[\boldsymbol{p} = \begin{bmatrix}    \boldsymbol{p_{1}^{T}}, \boldsymbol{p_{2}^{T}}, \cdots,\boldsymbol{p_{m}^{T}}\end{bmatrix}^{T}, \quad \boldsymbol{c} = \begin{bmatrix}    \boldsymbol{c_{1}^{T}}, \boldsymbol{c_{2}^{T}}, \cdots,\boldsymbol{c_{m}^{T}}\end{bmatrix}^{T}\]</span></p><p><span class="math display">\[\boldsymbol{A} = \begin{bmatrix}    \boldsymbol{1_{n}^{T}} \otimes \boldsymbol{I_{m}} \\    \boldsymbol{I_{n}} \otimes \boldsymbol{1_{m}^{T}} \\\end{bmatrix}\]</span></p><p>其中，符号"<span class="math inline">\(\otimes\)</span>"表示矩阵的kronecker's product。</p><p><span class="math display">\[\boldsymbol{c^{T}}\boldsymbol{p}=\sum_{i=1}^{m}\boldsymbol{c_{i}^{T}}\boldsymbol{p_{i}}=\sum_{i,j}p_{ij}c_{ij}\]</span></p><p><span class="math display">\[\boldsymbol{A}\boldsymbol{p} =\begin{bmatrix}    \boldsymbol{1_{n}^{T}} \otimes \boldsymbol{I_{m}} \\    \boldsymbol{I_{n}} \otimes \boldsymbol{1_{m}^{T}} \\\end{bmatrix}\boldsymbol{p}=\begin{bmatrix}    (\boldsymbol{1_{n}^{T}} \otimes \boldsymbol{I_{m}})\boldsymbol{p} \\    (\boldsymbol{I_{n}} \otimes \boldsymbol{1_{m}^{T}})\boldsymbol{p} \\\end{bmatrix} \in \mathbb{R}^{n+m}\]</span></p><p>其中，</p><p><span class="math display">\[\begin{split}    (\boldsymbol{1_{n}^{T}} \otimes \boldsymbol{I_{m}})\boldsymbol{p}&amp;= \begin{bmatrix}        \boldsymbol{I_{m}}, \boldsymbol{I_{m}},\cdots,\boldsymbol{I_{m}}    \end{bmatrix}_{m \times nm} \cdot \boldsymbol{p} \\    &amp;= \begin{bmatrix}        \sum_{j}p_{1 \cdot j} \\        \sum_{j}p_{2 \cdot j} \\        \vdots \\        \sum_{j}p_{n \cdot j}    \end{bmatrix}_{m \times 1} = \boldsymbol{P1_{m}} = \boldsymbol{a}\end{split}\]</span></p><p><span class="math display">\[\begin{split}    (\boldsymbol{I_{n}} \otimes \boldsymbol{1_{m}^{T}})\boldsymbol{p}&amp;= \begin{bmatrix}        \boldsymbol{1_{m}^{T}} &amp; &amp; &amp; \\        &amp; \boldsymbol{1_{m}^{T}} &amp; &amp; \\        &amp; &amp; \ddots &amp; \\        &amp; &amp; &amp; \boldsymbol{1_{m}^{T}}    \end{bmatrix}_{n \times nm} \cdot \boldsymbol{p} \\    &amp;= \begin{bmatrix}        \sum_{j}p_{j \cdot 1} \\        \sum_{j}p_{j \cdot 2} \\        \vdots \\        \sum_{j}p_{j \cdot m} \\    \end{bmatrix}_{n \times 1} = \boldsymbol{P^{T}1_{n}}=\boldsymbol{b}\end{split}\]</span></p><p>故有：</p><p><span class="math display">\[\boldsymbol{Ap}=\begin{bmatrix}    (\boldsymbol{1_{n}^{T}} \otimes \boldsymbol{I_{m}})\boldsymbol{p} \\    (\boldsymbol{I_{n}} \otimes \boldsymbol{1_{m}^{T}})\boldsymbol{p} \\\end{bmatrix}=\begin{bmatrix}    \boldsymbol{a} \\    \boldsymbol{b} \\\end{bmatrix} \overset{\text{def.}}{=} \boldsymbol{d} \in\mathbb{R^{n+m}}\]</span></p><p>故 Kantorovich-OT Problem 可以转化为等价形式(6):</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{\boldsymbol{p} \in \mathbb{R^{nm}}} \quad&amp;\boldsymbol{c^{T}p} \\        s.t. \quad &amp; \boldsymbol{Ap} = \boldsymbol{d} \\        &amp; \boldsymbol{p} \ge \boldsymbol{0} \\    \end{split}\end{equation}\]</span></p><p>以上问题(6)即为 Kantorovich-OT Problem的一般线性规划形式，其中等式约束也可以改为不等式约束。</p><h3 id="dual-linear-programming">Dual Linear Programming</h3><p>  接下来我们来讨论一下问题(6)的对偶问题，首先写出问题(6)的拉格朗日函数：</p><p><span class="math display">\[\mathbf{L}(\boldsymbol{p},\boldsymbol{h},\boldsymbol{u})= \boldsymbol{c^{T}p} + \boldsymbol{h^{T}} \left[ \boldsymbol{Ap} -\boldsymbol{d} \right] - \boldsymbol{u^{T}p}\]</span></p><p>原问题的无约束形式为：</p><p><span class="math display">\[\begin{split}    \min_{\boldsymbol{p}}\max_{\boldsymbol{h,u}} \quad &amp;\mathbf{L}(\boldsymbol{p},\boldsymbol{h},\boldsymbol{u}) \\    s.t. \quad &amp; \boldsymbol{u} \ge \boldsymbol{0}\end{split}\]</span></p><p>则原问题(6)的对偶问题(7)为：</p><p><span class="math display">\[\begin{equation}    \max_{\boldsymbol{h,u}}\min_{\boldsymbol{p}} \quad\mathbf{L}(\boldsymbol{p},\boldsymbol{h},\boldsymbol{u})\end{equation}\]</span></p><p>我们首先来考察一下内部极小化问题：</p><p><span class="math display">\[\begin{split}    \min_{p} \mathbf{L}(\boldsymbol{p},\boldsymbol{h},\boldsymbol{u})&amp;= \boldsymbol{c^{T}p} + \boldsymbol{h^{T}}\left( \boldsymbol{Ap} -\boldsymbol{d} \right) - \boldsymbol{u^{T}p} \\    &amp;= \left(  \boldsymbol{c^{T}} + \boldsymbol{h^{T}A} -\boldsymbol{u^{T}} \right)\boldsymbol{p} - \boldsymbol{h^{T}d} \\    &amp;= \left( \boldsymbol{c} + \boldsymbol{A^{T}h - \boldsymbol{u}}\right)^{T}\boldsymbol{p} - \boldsymbol{h^{T}d}\end{split}\]</span></p><p>令 <span class="math inline">\(\boldsymbol{h'}=-\boldsymbol{h}\)</span>，则有：</p><p><span class="math display">\[\min_{p}\mathbf{L}(\boldsymbol{p},\boldsymbol{h},\boldsymbol{u}) = \left(\boldsymbol{c} - \boldsymbol{u} - \boldsymbol{A^{T}h'}\right)^{T}\boldsymbol{p} + \boldsymbol{(h')^{T}d}\]</span></p><p><span class="math display">\[\min_{p}\mathbf{L}(\boldsymbol{p},\boldsymbol{h'},\boldsymbol{u}) = \left \{\begin{array}{rcl}\boldsymbol{(h')^{T}d}, &amp; {\boldsymbol{A^{T}h'} \leq\boldsymbol{c} - \boldsymbol{u}}\\-\infty,&amp; {\boldsymbol{A^{T}h'} \nleq \boldsymbol{c} -\boldsymbol{u}}\\\end{array} \right.\]</span></p><p>故问题(7)等价于:</p><p><span class="math display">\[\begin{split}    \max_{\boldsymbol{h',u}} \quad &amp; \boldsymbol{(h')^{T}d}\\    s.t. \quad &amp; \boldsymbol{A^{T}h'} \leq \boldsymbol{c} -\boldsymbol{u} \\    &amp; \boldsymbol{u} \ge \boldsymbol{0}\end{split}\]</span></p><p>合并两个等式约束得到原问题(6)的对偶问题(8):</p><p><span class="math display">\[\begin{equation}    \begin{split}    \max_{\boldsymbol{h'} \in \mathbb{R}^{n+m}} \quad &amp;\boldsymbol{d^{T}h'} \\    s.t. \quad &amp; \boldsymbol{A^{T}h'} \leq \boldsymbol{c} \\    &amp; \boldsymbol{h'} \ge \boldsymbol{0}\end{split}\end{equation}\]</span></p><h2 id="economic-interpretation-of-kantorovich-ot-dual-problem">Economicinterpretation of Kantorovich-OT Dual Problem</h2><p>  Kantorovich-OT Problem的对偶问题具有现实的经济解释，下面我们首先导出其对偶问题的形式。其原问题为：</p><p><span class="math display">\[\min_{\boldsymbol{P} \in\boldsymbol{U}(\boldsymbol{a},\boldsymbol{b})} \left&lt;\boldsymbol{C},\boldsymbol{P}\right&gt;_{F}=\sum_{i,j}p_{ij}c_{ij}\]</span></p><p><span class="math display">\[\boldsymbol{U}(\boldsymbol{a},\boldsymbol{b}) =\{ \boldsymbol{P} \in \mathbb{R}^{n \times m}_{+}: \boldsymbol{P}\mathbf{1}_{m}=\boldsymbol{a} \quad and \quad \boldsymbol{P^{T}}\mathbf{1}_{n}=\boldsymbol{b} \}\]</span></p><p>原问题的拉格朗日函数：</p><p><span class="math display">\[\mathbf{L}(\boldsymbol{P},\boldsymbol{f},\boldsymbol{g})= \left&lt; \boldsymbol{C},\boldsymbol{P} \right&gt; + \left&lt;\boldsymbol{a} - \boldsymbol{P1_{m}},\boldsymbol{f} \right&gt;+\left&lt;\boldsymbol{b} - \boldsymbol{P^{T}1_{n}},\boldsymbol{g}\right&gt;\]</span></p><p>其中，<span class="math inline">\(\boldsymbol{f} \in \mathbb{R}^{n},\boldsymbol{g} \in \mathbb{R}^{m}\)</span>。则原问题的无约束形式为：</p><p><span class="math display">\[\min_{\boldsymbol{P} \ge0}\max_{\boldsymbol{f},\boldsymbol{g}}\quad \left&lt;\boldsymbol{C},\boldsymbol{P} \right&gt; + \left&lt; \boldsymbol{a} -\boldsymbol{P1_{m}},\boldsymbol{f} \right&gt;+\left&lt; \boldsymbol{b} -\boldsymbol{P^{T}1_{n}},\boldsymbol{g} \right&gt;\]</span></p><p>上式等价于</p><p><span class="math display">\[\max_{\boldsymbol{f},\boldsymbol{g}}\left&lt; \boldsymbol{a},\boldsymbol{f} \right&gt; + \left&lt;\boldsymbol{b},\boldsymbol{g} \right&gt; + \min_{\boldsymbol{P} \ge\boldsymbol{0}} \left&lt; \boldsymbol{C -\boldsymbol{f1_{m}^{T}}-\boldsymbol{1_{n}g^{T}}}, \boldsymbol{P}\right&gt;\]</span></p><p>令 <span class="math inline">\(\boldsymbol{Q}=\boldsymbol{C -\boldsymbol{f1_{m}^{T}}-\boldsymbol{1_{n}g^{T}}} = \boldsymbol{C} -\boldsymbol{f}\oplus\boldsymbol{g}\)</span>，有：</p><p><span class="math display">\[\min_{\boldsymbol{P} \ge \boldsymbol{0}}\left&lt; \boldsymbol{Q},\boldsymbol{P} \right&gt; = \left \{\begin{array}{rcl}0, &amp; {\boldsymbol{Q} \ge \boldsymbol{0}}\\-\infty,&amp; {otherwise}\\\end{array} \right.\]</span></p><p>故原问题的对偶问题可以写成(9):</p><p><span class="math display">\[\begin{equation}    L_{\boldsymbol{C}}(\boldsymbol{a},\boldsymbol{b}) =\max_{(\boldsymbol{f},\boldsymbol{g}) \in\boldsymbol{R}(\boldsymbol{C})} \left&lt; \boldsymbol{f},\boldsymbol{a}\right&gt; + \left&lt; \boldsymbol{g,\boldsymbol{b}} \right&gt;\end{equation}\]</span></p><p><span class="math display">\[\boldsymbol{R}(\boldsymbol{C})\overset{def.}{=} \{ (\boldsymbol{f},\boldsymbol{g}) \in \mathbb{R}^{n}\times \mathbb{R}^{m}: \boldsymbol{f}\oplus\boldsymbol{g} \leq\boldsymbol{C} \}\]</span></p><p>  借助原问题的经济含义，我们来思考一下对偶问题的经济解释。在原问题中，我们需要将n 个木材工厂生产的木材运输到 m 个城市，各个木材工厂的产量为 <span class="math inline">\(\boldsymbol{a} \in\mathbb{R}_{+}^{n}\)</span>，各个城市的需求量为 <span class="math inline">\(\boldsymbol{b} \in\mathbb{R}_{+}^{m}\)</span>。在原问题中，我们所需要求解的运输矩阵 <span class="math inline">\(\boldsymbol{P}\)</span> 包含了从工厂 i 到城市 j的具体运输计划，即需要将 工厂 i 生产多少木材运输到城市 j。成本矩阵 <span class="math inline">\(\boldsymbol{C}\)</span> 则表示从工厂 i 到城市 j每单位木材运输所消耗的成本。约束条件 <span class="math inline">\(\boldsymbol{U}(\boldsymbol{a},\boldsymbol{b})\)</span>表示运输计划需要符合各个工厂的产量以及各个城市的需求量。<br>  <strong>中间商运输</strong><br>  考虑目标函数。木材供应商面对复杂的运输问题，他选择将运输的工作外包给某个中间商。该中间商会首先从各个木材工厂统一将木材进行回收，然后再分发给不同的城市。假设木材的回收价格为<span class="math inline">\(\boldsymbol{f} \in\mathbb{R}^{n}\)</span>，<span class="math inline">\(f_{i}\)</span>表示从木材工厂 i 回收单位木材的价格；木材的分发价格为 <span class="math inline">\(\boldsymbol{g} \in \mathbb{R}^{m}\)</span>，<span class="math inline">\(g_{i}\)</span> 表示将单位木材分发给城市 i的价格。中间商的目的自然是最大化它的利润，故对于中间商来说，这个运输的优化问题为：</p><p><span class="math display">\[\max_{\boldsymbol{f},\boldsymbol{g}}\quad \left&lt; \boldsymbol{f},\boldsymbol{a} \right&gt; + \left&lt;\boldsymbol{g},\boldsymbol{b} \right&gt;\]</span></p><p>  考虑约束条件。对于木材供应商来说，他认为如果中间商的定价满足 <span class="math inline">\(\boldsymbol{f} \oplus \boldsymbol{g} \leq\boldsymbol{C}\)</span>，则意味着自己一定不会吃亏，因为这意味着 <span class="math inline">\(\forall i,j, f_i+g_j \leq\boldsymbol{C}_{ij}\)</span>，即对于将单位木材从工厂 i 运输到城市j，中间商收取的费用一定是小于等于自己运输的成本。因此供应商将这条定价约束写进了招商合同。这样对于中间商来说，它所面对的最优化问题即为：</p><p><span class="math display">\[\begin{equation}    L_{\boldsymbol{C}}(\boldsymbol{a},\boldsymbol{b}) =\max_{(\boldsymbol{f},\boldsymbol{g}) \in\boldsymbol{R}(\boldsymbol{C})} \left&lt; \boldsymbol{f},\boldsymbol{a}\right&gt; + \left&lt; \boldsymbol{g,\boldsymbol{b}} \right&gt;\end{equation}\]</span></p><p><span class="math display">\[\boldsymbol{R}(\boldsymbol{C})\overset{def.}{=} \{ (\boldsymbol{f},\boldsymbol{g}) \in \mathbb{R}^{n}\times \mathbb{R}^{m}: \boldsymbol{f}\oplus\boldsymbol{g} \leq\boldsymbol{C} \}\]</span></p><p>  考虑经济收益。由于供应商面对的原问题本质上是一个线性规划问题，故其满足强对偶关系，这意味着中间商可以通过合理地设置收费价格<span class="math inline">\(\boldsymbol{f},\boldsymbol{g}\)</span>使得：</p><p><span class="math display">\[\min_{\boldsymbol{P} \in\boldsymbol{U}(\boldsymbol{a},\boldsymbol{b})} \left&lt;\boldsymbol{P},\boldsymbol{C} \right&gt; =\max_{(\boldsymbol{f},\boldsymbol{g}) \in\boldsymbol{R}(\boldsymbol{C})} \left&lt; \boldsymbol{f},\boldsymbol{a}\right&gt; + \left&lt; \boldsymbol{g},\boldsymbol{b}\right&gt;\]</span></p><p>  值得注意的是，中间商所设置的价格 <span class="math inline">\(\boldsymbol{f},\boldsymbol{g}\)</span>可以为负数，这也很好理解，中间商可以补贴的形式，从供应商那里收取木材，即将<span class="math inline">\(\boldsymbol{f}\)</span>的某些值设置为负数；而对于需要木材的城市，收取高额的分发费用，即增加<span class="math inline">\(\boldsymbol{g}\)</span>的某些分量的值，这样对于中间商来说，它仍然可以保证利润不变，但却给供应商营造了一种获利的错觉。</p><h2 id="reference">Reference</h2><ul><li><strong>[1] Book: Peyré G, Cuturi M. Computational optimaltransport[J]. Center for Research in Economics and Statistics WorkingPapers, 2017 (2017-86).</strong><br></li><li><strong>[2] Lecture: 李向东. 最优传输理论及其应用.BIMSA</strong></li></ul>]]></content>
      
      
      <categories>
          
          <category> 最优传输理论 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Optimal Transport-2.Wasserstein Distance</title>
      <link href="/2024/02/15/Optimal%20Transport-2.Wassertein%20Distance/"/>
      <url>/2024/02/15/Optimal%20Transport-2.Wassertein%20Distance/</url>
      
        <content type="html"><![CDATA[<h1 id="wasserstein-distance">Wasserstein Distance</h1><p>  在上一节 Monge-Kantorovich Problem中，我们介绍了最优传输问题。最优传输一个重要的应用是它可以用来衡量分布之间的距离，从而将距离的概念由点与点之间拓展到分布与分布之间。本节我们将介绍分布之间的距离定义，即Wasserstein Distance，以及为什么其能够用于表示分布之间的距离。</p><h2 id="metric-properties-on-probility-space">Metric Properties onProbility Space</h2><p>  在上一节中，我们从概率视角描述了最优传输问题。设 <span class="math inline">\(X,Y\)</span> 是服从分布 <span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta}\)</span>的两个随机变量，运输矩阵为 <span class="math inline">\(\boldsymbol{P}\)</span>，成本矩阵为 <span class="math inline">\(\boldsymbol{C}\)</span>，则分布 <span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta}\)</span>之间的最优传输问题可以被定义为：</p><p><span class="math display">\[L_{\boldsymbol{C}}(\boldsymbol{\alpha},\boldsymbol{\beta}):= \min_{\boldsymbol{P} \in\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta})} \left&lt;\boldsymbol{P},\boldsymbol{C} \right&gt; = \min_{(X,Y)} \{\mathbb{E}_{(X,Y)}(c(X,Y)): X \sim \boldsymbol{\alpha}, Y \sim\boldsymbol{\beta} \}\]</span></p><p>  <span class="math inline">\(L_{\boldsymbol{C}}(\boldsymbol{\alpha},\boldsymbol{\beta})\)</span>的含义是将分布 <span class="math inline">\(\boldsymbol{\alpha}\)</span>传输到分布 <span class="math inline">\(\boldsymbol{\beta}\)</span>所花费的最小成本，我们很自然地就会想到 <span class="math inline">\(L_{\boldsymbol{C}}(\boldsymbol{\alpha},\boldsymbol{\beta})\)</span>也许能够表示分布 <span class="math inline">\(\boldsymbol{\alpha}\)</span> 和 <span class="math inline">\(\boldsymbol{\beta}\)</span>之间的距离或相似度。当然，要说明这个问题，我们需要证明函数 <span class="math inline">\(L_{\boldsymbol{C}}\)</span>满足概率空间中距离函数的性质。<br>  设分布 $, $ 取自概率空间 <span class="math inline">\(\mathcal{X}\)</span>，<span class="math inline">\(W(\boldsymbol{\alpha},\boldsymbol{\beta})\)</span>是分布 <span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta}\)</span>之间的距离函数，如果函数 <span class="math inline">\(W\)</span>满足:</p><ul><li><strong>非负性(Non-negativity):</strong> 对 <span class="math inline">\(\forall \boldsymbol{\alpha},\boldsymbol{\beta} \in\mathcal{X}, W(\boldsymbol{\alpha},\boldsymbol{\beta}) \ge0.\)</span><br></li><li><strong>同一性(Identity of Indiscernibles):</strong> <span class="math inline">\(W(\boldsymbol{\alpha},\boldsymbol{\beta})=0\)</span>当且仅当 <span class="math inline">\(\boldsymbol{\alpha} =\boldsymbol{\beta}.\)</span><br></li><li><strong>对称性(Symmetry):</strong> <span class="math inline">\(\forall \boldsymbol{\alpha},\boldsymbol{\beta} \in\mathcal{X}, W(\boldsymbol{\alpha},\boldsymbol{\beta}) =W(\boldsymbol{\beta},\boldsymbol{\alpha}).\)</span><br></li><li><strong>三角不等式(Triangle Inequality):</strong> <span class="math inline">\(\forall\boldsymbol{\alpha},\boldsymbol{\beta},\boldsymbol{\gamma} \in\mathcal{X}, W(\boldsymbol{\alpha},\boldsymbol{\gamma}) \leqW(\boldsymbol{\alpha},\boldsymbol{\beta})+W(\boldsymbol{\beta},\boldsymbol{\gamma}).\)</span></li></ul><p>  学者们通过研究发现，当对成本矩阵 <span class="math inline">\(\boldsymbol{C}\)</span>设置一些条件后，可以使得概率空间中最优传输问题的解 <span class="math inline">\(L_{\boldsymbol{C}}(\boldsymbol{\alpha},\boldsymbol{\beta})\)</span>满足距离函数的性质，从而使得其可以用于衡量分布之间的距离。</p><h2 id="wasserstein-distance-1">Wasserstein Distance</h2><h3 id="definition">Definition</h3><p>  我们首先来定义离散分布下的 Wasserstein Distance。设 <span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta} \in\sum_{n}:=\{ \boldsymbol{x} \in \mathbb{R}^{n}_{+}:\boldsymbol{x^{T}}\mathbf{1}_{n}=1 \}\)</span>，设矩阵 <span class="math inline">\(\boldsymbol{D} \in \mathbb{R}^{n \timesn}\)</span> 是一个度量矩阵，即矩阵 <span class="math inline">\(\boldsymbol{D}\)</span> 满足:<br><strong>(1)</strong> <span class="math inline">\(\boldsymbol{D} \in\mathbb{R}^{n \times n}_{+};\)</span><br><strong>(2)</strong> <span class="math inline">\(\boldsymbol{D}_{i,j}=0\)</span>，当且仅当 <span class="math inline">\(i=j\)</span>;<br><strong>(3)</strong> <span class="math inline">\(\boldsymbol{D}\)</span>是对称矩阵;<br><strong>(4)</strong> <span class="math inline">\(\forall i,j,k \in \{1,\dotsb,n\}, \boldsymbol{D}_{i,k} \leq\boldsymbol{D}_{i,j}+\boldsymbol{D}_{j,k}\)</span>.<br>令成本矩阵 <span class="math inline">\(\boldsymbol{C} =\boldsymbol{D}^{p}= \left[ \boldsymbol{D}_{i,j}^{p} \right]_{n \times n}\in \mathbb{R}^{n \times n}_{+}(p \ge 1)\)</span>，定义：</p><p><span class="math display">\[W_{p}(\boldsymbol{\alpha},\boldsymbol{\beta}) :=L_{\boldsymbol{D}^{p}}(\boldsymbol{\alpha,\boldsymbol{\beta}})^{1/p}\]</span></p><p>则称 <span class="math inline">\(W_{p}(\boldsymbol{\alpha},\boldsymbol{\beta})\)</span>为概率分布 <span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta}\)</span>之间的 <strong>p-Wasserstein 距离</strong>。<br>  现在来证明<span class="math inline">\(W_{p}\)</span>可以作为概率空间<span class="math inline">\(\sum_{n}\)</span>上的距离函数。</p><h3 id="proof">Proof</h3><p><span class="math display">\[W_{p}(\boldsymbol{\alpha},\boldsymbol{\beta}) =L_{\boldsymbol{D}^{p}}(\boldsymbol{\alpha,\boldsymbol{\beta}})^{1/p} =\left( \min_{\boldsymbol{P} \in\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta})} \left&lt;\boldsymbol{P},\boldsymbol{D}^{p} \right&gt;\right)^{\frac{1}{p}}\]</span></p><p>其中 <span class="math inline">\(\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta})= \{ \boldsymbol{P} \in \mathbb{R}^{n \times n}_{+} :\boldsymbol{P}\mathbf{1}_{n}=\boldsymbol{\alpha} \quad and \quad\boldsymbol{P^{T}}\mathbf{1}_n=\boldsymbol{\beta} \}\)</span>.<br>  要证明 <span class="math inline">\(W_{p}\)</span>可以作为概率空间<span class="math inline">\(\sum_{n}\)</span>上的距离函数，则需要证明 <span class="math inline">\(W_{p}\)</span>满足概率空间中距离函数的性质，即非负性、同一性、对称性、三角不等式。<br>  <strong>(1) 非负性证明</strong><br>   <span class="math inline">\(\boldsymbol{P},\boldsymbol{D}^{p} \in\mathbb{R}^{n \times n}_{+} \Rightarrow \left&lt;\boldsymbol{P},\boldsymbol{D}^{p}\right&gt;=\sum_{ij}\boldsymbol{P}_{ij}\boldsymbol{D}^{p}_{ij} \ge 0\Rightarrow W_{p}(\boldsymbol{\alpha}, \boldsymbol{\beta}) \ge0.\)</span><br>  <strong>(2) 同一性证明</strong><br>  由度量矩阵的性质可知: <span class="math inline">\(\boldsymbol{D}_{i,i}=0, \forall i \in \{1,\dotsb,n \}\)</span>，则有 <span class="math inline">\(\boldsymbol{D}_{i,i}^{p}=0\)</span>，即成本矩阵<span class="math inline">\(\boldsymbol{D}^{p}\)</span>的对角线元素均为零。<br>  当 <span class="math inline">\(\boldsymbol{\alpha}=\boldsymbol{\beta}\)</span>时，可行域 <span class="math inline">\(\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\alpha})= \{ \boldsymbol{P} \in \mathbb{R}^{n \times n}_{+} :\boldsymbol{P}\mathbf{1}_{n}=\boldsymbol{P^{T}}\mathbf{1}_n=\boldsymbol{\alpha}\}\)</span>，则 <span class="math inline">\(\boldsymbol{P}^{*}=diag(\boldsymbol{\alpha}) \in\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\alpha})\)</span>，此时：</p><p><span class="math display">\[\left&lt;  \boldsymbol{P}^{*},\boldsymbol{D}^{p}\right&gt;=\sum_{i}\boldsymbol{\alpha}_{i}\boldsymbol{D}_{i,i}^{p}=0\RightarrowW_{p}(\boldsymbol{\alpha},\boldsymbol{\alpha})=0\]</span></p><p>  当 <span class="math inline">\(W_{p}(\boldsymbol{\alpha},\boldsymbol{\beta})=0\)</span>时，由于成本矩阵 <span class="math inline">\(\boldsymbol{D}^{p}\)</span>的非对角线元素均大于零，故运输矩阵 <span class="math inline">\(\boldsymbol{P}\)</span>的非对角线元素均为零，即运输矩阵 <span class="math inline">\(\boldsymbol{P}\)</span> 为对角矩阵，<span class="math inline">\(\boldsymbol{P}=\boldsymbol{P}^{T}\)</span>. 此时有<span class="math inline">\(\boldsymbol{P}\mathbf{1}_{n}=\boldsymbol{P^{T}}\mathbf{1}_n\)</span>，即<span class="math inline">\(\boldsymbol{\alpha}=\boldsymbol{\beta}\)</span>.<br>  <strong>(3) 对称性证明</strong><br>  设 <span class="math inline">\(\boldsymbol{P}^{*}\)</span> 为<span class="math inline">\(W_{p}(\boldsymbol{\alpha},\boldsymbol{\beta})\)</span>所对应的最优运输矩阵，则有:</p><p><span class="math display">\[W_{p}(\boldsymbol{\alpha},\boldsymbol{\beta})=\left&lt;  \boldsymbol{P}^{*},\boldsymbol{D}^{p}\right&gt;^{\frac{1}{p}}\]</span></p><p>  由于成本矩阵 <span class="math inline">\(\boldsymbol{D}^{p}\)</span> 是对称矩阵，故有：</p><p><span class="math display">\[W_{p}(\boldsymbol{\alpha},\boldsymbol{\beta})=\left&lt;  \boldsymbol{P}^{*},\boldsymbol{D}^{p}\right&gt;^{\frac{1}{p}}=\left&lt;  \boldsymbol{(P^{*})^{T}},\boldsymbol{D}^{p}\right&gt;^{\frac{1}{p}}\]</span></p><p>  <span class="math inline">\(\boldsymbol{(P^{*})^{T}}\mathbf{1}_{n}=\boldsymbol{\beta},\boldsymbol{P}^{*}\mathbf{1}_{n}=\boldsymbol{\alpha} \Rightarrow\boldsymbol{(P^{*})^{T}} \in\boldsymbol{U}(\boldsymbol{\beta},\boldsymbol{\alpha})\)</span>. 由于<span class="math inline">\(\boldsymbol{U}(\boldsymbol{\beta},\boldsymbol{\alpha})\)</span>与 <span class="math inline">\(\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta})\)</span>中的运输矩阵是对应转置的关系，故有：</p><p><span class="math display">\[W_{p}(\boldsymbol{\beta},\boldsymbol{\alpha})=\left(\min_{\boldsymbol{P} \in\boldsymbol{U}(\boldsymbol{\beta},\boldsymbol{\alpha})} \left&lt;\boldsymbol{P},\boldsymbol{D}^{p} \right&gt;\right)^{\frac{1}{p}}=\left&lt;  \boldsymbol{(P^{*})^{T}},\boldsymbol{D}^{p}\right&gt;^{\frac{1}{p}}\]</span></p><p><span class="math display">\[\RightarrowW_{p}(\boldsymbol{\alpha},\boldsymbol{\beta}) =W_{p}(\boldsymbol{\beta},\boldsymbol{\alpha})\]</span><br>  <strong>(4) 三角不等式性质证明</strong><br>  设 <span class="math inline">\(\boldsymbol{\gamma} \in\sum_{n}\)</span>, 现证明：<span class="math inline">\(W_{p}(\boldsymbol{\alpha},\boldsymbol{\gamma})\leqW_{p}(\boldsymbol{\alpha},\boldsymbol{\beta})+W_{p}(\boldsymbol{\beta},\boldsymbol{\gamma})\)</span>.<br>  设 <span class="math inline">\(\boldsymbol{P}\)</span> 是 <span class="math inline">\(W_{p}(\boldsymbol{\alpha},\boldsymbol{\beta})\)</span>所对应的最优运输矩阵，<span class="math inline">\(\boldsymbol{Q}\)</span> 是 <span class="math inline">\(W_{p}(\boldsymbol{\beta},\boldsymbol{\gamma})\)</span>所对应的最优运输矩阵，则有</p><p><span class="math display">\[\begin{split}    W_{p}(\boldsymbol{\alpha},\boldsymbol{\beta}) &amp;=\left&lt;  \boldsymbol{P},\boldsymbol{D}^{p} \right&gt;^{\frac{1}{p}} =\left(\sum_{ij}\boldsymbol{P}_{ij}\boldsymbol{D}^{p}_{ij}\right)^{\frac{1}{p}}  \\    W_{p}(\boldsymbol{\beta},\boldsymbol{\gamma}) &amp;=\left&lt;  \boldsymbol{Q},\boldsymbol{D}^{p} \right&gt;^{\frac{1}{p}} =\left(\sum_{ij}\boldsymbol{Q}_{ij}\boldsymbol{D}^{p}_{ij}\right)^{\frac{1}{p}}  \\\end{split}\]</span></p><p>  定义：</p><p><span class="math display">\[\tilde{\boldsymbol{\beta}} =[\tilde{\boldsymbol{\beta}}_{j}],\quad \tilde{\boldsymbol{\beta}}_{j} =\left \{ \begin{array}{lr}    \boldsymbol{\beta}_{j}, \quad\boldsymbol{\beta}_{j} &gt; 0 \\    1, \quad\boldsymbol{\beta}_{j} = 0\end{array} \right.\]</span></p><p><span class="math display">\[\boldsymbol{S} :=\boldsymbol{P}diag(1/\tilde{\boldsymbol{\beta}})\boldsymbol{Q} \in\mathbb{R}^{n \times n}_{+}\]</span></p><p>  则有：</p><p><span class="math display">\[\begin{split}    \boldsymbol{S}\mathbf{1}_{n} &amp;=\boldsymbol{P}diag(1/\tilde{\boldsymbol{\beta}})\boldsymbol{Q}\mathbf{1}_{n}=\boldsymbol{P}diag(1/\tilde{\boldsymbol{\beta}})\boldsymbol{\beta}  \\    &amp;=\boldsymbol{P}\boldsymbol{[\boldsymbol{\beta}_{j}/\tilde{\boldsymbol{\beta}}_{j}]_{n}}= \boldsymbol{P}\mathbf{1}_{Supp(\boldsymbol{\beta})} =\boldsymbol{P}\mathbf{1}_{n} \\    &amp;= \boldsymbol{\alpha}\end{split}\]</span></p><p>  同理可得：<span class="math inline">\(\boldsymbol{S}^{T}\mathbf{1}_{n}=\boldsymbol{\gamma}\)</span>，则可以得到:<span class="math inline">\(\boldsymbol{S} \in\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\gamma})\)</span>.</p><p><span class="math display">\[\begin{split}    W_{p}(\boldsymbol{\alpha}, \boldsymbol{\gamma}) &amp;= \left(\min_{\boldsymbol{P} \in\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\gamma})} \left&lt;\boldsymbol{P},\boldsymbol{D}^{p} \right&gt; \right)^{\frac{1}{p}} \leq\left&lt; \boldsymbol{S},\boldsymbol{D}^{p} \right&gt;^{\frac{1}{p}}  \\    &amp;= \left(  \sum_{ik}\boldsymbol{D}_{ik}^{p}\boldsymbol{S}_{ik}\right)^{\frac{1}{p}} =\left(  \sum_{ik}\boldsymbol{D}_{ik}^{p}\sum_{j}\frac{\boldsymbol{P}_{ij}\boldsymbol{Q}_{jk}}{\tilde{\boldsymbol{\beta}}_{j}}\right)^{\frac{1}{p}} =\left(  \sum_{ijk}\boldsymbol{D}_{ik}^{p}\frac{\boldsymbol{P}_{ij}\boldsymbol{Q}_{jk}}{\tilde{\boldsymbol{\beta}}_{j}}\right)^{\frac{1}{p}}  \\    &amp; \leq\left(  \sum_{ijk}(\boldsymbol{D}_{ij}+\boldsymbol{D}_{jk})^{p}\frac{\boldsymbol{P}_{ij}\boldsymbol{Q}_{jk}}{\tilde{\boldsymbol{\beta}}_{j}}\right)^{\frac{1}{p}} \leq\left(  \sum_{ijk}\boldsymbol{D}_{ij}^{p}\frac{\boldsymbol{P}_{ij}\boldsymbol{Q}_{jk}}{\tilde{\boldsymbol{\beta}}_{j}}\right)^{\frac{1}{p}} +\left(  \sum_{ijk}\boldsymbol{D}_{jk}^{p}\frac{\boldsymbol{P}_{ij}\boldsymbol{Q}_{jk}}{\tilde{\boldsymbol{\beta}}_{j}}\right)^{\frac{1}{p}}  \\    &amp;=\left(  \sum_{ij}\boldsymbol{D}_{ij}^{p}\boldsymbol{P}_{ij}\sum_{k}\frac{\boldsymbol{Q}_{jk}}{\tilde{\boldsymbol{\beta}}_{j}}\right)^{\frac{1}{p}} +\left(  \sum_{jk}\boldsymbol{D}_{jk}^{p}\boldsymbol{Q}_{jk}\sum_{i}\frac{\boldsymbol{P}_{ij}}{\tilde{\boldsymbol{\beta}}_{j}}\right)^{\frac{1}{p}}  \\    &amp;= \left(  \sum_{ij}\boldsymbol{D}_{ij}^{p}\boldsymbol{P}_{ij}\right)^{\frac{1}{p}} +\left(  \sum_{jk}\boldsymbol{D}_{jk}^{p}\boldsymbol{Q}_{jk}\right)^{\frac{1}{p}}  \\    &amp;= W_{p}(\boldsymbol{\alpha},\boldsymbol{\beta}) +W_{p}(\boldsymbol{\beta},\boldsymbol{\gamma})  \\\end{split}\]</span></p><p>  故有：</p><p><span class="math display">\[W_{p}(\boldsymbol{\alpha},\boldsymbol{\gamma})\leqW_{p}(\boldsymbol{\alpha},\boldsymbol{\beta})+W_{p}(\boldsymbol{\beta},\boldsymbol{\gamma})\]</span></p><p>  综上所述，<span class="math inline">\(W_{p}\)</span>可以作为概率空间<span class="math inline">\(\sum_{n}\)</span>上的距离函数。</p><h2 id="ground-cost">Ground Cost</h2><p>  证明了<span class="math inline">\(W_{p}\)</span>可以作为概率空间<span class="math inline">\(\sum_{n}\)</span>上的距离函数。接下来我们就可以考虑如何定义度量矩阵<span class="math inline">\(\boldsymbol{D}\)</span>，从而生成成本矩阵<span class="math inline">\(\boldsymbol{C}\)</span>, 得到成本矩阵<span class="math inline">\(\boldsymbol{C}\)</span>后，我们便可以来计算分布<span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta}\)</span>之间的 Wasserstein距离。当我们在欧式空间中考虑最优传输问题时，一种常用的生成成本矩阵<span class="math inline">\(\boldsymbol{C}\)</span>的方法是 <strong>GroundCost</strong>。<br>  Ground Cost 使用原始分布与目标分布的取值之差的 <span class="math inline">\(L_2\)</span> 范数来定义度量矩阵 <span class="math inline">\(\boldsymbol{D}\)</span>，容易验证矩阵 <span class="math inline">\(\boldsymbol{D}\)</span>满足度量矩阵的性质，然后使用度量矩阵的平方生成成本矩阵 <span class="math inline">\(\boldsymbol{C}\)</span>，即 <span class="math inline">\(\boldsymbol{C}=\boldsymbol{D}^2\)</span>，故Ground Cost 是欧式空间中的一种 2-Wasserstein 距离。<br>  仍然考虑离散分布 <span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta}\)</span>，设:</p><p><span class="math display">\[\boldsymbol{\alpha} = \begin{bmatrix}    \alpha_1 \\    \alpha_2 \\    \vdots \\    \alpha_n \\\end{bmatrix}, \quad \boldsymbol{\beta} = \begin{bmatrix}    \beta_1 \\    \beta_2 \\    \vdots \\    \beta_n \\\end{bmatrix}\]</span></p><p>则分布 <span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta}\)</span>的分布列可以写成：</p><style>.center {  width: auto;  display: table;  margin-left: auto;  margin-right: auto;}</style><p align="center"><font face="黑体" size="2."></font></p><div class="center"><table><thead><tr class="header"><th style="text-align: center;"></th><th style="text-align: center;">1</th><th style="text-align: center;">2</th><th style="text-align: center;"><span class="math inline">\(\cdots\)</span></th><th style="text-align: center;">n</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">p</td><td style="text-align: center;"><span class="math inline">\(\alpha_1\)</span></td><td style="text-align: center;"><span class="math inline">\(\alpha_2\)</span></td><td style="text-align: center;"><span class="math inline">\(\cdots\)</span></td><td style="text-align: center;"><span class="math inline">\(\alpha_n\)</span></td></tr></tbody></table><table><thead><tr class="header"><th style="text-align: center;"></th><th style="text-align: center;">1</th><th style="text-align: center;">2</th><th style="text-align: center;"><span class="math inline">\(\cdots\)</span></th><th style="text-align: center;">n</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">p</td><td style="text-align: center;"><span class="math inline">\(\beta_1\)</span></td><td style="text-align: center;"><span class="math inline">\(\beta_2\)</span></td><td style="text-align: center;"><span class="math inline">\(\cdots\)</span></td><td style="text-align: center;"><span class="math inline">\(\beta_n\)</span></td></tr></tbody></table></div><p>定义度量矩阵<span class="math inline">\(\boldsymbol{D}\)</span>为离散分布<span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta}\)</span>取值之差的<span class="math inline">\(L_2\)</span>范数：</p><p><span class="math display">\[\boldsymbol{D} =[\boldsymbol{D}_{ij}]_{n \times n}=[ ||i-j||_{2} ]_{n \times n} =\begin{bmatrix}    0 &amp; \cdots &amp; ||1-n||_{2}  \\    \vdots &amp; &amp; \vdots \\    ||n-1||_{2} &amp; \cdots &amp; 0 \\\end{bmatrix}\]</span></p><p>定义成本矩阵<span class="math inline">\(\boldsymbol{C}\)</span>为度量矩阵<span class="math inline">\(\boldsymbol{D}\)</span>的平方：</p><p><span class="math display">\[ \boldsymbol{C} = \boldsymbol{D}^{2} =[\boldsymbol{D}_{ij}^{2}]_{n \times n} = \begin{bmatrix}    0 &amp; \cdots &amp; ||1-n||_{2}^{2}  \\    \vdots &amp; &amp; \vdots \\    ||n-1||_{2}^{2} &amp; \cdots &amp; 0 \\\end{bmatrix}\]</span></p><p>概率分布 <span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta}\)</span> 之间的 Wasserstein 距离可以被定义为：</p><p><span class="math display">\[W_{2}(\boldsymbol{\alpha},\boldsymbol{\beta}) =L_{\boldsymbol{C}}(\boldsymbol{\alpha,\boldsymbol{\beta}})^{1/2} =\left( \min_{\boldsymbol{P} \in\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta})} \left&lt;\boldsymbol{P},\boldsymbol{C} \right&gt;\right)^{\frac{1}{2}}\]</span></p><h2 id="example">Example</h2><p>  我们用一个实际的例子来展示如何基于 Ground Cost 来计算离散分布之间的Wasserstein 距离。我们将使用Python中专门用于OT问题的库<strong>POT</strong> 来完成这个实例的计算。首先导入所需要的包：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> ot<br></code></pre></td></tr></tbody></table></figure><p>  假设离散分布 <span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta} \in \sum_{5}:=\{ \boldsymbol{x} \in\mathbb{R}^{5}_{+}: \boldsymbol{x^{T}}\mathbf{1}_{5}=1 \}\)</span>：</p><p><span class="math display">\[\boldsymbol{\alpha} = \begin{bmatrix}    0.1 \\    0.3 \\    0.2 \\      0.1 \\    0.3 \\\end{bmatrix}, \quad \boldsymbol{\beta} = \begin{bmatrix}    0.1 \\    0.3 \\    0.2 \\    0.3 \\    0.1 \\\end{bmatrix}\]</span></p><p>  画出离散分布 <span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta}\)</span> 的概率分布直方图：</p><div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> pbar(x,y,color,title):</span><span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    plt.bar(x,y, width<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span>color,alpha<span class="op">=</span><span class="fl">0.7</span>)</span><span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    plt.title(title)</span><span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'Value'</span>)</span><span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'Probability'</span>)</span><span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 1*2 plot</span></span><span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> multiplot(x,y_1,y_2):</span><span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">4</span>))</span><span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">1</span>)</span><span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    pbar(x,y_1, color<span class="op">=</span><span class="st">"blue"</span>, title<span class="op">=</span><span class="st">'alpha distribution'</span>)</span><span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>)</span><span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    pbar(x,y_2, color<span class="op">=</span><span class="st">"green"</span>, title<span class="op">=</span><span class="st">'beta distribution'</span>)</span><span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    plt.show()</span><span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span><span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># values of probalility distribution</span></span><span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.array([<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>])</span><span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># probability vector</span></span><span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> np.array([<span class="fl">0.1</span>,<span class="fl">0.3</span>,<span class="fl">0.2</span>,<span class="fl">0.1</span>,<span class="fl">0.3</span>])</span><span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> np.array([<span class="fl">0.1</span>,<span class="fl">0.3</span>,<span class="fl">0.2</span>,<span class="fl">0.3</span>,<span class="fl">0.1</span>])</span><span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># draw distribution barplot</span></span><span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>multiplot(x,a,b)</span></code></pre></div><p>得到的图像为：</p><center><img src="https://s2.loli.net/2024/03/04/amv6pIF52GMkEds.png" width="60%" height="60%"><div data-align="center">Image1: 原始分布与目标分布的直方图</div></center><p><br></p><p>  基于 Ground Cost 我们可以定义成本矩阵 <span class="math inline">\(\boldsymbol{C}\)</span>，相应的代码为：</p><div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ground cost</span></span><span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ground_cost(n):</span><span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    C <span class="op">=</span> np.zeros((n, n))</span><span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span><span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(n):</span><span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> i<span class="op">+</span><span class="dv">1</span></span><span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> j<span class="op">+</span><span class="dv">1</span></span><span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>            C[i][j] <span class="op">=</span> (x<span class="op">-</span>y)<span class="op">**</span><span class="dv">2</span></span><span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> C</span><span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span><span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> ground_cost(<span class="dv">5</span>)</span></code></pre></div><p>  得到的成本矩阵<span class="math inline">\(\boldsymbol{C}\)</span>为：</p><p><span class="math display">\[\boldsymbol{C} = \begin{bmatrix}    0 &amp; 1 &amp; 4 &amp; 9 &amp; 16 \\    1 &amp; 0 &amp; 1 &amp; 4 &amp; 9 \\    4 &amp; 1 &amp; 0 &amp; 1 &amp; 4 \\    9 &amp; 4 &amp; 1 &amp; 0 &amp; 1 \\    16 &amp; 9 &amp; 4 &amp; 1 &amp; 0 \\\end{bmatrix}\]</span></p><p>则概率分布 <span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta}\)</span> 之间的 Wasserstein 距离可以被定义为：</p><p><span class="math display">\[W_{2}(\boldsymbol{\alpha},\boldsymbol{\beta}) =L_{\boldsymbol{C}}(\boldsymbol{\alpha,\boldsymbol{\beta}})^{1/2} =\left( \min_{\boldsymbol{P} \in\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta})} \left&lt;\boldsymbol{P},\boldsymbol{C} \right&gt;\right)^{\frac{1}{2}}\]</span></p><p>  我们可以使用POT库的API来求解离散分布 <span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta}\)</span>之间的最优传输矩阵 <span class="math inline">\(P^{*}\)</span> 以及Wasserstein 距离 <span class="math inline">\(W_{2}(\boldsymbol{\alpha},\boldsymbol{\beta})\)</span>，其代码如下：</p><div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># optimal transport matrix</span></span><span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> ot.emd(a, b, C)</span><span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># wasserstein distence</span></span><span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>wasserstein_distence <span class="op">=</span> ot.emd2(a, b, C)</span><span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span><span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(P.<span class="bu">round</span>(<span class="dv">4</span>))</span><span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">round</span>(np.sqrt(wasserstein_distence),<span class="dv">4</span>))</span></code></pre></div><p>求解结果如下：</p><p><span class="math display">\[\boldsymbol{P}^{*} = \begin{bmatrix}    0.1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\    0 &amp; 0.3 &amp; 0 &amp; 0 &amp; 0 \\    0 &amp; 0 &amp; 0.2 &amp; 0 &amp; 0 \\    0 &amp; 0 &amp; 0 &amp; 0.1 &amp; 0 \\    0 &amp; 0 &amp; 0 &amp; 0.2 &amp; 0.1 \\\end{bmatrix},\quadW_{2}(\boldsymbol{\alpha},\boldsymbol{\beta})=0.4472\]</span></p><h2 id="reference">Reference</h2><ul><li><strong>[1] Book: Peyré G, Cuturi M. Computational optimaltransport[J]. Center for Research in Economics and Statistics WorkingPapers, 2017 (2017-86).</strong></li></ul>]]></content>
      
      
      <categories>
          
          <category> 最优传输理论 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Optimal Transport-1.Monge-Kantorovich Problem</title>
      <link href="/2024/02/07/Optimal%20Transport-1-Monge-Kantorovich%20Problem/"/>
      <url>/2024/02/07/Optimal%20Transport-1-Monge-Kantorovich%20Problem/</url>
      
        <content type="html"><![CDATA[<h1 id="monge-kantorovich-problem">Monge-Kantorovich Problem</h1><p>  最优传输理论(Optimal TransportTheroy)是应用数学的一个分支，主要研究的是概率分布之间的最优转移方式以及相关的距离度量。它的核心思想是通过最小化两个概率分布之间的转移成本，来定义这两个分布之间的距离。<br>  最优传输问题的历史可以追溯到18世纪。著名法国数学家 Monge在1781年提出了最优传输问题的早期形式，即Monge-Problem。他关注的是如何以最小的成本将一个土堆移动到另一个位置，这被看作是最优传输理论的起源。时至今日，很多介绍最优传输理论的科普文章依然喜欢使用“土堆移动、推土机”等例子来论述最优传输理论。Monge的工作奠定了这一理论的基础。另外一个对最优传输理论的发展具有关键作用的是前苏联数学家Kantorovich。Kantorovich在20世纪40年代从实际经济资源分配问题中重新导出了最优传输问题，他基于测度论对最优传输问题进行了严格的定义，并推导出了一系列重要的理论成果。Kantorovich的工作对最优传输理论的数学形式和理论基础的建立起到了关键作用。由于在线性规划和资源分配方面的重要贡献，Kantorovich于1975年获得诺贝尔经济学奖。此外还有多位数学家对最优传输理论的发展做出了重要贡献，Wassertein、Villani、Berg等数学家从理论、应用、数值计算等方面丰富了最优传输理论。<br>  最优传输理论的应用涵盖多个领域：</p><ul><li><strong>1.图像处理和计算机视觉:</strong>在图像处理中，最优传输理论被用来衡量图像之间的相似性，从而进行图像匹配、图像检索等任务。在计算机视觉领域，它被用于解决图像生成和变换的问题。<br></li><li><strong>2.机器学习:</strong>在机器学习中，最优传输理论被应用于生成模型、领域自适应等问题，以提高模型的泛化性能。<br></li><li><strong>3.经济学和金融学:</strong>在经济学中，最优传输理论被用于研究资源的分配和经济结构的演变。在金融学中，它可以用来量化不同资产之间的差异和联系。<br></li><li><strong>4.统计学和信息论:</strong>最优传输理论在统计学和信息论中也有广泛的应用，尤其是在测度空间中定义概率分布之间的距离。</li></ul><p>  总的来说，最优传输理论在多个领域都展现了广泛的应用，其在处理概率分布之间的关系和相似性的能力为研究者提供了有力的工具。</p><h2 id="monge-problem">Monge Problem</h2><p>  GaspardMonge(1746-1818)是法国著名的数学家和物理学家，他在数学、物理学和工程学领域做出了突出的贡献，被认为是现代微分几何的奠基人之一，对现代科学和数学的发展产生了深远的影响。<br>  在18世纪，第一次工业革命正在欧洲如火如荼地进行着，经济与军事活动催生了大量的数学、物理理论的诞生。最优传输理论的萌芽便来源于军事活动中所遇到的实际问题。彼时，英国与法国正在争夺欧洲霸权，为此进行了一系列的战争。在过去的战争中，防御工事的修建至关重要，修建防御工事需要将大量的土堆从某地转移到阵地，并堆砌成确定的形状。修建工事需要消耗大量的人力物力，这便催生了一个实际问题，<strong>如何将某一形状的土堆移动到另一个地方并堆成另一种形状，使得这整个过程中所消耗的资源最少。</strong><br>  Monge此时正在法国军队中担任工程研究人员，负责为军队提供有关土木工程和防御工事的重要建议。Monge敏锐地注意到了这个问题，并对其展开了研究。1781年，Monge发表了著作——<strong>Mémoiresur la théorie de déblais et deremblais(关于挖掘和填充的备忘录).</strong>Monge在著作中对该问题进行了论述，并提出了一种解决方案。<br>  Monge首先对所要研究的问题做出了一系列假设：</p><ul><li><strong>所要运输的土堆都是由质量相等的不可再分的分子所构成的，且土堆中分子的分布是均匀的。</strong></li><li><strong>每个分子的运输价格都是相等的，与该分子的重量和它所被传输的距离成正比。</strong></li><li><strong>总的运输价格是所运输的每个分子的质量乘以分子所传输的距离的总和。</strong></li></ul><p>基于以上的假设，Monge提出了"前向运输法"，并给出了从一维到三维的实例。</p><h3 id="点到点的最优传输">点到点的最优传输</h3><p>  我们首先来考虑最简单的两点运输情形，如下图1所示：</p><center><img src="https://s2.loli.net/2024/02/09/HvhSCsApP1a5Q38.jpg" width="40%" height="60%"><div data-align="center">Image1: 两点运输</div></center><p>A区域中有两个分子需要运输到B区域的指定位置，总共有两种运输方案，图1中分别用绿色与橙色的箭头表示。由三角形两边之和大于第三边可知，橙色方案的运输距离要大于绿色方案，又因为分子的质量是相同的，所有我们可以很容易得到，绿色方案的运输效率要高于橙色方案。<br>  再来考虑多点的情形，如下图2所示</p><center><img src="https://s2.loli.net/2024/02/09/vkGiCydSJEOT7BF.jpg" width="60%" height="60%"><div data-align="center">Image2: 多点运输</div></center><p>我们需要将点1、2、3运输到点4、5、6处，图2中给出了A、B、C三种运输方案。首先来比较方案A、B，方案A，B的不同在于点1、3到点5、6的运输路线，基于上文两点运输的思考，我们可以得知B方案的运输效率要高于方案A，同理，C方案的运输效率要高于B方案。<br>  基于以上的思考，Monge认为点到点的运输要遵循"前向法"，即运输路线不存在交叉。Monge同时将这种思想拓展到更高维的情形。</p><h3 id="平面到平面的最优运输">平面到平面的最优运输</h3><p>  现在考虑将某一平面区域运输到另一个平面区域，基于“前向法”，Monge给出了如下图3所示的运输方案：</p><center><img src="https://s2.loli.net/2024/02/09/2JhmZywrsRd9zpF.jpg" width="60%" height="60%"><div data-align="center">Image3: 平面运输</div></center><p>现在需要将平面区域Ⅰ中的分子运输到平面区域Ⅱ，<strong>Monge认为区域Ⅰ中的A、C两点要运输到区域Ⅱ的B、D两点，直线AB、CD相较于O点，由O点出发可以确定两条边界射线<span class="math inline">\(l_3,l_4\)</span>。在锥型区域中，可以用很多条射线将运输平面进行分割。例如夹角"非常小"的射线<span class="math inline">\(l_1,l_2\)</span>与区域Ⅰ相交于<span class="math inline">\(M_1、M_3、H_1、H_3\)</span>，与区域Ⅱ相交于<span class="math inline">\(M_4、M_6、H_4、H_6\)</span>，分割出区域<span class="math inline">\(M_1M_3H_1H_3\)</span>和<span class="math inline">\(M_4M_6H_4H_6\)</span>，根据"前向法"，区域Ⅰ中<span class="math inline">\(M_1M_3H_1H_3\)</span>的分子要运输到区域Ⅱ中<span class="math inline">\(M_4M_6H_4H_6\)</span>中，同时运输时也要遵循“前向法”，即橙色区域要运输到橙色区域，绿色区域要运送到绿色区域。由无数个分割出的小区域依据“前向法”进行运输，这种运输方式的所消耗的资源最少。</strong><br>  Monge借助微分几何的知识证明了这些从O点出发的运输射线都垂直于某一曲线R，并通过解析几何的知识求出了曲线R的解析式，以及最优运输的映射方程<span class="math inline">\(y =T(x)\)</span>。基于“前向法”，Monge同时也对三维运输的映射方程进行了求解，具体的求解过程这里不作详细的介绍，有兴趣的读者可自行查阅相关资料。</p><center><img src="https://s2.loli.net/2024/02/11/FG4H59woPsLAJMy.png" width="60%" height="60%"><div data-align="center">Image4: Monge著作中的作图</div></center><h3 id="待解决的问题">待解决的问题</h3><p>  虽然Monge借助“前向法”给出了最优传输问题的一种解决方法，但他同时也在著作中承认，实际的运输问题是非常复杂的，他的解法只是一种非常理想化的方法，并且这种方法也存在着缺陷，主要有以下几个方面:</p><ul><li><strong>1.被运输的每个分子的质量可能是不同的。</strong></li><li><strong>2.每个区域的密度有可能不同，即面积相同的区域所含有的分子数量可能不相等。</strong></li><li><strong>3.当目标区域是非凸区域时，“前向法”在中间区域的映射方程是无解的。</strong></li></ul><p>  在Monge-Problem被提出后，后世的学者也不断地对这一问题进行研究，但并没有取得突破性的进展。</p><h2 id="kantorovich-relaxation">Kantorovich Relaxation</h2><p>  19世纪-20世纪初，集合论、测度论、概率论等数学分支得到了充分的发展壮大，Monge-Problem的解决也迎来了转机。前苏联数学家kantorovich在解决Monge-Problem中起到了不可忽视的作用。然而，有趣的是，Kantorovich最初在解决这个问题时，并没有意识到自己所面对的问题与Monge-Problem之间的关联，直到将近10年后，Kantorovich才在一篇新的论文中阐述了自己的方法可以解决Monge-Problem。<br>  1938年，圣彼得堡胶合板托拉斯的研究人员找到圣彼得堡大学数学系，想要圣彼得堡大学的数学家们帮助解决在生产中所遇到的一个实际问题。<strong>托拉斯有八台不同类型的机器，每台机器能够生产五种不同型号的胶合板。每种类型的机器生产这五种胶合板的效率不同，各种不同的胶合板的产量是由现有的需求量决定的。为了在最短的时间内生产这些胶合板，应该如何给每种类型的机器分配生产任务？</strong><br>  与高深的数学理论相比，这看起来是一个非常简单的数学问题，仅仅只需要求解方程组而已，然而要求出这个问题的最优解却并不是一个简单的工作，因为当时有关线性规划的理论还没有被提出。彼时，26岁的Kantorovich正在圣彼得堡大学担任教职，这个问题最终交由他来研究。1939年，Kantorovich发表了论文《数学方法中的问题理论》，以经济资源分配与运输问题为背景，正式提出了"最优传输问题"，这一工作被认为是线性规划理论的先驱性工作。1947年，kantorovich发表论文《关于数学规划问题的一般理论》，正式提出了线性规划理论及其一般解法。</p><h3 id="最优传输问题">最优传输问题</h3><p>  我们来考虑一个实际问题，有 n 个生产木材的工厂，生产的木材需要供应给m 个城市，每个工厂每个月的产能是固定的，记为 <span class="math inline">\(\boldsymbol{a}, \boldsymbol{a} \in\mathbb{R}^{n}\)</span>；每个城市每个月木材的需求量也是固定的，记为<span class="math inline">\(\boldsymbol{b}, \boldsymbol{b} \in\mathbb{R}^{m}\)</span>。记 <span class="math inline">\(\boldsymbol{P} =[p_{ij}]_{n \times m} \in \mathbb{R}^{n \times m}_{+}\)</span>表示运输方案，其中 <span class="math inline">\(p_{ij}\)</span> 表示第 i个工厂运输到第 j 个城市的木材量。记 <span class="math inline">\(\boldsymbol{C} = [c_{ij}]_{n \times m} \in\mathbb{R}^{n \times m}_{+}\)</span> 表示运输成本，其中 <span class="math inline">\(c_{ij}\)</span>表示将单位木材从第i个工厂运输到第j个城市的成本。记：</p><p><span class="math display">\[\boldsymbol{U}(\boldsymbol{a},\boldsymbol{b}) =\{ \boldsymbol{P} \in \mathbb{R}^{n \times m}_{+}: \boldsymbol{P}\mathbf{1}_{m}=\boldsymbol{a} \quad and \quad \boldsymbol{P^{T}}\mathbf{1}_{n}=\boldsymbol{b} \}\]</span></p><p>  则最优传输问题可以被定义为：</p><p><span class="math display">\[L_{\boldsymbol{C}}(\boldsymbol{a},\boldsymbol{b}) \quad \overset{\text{def.}}{=} \quad\min_{\boldsymbol{P} \in \boldsymbol{U}(\boldsymbol{a},\boldsymbol{b})}\left&lt; \boldsymbol{C},\boldsymbol{P}\right&gt;_{F}=\sum_{i,j}p_{ij}c_{ij}\]</span></p><p><span class="math inline">\(L_{\boldsymbol{C}}(\boldsymbol{a},\boldsymbol{b})\)</span>即为最优运输方案。由于目标函数是线性函数，且<span class="math inline">\(\boldsymbol{U}(\boldsymbol{a},\boldsymbol{b})\)</span>为 <span class="math inline">\(n+m\)</span>个等式定义的凸多胞体，故最优传输问题是一个典型的线性规划问题。</p><h3 id="概率视角">概率视角</h3><p>  Kantorovich使用测度论定义了被运输的对象。在Monge的理论中，运输是确定性的，即每个分子都不可再分，且会被整体被运输到另一个位置，<strong>Kantorovich放松了这种确定性条件，他认为每个分子是可以进行切分的，一个分子所包含的质量可以被运输到不同的位置，这种运输是具有概率性的</strong>。</p><h4 id="离散概率分布运输问题">离散概率分布运输问题</h4><p>  设 <span class="math inline">\(X, Y\)</span>是两个服从多项分布的随机变量，取值于 <span class="math inline">\(\{1,2,\dotsb, d \}\)</span>。<span class="math inline">\(X,Y\)</span>的概率分布 <span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta}\)</span>取自概率单纯形 <span class="math inline">\(\sum_{d}:=\{ x \in\mathbb{R}^{d}_{+}: \boldsymbol{x^{T}}\mathbf{1}_{d}=1\}\)</span>。运输矩阵 <span class="math inline">\(\boldsymbol{P} \in\mathbb{R}^{d \times d}_{+}\)</span>。记:</p><p><span class="math display">\[\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta}):= \{ \boldsymbol{P} \in \mathbb{R}^{d \times d}_{+} :\boldsymbol{P}\mathbf{1}_{d}=\boldsymbol{\alpha} \quad and \quad\boldsymbol{P^{T}}\mathbf{1}_d=\boldsymbol{\beta} \}\]</span></p><p>从概率视角看，集合 <span class="math inline">\(\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta})\)</span>包含了随机变量<span class="math inline">\(X,Y\)</span>所有可能的联合分布<span class="math inline">\(\pi(X,Y)\)</span>，即矩阵<span class="math inline">\(\boldsymbol{P}=[p_{ij}]_{d \times d} =[\pi(x=i,y=j)]\)</span>。设成本矩阵为<span class="math inline">\(\boldsymbol{C} \in \mathbb{R}^{d \timesd}_{+}\)</span>。则多项分布 <span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta}\)</span>之间的最优传输问题可以被定义为：</p><p><span class="math display">\[L_{\boldsymbol{C}}(\boldsymbol{\alpha},\boldsymbol{\beta}):= \min_{\boldsymbol{P} \in\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta})} \left&lt;\boldsymbol{P},\boldsymbol{C} \right&gt; = \min_{(X,Y)} \{\mathbb{E}_{(X,Y)}(c(X,Y)): X \sim \boldsymbol{\alpha}, Y \sim\boldsymbol{\beta} \}\]</span></p><p><span class="math inline">\((X,Y)\)</span>表示取值于<span class="math inline">\(\mathcal{X} \times\mathcal{Y}\)</span>的联合分布。</p><h4 id="连续概率分布运输问题">连续概率分布运输问题</h4><p>  连续分布的运输问题与离散问题相似，不同点在于随机变量 <span class="math inline">\(X,Y\)</span> 服从的是连续分布 <span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta}\)</span>，最优运输同样可以定义为：</p><p><span class="math display">\[L_{\boldsymbol{C}}(\boldsymbol{\alpha},\boldsymbol{\beta}):= \min_{(X,Y)} \{ \mathbb{E}_{(X,Y)}(c(X,Y)): X \sim\boldsymbol{\alpha}, Y \sim \boldsymbol{\beta} \}\]</span></p><h3 id="局部前向法">局部前向法</h3><p>  通过求解离散分布和连续分布的最优传输问题，我们可以发现最优传输方案与Monge的"前向法"存在联系，下图5是最优传输的结果实例：</p><center><img src="https://s2.loli.net/2024/02/12/BZ8uThM5CcbQzm4.png" width="60%" height="60%"><div data-align="center">Image5: 概率分布最优传输结果实例</div></center><p>从右图离散分布的最优运输结果来看，最优运输是满足“局部前向法"的，即在满足边界分布的条件下，遵循前向运输法则。</p><h2 id="references">References</h2><ul><li><strong>[1] Book: Peyré G, Cuturi M. Computational optimaltransport[J]. Center for Research in Economics and Statistics WorkingPapers, 2017 (2017-86).</strong><br></li><li><strong>[2] Lecture: 李向东. 最优传输理论及其应用.BIMSA</strong><br></li><li><strong>[3] Paper: Cuturi M. Sinkhorn distances: Lightspeedcomputation of optimal transport[J]. Advances in neural informationprocessing systems, 2013, 26.</strong></li></ul>]]></content>
      
      
      <categories>
          
          <category> 最优传输理论 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>机器学习-7-线性判别分析</title>
      <link href="/2024/02/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-7-%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/"/>
      <url>/2024/02/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-7-%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<h1 id="线性判别分析">线性判别分析</h1><p>  线性判别分析(Linear DiscriminantAnalysis，简称LDA)是一种在机器学习和统计学中常用于分类和降维的方法。它的主要目标是在特征空间中找到一个合适的投影方向，将高维数据点投影到低维空间中，使得这些数据点易于分类。LDA在特征选择、降维和模式识别等领域都有广泛的应用。  线性判别分析最早由著名统计学家<span class="math inline">\(RA.Fisher\)</span>于1936年提出，他的工作被认为是<span class="math inline">\(LDA\)</span>的奠基。线性判别分析经过多个阶段的发展，从最初的二分类问题到多分类问题，以及对不同数据类型的适应，一直在模式识别与机器学习领域中发挥着重要作用。同时，他也启发了其他降维和分类方法的发展。</p><h2 id="基本思想">基本思想</h2><p>  线性判别分析的基本思想为：在<span class="math inline">\(n\)</span>维特征空间中，找到一个最佳的投影方向，使得在将训练集中的数据点投影到该方向上后，类别间的散度较大，类别内的散点较小，这样我们可以在该投影方向上找到一个分界点，能够对训练数据集完全正确分类。对于新的实例，将其投影到最佳投影方向上，利用分界点对其进行分类。为了寻找到最佳的投影方向，需要设置与类间散度和类内散度有关的损失函数，使得在最小化损失函数的过程中，类间散度增大而类内散度减小，这样最终得到的投影方向便是最佳投影方向。线性判别分析的基本思想可用下图1来描述：</p><center><img src="https://s2.loli.net/2023/10/09/g2xVFLyehT3D8NS.jpg" width="60%" height="60%"><div data-align="center">Image1: 线性判别分析的基本思想</div></center><h2 id="模型">模型</h2><p>  线性判别分析可以应用于二分类问题或者多分类问题，这里我们主要讨论呢二分类问题下的线性判别模型。</p><p><strong>输入</strong></p><ul><li><strong>输入空间:</strong> <span class="math inline">\(\mathcal{X} =\mathbb{R}^{n}\)</span><br></li><li><strong>输入实例:</strong> <span class="math inline">\(x =\begin{bmatrix}  x^{(1)},x^{(2)},\dots,x^{(n)} \\ \end{bmatrix} \in\mathcal{X}\)</span></li></ul><p><strong>输出</strong></p><ul><li><strong>输出空间:</strong> <span class="math inline">\(\mathcal{Y} =\{-1,+1\}\)</span><br></li><li><strong>输出实例:</strong> <span class="math inline">\(y \in\mathcal{Y}\)</span></li></ul><p>  其中输出空间<span class="math inline">\(\mathcal{Y}\)</span>只包含+1和-1的一个集合，+1与-1分别代表二分类问题中的正类<span class="math inline">\(C_1\)</span>与负类<span class="math inline">\(C_2\)</span>。输出实例<span class="math inline">\(y\)</span>代表输出实例<span class="math inline">\(x\)</span>的类别。</p><p><strong>数据集</strong><br>  感知机模型的训练数据集<span class="math inline">\(T_{train}\)</span>为：</p><p><span class="math display">\[T_{train}=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}\]</span></p><p>  其中，<span class="math inline">\(x_i \in\mathcal{X}=\mathbb{R}^{n},y_i \in\mathcal{Y}=\{+1,-1\},i=1,2,\dots,N\)</span>，<span class="math inline">\(N\)</span>表示训练数据的样本容量。</p><p><strong>模型</strong><br>  设最佳投影方向为<span class="math inline">\(\hat{w}\)</span>，且<span class="math inline">\(||\hat{w}||_{2}=1\)</span>，正类与负类的样本均值点分别为：</p><p><span class="math display">\[\bar{x}_{C_1}=\frac{1}{N_1}\sum_{x_i \inC_1}x_i,\space \bar{x}_{C_2}=\frac{1}{N_2}\sum_{x_i \inC_2}x_i\]</span></p><p>  其中，<span class="math inline">\(N_1\)</span>表示训练集中正类样本的样本容量，<span class="math inline">\(N_2\)</span>表示训练集中负类样本的样本容量。将<span class="math inline">\(\bar{x}_{C_1}\)</span>与<span class="math inline">\(\bar{x}_{C_2}\)</span>投影到最佳投影方向<span class="math inline">\(\hat{w}\)</span>后的投影距离分别为<span class="math inline">\(\hat{w}^{T}\bar{x}_{C_1}\)</span>和<span class="math inline">\(\hat{w}^{T}\bar{x}_{C_2}\)</span>，设置分界点<span class="math inline">\(threshold\)</span>为：</p><p><span class="math display">\[threshold =\frac{\hat{w}^{T}\bar{x}_{C_1}+\hat{w}^{T}\bar{x}_{C_2}}{2}\]</span></p><p>  对于新的实例点<span class="math inline">\(x\)</span>，同样将其投影到最佳投影方向<span class="math inline">\(\hat{w}\)</span>，则投影距离为<span class="math inline">\(\hat{w}^{T}x\)</span>，其类别的判断准则为：</p><p><span class="math display">\[\hat{y} = \left \{\begin{array}{rcl}+1, &amp; {\hat{w}^{T}x &gt; threshold}\\-1, &amp; {\hat{w}^{T}x \leq threshold}\\\end{array} \right.\]</span></p><p><strong>假设空间</strong><br>  模型的假设空间<span class="math inline">\(\mathcal{H}\)</span>实际上是特征空间中所有的投影方向：</p><p><span class="math display">\[\mathcal{H} = \{w \vert w \in\mathbb{R}^{n}\}\]</span></p><p>  此时模型的参数空间<span class="math inline">\(\Theta =\mathcal{H}\)</span>.</p><h2 id="策略">策略</h2><p>  前文提到，线性判别分析的最佳投影方向要满足使得训练集样本在投影后有类间散度大，而类内散度小的特点，我们可以据此来设定损失函数。<br>  设特征空间中任意投影方向为<span class="math inline">\(w\)</span>，且<span class="math inline">\(||w||_{2}=1\)</span>，则特征空间中的数据点<span class="math inline">\(x\)</span>在<span class="math inline">\(w\)</span>上的投影距离为<span class="math inline">\(w^{T}x\)</span>.将训练集中的数据点投影到该方向上，令<span class="math inline">\(z_i=w^{T}x_i\)</span>，则训练数据集中正类与负类在投影到<span class="math inline">\(w\)</span>后的平均投影距离分别为：</p><p><span class="math display">\[\bar{z}_1 =\frac{1}{N_1}\sum_{i=1}^{N_1}z_i=\frac{1}{N_1}\sum_{x_i \inC_1}w^{T}x_i\]</span></p><p><span class="math display">\[\bar{z}_2 =\frac{1}{N_2}\sum_{i=1}^{N_2}z_i=\frac{1}{N_2}\sum_{x_i \inC_2}w^{T}x_i\]</span></p><p>  <span class="math inline">\(\bar{z}_1\)</span>与<span class="math inline">\(\bar{z}_2\)</span>的差的绝对值表示投影后正类数据与负类数据的中心点之间的距离，我们可以据此来表示类间散度，这个距离越大，说明投影后两个数据整体分离地越远。为了求导的方便，我们用平方代替绝对值，这样我们可以定义类间散度：</p><p><span class="math display">\[S_{be}=(\bar{z}_1-\bar{z}_2)^2\]</span></p><p>  在考虑类间散度的同时，我们也希望投影后，同一个类别的数据尽量聚拢，即类内散度较小，我们可以用投影距离的组内方差来描述类内散度。投影后正类和负类数据点的投影距离的组内方差分别为：</p><p><span class="math display">\[S_1 =\frac{1}{N_1}\sum_{i=1}^{N_1}(z_i-\bar{z}_1)(z_i-\bar{z}_1)^T=\frac{1}{N_1}\sum_{x_i\in C_1}(w^T x_i-\frac{1}{N_1}\sum_{x_i \in C_1}w^{T}x_i)(w^Tx_i-\frac{1}{N_1}\sum_{x_i \in C_1}w^{T}x_i)^T\]</span></p><p><span class="math display">\[S_2 =\frac{1}{N_2}\sum_{i=1}^{N_2}(z_i-\bar{z}_2)(z_i-\bar{z}_2)^T=\frac{1}{N_2}\sum_{x_i\in C_2}(w^T x_i-\frac{1}{N_2}\sum_{x_i \in C_2}w^{T}x_i)(w^Tx_i-\frac{1}{N_2}\sum_{x_i \in C_2}w^{T}x_i)^T\]</span></p><p>  我们希望正类与负类样本在投影后的组内方差均较小，因此我们可以将类内散度定义为：</p><p><span class="math display">\[S_{in}=S_1+S_2\]</span></p><p>  根据判别分析的基本思想，我们希望投影后数据点有类间散度大，类内散度小的特点，因此我们可以将损失函数定义为：</p><p><span class="math display">\[L(w)=-\frac{(\bar{z}_1-\bar{z}_2)^2}{S_1+S_2}\]</span></p><p>  当我们最小化损失函数<span class="math inline">\(L(w)\)</span>时，可以在增大类间散度的同时，减小类内散度。损失函数的最小值点<span class="math inline">\(\hat{w}\)</span>便是我们要寻找的最佳投影方向。<br>  我们对类间散度与类内散度做一下化简，以简化损失函数，便于优化。</p><p><span class="math display">\[\begin{align*}    \bar{z}_1-\bar{z}_2 &amp;=  \frac{1}{N_1}\sum_{x_i \inC_1}w^{T}x_i-\frac{1}{N_2}\sum_{x_i \in C_2}w^{T}x_i  \\    &amp;= w^T \left(\frac{1}{N_1}\sum_{i=1}^{N_1}x_i-\frac{1}{N_2}\sum_{i=1}^{N_2}x_i\right) \\    &amp;= w^T(\bar{x}_{C_1}-\bar{x}_{C_2})\end{align*}\]</span></p><p><span class="math display">\[\begin{align*}    S_1 &amp;= \frac{1}{N_1}\sum_{x_i \in C_1}(w^Tx_i-\frac{1}{N_1}\sum_{x_i \in C_1}w^{T}x_i)(w^Tx_i-\frac{1}{N_1}\sum_{x_i \in C_1}w^{T}x_i)^T \\    &amp;=\frac{1}{N_1}\sum_{i=1}^{N_1}w^{T}(x_i-\bar{x}_{C_1})(x_i-\bar{x}_{C_1})^{T}w  \\    &amp;= w^{T} \left(\frac{1}{N_1}\sum_{i=1}^{N_1}(x_i-\bar{x}_{C_1})(x_i-\bar{x}_{C_1})^{T}\right)w\end{align*}\]</span></p><p>  记<span class="math inline">\(S_{C_1}=\frac{1}{N_1}\sum_{i=1}^{N_1}(x_i-\bar{x}_{C_1})(x_i-\bar{x}_{C_1})^{T}\)</span>，表示投影前正类的组内方差，则投影后正类的组内方差为：</p><p><span class="math display">\[S_1=w^{T}S_{C_1}w\]</span></p><p>  同理可得：</p><p><span class="math display">\[S_2=w^{T}S_{C_2}w\]</span></p><p><span class="math display">\[S_{C_2}=\frac{1}{N_1}\sum_{i=1}^{N_1}(x_i-\bar{x}_{C_1})(x_i-\bar{x}_{C_1})^{T}\]</span></p><p>  则类内散度可以化简为：</p><p><span class="math display">\[S_1+S_2=w^{T}S_{C_1}w+w^{T}S_{C_2}w=w^{T}(S_{C_1}+S_{C_2})w\]</span></p><p><strong>损失函数</strong>   化简后，最终得到的损失函数为：</p><p><span class="math display">\[\begin{align*}    L(w)&amp;=-\frac{w^{T}(\bar{x}_{C_1}-\bar{x}_{C_2})(\bar{x}_{C_1}-\bar{x}_{C_2})^{T}w}{w^{T}(S_{C_1}+S_{C_2})w}\\    &amp;= -\frac{w^{T}Aw}{w^{T}Bw}  \\\end{align*}\]</span></p><p>  其中，<span class="math inline">\(A=(\bar{x}_{C_1}-\bar{x}_{C_2})(\bar{x}_{C_1}-\bar{x}_{C_2})^{T},B=S_{C_1}+S_{C_2}\)</span>.</p><h2 id="算法">算法</h2><p>  我们需要解决的优化问题为：</p><p><span class="math display">\[\min_{w}L(w)=-\frac{w^{T}Aw}{w^{T}Bw}\]</span></p><p>  对<span class="math inline">\(w\)</span>求一阶偏导：并令其为零：</p><p><span class="math display">\[\begin{align*}    \frac{\partial L(w)}{\partial w} &amp;= -\frac{\partial(w^{T}Aw)(w^{T}Bw)^{-1}}{\partial w} \\    &amp;= (2Aw)(w^{T}Bw)^{-1}-(w^{T}Aw)(w^{T}Bw)^{-2}(2Bw)=0  \\\end{align*}\]</span></p><p><span class="math display">\[\RightarrowAw(w^{T}Bw)-(w^{T}Aw)(Bw)=0\]</span></p><p><span class="math display">\[\Rightarrow(w^{T}Aw)Bw=Aw(w^{T}Bw)\]</span></p><p><span class="math display">\[\Rightarrow w=\frac{w^{T}Bw}{w^{T}Aw}B^{-1}Aw\]</span></p><p>  由于我们只需要求得最佳投影方向，而不需要关系其长度，因此有：</p><p><span class="math display">\[w \varpropto B^{-1}Aw\]</span></p><p>  表示<span class="math inline">\(w\)</span>的方向与<span class="math inline">\(B^{-1}Aw\)</span>一致。由于<span class="math inline">\(Aw=(\bar{x}_{C_1}-\bar{x}_{C_2})(\bar{x}_{C_1}-\bar{x}_{C_2})^{T}w\)</span>，而<span class="math inline">\((\bar{x}_{C_1}-\bar{x}_{C_2})^{T}w \in\mathbb{R}\)</span>为标量，故有：</p><p><span class="math display">\[w \varproptoB^{-1}(\bar{x}_{C_1}-\bar{x}_{C_2})\]</span></p><p>  <span class="math inline">\(\becauseB=S_{C_1}+S_{C_2}\)</span>，因此我们求得的最佳投影方向为：</p><p><span class="math display">\[w \varpropto(S_{C_1}+S_{C_2})^{-1}(\bar{x}_{C_1}-\bar{x}_{C_2})\]</span></p><p><span class="math display">\[\hat{w} =\frac{(S_{C_1}+S_{C_2})^{-1}(\bar{x}_{C_1}-\bar{x}_{C_2})}{||(S_{C_1}+S_{C_2})^{-1}(\bar{x}_{C_1}-\bar{x}_{C_2})||_2}\]</span></p><h2 id="线性判别分析实例及python实现">线性判别分析实例及Python实现</h2><p>  首先生成训练数据。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Data Generation</span><br>np.random.seed(<span class="hljs-number">520</span>)<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">data_generation</span>(<span class="hljs-params">n,basepoint,ylabel</span>):<br>    x1 = np.random.random((n,))+np.array([basepoint[<span class="hljs-number">0</span>]]*n)<br>    x2 = np.random.random((n,))+np.array([basepoint[<span class="hljs-number">1</span>]]*n)<br>    y = np.array([ylabel]*n)<br>    data = pd.DataFrame({<span class="hljs-string">"x1"</span>:x1,<span class="hljs-string">"x2"</span>:x2,<span class="hljs-string">"y"</span>:y})<br>    <span class="hljs-keyword">return</span> data<br><br><span class="hljs-comment"># positive data</span><br>positivedata = data_generation(n=<span class="hljs-number">30</span>,basepoint=(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>),ylabel=<span class="hljs-number">1</span>)<br><span class="hljs-comment"># negative data</span><br>negativedata = data_generation(n=<span class="hljs-number">20</span>,basepoint=(<span class="hljs-number">2</span>,<span class="hljs-number">1</span>),ylabel=<span class="hljs-number">0</span>)<br><span class="hljs-comment"># train data</span><br>train_data = pd.concat([positivedata,negativedata])<br>train_data = shuffle(train_data)<br>train_data.index = <span class="hljs-built_in">range</span>(train_data.shape[<span class="hljs-number">0</span>])<br>train_data.head(<span class="hljs-number">5</span>)<br><br><span class="hljs-comment"># train data scatter plot</span><br>plt.scatter(x=positivedata[<span class="hljs-string">"x1"</span>],y=positivedata[<span class="hljs-string">"x2"</span>],marker=<span class="hljs-string">"o"</span>,c=<span class="hljs-string">"green"</span>,label=<span class="hljs-string">"positive data"</span>)<br>plt.scatter(x=negativedata[<span class="hljs-string">"x1"</span>],y=negativedata[<span class="hljs-string">"x2"</span>],marker=<span class="hljs-string">"x"</span>,c=<span class="hljs-string">"red"</span>,label=<span class="hljs-string">"negative data"</span>)<br>plt.xlabel(<span class="hljs-string">"x1"</span>)<br>plt.ylabel(<span class="hljs-string">"x2"</span>)<br>plt.xlim((<span class="hljs-number">0</span>,<span class="hljs-number">4</span>))<br>plt.ylim((<span class="hljs-number">0</span>,<span class="hljs-number">4</span>))<br>plt.xticks(np.arange(<span class="hljs-number">0</span>,<span class="hljs-number">4</span>,<span class="hljs-number">0.5</span>))<br>plt.yticks(np.arange(<span class="hljs-number">0</span>,<span class="hljs-number">4</span>,<span class="hljs-number">0.5</span>))<br>plt.legend()<br></code></pre></td></tr></tbody></table></figure><center><img src="https://s2.loli.net/2024/02/03/YtmOUQ8PNdGD1Mi.png" width="60%" height="60%"><div data-align="center">Image2: 训练数据</div></center><p>  利用前文所提出的算法，计算线性判别模型的参数 <span class="math inline">\(w\)</span> .</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">LDA_param_solving</span>(<span class="hljs-params">train_data</span>):<br>    positive_data = train_data[train_data[<span class="hljs-string">"y"</span>]==<span class="hljs-number">1</span>]<br>    negative_data = train_data[train_data[<span class="hljs-string">"y"</span>]==<span class="hljs-number">0</span>]<br><br>    pos_X = np.array(positive_data.iloc[:,:-<span class="hljs-number">1</span>])<br>    neg_X = np.array(negative_data.iloc[:,:-<span class="hljs-number">1</span>])<br><br>    pos_X_mean = np.mean(pos_X,axis=<span class="hljs-number">0</span>)<br>    pos_X_var = np.cov(pos_X,rowvar=<span class="hljs-literal">False</span>)<br>    neg_X_mean = np.mean(neg_X,axis=<span class="hljs-number">0</span>)<br>    neg_X_var = np.cov(neg_X,rowvar=<span class="hljs-literal">False</span>)<br><br>    w = np.dot(np.linalg.inv(pos_X_var+neg_X_var),pos_X_mean-neg_X_mean)<br>    w = w/np.linalg.norm(w)<br><br>    <span class="hljs-keyword">return</span> w.<span class="hljs-built_in">round</span>(<span class="hljs-number">8</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">LDA_threshold</span>(<span class="hljs-params">train_data</span>):<br>    w = LDA_param_solving(train_data)<br><br>    positive_data = train_data[train_data[<span class="hljs-string">"y"</span>]==<span class="hljs-number">1</span>]<br>    negative_data = train_data[train_data[<span class="hljs-string">"y"</span>]==<span class="hljs-number">0</span>]<br>    pos_X = np.array(positive_data.iloc[:,:-<span class="hljs-number">1</span>])<br>    neg_X = np.array(negative_data.iloc[:,:-<span class="hljs-number">1</span>])<br>    pos_X_mean = np.mean(pos_X,axis=<span class="hljs-number">0</span>)<br>    neg_X_mean = np.mean(neg_X,axis=<span class="hljs-number">0</span>)<br><br>    t = (np.dot(w,pos_X_mean)+np.dot(w,neg_X_mean))/<span class="hljs-number">2</span><br><br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">round</span>(t,<span class="hljs-number">8</span>)<br><br>w_hat = LDA_param_solving(train_data=train_data)<br>threshold = LDA_threshold(train_data)<br></code></pre></td></tr></tbody></table></figure><p>  得到的模型参数及threshold为:</p><p><span class="math display">\[w = \begin{bmatrix}    -0.8235 \\    0.5673 \\\end{bmatrix}, \quad threshlod=-0.4689\]</span></p><p>  画出模型的决策边界：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python">x1_line = np.linspace(-<span class="hljs-number">3</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1000</span>)<br>x2_line = (w_hat[<span class="hljs-number">1</span>]/w_hat[<span class="hljs-number">0</span>])*x1_line<br>theta = math.atan(w_hat[<span class="hljs-number">1</span>]/w_hat[<span class="hljs-number">0</span>])<br>threshold_x1 = -math.cos(-theta)*threshold<br>threshold_x2 = math.sin(-theta)*threshold<br>decision_boundary_x2 = (-w_hat[<span class="hljs-number">0</span>]/w_hat[<span class="hljs-number">1</span>])*x1_line+(threshold_x2+(w_hat[<span class="hljs-number">0</span>]/w_hat[<span class="hljs-number">1</span>])*threshold_x1)<br>plt.figure(figsize=(<span class="hljs-number">7</span>, <span class="hljs-number">7</span>))<br>plt.scatter(x=threshold_x1,y=threshold_x2,marker=<span class="hljs-string">"p"</span>,c=<span class="hljs-string">"black"</span>,label=<span class="hljs-string">"threshold"</span>,s=<span class="hljs-number">100</span>,alpha=<span class="hljs-number">1</span>)<br>plt.scatter(x=positivedata[<span class="hljs-string">"x1"</span>],y=positivedata[<span class="hljs-string">"x2"</span>],marker=<span class="hljs-string">"o"</span>,c=<span class="hljs-string">"green"</span>,label=<span class="hljs-string">"positive data"</span>)<br>plt.scatter(x=negativedata[<span class="hljs-string">"x1"</span>],y=negativedata[<span class="hljs-string">"x2"</span>],marker=<span class="hljs-string">"x"</span>,c=<span class="hljs-string">"red"</span>,label=<span class="hljs-string">"negative data"</span>)<br>plt.plot(x1_line,x2_line,c=<span class="hljs-string">"blue"</span>,label=<span class="hljs-string">"Best Project Direction"</span>)<br>plt.plot(x1_line,decision_boundary_x2,c=<span class="hljs-string">"purple"</span>,label=<span class="hljs-string">"decision boundary"</span>)<br>ax = plt.subplot()<br>ax.spines[<span class="hljs-string">'top'</span>].set_visible(<span class="hljs-literal">False</span>)<br>ax.spines[<span class="hljs-string">'right'</span>].set_visible(<span class="hljs-literal">False</span>)<br>ax.spines[<span class="hljs-string">'bottom'</span>].set_position((<span class="hljs-string">'data'</span>,<span class="hljs-number">0</span>))<br>ax.spines[<span class="hljs-string">'left'</span>].set_position((<span class="hljs-string">'data'</span>,<span class="hljs-number">0</span>))<br>plt.xlim((-<span class="hljs-number">3</span>,<span class="hljs-number">3</span>))<br>plt.ylim((-<span class="hljs-number">3</span>,<span class="hljs-number">3</span>))<br>plt.xticks(np.arange(-<span class="hljs-number">3</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>))<br>plt.yticks(np.arange(-<span class="hljs-number">3</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>))<br>plt.legend(loc=<span class="hljs-string">"lower left"</span>)<br></code></pre></td></tr></tbody></table></figure><center><img src="https://s2.loli.net/2024/02/03/v5zwfZsA6dYotO1.png" width="60%" height="60%"><div data-align="center">Image3: 决策边界</div></center><h2 id="参考">参考</h2><p><strong>[1] Video: bilibili,shuhuai008,线性判别分析</strong><br><strong>[2] Blog: CSDN,SongGu1996,线性判别分析(Linear DiscriminantAnalysis，LDA)</strong></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>机器学习-6.隐马尔可夫模型</title>
      <link href="/2023/12/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-6-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/"/>
      <url>/2023/12/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-6-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="隐马尔可夫模型">隐马尔可夫模型</h1><p>  隐马尔可夫模型(hidden Markovmodel,HMM)是一种用于对时许数据建模的概率图模型。它主要应用于对观察序列的概率分布进行建模，这些观察序列背后存在一个不可见的状态序列。HMM的主要思想可以总结如下：</p><ul><li><strong>状态和观察:</strong>HMM包含两种类型的变量，即隐藏的状态序列和可见的观察序列。状态序列表示系统内部的状态，而观察序列是我们可以观察的外部现象。<br></li><li><strong>马尔可夫性质:</strong>HMM假设状态序列是一个马尔可夫链，即系统的未来状态只依赖于当前状态，而与过去的状态无关。这意味着在给定当前当前状态下，未来状态与过去状态的信息是独立的。</li><li><strong>状态转移概率:</strong>HMM用状态转移概率描述系统从一个状态转移到另一个状态的可能性。这些概率被组织成状态转移矩阵，矩阵的元素表示从一个状态转移到另一个状态的概率。</li><li><strong>观察概率:</strong>对于每个状态，HMM定义了生成每个观察值的概率分布。这些概率被组织成观察概率矩阵。</li><li><strong>初始概率:</strong>HMM还需要定义系统在初始时刻处于每个状态的概率，这些概率称为初始概率。</li><li><strong>前向算法和后向算法:</strong>HMM使用前向算法和后向算法来计算给定观测序列的概率。<br></li><li><strong>Baum-Welch算法:</strong>用于无监督学习的算法，通过观察序列来调整模型参数，使其更好地匹配观察数据。Baum-Welch算法本质上就是EM算法。</li></ul><p>  隐马尔可夫模型在各个领域都具有重要的应用，包括<strong>时序数据建模，语音识别，自然语言处理，生物信息学等</strong>。总体而言，HMM在多个领域中都发挥着关键的作用，为时序数据建模和分析提供了灵活而强大的工具。</p><h2 id="基本概念">基本概念</h2><h3 id="马尔可夫链mc">马尔可夫链(MC)</h3><p>  设有随机序列 <span class="math inline">\(S =\{S_1,S_2,\dots,S_t,S_{t+1},\dots \}\)</span>，若 <span class="math inline">\(S_{t+1}\)</span> 只依赖于前一时刻 <span class="math inline">\(S_t\)</span>，不依赖于 <span class="math inline">\(S_1,S_2,\dots,S_{t-1}\)</span>，即：</p><p><span class="math display">\[P(S_{t+1} | S_1,S_2,\dots,S_t)=P(S_{t+1}| S_t)\]</span></p><p>  则称随机序列<span class="math inline">\(S\)</span>为马尔可夫链(Markov Chain)。</p><h3 id="隐马尔可夫模型的定义">隐马尔可夫模型的定义</h3><p>  <strong>定义1(隐马尔可夫模型)</strong>隐马尔可夫模型是关于时序数据的概率模型，描述由一个隐藏的马尔可夫链随机生成不可观测的状态序列，再由各个状态生成一个观测从而产生观测序列的过程。隐藏的马尔可夫链随机生成的状态的序列，称为状态序列(statesequence)；每个状态生成一个观测，而由此产生的观测的随机序列，称为观测序列(observationsequence)。序列的每一个位置又可以看作是一个时刻。</p><p>  HMM模型可以用如下的概率图表示：</p><center><img src="https://s2.loli.net/2023/11/26/w4LvBDr9m7oXkf3.jpg" width="80%" height="60%"><div data-align="center">Image1: HMM模型的概率图</div></center><h3 id="模型参数">模型参数</h3><p>  隐马尔可夫模型由<strong>初始概率分布</strong>、<strong>状态转移概率分布</strong>、以及<strong>观测概率分布</strong>确定。下面我们来介绍这些模型参数的含义，在这之前先做一些符号定义。</p><p><strong>状态序列</strong><br>  设 <span class="math inline">\(I\)</span>为状态序列，<span class="math inline">\(Q\)</span> 是所有可能状态的集合，记为：</p><p><span class="math display">\[Q=\{q_1,q_2,\dots,q_N\},\quadI=\{i_1,i_2,\dots,i_{T}\}, \forall i \in Q\]</span></p><p>其中，<span class="math inline">\(N\)</span>是可能的状态数。</p><p><strong>观测序列</strong><br>  设 <span class="math inline">\(O\)</span> 是 状态序列 <span class="math inline">\(I\)</span> 所对应的观测序列，<span class="math inline">\(V\)</span> 是所有可能的观测的集合，记为：</p><p><span class="math display">\[V = \{v_1,v_2,\dots,v_{M}\},\quadO=\{o_1,o_2,\dots,o_{T}\},\forall o \in V\]</span></p><p><strong>状态转移概率矩阵</strong><br>  设 <span class="math inline">\(A\)</span> 为状态转移概率矩阵：</p><p><span class="math display">\[A = [a_{ij}]_{N \times N}\]</span></p><p>其中，</p><p><span class="math display">\[a_{ij} = P(i_{t+1}=q_{j} |i_{t}=q_{i}),\quad i,j=1,2,\dots,N\]</span></p><p>即系统在时刻 <span class="math inline">\(t\)</span> 处于状态 <span class="math inline">\(q_{i}\)</span> 的条件下在时刻 <span class="math inline">\(t+1\)</span> 转移到状态 <span class="math inline">\(q_{j}\)</span> 的概率。</p><p><strong>观测概率矩阵</strong><br>  设 <span class="math inline">\(B\)</span> 是观测概率矩阵：</p><p><span class="math display">\[B=[b_{j}(k)]_{N \times M}\]</span></p><p>其中，</p><p><span class="math display">\[b_{j}(k)=P(o_{t}=v_{k} |i_{t}=q_{j}),\quad k=1,2,\dots,M;\quad j=1,2,\dots,N\]</span></p><p>即系统在时刻 <span class="math inline">\(t\)</span> 处于状态 <span class="math inline">\(q_{j}\)</span> 的条件下生成观测 <span class="math inline">\(v_{k}\)</span> 的概率。</p><p><strong>初始状态概率向量</strong><br>  设 <span class="math inline">\(\pi\)</span> 是初始状态概率向量：</p><p><span class="math display">\[\pi = \begin{bmatrix}    \pi_1,\pi_2,\dots,\pi_N\end{bmatrix}^{T}\]</span></p><p>其中，</p><p><span class="math display">\[\pi_{i}=P(i_{1}=q_{i}),\quadi=1,2,\dots,N\]</span></p><p>  隐马尔可夫模型由<strong>初始状态概率向量<span class="math inline">\(\pi\)</span></strong>、<strong>状态转移概率矩阵<span class="math inline">\(A\)</span></strong>、<strong>观测概率矩阵<span class="math inline">\(B\)</span></strong>决定。因此，隐马尔可夫模型的参数<span class="math inline">\(\lambda\)</span>可用三元符号表示，即：</p><p><span class="math display">\[\lambda = (A,B,\pi)\]</span></p><p>  <span class="math inline">\(A,B,\pi\)</span>称为隐马尔可夫模型的三要素。</p><h2 id="模型假设">模型假设</h2><p>  隐马尔可夫模型有两个基本假设：<br>  (1) 齐次马尔可夫性假设，即假设隐藏的马尔可夫链在任意时刻 <span class="math inline">\(t\)</span>的状态只依赖于前一时刻的状态，与其他时刻的状态及观测无关，也与时刻 <span class="math inline">\(t\)</span> 无关：</p><p><span class="math display">\[P(i_{t} |i_{t-1},\dots,i_{1};o_{t-1},\dots,o_{1})=P(i_{t} | i_{t-1}),\quadt=1,2,\dots,T\]</span></p><p>  (2)观测独立性假设，即假设任意时刻的观测只依赖于该时刻的马尔可夫链的状态，与其他观测及状态无关：</p><p><span class="math display">\[P(o_{t} |i_{t},\dots,i_{1};o_{t-1},\dots,o_{1})=P(o_{t} | i_{t})\]</span></p><h2 id="基本问题">基本问题</h2><p>  隐马尔可夫模型有3个基本问题，包括概率计算问题、学习问题、解码问题。</p><h3 id="概率计算问题">(1) 概率计算问题</h3><p>  给定模型参数 <span class="math inline">\(\lambda=(A,B,\pi)\)</span>和观测序列 <span class="math inline">\(O=\{o_1,o_2,\dots,o_{T}\}\)</span>，计算在给定模型参数<span class="math inline">\(\lambda\)</span> 的条件下观测序列 <span class="math inline">\(O\)</span> 出现的概率 <span class="math inline">\(P(O | \lambda)\)</span>。</p><h3 id="学习问题">(2) 学习问题</h3><p>  已知观测序列 <span class="math inline">\(O=\{o_1,o_2,\dots,o_{T}\}\)</span>，估计模型参数<span class="math inline">\(\lambda=(A,B,\pi)\)</span>，使得在该模型下观测序列概率<span class="math inline">\(P(O | \lambda)\)</span> 最大，即：</p><p><span class="math display">\[\hat{\lambda}=\arg\max_{\lambda} P(O |\lambda)\]</span></p><p>即用极大似然估计的方法估计参数。</p><h3 id="解码问题">(3) 解码问题</h3><p>  已知模型参数 <span class="math inline">\(\lambda=(A,B,\pi)\)</span>和观测序列 <span class="math inline">\(O=\{o_1,o_2,\dots,o_{T}\}\)</span>，求给定观测序列条件下概率<span class="math inline">\(P(I | O)\)</span> 最大的状态序列 <span class="math inline">\(I=\{i_1,i_2,\dots,i_{T}\}\)</span>，即：</p><p><span class="math display">\[\hat{I} = \arg\max_{I} P(I |O)\]</span></p><p>根据所预测的<span class="math inline">\(I\)</span>的时刻不同，解码问题又可分为预测问题与滤波问题：</p><ul><li>预测问题：<span class="math inline">\(\hat{i}_{t+1} = \arg\maxP(i_{t+1} | o_1,\dots,o_{t})\)</span><br></li><li>滤波问题：<span class="math inline">\(\hat{i}_{t} = \arg\max P(i_{t}| o_1,\dots,o_{t})\)</span></li></ul><h2 id="概率计算问题-1">概率计算问题</h2><p>  已知模型参数 <span class="math inline">\(\lambda=(A,B,\pi)\)</span>和观测序列 <span class="math inline">\(O=\{o_1,o_2,\dots,o_{T}\}\)</span>，计算 <span class="math inline">\(P(O | \lambda)\)</span>。概率计算问题主要有<strong>直接计算法、前向计算法、后向计算法</strong>。</p><h3 id="直接计算法">直接计算法</h3><p>  直接计算法的思路是通过列举所有可能的长度为 <span class="math inline">\(T\)</span> 状态序列 <span class="math inline">\(I=\{i_1,i_2,\dots,i_{T}\}\)</span>，求各个状态序列<span class="math inline">\(I\)</span> 与观测序列 <span class="math inline">\(O=\{o_1,o_2,\dots,o_{T}\}\)</span> 的联合概率<span class="math inline">\(P(O,I |\lambda)\)</span>，然后对所有可能的状态序列求和，得到 <span class="math inline">\(P(O | \lambda)\)</span>。计算过程如下：</p><p><span class="math display">\[P(O | \lambda) = \sum_{I}P(I,O |\lambda)=\sum_{I}P(O | I,\lambda)P(I | \lambda)\]</span></p><p><span class="math display">\[\begin{split}    P(I | \lambda) &amp;= P(i_1,i_2,\dots,i_{T} | \lambda) \\    &amp;= P(i_{T} | i_1,\dots,i_{T-1};\lambda)P(i_1,\dots,i_{T-1} |\lambda) \\    &amp;= P(i_{T} | i_{T-1};\lambda)P(i_1,\dots,i_{T-1} | \lambda) \\    &amp;= \left( a_{i_{T-1}i_{T}} \right) \left(\pi_{i_1}a_{i_{1}i_{2}} \dotsb a_{i_{T-2}i_{T-1}} \right) \\    &amp;= \pi_{i_1}a_{i_{1}i_{2}}a_{i_{2}i_{3}} \dotsb a_{i_{T-1}i_{T}}\end{split}\]</span></p><p><span class="math display">\[\begin{split}    P(O | I,\lambda) &amp;= P(o_1,o_2,\dots,o_{T} |i_1,i_2,\dots,i_{T};\lambda) \\    &amp;= b_{i_1}(o_1)b_{i_2}(o_2) \dotsb b_{i_T}(o_T)\end{split}\]</span></p><p><span class="math display">\[\begin{split}    P(O | \lambda) &amp;= \sum_{I}P(O | I,\lambda)P(I | \lambda) \\    &amp;=\sum_{i_1,i_2,\dots,i_T}\pi_{i1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2)\dotsb a_{i_{T-1}i_{T}}b_{i_{T}}(o_{T})\end{split}\]</span></p><p>  直接计算法的思路非常直观，容易理解，但缺点是计算量很大，是 <span class="math inline">\(O(TN^{T})\)</span>阶的，随着时间的推移呈指数型增长，这种算法在实际中是不可取的。实际上，在概率计算问题中，我们更常用的是前向计算法和后向计算法。</p><h3 id="前向计算法">前向计算法</h3><p>  在导出前向算法之前，我们首先来定义<strong>前向概率:</strong></p><p><span class="math display">\[\alpha_{t}(i)=P(o_1,o_2,\dots,o_{t};i_{t}=q_{i} |\lambda)\]</span></p><p>前向概率表示在给定模型参数 <span class="math inline">\(\lambda\)</span> 的条件下，到时刻 <span class="math inline">\(t\)</span> 部分观测序列为 <span class="math inline">\(o_1,o_2,\dots,o_{t}\)</span> 且状态为 <span class="math inline">\(q_{i}\)</span>的概率。则观测序列概率可以表示为：</p><p><span class="math display">\[P(O |\lambda)=\sum_{i=1}^{N}P(o_1,o_2,\dots,o_{T};i_{T}=q_{k} |\lambda)=\sum_{i=1}^{N}\alpha_{T}(k)\]</span></p><p>  前向计算法的主要思想是递推地求得前向概率 <span class="math inline">\(\alpha_{t}(i)\)</span> 及观测序列概率 <span class="math inline">\(P(O |\lambda)\)</span>，前向计算法的过程可用下图理解：</p><center><img src="https://s2.loli.net/2023/11/27/ry1pAg8RZwhTGkV.jpg" width="60%" height="50%"><div data-align="center">Image2: 前向递推</div></center><p>  接下来我们需要找到 <span class="math inline">\(\alpha_{t}(i)\)</span> 和 <span class="math inline">\(\alpha_{t+1}(j)\)</span> 之间的递推关系式：</p><p><span class="math display">\[\begin{split}    \alpha_{t+1}(j) &amp;= P(o_1,\dots,o_{t},o_{t+1};i_{t+1}=q_{j} |\lambda) \\    &amp;=\sum_{i=1}^{N}P(o_1,\dots,o_{t},o_{t+1};i_{t}=q_{i},i_{t+1}=q_{j} |\lambda) \\    &amp;= \sum_{i=1}^{N} P(o_{t+1} |o_1,\dots,o_{t};i_{t}=q_{i},i_{t+1}=q_{j};\lambda)P(o_1,\dots,o_{t};i_{t}=q_{i},i_{t+1}=q_{j}| \lambda) \\    &amp;=\sum_{i=1}^{N}P(o_{t+1}|i_{t+1}=q_{j};\lambda)P(i_{t+1}=q_{j}|o_1,\dots,o_{t};i_{t}=q_{i};\lambda)P(o_1,\dots,o_{t};i_{t}=q_{i}| \lambda) \\    &amp;= \sum_{i=1}^{N}P(o_{t+1}|i_{t+1}=q_{j};\lambda)P(i_{t+1}=q_{j}| i_{t}=q_i;\lambda)P(o_1,\dots,o_{t};i_{t}=q_{i} | \lambda) \\    &amp;= \sum_{i=1}^{N}b_{j}(o_{t+1})a_{ij}\alpha_{t}(i)\end{split}\]</span></p><p>  当 <span class="math inline">\(t=1\)</span> 时，有：</p><p><span class="math display">\[\alpha_{1}(i)=\pi_{i}b_{i}(o_1)\]</span></p><p>  综上所述，前向计算法的递推关系式可以总计为：</p><p><span class="math display">\[\begin{split}    \alpha_{1}(i) &amp;= \pi_{i}b_{i}(o_1),\quad i=1,2,\dots,N \\    \alpha_{t+1}(j) &amp;=b_{j}(o_{t+1})\sum_{i=1}^{N}a_{ij}\alpha_{t}(i),\quad j=1,2,\dots,N\end{split}\]</span></p><h4 id="前向算法">前向算法</h4><p>  <strong>观测序列概率的前向算法</strong><br>  输入：隐马尔可夫模型的参数 <span class="math inline">\(\lambda=(A,B,\pi)\)</span>，观测序列 O；<br>  输出：观测序列概率 <span class="math inline">\(P(O |\lambda)\)</span>。<br>  (1) 初值</p><p><span class="math display">\[\alpha_{1}(i)=\pi_{i}b_{i}(o_1),\quadi=1,2,\dots,N\]</span></p><p>  (2) 递推 <span class="math inline">\(\quad\)</span> 对 <span class="math inline">\(t=1,2,\dots,T-1,\)</span></p><p><span class="math display">\[\alpha_{t+1}(j) =b_{j}(o_{t+1})\sum_{i=1}^{N}a_{ij}\alpha_{t}(i),\quadj=1,2,\dots,N\]</span></p><p>  (3) 终止</p><p><span class="math display">\[P(O |\lambda)=\sum_{i=1}^{N}\alpha_{T}(i)\]</span></p><p>  前向算法的计算复杂度为 <span class="math inline">\(O(TN^{2})\)</span>，优于直接计算法。</p><h3 id="后向计算法">后向计算法</h3><p>  后向计算法与前向计算法大致相同，不同点在于后向计算法是从后向前递推。我们首先来定义<strong>后向概率:</strong></p><p><span class="math display">\[\beta_{t}(i) =P(o_{t+1},o_{t+2},\dots,o_{T} | i_{t}=q_{i};\lambda)\]</span></p><p>后向概率表示在给定模型参数 <span class="math inline">\(\lambda\)</span>，系统到时刻 <span class="math inline">\(t\)</span> 状态为 <span class="math inline">\(q_{i}\)</span> 的条件下，从 <span class="math inline">\(t+1\)</span> 到 <span class="math inline">\(T\)</span> 的部分观测序列为 <span class="math inline">\(o_{t+1},o_{t+2},\dots,o_{T}\)</span>的概率。则观测序列概率可以表示为：</p><p><span class="math display">\[\begin{split}    P(O | \lambda) &amp;= P(o_1,o_2,\dots,o_{T} | \lambda) \\    &amp;= \sum_{i=1}^{N} P(o_1,o_2,\dots,o_{T};i_{1}=q_{i} | \lambda)\\    &amp;= \sum_{i=1}^{N} P(o_1,o_2,\dots,o_{T} |i_{1}=q_{i};\lambda)P(i_1=q_{i} | \lambda) \\    &amp;= \sum_{i=1}^{N} P(o_1 |o_2,\dots,o_{T};i_{1}=q_{i};\lambda)P(o_2,\dots,o_{T} |i_{1}=q_{i};\lambda)\pi_{i} \\      &amp;= \sum_{i=1}^{N}P(o_1 | i_{1}=q_{i};\lambda)\beta_{1}(i)\pi_{i}\\    &amp;= \sum_{i=1}^{N} \pi_{i} b_{i}(o_1) \beta_{1}(i)\end{split}\]</span></p><p>  后向计算法的主要思想是递推地求得后向概率 <span class="math inline">\(\beta_{t}(i)\)</span> 及观测序列概率 <span class="math inline">\(P(O |\lambda)\)</span>，后向计算法的过程可用下图理解：</p><center><img src="https://s2.loli.net/2023/11/27/9jd7FOnlkuo2rM4.jpg" width="60%" height="60%"><div data-align="center">Image3: 后向递推</div></center><p>  之后我们来导出 <span class="math inline">\(\beta_{t}(i)\)</span> 和<span class="math inline">\(\beta_{t+1}(j)\)</span>之间的递推关系式：</p><p><span class="math display">\[\begin{split}    \beta_{t}(i) &amp;= P(o_{t+1},\dots,o_{T} | i_{t}=q_{i};\lambda) \\    &amp;= \sum_{j=1}^{N}P(o_{t+1},\dots,o_{T};i_{t+1}=q_{j} |i_{t}=q_{i};\lambda) \\    &amp;= \sum_{j=1}^{N}P(o_{t+1},\dots,o_{T} |i_{t+1}=q_{j},i_{t}=q_{i};\lambda)P(i_{t+1}=q_{j} | i_{t}=q_{i};\lambda)\\    &amp;= \sum_{j=1}^{N}P(o_{t+1},\dots,o_{T} |i_{t+1}=q_{j};\lambda)a_{ij} \\    &amp;= \sum_{j=1}^{N}P(o_{t+1} |o_{t+2},\dots,o_{T};i_{t+1}=q_{j};\lambda)P(o_{t+2},\dots,o_{T} |i_{t+1}=q_{j};\lambda)a_{ij} \\    &amp;= \sum_{j=1}^{N}P(o_{t+1} | i_{t+1}=q_{j};\lambda)\beta_{t+1}(j) a_{ij}  \\    &amp;= \sum_{j=1}^{N} b_{j}(o_{t+1})a_{ij} \beta_{t+1}(j)\end{split}\]</span></p><p>  当 <span class="math inline">\(t=T\)</span>时，给定初始的后向概率：</p><p><span class="math display">\[\beta_{T}(i) = 1,\quadi=1,2,\dots,N\]</span></p><p>  综上所述，后向计算法的递推关系式可以总计为：</p><p><span class="math display">\[\begin{split}    \beta_{T}(i) &amp;= 1,\quad i=1,2,\dots,N \\    \beta_{t}(i) &amp;= \sum_{j=1}^{N} b_{j}(o_{t+1})a_{ij}\beta_{t+1}(j),\quad t=T-1,\dots,1;i=1,\dots,N\end{split}\]</span></p><h4 id="后向算法">后向算法</h4><p><strong>观测序列概率的后向算法</strong><br>  输入：隐马尔可夫模型参数 <span class="math inline">\(\lambda=(A,B,\pi)\)</span>，观测序列 <span class="math inline">\(O\)</span>;<br>  输出：观测序列概率 <span class="math inline">\(P(O |\lambda)\)</span>。<br>  (1) 初值</p><p><span class="math display">\[\beta_{T}(i) = 1,\quadi=1,2,\dots,N\]</span></p><p>  (2) 递推 <span class="math inline">\(\quad\)</span> 对 <span class="math inline">\(t=T-1,T-2,\dots,1\)</span></p><p><span class="math display">\[\beta_{t}(i) = \sum_{j=1}^{N}b_{j}(o_{t+1})a_{ij} \beta_{t+1}(j),\quad i=1,\dots,N\]</span></p><p>  (3) 终止</p><p><span class="math display">\[P(O | \lambda) = \sum_{i=1}^{N} \pi_{i}b_{i}(o_1) \beta_{1}(i)\]</span></p><p>  后向算法的计算复杂度为 <span class="math inline">\(O(TN^{2})\)</span>，与前向算法相同，优于直接计算法。</p><h2 id="学习问题-1">学习问题</h2><p>  隐马尔可夫模型的学习，根据训练数据是包括观测序列和对应的状态序列还是只有观测序列，可以分为监督学习与无监督学习。监督学习主要利用极大似然法来估计模型参数，无监督学习则是利用<span class="math inline">\(Baum-Welch\)</span> 算法，也就是 <span class="math inline">\(EM\)</span> 算法来估计参数。</p><h3 id="监督学习方法">监督学习方法</h3><p>  假设已给训练数据包含 <span class="math inline">\(S\)</span>个长度相同的观测序列和对应的状态序列 <span class="math inline">\(\{(O_1,I_1),(O_2,I_2),\dotsb,(O_S,I_S)\}\)</span>，可以利用极大似然估计法来估计隐马尔可夫模型的参数。</p><p>  <strong>1.转移概率 <span class="math inline">\(a_{ij}\)</span>的估计</strong><br>  设样本中时刻 <span class="math inline">\(t\)</span> 处于状态 <span class="math inline">\(q_i\)</span> 且时刻 <span class="math inline">\(t+1\)</span> 转移到状态 <span class="math inline">\(q_j\)</span> 的频数为 <span class="math inline">\(A_{ij}\)</span>，那么状态转移概率 <span class="math inline">\(a_{ij}\)</span> 的估计为：</p><p><span class="math display">\[\hat{a}_{ij}=\frac{A_{ij}}{\sum_{j=1}^{N}A_{ij}},\quadi=1,2,\dotsb,N;\quad j=1,2,\dotsb,N\]</span></p><p>  <strong>2.观测概率 <span class="math inline">\(b_{j}(k)\)</span>的估计</strong><br>  设样本中状态为 <span class="math inline">\(q_j\)</span> 并观测为 <span class="math inline">\(v_k\)</span> 的频数是 <span class="math inline">\(B_{jk}\)</span>，那么状态为 <span class="math inline">\(q_j\)</span> 观测为 <span class="math inline">\(v_k\)</span> 的概率 <span class="math inline">\(b_{j}(k)\)</span> 的估计为</p><p><span class="math display">\[\hat{b}_{j}(k)=\frac{B_{jk}}{\sum_{k=1}^{M}B_{jk}},\quadj=1,2,\quad,N;\quad k=1,2,\dotsb,M\]</span></p><p>  <strong>3.初始状态概率 <span class="math inline">\(\pi_{i}\)</span>的估计</strong><br>  设样本中初始状态为 <span class="math inline">\(q_i\)</span> 的频数为<span class="math inline">\(Q_i\)</span>，则初始状态概率 <span class="math inline">\(\pi_i\)</span> 的估计为</p><p><span class="math display">\[\hat{\pi}_{i}=\frac{Q_{i}}{S},\quadi=1,2,\dotsb,N\]</span></p><h3 id="无监督学习方法">无监督学习方法</h3><p>  虽然监督学习的方法操作十分简便，也非常容易理解，但监督学习需要对训练数据进行标注，而人工标注训练数据往往代价很高，因此，有时就会利用无监督学习的方法。无监督学习所使用的算法为<span class="math inline">\(Baum-Welch\)</span>，实际上为 <span class="math inline">\(EM\)</span> 算法。   假设给定训练数据只包含 <span class="math inline">\(S\)</span> 个长度为 <span class="math inline">\(T\)</span> 的观测序列 <span class="math inline">\(\{O_1,O_2,\dotsb,O_S\}\)</span>而没有对应的状态序列，目标是学习隐马尔可夫模型 <span class="math inline">\(\lambda=(A,B,\pi)\)</span>的参数。我们将观测序列数据看作观测数据 <span class="math inline">\(O\)</span>，状态序列数据看作不可观测的隐数据 <span class="math inline">\(I\)</span>，那么隐马尔可夫模型实际上是一个含有隐变量的概率模型</p><p><span class="math display">\[P(O | \lambda)=\sum_{I}P(O |I,\lambda)P(I | \lambda)\]</span></p><p>它的参数学习可以由 <span class="math inline">\(EM\)</span>算法实现。</p><p>  <strong>1.确定完全数据的对数似然函数</strong><br>  所有的观测数据写成 <span class="math inline">\(O=(o_1,o_2,\dotsb,o_{T})\)</span>，所有隐数据写成<span class="math inline">\(I=(i_1,i_2,\dotsb,i_{T})\)</span>，完全数据为<span class="math inline">\((O,I)=(o_1,o_2,\dotsb,o_{T};i_1,i_2,\dotsb,i_{T})\)</span>。完全数据的对数似然函数为：</p><p><span class="math display">\[L(\lambda) = \log{P(O,I |\lambda)}\]</span></p><p>  <strong>2.EM 算法的 E 步：求 <span class="math inline">\(Q\)</span>函数 <span class="math inline">\(Q(\lambda,\bar{\lambda})\)</span></strong><br>  由 <span class="math inline">\(Q\)</span> 函数的定义得</p><p><span class="math display">\[\begin{split}    Q(\lambda,\bar{\lambda}) &amp;= E_{I}[\log{P(O,I|\lambda)} |O,\bar{\lambda}]  \\    &amp;=\sum_{I}\frac{\log{P(O,I |\lambda)}P(O,I|\bar{\lambda})}{P(O|\bar{\lambda})}\end{split}\]</span></p><p>其中，<span class="math inline">\(\bar{\lambda}\)</span>是隐马尔可夫模型当前的估计值，<span class="math inline">\(\lambda\)</span>是下一步要极大化的隐马尔可夫模型参数。由于 <span class="math inline">\(P(O | \bar{\lambda})\)</span>为常数，对优化没有影响，可以舍去；同时由概率计算中的直接计算法可得：</p><p><span class="math display">\[P(O,I |\lambda)=\pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2) \dotsba_{i_{T-1}i_{T}}b_{i_{T}}(o_{T})\]</span></p><p>于是函数 <span class="math inline">\(Q(\lambda,\bar{\lambda})\)</span> 可以写成：</p><p><span class="math display">\[\begin{split}    Q(\lambda,\bar{\lambda}) &amp;=\sum_{I}\log{P(O,I|\lambda)P(O,I|\bar{\lambda})} \\    &amp;= \sum_{I} P(O,I | \bar{\lambda}) \log{[\pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2) \dotsba_{i_{T-1}i_{T}}b_{i_{T}}(o_{T}) ]}  \\    &amp;= \sum_{I} P(O,I | \bar{\lambda}) \log{\left[\pi_{i_1}\left(\prod_{t=1}^{T-1}a_{i_{t}i_{t+1}}\right)\left(\prod_{t=1}^{T}b_{i_t}(o_t) \right) \right]} \\    &amp;=\sum_{I}P(O,I | \bar{\lambda}) \left[\log(\pi_{i})+\sum_{t=1}^{T-1}\log(a_{i_{t}i_{t+1}})+\sum_{t=1}^{T}\log(b_{i_t}(o_t))\right] \\    &amp;=\sum_{I}\log(\pi_{i})P(O,I | \bar{\lambda})+\sum_{I}\left(\sum_{t=1}^{T-1}\log(a_{i_{t}i_{t+1}}) \right)P(O,I | \bar{\lambda}) \\    &amp;+ \sum_{I}\left( \sum_{t=1}^{T}\log(b_{i_t}(o_t)) \right)P(O,I| \bar{\lambda})\end{split}\]</span></p><p>式中求和都是对所有数据的序列总长度 <span class="math inline">\(T\)</span> 进行的。通过观察 <span class="math inline">\(Q(\lambda,\bar{\lambda})\)</span> 的计算式可以看出<span class="math inline">\(Q(\lambda,\bar{\lambda})\)</span> 的第一项<span class="math inline">\(\sum_{I}\log(\pi_{i})P(O,I |\bar{\lambda})\)</span> 只与参数 <span class="math inline">\(\lambda\)</span> 中的初始概率 <span class="math inline">\(\pi\)</span> 有关；第二项 <span class="math inline">\(\sum_{I}\left(\sum_{t=1}^{T-1}\log(a_{i_{t}i_{t+1}}) \right)P(O,I |\bar{\lambda})\)</span> 只与参数 <span class="math inline">\(\lambda\)</span> 中的状态转移概率矩阵 <span class="math inline">\(A\)</span> 有关；第三项 <span class="math inline">\(\sum_{I}\left( \sum_{t=1}^{T}\log(b_{i_t}(o_t))\right)P(O,I | \bar{\lambda})\)</span> 只与参数 <span class="math inline">\(\lambda\)</span> 中的观测概率矩阵 <span class="math inline">\(B\)</span> 有关。因此，可以令：</p><p><span class="math display">\[\begin{split}    Q_{1}(\pi,\bar{\lambda}) &amp;= \sum_{I}\log(\pi_{i})P(O,I |\bar{\lambda}) \\    Q_{2}(A,\bar{\lambda}) &amp;= \sum_{I}\left(\sum_{t=1}^{T-1}\log(a_{i_{t}i_{t+1}}) \right)P(O,I | \bar{\lambda}) \\    Q_{3}(B,\bar{\lambda}) &amp;= \sum_{I}\left(\sum_{t=1}^{T}\log(b_{i_t}(o_t)) \right)P(O,I | \bar{\lambda})\end{split}\]</span></p><p>则 <span class="math inline">\(Q(\lambda,\bar{\lambda})\)</span>可以写成：</p><p><span class="math display">\[Q(\lambda,\bar{\lambda})=Q_{1}(\pi,\bar{\lambda})+Q_{2}(A,\bar{\lambda})+Q_{3}(B,\bar{\lambda})\]</span></p><p>  <strong>3.EM 算法的 M 步：极大化 <span class="math inline">\(Q\)</span> 函数 <span class="math inline">\(Q(\lambda,\bar{\lambda})\)</span> 求模型参数 <span class="math inline">\(A,B,\pi\)</span></strong><br>  通过极大化 <span class="math inline">\(Q(\lambda,\bar{\lambda})\)</span> 可以得到模型参数<span class="math inline">\(A,B,\pi\)</span> 的估计值。<br>  <strong>(1) 估计初始状态概率 <span class="math inline">\(\pi\)</span></strong><br>  <span class="math inline">\(Q(\lambda,\bar{\lambda})\)</span> 中只有<span class="math inline">\(Q_{1}(\pi,\bar{\lambda})\)</span>与初始状态概率 <span class="math inline">\(\pi\)</span>有关，因此优化问题可以写成：</p><p><span class="math display">\[\begin{split}    &amp; \max_{\pi} \quad Q_{1}(\pi,\bar{\lambda}) \\    &amp; \space s.t. \quad \sum_{i=1}^{N} \pi_{i}=1 \\\end{split}\]</span></p><p>优化问题的拉格朗日函数：</p><p><span class="math display">\[L(\pi,\gamma)=\sum_{I}\log(\pi_{i})P(O,I| \bar{\lambda})+\gamma \left( \sum_{i=1}^{N}\pi_{i}-1\right)\]</span></p><p>由费马定理得：</p><p><span class="math display">\[\frac{\partial{L(\pi,\gamma)}}{\partial{\pi_{i}}}= \frac{P(O,i_1=i | \bar{\lambda})}{\pi_{i}}+\gamma=0,\quadi=1,2,\dotsb,N\]</span></p><p>得到：</p><p><span class="math display">\[\gamma\pi_{i}=-P(O,i_1=i|\bar{\lambda})\]</span></p><p>两边同时对 <span class="math inline">\(i\)</span> 求和得：</p><p><span class="math display">\[\begin{split}    &amp; \gamma\sum_{i=1}^{N}\pi_{i} =-\sum_{i=1}^{N}P(O,i_{1}=i|\bar{\lambda})=P(O | \bar{\lambda}) \\    &amp; \Rightarrow \gamma = P(O | \bar{\lambda}) \\\end{split}\]</span></p><p>从而得到 <span class="math inline">\(\pi_{i}\)</span>的估计值为：</p><p><span class="math display">\[\hat{\pi}_{i}=\frac{P(O,i_1=i|\bar{\lambda})}{P(O| \bar{\lambda})}\]</span></p><p>  <strong>(2) 估计状态转移矩阵 <span class="math inline">\(A\)</span></strong><br>  <span class="math inline">\(Q(\lambda,\bar{\lambda})\)</span> 中只有<span class="math inline">\(Q_{2}(A,\bar{\lambda})\)</span>与状态转移概率矩阵 <span class="math inline">\(A\)</span>有关，因此优化问题可以写成：</p><p><span class="math display">\[\begin{split}    &amp; \max_{A} \quad Q_{2}(A,\bar{\lambda}) \\    &amp; \space s.t. \quad \sum_{j=1}^{N}a_{ij}=1 \\\end{split}\]</span></p><p>优化问题的拉格朗日函数：</p><p><span class="math display">\[\begin{split}    L(A,\gamma) &amp;= \sum_{I}\left(\sum_{t=1}^{T-1}\log(a_{i_{t}i_{t+1}}) \right)P(O,I |\bar{\lambda})+\gamma\left( \sum_{j=1}^{N}a_{ij}-1 \right) \\    &amp;=\sum_{t=1}^{T-1}\sum_{i=1}^{N}\sum_{j=1}^{N}\left(P(O,i_{t}=i,i_{t+1}=j|\bar{\lambda})\log{a_{ij}} \right)+\gamma\left(\sum_{j=1}^{N}a_{ij}-1 \right)\end{split}\]</span></p><p>由费马定理得：</p><p><span class="math display">\[\frac{\partial{L(A,\gamma)}}{\partial{a_{ij}}}=\frac{\sum_{t=1}^{T-1}P(O,i_{t}=i,i_{t+1}=j|\bar{\lambda})}{a_{ij}}+\gamma=0,\quadi,j=1,\dotsb,N\]</span></p><p>得到：</p><p><span class="math display">\[\gammaa_{ij}=-\sum_{t=1}^{T-1}P(O,i_{t}=i,i_{t+1}=j|\bar{\lambda})\]</span></p><p>两边同时对 <span class="math inline">\(j\)</span> 求和：</p><p><span class="math display">\[\begin{split}    &amp;\gamma\sum_{j=1}^{N}a_{ij}=-\sum_{t=1}^{T-1}\sum_{j=1}^{N}P(O,i_{t}=i,i_{t+1}=j|\bar{\lambda})\\    &amp; \Rightarrow \gamma =-\sum_{t=1}^{T-1}P(O,i_{t}=i|\bar{\lambda})\end{split}\]</span></p><p>从而得到状态转移概率矩阵元素 <span class="math inline">\(a_{ij}\)</span> 的估计值为：</p><p><span class="math display">\[\hat{a}_{ij}=\frac{\sum_{t=1}^{T-1}P(O,i_{t}=i,i_{t+1}=j|\bar{\lambda})}{\sum_{t=1}^{T-1}P(O,i_{t}=i|\bar{\lambda})}\]</span></p><p>  <strong>(3) 估计观测概率矩阵 <span class="math inline">\(B\)</span></strong><br>  <span class="math inline">\(Q(\lambda,\bar{\lambda})\)</span> 中只有<span class="math inline">\(Q_{3}(B,\bar{\lambda})\)</span>与观测概率矩阵 <span class="math inline">\(B\)</span>有关，因此优化问题可以写成：</p><p><span class="math display">\[\begin{split}    &amp; \max_{B} \quad Q_{3}(B,\bar{\lambda}) \\    &amp; \space s.t. \quad \sum_{k=1}^{M}b_{j}(v_{k})=1 \\\end{split}\]</span></p><p>优化问题的拉格朗日函数：</p><p><span class="math display">\[\begin{split}    L(B,\gamma) &amp;= \sum_{I}\left( \sum_{t=1}^{T}\log(b_{i_t}(o_t))\right)P(O,I | \bar{\lambda})+\gamma\left( \sum_{k=1}^{M}b_{j}(v_{k})-1\right) \\    &amp;=\sum_{t=1}^{T}\sum_{j=1}^{N}\left(P(O,i_{t}=j|\bar{\lambda})\log{b_{j}(o_{t})} \right)+\gamma\left(\sum_{k=1}^{M}b_{j}(v_{k})-1 \right)\end{split}\]</span></p><p>由费马定理得：</p><p><span class="math display">\[\frac{\partial{L(B,\gamma)}}{\partial{b_{j}(v_{k})}}=\frac{\sum_{t=1}^{T}P(O,i_{t}=j|\bar{\lambda})I(o_{t}=v_{k})}{b_{j}(v_{k})}+\gamma=0,\quadj=1,\dotsb,N;k=1,\dotsb,M\]</span></p><p>得到：</p><p><span class="math display">\[\gammab_{j}(v_{k})=-\sum_{t=1}^{T}P(O,i_{t}=j|\bar{\lambda})I(o_{t}=v_{k})\]</span></p><p>两边同时对 <span class="math inline">\(k\)</span> 求和：</p><p><span class="math display">\[\begin{split}    &amp;\gamma\sum_{k=1}^{M}b_{j}(v_{k})=-\sum_{k=1}^{M}\sum_{t=1}^{T}P(O,i_{t}=j|\bar{\lambda})I(o_{t}=v_{k})\\    &amp; \Rightarrow \gamma = -\sum_{t=1}^{T}P(O,i_{t}=j|\bar{\lambda})\end{split}\]</span></p><p>从而得到观测概率矩阵元素的 <span class="math inline">\(b_{j}(k)\)</span> 的估计值为：</p><p><span class="math display">\[\hat{b}_{j}(k)=\frac{\sum_{t=1}^{T}P(O,i_{t}=j|\bar{\lambda})I(o_{t}=v_{k})}{\sum_{t=1}^{T}P(O,i_{t}=j|\bar{\lambda})}\]</span></p><h4 id="baum-welch-算法">Baum-Welch 算法</h4><p>  通过以上的推导，我们可以总结出无监督学习下隐马尔可夫参数估计的一种算法，其被称为Baum-Welch 算法，本质上是 EM 算法在隐马尔可夫模型学习中的具体实现。</p><p>  <strong>Baum-Welch 算法</strong><br>  输入：观测数据 <span class="math inline">\(O=(o_1,o_2,\dotsb,o_{T})\)</span>；<br>  输出：隐马尔可夫模型参数 <span class="math inline">\(\lambda=(A,B,\pi)\)</span>。<br>  (1) 初始化。对 <span class="math inline">\(n=0\)</span>，选取 <span class="math inline">\(a_{ij}^{(0)},b_{j}^{(0)}(k),\pi_{i}^{(0)}\)</span>，得到模型<span class="math inline">\(\lambda^{(0)}=(A^{(0)},B^{(0)},\pi^{(0)})\)</span>。<br>  (2) 递推。对 <span class="math inline">\(n=1,2,\dotsb\)</span>，</p><p><span class="math display">\[a_{ij}^{(n+1)}=\frac{\sum_{t=1}^{T-1}P(O,i_{t}=i,i_{t+1}=j|\lambda^{(n)})}{\sum_{t=1}^{T-1}P(O,i_{t}=i|\lambda^{(n)})}\]</span></p><p><span class="math display">\[b_{j}^{(n+1)}(k)=\frac{\sum_{t=1}^{T}P(O,i_{t}=j|\lambda^{(n)})I(o_{t}=v_{k})}{\sum_{t=1}^{T}P(O,i_{t}=j|\lambda^{(n)})}\]</span></p><p><span class="math display">\[\pi_{i}^{(n+1)}=\frac{P(O,i_1=i|\lambda^{(n)})}{P(O| \lambda^{(n)})}\]</span></p><p>  (3) 终止。得到模型参数 <span class="math inline">\(\lambda^{(n+1)}=(A^{(n+1)},B^{(n+1)},\pi^{(n+1)})\)</span></p><h2 id="解码问题-1">解码问题</h2><p>  解码问题主要是研究给定观测序列下最有可能出现的状态序列。已知模型参数<span class="math inline">\(\lambda=(A,B,\pi)\)</span> 和观测序列 <span class="math inline">\(O=(o_1,o_2,\dotsb,o_{T})\)</span>，求某一观测序列<span class="math inline">\(I=(i_1,i_2,\dotsb,i_{T})\)</span>使得条件概率 <span class="math inline">\(P(I | O)\)</span>最大。隐马尔可夫模型中的解码问题主要使用维特比算法。</p><h3 id="维特比算法">维特比算法</h3><p>  维特比算法实际是用动态规划(dynamicprogramming)解隐马尔可夫模型解码问题，即用动态规划求概率最大路径(最优路径)。这时一条路径对应着一个状态序列。维特比算法的思想可以用下图表示：</p><center><img src="https://s2.loli.net/2023/12/06/E87VbUDGMmWHFJp.jpg" width="60%" height="60%"><div data-align="center">Image3: 最优路径</div></center><p>  根据动态规划的原理，最优路径具有这样的特性：<strong>如果最优路径在<span class="math inline">\(t\)</span> 时刻通过结点 <span class="math inline">\(i_{t}^{*}\)</span>，那么这一路径从结点 <span class="math inline">\(i_{t}^{*}\)</span> 到终点 <span class="math inline">\(i_{T}^{*}\)</span> 的部分路径，对于从 <span class="math inline">\(i_{t}^{*}\)</span> 到 <span class="math inline">\(i_{T}^{*}\)</span>的所有可能的部分路径来说，必须是最优的。</strong>依据这一原理，我们只需要从时刻 <span class="math inline">\(t=1\)</span>开始，递推地计算在时刻 <span class="math inline">\(t\)</span> 状态为<span class="math inline">\(i\)</span>的各条部分路径的最大概率，直至得到时刻 <span class="math inline">\(t=T\)</span> 状态为 <span class="math inline">\(i\)</span> 的各条路径的最大概率。时刻 <span class="math inline">\(t=T\)</span> 的最大概率即为最优路径的概率 <span class="math inline">\(P^{*}\)</span>，最优路径的终结点 <span class="math inline">\(i_{T}^{*}\)</span>也同时得到。之后，为了找出最优路径的各个结点，从终结点 <span class="math inline">\(i_{T}^{*}\)</span> 开始，由后向前逐步求得结点<span class="math inline">\(i_{T-1}^{*},\dotsb,i_{1}^{*}\)</span>，得到最优路径<span class="math inline">\(I^{*}=(i_{1}^{*},i_{2}^{*},\dotsb,i_{T}^{*})\)</span>。这就是维特比算法。</p><p>  首先导入两个变量 <span class="math inline">\(\delta\)</span> 和<span class="math inline">\(\psi\)</span>。定义在时刻 <span class="math inline">\(t\)</span> 状态为 <span class="math inline">\(i\)</span> 的所有单个路径 <span class="math inline">\((i_1,i_2,\dotsb,i_{t})\)</span>中概率最大值为：</p><p><span class="math display">\[\delta_{t}(i)=\max_{i_1,i_2,\dotsb,i_{t-1}}P(i_{t}=i,i_{t-1},\dotsb,i_1;o_{t},\dotsb,o_1| \lambda),\quad i=1,2,\dotsb,N\]</span></p><p>  由定义可得变量 <span class="math inline">\(\delta\)</span>的递推公式：</p><p><span class="math display">\[\begin{split}    \delta_{t+1}(i) &amp;=\max_{i_1,i_2,\dotsb,i_{t}}P(i_{t+1}=i,i_{t},\dotsb,i_1;o_{t+1},\dotsb,o_1| \lambda) \\    &amp;= \max_{1 \leq j \leq N}[\delta_{t}a_{ji}]b_{i}(o_{t+1}),\quadi=1,2,\dotsb,N; \space t=1,2,\dotsb,T-1\end{split}\]</span></p><p>  定义在时刻 <span class="math inline">\(t\)</span> 状态为 <span class="math inline">\(i\)</span> 的所有单个路径 <span class="math inline">\((i_1,i_2,\dotsb,i_{t-1},i)\)</span>中概率最大的路径的第 <span class="math inline">\(t-1\)</span>个结点为</p><p><span class="math display">\[\psi_{t}(i)=\arg\max_{1 \leq j \leqN}[\delta_{t-1}(j)a_{ji}],\quad i=1,2,\dotsb,N\]</span></p><p>  <strong>维特比算法</strong><br>  输入：模型参数 <span class="math inline">\(\lambda=(A,B,\pi)\)</span>和观测 <span class="math inline">\(O=(o_1,o_2,\dotsb,o_{T})\)</span>；<br>  输出：最优路径 <span class="math inline">\(I^{*}=(i_{1}^{*},i_{2}^{*},\dotsb,i_{T}^{*})\)</span>.<br>  (1) 初始化</p><p><span class="math display">\[\delta_{1}(i)=\pi_{i}b_{i}(o_1),\quadi=1,2,\dotsb,N\]</span></p><p><span class="math display">\[\psi_{1}(i)=0,\quadi=1,2,\dotsb,N\]</span></p><p>  (2) 递推。对 <span class="math inline">\(t=2,3,\dotsb,T\)</span></p><p><span class="math display">\[\delta_{t}(i)=\max_{1 \leq j \leqN}[\delta_{t-1}(j)a_{ji}]b_{i}(o_{t}),\quad i=1,2,\dotsb,N\]</span></p><p><span class="math display">\[\psi_{t}(i)=\arg\max_{1 \leq j \leqN}[\delta_{t-1}(j)a_{ji}],\quad i=1,2,\dotsb,N\]</span></p><p>  (3) 终止</p><p><span class="math display">\[P^{*}=\max_{1 \leq i \leq N}\delta_{T}(i)\]</span></p><p><span class="math display">\[i_{T}^{*}=\arg\max_{1 \leq i \leqN}[\delta_{T}(i)]\]</span></p><p>  (4) 最优路径回溯。对 <span class="math inline">\(t=T-1,T-2,\dotsb,1\)</span></p><p><span class="math display">\[i_{t}^{*}=\psi_{t+1}(i_{t+1}^{*})\]</span></p><p>求得最优路径 <span class="math inline">\(I^{*}=(i_{1}^{*},i_{2}^{*},\dotsb,i_{T}^{*})\)</span>。</p><h2 id="参考">参考</h2><p><strong>[1] Book: 李航,统计学习方法(第2版)</strong><br><strong>[2] Book: 董平,机器学习中的统计思维(Python实现)</strong><br><strong>[3] Video: bilibili,简博士,隐马尔可夫系列</strong><br><strong>[4] Video: bilibili,shuhuai008,隐马尔可夫系列</strong></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>矩阵分析-6.矩阵的等价与相似</title>
      <link href="/2023/11/20/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-6-%E7%9F%A9%E9%98%B5%E7%9A%84%E7%AD%89%E4%BB%B7%E4%B8%8E%E7%9B%B8%E4%BC%BC/"/>
      <url>/2023/11/20/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-6-%E7%9F%A9%E9%98%B5%E7%9A%84%E7%AD%89%E4%BB%B7%E4%B8%8E%E7%9B%B8%E4%BC%BC/</url>
      
        <content type="html"><![CDATA[<h1 id="矩阵的等价与相似">矩阵的等价与相似</h1><h2 id="矩阵等价">矩阵等价</h2><h3 id="定义">定义</h3><p>  设矩阵<span class="math inline">\(A,B \in \mathbb{F}^{m \timesn}\)</span>，若存在可逆矩阵 <span class="math inline">\(P \in\mathbb{F}^{n \times n}, Q \in \mathbb{F}^{m \times m}\)</span>，使得<span class="math inline">\(AP=QB\)</span>，则称矩阵<span class="math inline">\(A,B\)</span>等价。</p><p>[注]：由于<span class="math inline">\(AP=QB\)</span>，且<span class="math inline">\(Q\)</span>可逆，可得 <span class="math inline">\(Q^{-1}AP=B\)</span>. <strong>因此矩阵<span class="math inline">\(B\)</span>是由矩阵<span class="math inline">\(A\)</span>进行有限次初等变换后得到的新矩阵.</strong></p><h3 id="几何意义">几何意义</h3><p>  令：</p><p><span class="math display">\[P = \begin{bmatrix}    p_1,p_2,\dots,p_n\end{bmatrix}, p_{i} \in \mathbb{F}^{n}, i=1,2,\dots,n\]</span></p><p><span class="math display">\[Q = \begin{bmatrix}    q_1,q_2,\dots,q_{m}\end{bmatrix}, q_j \in \mathbb{F}^{m},i=1,2,\dots,m\]</span></p><p>  <span class="math inline">\(\because AP=QB\)</span>，故有：</p><p><span class="math display">\[A\begin{bmatrix}    p_1,p_2,\dots,p_n \\\end{bmatrix}=\begin{bmatrix}    q_1,q_2,\dots,q_m\end{bmatrix}B\]</span></p><p>  矩阵<span class="math inline">\(A \in \mathbb{F}^{m \timesn}\)</span>，可视为线性映射：</p><p><span class="math display">\[\begin{split}    \mathbb{F}^{n} &amp; \rightarrow \mathbb{F}^{m} \\     x &amp; \rightarrow y=Ax  \end{split}\]</span></p><p>  入口基：<span class="math inline">\(\begin{bmatrix}  p_1,p_2,\dots,p_n\end{bmatrix}\)</span>，出口基：<span class="math inline">\(\begin{bmatrix}  q_1,q_2,\dots,q_{m}\end{bmatrix}\)</span>，由线性映射的概念可以得到矩阵等价的几何意义是：<strong>线性映射<span class="math inline">\(A\)</span>在入口基<span class="math inline">\(P\)</span>和出口基<span class="math inline">\(Q\)</span>下的矩阵表示为<span class="math inline">\(B\)</span>.</strong></p>]]></content>
      
      
      <categories>
          
          <category> 矩阵分析 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>机器学习-5.逻辑回归</title>
      <link href="/2023/11/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-5-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
      <url>/2023/11/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-5-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</url>
      
        <content type="html"><![CDATA[<h1 id="逻辑回归">逻辑回归</h1><p>  逻辑回归(LogisticRegression)是一种用于解决二分类问题的机器学习方法。虽然逻辑回归在名称中含有“回归”一词，但其实际上主要用于分类问题。逻辑回归的主要思想是将由线性回归所得的值通过一个逻辑函数映射到0和1之间的概率值，从而将线性回归模型转化为一个分类模型。逻辑回归最早是由统计学家和生物学家使用，用于建立生物学实验结果与概率的关系。后来，随着计算机科学的发展，逻辑回归成为机器学习领域的重要算法之一，被广泛用于分类问题。逻辑回归的优势在于简单易懂、计算效率高，特别适用于大规模数据集。然而，在处理复杂非线性关系的问题上，逻辑回归可能受到限制，这时候更复杂的模型（如支持向量机、深度学习等）可能更为适用。</p><h2 id="基本思想">基本思想</h2><p>  逻辑回归的基本思想是通过逻辑函数(也称为sigmoid函数)将线性回归的结果映射为一个概率值，然后利用概率值解决二分类问题。逻辑回归的基本思想中包含着三个主要因素：<strong>线性回归</strong>、<strong>逻辑函数</strong>、<strong>决策边界</strong>。<br>  线性回归很好理解，给定一个输入的特征向量 <span class="math inline">\(x=\begin{bmatrix}  x_1,x_2,\dots,x_n \\\end{bmatrix}^{T}\)</span>，以及权重向量 <span class="math inline">\(w=\begin{bmatrix}  w_1,w_2,\dots,w_n \\\end{bmatrix}^{T}\)</span>，以及偏置项 <span class="math inline">\(b\)</span>，可以计算线性回归结果：</p><p><span class="math display">\[z =w^{T}x+b=w_1x_1+w_2x_2+\dots+w_nx_n+b\]</span></p><p>  逻辑函数<span class="math inline">\(f\)</span>的作用是将线性回归的结果映射到一个概率值，即：</p><p><span class="math display">\[f: z \rightarrow p \in[0,1]\]</span></p><p>在实际中，我们的逻辑函数一般为<span class="math inline">\(sigmoid\)</span>函数，其函数形式为：</p><p><span class="math display">\[sigmoid(z)=\frac{1}{1+e^{-z}}\]</span></p><p>其函数图像为：</p><center><img src="https://s2.loli.net/2023/11/15/4rRHc3iUGn2FDKx.jpg" width="60%" height="60%"><div data-align="center">Image1: sigmoid函数图像</div></center><p>  sigmoid函数的主要优点如下：</p><ul><li><strong>输出范围为(0,1):</strong>sigmoid函数的输出范围在0和1之间，这与概率的范围一致。这使得逻辑回归的输出可以被解释为属于某个类别的概率。</li><li><strong>可导性:</strong>sigmoid函数是可导的，这使得使用梯度下降等优化算法来最小化损失函数成为可能。梯度下降等优化方法对于机器学习模型的训练非常重要，而Sigmoid函数的可导性使得模型参数可以通过梯度下降等优化方法进行有效地更新。<br></li><li><strong>单调递增性:</strong>sigmoid函数是单调递增的，这意味着输入变量的增加必然导致输出的增加。这一特性有助于模型学习输入特征与输出概率之间的关系，使得模型更容易收敛。</li><li><strong>数学上平滑:</strong>sigmoid函数的平滑性质有助于在优化过程中避免梯度爆炸或梯度消失的问题，这在深度学习等领域尤为重要。</li><li><strong>对异常值的鲁棒性:</strong>sigmoid函数在极端值上趋于饱和，对于一些异常值的影响相对较小。这有助于模型对于噪声或异常值的鲁棒性。</li></ul><p>  决策边界是指当我们通过逻辑函数得到概率值 <span class="math inline">\(p\)</span> 后，我们如何判别实例<span class="math inline">\(x\)</span>属于哪一个类别。当我们在面对的是二分类问题时，设<span class="math inline">\(y \in \{0,1\}\)</span> 表示实例<span class="math inline">\(x\)</span>的类别，概率值<span class="math inline">\(p\)</span>表示条件概率：</p><p><span class="math display">\[p = P(y=1 | x)\]</span></p><p>则我们选用的决策边界为：</p><p><span class="math display">\[\hat{y}= \left \{\begin{array}{rcl}1, &amp; {p &gt;0.5}\\0,&amp; {p \leq 0.5}\\\end{array} \right.\]</span></p><p>  其中，<span class="math inline">\(\hat{y}\)</span>为实例<span class="math inline">\(x\)</span>的预测类别。</p><h2 id="模型">模型</h2><p><strong>输入</strong></p><ul><li><strong>输入空间:</strong> <span class="math inline">\(\mathcal{X}\in \mathbb{R}^{n}\)</span>.<br></li><li><strong>输入实例:</strong> <span class="math inline">\(x =\begin{bmatrix}  x^{(1)},x^{(2)},\dots,x^{(n)} \\ \end{bmatrix}^T \in\mathcal{X}\)</span>.</li></ul><p>  其中，输入空间<span class="math inline">\(\mathcal{X}\)</span>为<span class="math inline">\(n\)</span>维实数空间的子集，输入实例<span class="math inline">\(x\)</span>为<span class="math inline">\(n\)</span>维特征向量。</p><p><strong>输出</strong><br>  由于我们只考虑二分类问题，因此实例点的类别只有正类与负类两种。</p><ul><li><strong>输出空间:</strong> <span class="math inline">\(\mathcal{Y} =\{ 0,1 \}\)</span><br></li><li><strong>输出实例:</strong> <span class="math inline">\(y \in\mathcal{Y}\)</span>.</li></ul><p>  其中，输出空间<span class="math inline">\(\mathcal{Y}\)</span>为只包含1，0两个元素的集合，1与0分别代表二分类问题中的正类与负类。输出实例<span class="math inline">\(y\)</span>代表相对应的输出实例<span class="math inline">\(x\)</span>的类别。</p><p><strong>数据集</strong><br>  逻辑回归的训练数据集<span class="math inline">\(T_{train}\)</span>为：</p><p><span class="math display">\[T_{train}=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N) \}\]</span></p><p>  其中，<span class="math inline">\(x_i \in\mathcal{X}=\mathbb{R}^{n},y_i \in\mathcal{Y}=\{0,1\},i=1,2,\dots,N\)</span>，<span class="math inline">\(N\)</span>表示训练数据的样本容量。</p><p><strong>模型</strong><br>  逻辑回归的模型形式表现为条件概率分布：</p><p><span class="math display">\[\begin{split}    P(Y=1|X) &amp;= \frac{1}{1+e^{-(w^{T}x+b)}} \\    P(Y=0|X) &amp;= \frac{e^{-(w^{T}x+b)}}{1+e^{-(w^{T}x+b)}}\end{split}\]</span></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>机器学习-4.交叉熵与KL散度</title>
      <link href="/2023/11/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-4-%E4%BA%A4%E5%8F%89%E7%86%B5%E4%B8%8EKL%E6%95%A3%E5%BA%A6/"/>
      <url>/2023/11/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-4-%E4%BA%A4%E5%8F%89%E7%86%B5%E4%B8%8EKL%E6%95%A3%E5%BA%A6/</url>
      
        <content type="html"><![CDATA[<h1 id="交叉熵与kl散度">交叉熵与KL散度</h1><p>  在机器学习中，我们经常使用信息熵、交叉熵、KL散度等概率，例如在决策树中，我们使用基于信息熵的信息增益来构造树形结构；而交叉熵常被用于分类问题的损失函数；KL散度则被用于衡量两个分布之间的差异。本文将会介绍这些常用的概率，以便于我们今后学习相应的机器学习模型。</p><h2 id="信息熵-entropy">信息熵 (entropy)</h2><h3 id="熵">熵</h3><p>  在现实中，我们会接触到各种各样的信息，如何对信息进行量化便成为了一个重要的问题。信息论是应用数学的一个分支，由美国数学家香农提出并发展壮大，主要研究的是对一个事件包含信息的多少进行量化。<br>  信息论的基本思想是一个小概率事件发生了，要比大概率事件发生，提供的信息更多。在信息论中，我们认为事件的信息量具有以下三条性质：</p><ul><li>大概率事件所包含的信息量较小。<br></li><li>小概率事件所包含的信息量较大。</li><li>独立事件的信息量可以进行累加。</li></ul><p>  由以上三条性质，我们定义了某一事件<span class="math inline">\(X=x\)</span>的<strong>自信息量</strong>(self-information)为:</p><p><span class="math display">\[I(x)=-\log{P(x)}\]</span></p><p>  其中<span class="math inline">\(X\)</span>为随机变量，表示某一事件；<span class="math inline">\(x\)</span>为随机变量<span class="math inline">\(X\)</span>的取值。当上式中的<span class="math inline">\(\log\)</span>以2为底数时，<span class="math inline">\(I(x)\)</span>的单位是比特(bit)或者香农(shannons)；当<span class="math inline">\(\log\)</span>以2为底数时，<span class="math inline">\(I(x)\)</span>单位是奈特(nats)。这两个单位之间可以通过对数换底公式相互转换。<br>  自信息量表示单个事件的信息量。若我们已知事件<span class="math inline">\(X\)</span>服从某一概率分布<span class="math inline">\(P(X)\)</span>，我们可以使用<strong>香农熵</strong>(Shannonentropy)来对整个概率分布所包含的信息总量进行量化：</p><p><span class="math display">\[H(X)=\mathbb{E}_{X \simP}[I(x)]\]</span></p><p>  若随机变量<span class="math inline">\(X\)</span>为离散型随机变量，则熵可以写为求和形式：</p><p><span class="math display">\[H(X)=\sum_{i=1}^{N}P(x_i)I(x_i)=-\sum_{i=1}^{N}P(x_i)\log{P(x_i)}\]</span></p><p>  若随机变量<span class="math inline">\(X\)</span>为连续型随机变量，则熵可以写为积分形式：</p><p><span class="math display">\[H(X)=\int_{\mathcal{X}}P(x)I(x)dx=-\int_{\mathcal{X}}P(x)\log{P(x)}dx\]</span></p><h3 id="联合熵">联合熵</h3><p>  若有两个随机变量<span class="math inline">\(X,Y\)</span>，且服从某个联合分布<span class="math inline">\(P(X,Y)\)</span>，我们可以使用联合熵来对联合概率分布所包含的信息量进行量化：</p><p><span class="math display">\[H(X,Y)=-\sum_{x \in \mathcal{X}}\sum_{y\in \mathcal{Y}}P(x,y)\log{P(x,y)}\]</span></p><p>  以上给出的是<span class="math inline">\(X,Y\)</span>为离散型随机变量的联合熵，若<span class="math inline">\(X,Y\)</span>为连续型随机变量，则依照熵的形式，联合熵也可以写成积分形式：</p><p><span class="math display">\[H(X,Y)=-\int_{\mathcal{X}}\int_{\mathcal{Y}}P(x,y)\log{P(x,y)}dydx\]</span></p><h3 id="条件熵">条件熵</h3><p>  在数理统计中，我们还学习了条件概率分布，表示在某个事件发生后，另一个事件所发生的概率，在信息论中，用条件熵来表示条件概率分布所包含的信息。若有两个离散随机变量<span class="math inline">\(X,Y\)</span>，且已知联合概率分布<span class="math inline">\(P(X,Y)\)</span>，条件概率分布<span class="math inline">\(P(Y|X)\)</span>，则该条件分布的条件熵为：</p><p><span class="math display">\[H(Y|X)=-\sum_{x \in \mathcal{X}}\sum_{y\in \mathcal{Y}}P(x,y)\log{P(y|x)}\]</span></p><p>  当<span class="math inline">\(X,Y\)</span>为连续型随机变量时，上式可以写出积分形式：</p><p><span class="math display">\[H(Y|X)=-\int_{\mathcal{X}}\int_{\mathcal{Y}}P(x,y)\log{P(y|x)dydx}\]</span></p><h3 id="最大熵思想">最大熵思想</h3><p>  既然信息熵可以用来表示信息量的大小，人们自然希望找到包含信息量最大的概率分布。当我们的概率分布中存在未知参数时，可以使用<strong>最大化分布的熵</strong>的思想来估计未知参数，这类方法在机器学习中被称为最大熵学习，这里不展开讨论最大熵学习，有兴趣的读者可查阅相关资料进行了解。<br>  通过最大熵思想，我们有一个非常有意思的发现：<strong>若有定义在整个实数轴上的随机变量<span class="math inline">\(X\)</span>，其均值为<span class="math inline">\(\mu\)</span>，方差为<span class="math inline">\(\sigma^{2}\)</span>，当<span class="math inline">\(X \sim N(\mu,\sigma^2)\)</span>时，熵 <span class="math inline">\(H(X)\)</span> 最大.</strong>这个结论的证明放在目录，有兴趣的读者可自行阅读。高斯分布具有最大的信息熵，这也是为什么在现实生活中大量随机事件都服从高斯分布，因为自然界总是偏向于制造最大的不确定性，从而包含最多的信息，即最大的熵。</p><h2 id="交叉熵cross-entropy">交叉熵(cross entropy)</h2><p>  在机器学习中，我们常常需要比较两个分布之间的相似程度，例如当我们用一个估计的分布去近似真实分布时，我们自然希望这两个分布越相似越好。交叉熵便是衡量两个分布之间相似程度的一种度量方法，因此经常被用作机器学习模型的损失函数。<br>  为了理解交叉熵为什么能够度量两个分布的相似程度，我们借助贝叶斯统计的思想来进行解释。根据贝叶斯思想，对于某一个事件<span class="math inline">\(X\)</span>，我们会有一个先验的认知，即<span class="math inline">\(X\)</span>的先验分布<span class="math inline">\(P_0(x)\)</span>，在先验认知下，事件<span class="math inline">\(X\)</span>带给我们的信息量为：</p><p><span class="math display">\[I_{0}(x)=-\log{P_{0}(x)}\]</span></p><p>  假设事件<span class="math inline">\(X\)</span>的真实分布为<span class="math inline">\(P_{1}(x)\)</span>，则<strong>我们在主观认知下，通过客观事实所得到的事件<span class="math inline">\(X\)</span>的概率分布所包含的信息总量</strong>为：</p><p><span class="math display">\[H(P_0,P_1)=-\sum_{i=1}^{N}P_{1}(x)\log{P_{0}(x)}dx\]</span></p><p>  当<span class="math inline">\(X\)</span>为连续型随机变量时，其积分形式为：</p><p><span class="math display">\[H(P_{0},P_{1})=-\int_{\mathcal{X}}P_{1}(x)\log{P_{0}(x)}dx\]</span></p><p>  上式即为分布<span class="math inline">\(P_0,P_1\)</span>的<strong>交叉熵</strong>。若交叉熵较大，说明在已有主观先验认知下，事件<span class="math inline">\(X\)</span>的实际情况带给我们的信息量较大，说明我们的先验认知与实际情况差别较大，即<span class="math inline">\(P_{0}(x)\)</span>与<span class="math inline">\(P_{1}(x)\)</span>的相似度较低；若交叉熵较小，说明在已有主观先验认知下，事件<span class="math inline">\(X\)</span>的实际情况带给我们的信息量较小，说明我们的先验认知与实际情况差别较小，即<span class="math inline">\(P_{0}(x)\)</span>与<span class="math inline">\(P_{1}(x)\)</span>的相似度较高。<br>  在机器学习中，我们希望学习到的概率分布<span class="math inline">\(P_{m}\)</span>与训练数据所估计的真实分布<span class="math inline">\(P_{t}\)</span>足够相似，这时我们通常将分布<span class="math inline">\(P_{m}\)</span>与<span class="math inline">\(P_{t}\)</span>的交叉熵作为损失函数，例如逻辑回归模型，通过最小化交叉熵来调整<span class="math inline">\(P_{m}\)</span>，使得<span class="math inline">\(P_{m}\)</span>与真实分布<span class="math inline">\(P_{t}\)</span>足够相似。<br>  另外，有一个非常有意思的结论，<strong>最小化交叉熵实际上是等价于极大似然估计</strong>，这个结论的证明将会放在附录。</p><h2 id="kl散度kl-divergence">KL散度(KL Divergence)</h2><p>  上文介绍了交叉熵，其可以用来衡量两个分布的相似程度。但交叉熵存在一个问题，即若我们主观的先验认知与客观实际完全一致，即<span class="math inline">\(P_{0}=P_{1}\)</span>，此时我们实际上并没有得到任何信息，但交叉熵的计算结果为<span class="math inline">\(P_{0}\)</span>的信息熵，并不为<span class="math inline">\(0\)</span>。因此，我们可以转而考虑信息的增量，KL散度实际上就时在考虑信息熵的增量，我们先给出两个分布<span class="math inline">\(P_0,P_1\)</span>的KL散度的计算公式：</p><p><span class="math display">\[KL(P_1||P_0)=\mathbb{E}_{X \simP_1}[\log{\frac{P_{1}(x)}{P_{0}(x)}}]\]</span></p><p>  通过将上式进行调整，我们可以将<span class="math inline">\(P_{0}\)</span>与<span class="math inline">\(P_{1}\)</span>的KL散度写成<span class="math inline">\(P_{0}\)</span>与<span class="math inline">\(P_{1}\)</span>的交叉熵<span class="math inline">\(H(P_{0},P_{1})\)</span>与<span class="math inline">\(P_{0}\)</span>的信息熵之差：</p><p><span class="math display">\[KL(P_{1}||P_{0})=H(P_{0},P_{1})-H(P_{0})\]</span></p><p>  通过上式我们可以发现，当<span class="math inline">\(P_1\)</span>与<span class="math inline">\(P_0\)</span>的KL散度较大，说明在已有主观认知下，我们从客观事件获得信息增量较大，说明我们的主观认知与客观现实不一致，即<span class="math inline">\(P_1\)</span>与<span class="math inline">\(P_0\)</span>的相似度较低；当<span class="math inline">\(P_1\)</span>与<span class="math inline">\(P_0\)</span>的KL散度较小，说明在已有主观认知下，我们从客观事件获得信息增量较小，说明我们的主观认知与客观现实较为一致，即<span class="math inline">\(P_1\)</span>与<span class="math inline">\(P_0\)</span>的相似度较大；当<span class="math inline">\(P_1\)</span>与<span class="math inline">\(P_0\)</span>的KL散度等于0时，说明已有主观认知下，我们从客观事件中没有获得额外的信息，说明我们的主观认知与客观现实完全一致，即<span class="math inline">\(P_0=P_1\)</span>.<br>  KL散度衡量了一种信息增益，因此也被称为<strong>相对熵</strong>。在机器学习中我们同样可以使用KL散度作为损失函数。</p><h2 id="附录">附录</h2><h3 id="一-高斯分布具有最大的信息熵">(一) 高斯分布具有最大的信息熵</h3><p><strong>结论:</strong> 已知定义在整个实数轴上的连续型随机变量<span class="math inline">\(X\)</span>的均值为<span class="math inline">\(\mu\)</span>，方差为<span class="math inline">\(\sigma^{2}\)</span>，当<span class="math inline">\(X \sim N(\mu,\sigma^{2})\)</span>，<span class="math inline">\(X\)</span>的信息熵<span class="math inline">\(H(X)\)</span>最大。<br><strong>证明:</strong><br>  设随机变量<span class="math inline">\(X\)</span>的概率密度函数为<span class="math inline">\(p(x)\)</span>，由题意可知：</p><p><span class="math display">\[E(X)=\int_{-\infty}^{+\infty}xp(x)dx=\mu,\quadVar(X)=\int_{-\infty}^{+\infty}(x-\mu)^{2}p(x)dx=\sigma^{2}\]</span></p><p>  根据最大熵思想，可以得到如下一个带约束的优化问题：</p><p><span class="math display">\[\begin{split}    \max_{p(x)} \quad &amp;H(X)=-\int_{-\infty}^{+\infty}p(x)\ln{p(x)}dx  \\    s.t. \quad &amp; \int_{-\infty}^{+\infty}p(x)=1 \\    &amp; \int_{-\infty}^{+\infty}xp(x)dx=\mu  \\    &amp; \int_{-\infty}^{+\infty}(x-\mu)^{2}p(x)dx=\sigma^{2}\end{split}\]</span></p><p>  上述原问题的拉格朗日函数为：</p><p><span class="math display">\[\begin{split}    Q(p(x),\lambda_1,\lambda_2,\lambda_3) =&amp;-\int_{-\infty}^{+\infty}p(x)\ln{p(x)}dx+\lambda_1 \left(\int_{-\infty}^{+\infty}p(x)-1 \right) \\    &amp;+ \lambda_2 \left( \int_{-\infty}^{+\infty}xp(x)dx-\mu\right)+\lambda_3 \left(\int_{-\infty}^{+\infty}(x-\mu)^{2}p(x)dx-\sigma^{2} \right)\end{split}\]</span></p><p>  令<span class="math inline">\(\lambda=\begin{bmatrix}  \lambda_1,\lambda_2,\lambda_3\end{bmatrix}^{T}\)</span>，则原问题的无约束形式可以写为</p><p><span class="math display">\[\begin{split}     \max_{p(x)}\min_{\lambda} \quad &amp;Q(p(x),\lambda_1,\lambda_2,\lambda_3)  \\     \space s.t. \quad &amp; \lambda \ge 0\end{split}\]</span></p><p>  则原问题的对偶问题为：</p><p><span class="math display">\[\begin{split}     \min_{\lambda}\max_{p(x)} \quad &amp;Q(p(x),\lambda_1,\lambda_2,\lambda_3)  \\     \space s.t. \quad &amp; \lambda \ge 0\end{split}\]</span></p><p>  设<span class="math inline">\(p^{*}(x)\)</span>为原问题最优解，<span class="math inline">\(\lambda^{*}\)</span>为对偶问题最优解，由KKT条件得：</p><p><span class="math display">\[\frac{\partial Q}{\partial p(x)}|_{p(x)=p^{*}(x)}=-[\ln{p^{*}(x)+1}]+\lambda_1^{*}+\lambda_2^{*}x+\lambda_3^{*}(x-\mu)^{2}=0\]</span></p><p><span class="math display">\[\Rightarrowp^{*}(x)=e^{\lambda_1^{*}-1+\lambda_2^{*}x+\lambda_3^{*}(x-\mu)^{2}}\]</span></p><p>  之后对<span class="math inline">\(p^{*}(x)\)</span>做一些变形：</p><p><span class="math display">\[\begin{split}    p^{*}(x) &amp;=e^{\lambda_1^{*}-1+\lambda_2^{*}x+\lambda_3^{*}(x-\mu)^{2}}=e^{\lambda_1^{*}-1}e^{\lambda_3^{*}x^2+(\lambda_2^{*}-2\mu\lambda_3^{*})x+\mu^{2}\lambda_3^{*}}\\    &amp;=Ce^{\lambda_3^{*}[x^2+(\frac{\lambda_2^{*}}{\lambda_3^{*}}-2\mu)x+\mu^2]}= Ce^{\lambda_3^{*}[x-(\mu-\frac{\lambda_2^{*}}{2\lambda_3^{*}})]^{2}}\end{split}\]</span></p><p>  由密度函数<span class="math inline">\(p^{*}(x)\)</span>的非负性以及正则性可知：<span class="math inline">\(C&gt;0,\lambda_3^{*}&lt;0\)</span>；指数部分中的二次函数<span class="math inline">\([x-(\mu-\frac{\lambda_2^{*}}{2\lambda_3^{*}})]^{2}\)</span>表明 <span class="math inline">\(p^{*}(x)\)</span>的对称轴为 <span class="math inline">\(\mu-\frac{\lambda_2^{*}}{2\lambda_3^{*}}\)</span>，由于对称的密度函数，其对称轴一定等于均值可知：<span class="math inline">\(\lambda_2^{*}=0\)</span>。<br>  由于<span class="math inline">\(\lambda_3^{*}&gt;0\)</span>，可设<span class="math inline">\(\lambda_3^{*}=-\beta\)</span>，则<span class="math inline">\(p^{*}(x)\)</span>可化简为：</p><p><span class="math display">\[p^{*}(x)=Ce^{-\beta(x-\mu)^{2}}\]</span></p><p>  将<span class="math inline">\(p^{*}(x)\)</span>代入正则化约束得：</p><p><span class="math display">\[\begin{split}    1 &amp;=\int_{-\infty}^{+\infty}p^{*}(x)dx=C\int_{-\infty}^{+\infty}e^{-\beta(x-\mu)^{2}}dx\\    &amp;=C\int_{-\infty}^{+\infty}e^{-\betay^{2}}dy=\frac{C}{\sqrt{\beta}}\int_{0}^{+\infty}z^{-\frac{1}{2}}e^{-z}dz \\    &amp;=\frac{C}{\sqrt{\beta}}\Gamma \left( \frac{1}{2} \right) =C\sqrt{\frac{\pi}{\beta}}\end{split}\]</span></p><p>  从而得：<span class="math inline">\(C=\sqrt{\frac{\beta}{\pi}}\)</span>，再利用方差约束条件得：</p><p><span class="math display">\[\begin{split}    \sigma^{2} &amp;= \int_{-\infty}^{+\infty}(x-\mu)^2p^{*}(x)dx =C\int_{-\infty}^{+\infty}(x-\mu)^{2}e^{-\beta(x-\mu)^{2}}dx  \\    &amp;=2C\int_{0}^{+\infty}y^{2}e^{-\beta  y^{2}}dx = \frac{C}{\beta\sqrt{\beta}} \int_{0}^{+\infty} z^{\frac{1}{2}}e^{-z}dz \\    &amp;= \frac{C}{\beta \sqrt{\beta}} \Gamma \left( \frac{3}{2}\right) = \sqrt{\frac{\beta}{\pi}} \cdot \frac{1}{\beta \sqrt{\beta}}\cdot \frac{\sqrt{\pi}}{2}= \frac{1}{2\beta}\end{split}\]</span></p><p>  从而得：<span class="math inline">\(\beta =\frac{1}{2\sigma^{2}}\)</span>，联立：</p><p><span class="math display">\[\left \{\begin{array}{l}p^{*}(x)=Ce^{-\beta(x-\mu)^{2}}  \\C=\sqrt{\frac{\beta}{\pi}}  \\\beta = \frac{1}{2\sigma^{2}} \\\end{array} \right.\]</span></p><p>  解得：</p><p><span class="math display">\[p^{*}(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}}\]</span></p><p>  即当<span class="math inline">\(X \simN(\mu,\sigma^{2})\)</span>时，熵<span class="math inline">\(H(X)\)</span>最大，证毕.</p><h3 id="二-最小化交叉熵等价于极大似然估计">(二)最小化交叉熵等价于极大似然估计</h3><p><strong>结论：</strong> 假设我们从训练数据集<span class="math inline">\(T_{train}\)</span>中所得到的数据的经验分布为<span class="math inline">\(P_{t}(x)=\frac{1}{N}\)</span>(最大熵思想)，<span class="math inline">\(N\)</span>为训练数据的样本容量，我们所需要学习的模型为<span class="math inline">\(P_{m}(x;\theta)\)</span>，则有：</p><p><span class="math display">\[\min_{\theta}H(P_{t}(x),P_{m}(x;\theta)) \Leftrightarrow \max_{\theta} \prod_{x \inT} P_{m}(x;\theta)\]</span></p><p><strong>证明:</strong></p><p><span class="math display">\[\begin{split}    \max_{\theta} \prod_{x \in T} P_{m}(x;\theta) &amp; \Leftrightarrow\max_{\theta} \sum_{x \in T} \log{P_{m}(x;\theta)} \Leftrightarrow-\min_{\theta} \sum_{x \in T} \log{P_{m}(x;\theta)} \\    &amp; \Leftrightarrow -\min_{\theta} \sum_{i=1}^{N}\frac{1}{N}\log{P_{m}(x;\theta)} \Leftrightarrow \min_{\theta}\mathbb{E}_{X \sim P_{t}}[-\log{P_{m}(x;\theta)}]  \\    &amp; \Leftrightarrow \min_{\theta} H(P_{t}(x),P_{m}(x;\theta))\end{split}\]</span></p><p>  证毕.</p><h2 id="参考">参考</h2><p><strong>[1] Book:董平,机器学习中的统计思维(Python实现)</strong><br><strong>[2]Blog：知乎,康斯坦丁,一篇文章讲清楚交叉熵和KL散度</strong></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>深度学习-2-Paper:生成对抗网络GANs——深度学习二十年间最酷的idea!</title>
      <link href="/2023/11/06/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-2-Paper-%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9CGANs%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8D%81%E5%B9%B4%E9%97%B4%E6%9C%80%E9%85%B7%E7%9A%84idea/"/>
      <url>/2023/11/06/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-2-Paper-%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9CGANs%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8D%81%E5%B9%B4%E9%97%B4%E6%9C%80%E9%85%B7%E7%9A%84idea/</url>
      
        <content type="html"><![CDATA[<h1 id="generative-adversarial-nets">Generative Adversarial Nets</h1><p>  生成对抗网络(GANs)是一种深度学习框架，由 lan Goodfellow和他的同事们于2014年提出，论文成果发表于人工智能顶会NIPS(NeuralInformation ProcessingSystems)。GANs被认为是深度学习领域的一项重大突破，其应用涵盖图像生成、图像修复、语音和文本合成、风格迁移和艺术创作、欺骗检测等多个领域。图灵奖得主，被誉为深度学习三巨头之一的Yann LeCunn 称赞GANs为 "the coolest idea in deep learning in the last 20years."。接下来，我们就来一起欣赏一下深度学习这二十年间最酷的idea！</p><p>  论文链接：https://arxiv.org/abs/1406.2661</p><h2 id="摘要">摘要</h2><p>  图1是 lan Goodfellow 在GANs的原始论文中所写的摘要全文：</p><center><img src="https://s2.loli.net/2023/11/06/fzbnJkFKo4cqOug.png" width="80%" height="80%"><div data-align="center">Image1: The Abstract of GANs</div></center><p>  摘要显示这篇论文提出了一种全新的基于对抗过程的生成模型框架。在这个框架中，存在着两个基于神经网络的模型：生成模型<span class="math inline">\(G\)</span>与判别模型<span class="math inline">\(D\)</span>。生成模型<span class="math inline">\(G\)</span>的作用是估计数据的真实分布；判别模型<span class="math inline">\(D\)</span>是用于判断所输入的样本来自于真实数据而非<span class="math inline">\(G\)</span>所生成的概率。生成模型<span class="math inline">\(G\)</span>的训练过程是最大化判别模型<span class="math inline">\(D\)</span>犯错的概率；判别模型<span class="math inline">\(D\)</span>的训练过程是最小化<span class="math inline">\(D\)</span>犯错的概率，整个GANs的训练过程可视为Minimax的二元博弈过程。通过理论分析发现，存在一个唯一的最优解，使得生成模型<span class="math inline">\(G\)</span>能够正确模拟训练数据的分布，同时判别模型所给出的概率几乎处处为<span class="math inline">\(\frac{1}{2}\)</span>，即几乎不能分辨所输入的样本是来自于真实数据分布还是生成模型<span class="math inline">\(G\)</span>。</p><h2 id="背景及主要思想">背景及主要思想</h2><h3 id="背景">背景</h3><p>  深度学习的目标是构建模型来表示在人工智能的应用中遇到的数据的概率分布，例如图像、音频、自然语言的语料等。在2014年之前，判别模型在这方面占据着主导，这些模型通常是利用反向传播算法、Dropout、ReLU等技术，直接学习一个从高维特征空间到实例类别的映射。与判别模型相比，生成模型的发展则有些相形见绌。这一方面是由于在与最大似然估计相关的策略中，有许多难以解决的概率计算问题；另一方面，判别模型在NLP任务中也难以利用分段线性单元的优势。</p><h3 id="主要思想">主要思想</h3><p>  本文的作者表示他们所提出的新的生成模型能够避开这些困难。生成对抗网络的主要思想可以概括为两个字——“对抗”，具体而言，在GANs框架的训练过程中，生成模型<span class="math inline">\(G\)</span>的训练目标是最大化判别模型犯错的概率，即希望由生成模型<span class="math inline">\(G\)</span>所生成的样本能够成功“骗过”判别模型；判别模型<span class="math inline">\(D\)</span>的训练目标是最小化自身犯错的概率，即希望判别模型的“鉴伪”能力越高越好。当我们将生成模型<span class="math inline">\(G\)</span>所生成的样本用于训练判别模型<span class="math inline">\(D\)</span>，并交替着训练这两个模型，便会引发这两个模型之间的“对抗”，它们为了达到自身的训练目标便会在对抗中提升各自的性能。最终，通过若干次训练，我们能得到一个性能非常好的生成模型<span class="math inline">\(G\)</span>，它所生成的样本与真实样本十分接近，以至于判别模型无法在所给的的参数量下分辨二者的区别，即生成模型所生成的样本几乎能够反映真实的数据分布。<br>  在GANs的论文中，lan Goodfellow用一个十分形象的比喻来说明生成对抗网络的基本思想。假设我们的目标是能够制造足够逼真的假钞，我们只需要找来两个队伍，一方是制造假钞的犯罪集团，另一方是警察队伍，犯罪集团的目标是制造假钞并在不被发现的情况下使用假钞；警察队伍的目标是鉴别假钞。这样，我们并不需要做太多的事情，只需要将这两方放在一起，让他们彼此对抗。在对抗的过程中，警察队伍鉴别假钞的能力会越来越强，犯罪集团所制造的假钞也会越来越逼真，最终我们便能得到足够逼真的假钞。</p><h2 id="模型构成">模型构成</h2><p>  在论文中，为了让生成模型<span class="math inline">\(G\)</span>能够学习到真实数据(训练数据)的分布<span class="math inline">\(p_{data}\)</span>，作者首先定义了输入噪声变量的先验分布<span class="math inline">\(p_{z}(z)\)</span>；生成模型<span class="math inline">\(G(z;\theta_{g})\)</span>由参数为<span class="math inline">\(\theta_{g}\)</span>的神经网络定义，其作用是将噪声变量<span class="math inline">\(z\)</span>映射到数据空间<span class="math inline">\(\mathcal{X}\)</span>:</p><p><span class="math display">\[G(z;\theta_{g}): z \rightarrowx\]</span></p><p>  由生成模型<span class="math inline">\(G(z;\theta_{g})\)</span>生成的数据<span class="math inline">\(x\)</span>的概率分布为<span class="math inline">\(p_{g}\)</span>。同时，作者定义了判别模型<span class="math inline">\(D(x;\theta_{d})\)</span>，判别模型<span class="math inline">\(D(x;\theta_{d})\)</span>由参数为<span class="math inline">\(\theta_{d}\)</span>的神经网络构成，其作用是给出输入数据<span class="math inline">\(x\)</span>是来自于真实数据分布<span class="math inline">\(p_{data}\)</span>而非生成模型<span class="math inline">\(G\)</span>的概率<span class="math inline">\(p\)</span>：</p><p><span class="math display">\[D(x;\theta_{d}): x \rightarrowp\]</span></p><p>  根据GANs的基本思想，在模型的训练过程中，生成模型<span class="math inline">\(G\)</span>的训练目标是最大化判别模型<span class="math inline">\(D\)</span>犯错的概率；判别模型<span class="math inline">\(D\)</span>的训练目标是最小化自身犯错的概率。其训练目标构成了一个minimax 的博弈过程。作者定义了训练的目标函数<span class="math inline">\(V(D,G)\)</span>，训练目标可以写为(1)式：</p><p><span class="math display">\[\begin{equation}\min_{G} \max_{D} V(D,G)=\mathbb{E}_{x \simp_{data}(x)}[\log{D(x)}]+\mathbb{E}_{z \sim p_{z}(z)}[\log{(1-D(G(z)))}]\end{equation}\]</span></p><p>  从目标函数可以得出，若固定<span class="math inline">\(G\)</span>，则判别模型<span class="math inline">\(D\)</span>为(2)式：</p><p><span class="math display">\[\begin{equation}    \begin{split}        D = \arg \max_{D} V(D,G) &amp;= \mathbb{E}_{x \simp_{data}(x)}[\log{D(x)}]+\mathbb{E}_{z \sim p_{z}(z)}[\log{(1-D(G(z)))}]\\        &amp;= \mathbb{E}_{x \sim p_{data}(x)}[\log{D(x)}]+\mathbb{E}_{x\sim p_{g}(x)}[\log{(1-D(x))}] \\    \end{split}\end{equation}\]</span></p><p>  根据(2)式，在训练过程中判别模型<span class="math inline">\(D\)</span>会调整参数<span class="math inline">\(\theta_{d}\)</span>，使得(2)式中的<span class="math inline">\(D(x)\)</span>较大，<span class="math inline">\(D(G(z))\)</span>较小，其含义是若判别模型<span class="math inline">\(D\)</span>的输入数据<span class="math inline">\(x\)</span>来自于真实的数据分布<span class="math inline">\(p_{data}\)</span>，则模型的输出概率值较大；若输入数据<span class="math inline">\(x\)</span>来自于生成模型定义的分布<span class="math inline">\(p_{g}\)</span>，则模型的输出概率值较小。(2)式实际上就是二分类问题中的交叉熵目标函数，通过(2)式的优化，可以得到一个分类性能更好的判别模型<span class="math inline">\(D\)</span>。<br>  若固定<span class="math inline">\(D\)</span>，则生成模型<span class="math inline">\(G\)</span>为(3)式：</p><p><span class="math display">\[\begin{equation}    \begin{split}        G &amp;= \arg \min_{G} V(D,G) = \mathbb{E}_{x \simp_{data}(x)}[\log{D(x)}]+\mathbb{E}_{z \sim p_{z}(z)}[\log{(1-D(G(z)))}]\\        &amp; \Leftrightarrow \arg \min_{G} \mathbb{E}_{z \simp_{z}(z)}[\log{(1-D(G(z)))}]=\mathbb{E}_{x \simp_{g}(x)}[\log{(1-D(x))}] \\    \end{split}\end{equation}\]</span></p><p>  根据(3)式，在训练过程中生成模型<span class="math inline">\(G\)</span>会调整参数<span class="math inline">\(\theta_{g}\)</span>，使得(3)式中的<span class="math inline">\(D(G(z))\)</span>较大，其含义是将从生成模型<span class="math inline">\(G\)</span>所定义的分布<span class="math inline">\(p_{g}\)</span>中给出的数据<span class="math inline">\(x\)</span>输入到判别模型<span class="math inline">\(D\)</span>中，判别模型输出的概率值较大，即判别模型误认为数据<span class="math inline">\(x\)</span>来自于真实的数据分布<span class="math inline">\(p_{data}\)</span>，这表明生成数据分布<span class="math inline">\(p_{g}\)</span>与真实数据分布<span class="math inline">\(p_{data}\)</span>足够相似，以至于当前的判别模型<span class="math inline">\(D\)</span>无法分辨这两个分布所产生的数据<span class="math inline">\(x\)</span>。</p><h2 id="优化算法">优化算法</h2><p>  lan Goodfellow 在GANs的原始论文中给出的目标函数的优化算法如下</p><center><img src="https://s2.loli.net/2023/11/07/TNJ4mGcanZewflz.png" width="80%" height="80%"><div data-align="center">Image2: 优化算法</div></center><p>  优化算法的基本思路是利用小批量随机梯度下降算法对目标函数进行优化。对于目标函数中的生成模型与判别模型，每次迭代时固定其中一个模型，利用SGD对另一个模型进行参数更新，彼此循环迭代，直至收敛。优化算法中需要特别注意的有以下几点：</p><ul><li>在刚开始迭代时，应该首先固定生成模型<span class="math inline">\(G\)</span>，对判别模型<span class="math inline">\(D\)</span>进行更新。这是因为在开始训练时，判别模型<span class="math inline">\(D\)</span>的参数是随机初始化的，其不具备对样本进行正确分类的能力，而判别模型分类的结果又会直接影响生成模型<span class="math inline">\(G\)</span>的训练，若首先更新生成模型<span class="math inline">\(G\)</span>，则生成模型一开始便有可能完全“骗过”判别模型，导致训练无法成功进行。<br></li><li>在训练过程中，每更新<span class="math inline">\(k\)</span>次判别模型<span class="math inline">\(D\)</span>,再更新1次生成模型<span class="math inline">\(G\)</span>。这会使得只要生成模型<span class="math inline">\(G\)</span>变化得足够缓慢，判别模型<span class="math inline">\(D\)</span>就会维持再其最优解附近。</li><li>在实际对生成模型<span class="math inline">\(G\)</span>的更新中，并不使用(3)式中的最小化 <span class="math inline">\(\log{(1-D(x))}\)</span>，而是最大化 <span class="math inline">\(\log{D(x)}\)</span>。这是因为在开始训练时，判别模型<span class="math inline">\(D\)</span>要强于生成模型<span class="math inline">\(G\)</span>，使得<span class="math inline">\(D(x)(x\sim p_{g})\)</span>的值较小，此时 <span class="math inline">\(\log{(1-D(x))}\)</span> 对<span class="math inline">\(D(x)\)</span>的梯度很小，训练会非常缓慢；而 <span class="math inline">\(\log{D(x)}\)</span> 在<span class="math inline">\(D(x)\)</span>较小时，梯度较大，更有利于参数更新。</li></ul><h2 id="理论分析">理论分析</h2><p>  lanGoodfellow在论文中对目标函数的理论最优解以及算法收敛性进行了分析，得出了非常有意义的结果。</p><h3 id="总体最优解">总体最优解</h3><p>  <strong>结论</strong>: 当<span class="math inline">\(p_{g}=p_{data}\)</span>时，目标函数达到总体最优。<br>  <strong>证明</strong>:<br>  训练目标：</p><p><span class="math display">\[\min_{G} \max_{D} V(D,G)=\mathbb{E}_{x\sim p_{data}(x)}[\log{D(x)}]+\mathbb{E}_{z \simp_{z}(z)}[\log{(1-D(G(z)))}]\]</span></p><p>  当固定生成模型<span class="math inline">\(G\)</span>时，<span class="math inline">\(D^{*}=\arg \max_{D} V(D,G)\)</span>，其中:</p><p><span class="math display">\[\begin{split}    V(D,G) &amp;= \mathbb{E}_{x \sim p_{data}}[\log{D(x)}]+\mathbb{E}_{x\sim p_{g}}[\log{(1-D(x))}] \\    &amp;= \int_{x} p_{data}(x)\log{D(x)} dx + \int_{x}p_{g}(x)\log{(1-D(x))} dx \\    &amp;= \int_{x} [p_{data}(x)\log{D(x)}+p_{g}(x)\log{(1-D(x))}] dx\end{split}\]</span></p><p><span class="math display">\[\begin{split}    \max_{D} V(D,G) &amp;= \max_{D} \int_{x}[p_{data}(x)\log{D(x)}+p_{g}(x)\log{(1-D(x))}] dx \\    &amp; \Leftrightarrow \max_{D} \spacep_{data}(x)\log{D(x)}+p_{g}(x)\log{(1-D(x))} \triangleq L(D)\end{split} \]</span></p><p><span class="math display">\[\frac{\partial L(D)}{\partialD}=\frac{p_{data}(x)}{D(x)}-\frac{p_{g}(x)}{1-D(x)}=0 \RightarrowD^{*}(x)=\frac{p_{data}(x)}{p_{data}(x)+p_{g}(x)}\]</span></p><p><span class="math display">\[\begin{split}    \max_{D} V(D,G) &amp;= V(D^{*},G) = \mathbb{E}_{x \simp_{data}}[\log{D^{*}(x)}]+\mathbb{E}_{x \sim p_{g}}[\log{(1-D^{*}(x))}]\\    &amp;= \int_x p_{data}(x)\log{\left(\frac{p_{data}(x)}{p_{data}(x)+p_{g}(x)} \right)} dx + \int_{x}p_{g}(x)\log{\left( \frac{p_{g}(x)}{p_{data}(x)+p_{g}(x)} \right)}dx \\    &amp;= -2\log{2}+\int_x p_{data}(x)\log{\left(\frac{p_{data}(x)}{(p_{data}(x)+p_{g}(x))/2} \right)} dx + \int_{x}p_{g}(x)\log{\left( \frac{p_{g}(x)}{(p_{data}(x)+p_{g}(x))/2}\right)}dx  \\    &amp;= -2\log{2}+ KL \left( p_{data}(x) ||\frac{p_{data}(x)+p_{g}(x)}{2} \right) + KL \left( p_{g}(x) ||\frac{p_{data}(x)+p_{g}(x)}{2} \right) \\    &amp;= -2\log{2}+2JS(p_{data}(x)||p_{g}(x))\end{split}\]</span></p><p>  完成对<span class="math inline">\(D\)</span>的最大化后，再对<span class="math inline">\(G\)</span>进行极小化：</p><p><span class="math display">\[\min_{G} \max_{D} V(D,G) \Leftrightarrow\min_{G} \space J(G)=-2\log{2}+2JS(p_{data}(x)||p_{g}(x))\]</span></p><p>  当 <span class="math inline">\(p_{g}(x)=p_{data}(x)\)</span>时，<span class="math inline">\(JS(p_{data}(x)||p_{g}(x))_{min}=0\)</span>，<span class="math inline">\(J(G)\)</span>达到最小，证毕。</p><h3 id="算法收敛性">算法收敛性</h3><p><strong>结论:</strong> 若生成模型<span class="math inline">\(G\)</span>和判别模型<span class="math inline">\(D\)</span>拥有足够的参数量，并且在上文优化算法的每一步中，在给定生成模型<span class="math inline">\(G\)</span>下，判别模型<span class="math inline">\(D\)</span>都达到了其最优解，从而有：</p><p><span class="math display">\[p_{g}= \arg\min_{p_{g}} \mathbb{E}_{x\sim p_{data}}[\log{D^{*}_{G}(x)}]+\mathbb{E}_{x \simp_{g}}[\log{(1-D^{*}_{G}(x))}]\]</span></p><p>利用上式对<span class="math inline">\(p_{g}\)</span>进行迭代更新，最终<span class="math inline">\(p_{g}\)</span>会收敛到<span class="math inline">\(p_{data}\)</span>.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>机器学习-3.硬间隔线性支持向量机</title>
      <link href="/2023/10/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-3-%E7%A1%AC%E9%97%B4%E9%9A%94%E7%BA%BF%E6%80%A7%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
      <url>/2023/10/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-3-%E7%A1%AC%E9%97%B4%E9%9A%94%E7%BA%BF%E6%80%A7%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</url>
      
        <content type="html"><![CDATA[<h1 id="硬间隔线性支持向量机">硬间隔线性支持向量机</h1><p>  支持向量机(Support Vector Machine, SVM)是 <span class="math inline">\(Cortes\)</span>和<span class="math inline">\(Vapnik\)</span>于1995年在 <span class="math inline">\(Machine \space Learning\)</span>期刊上提出的分类模型，在自然语言处理、计算机视觉以及生物信息中有着重要的应用。支持向量机的模型形式与感知机相同，但有别于感知机模型的是，支持向量机学习通过添加约束条件，能够得到最优的分离超平面。支持向量机学习的目标是找到一个线性超平面，能够最大程度地分离训练数据集中不同类别的数据点，并且在分离过程中最大化间隔，即数据点与决策边界之间的距离最大化。<br>  根据处理的问题不同，支持向量机可大致分为硬间隔线性支持向量机、软间隔线性支持向量机，非线性支持向量机。本节主要介绍二分类问题下的硬间隔线性支持向量机。</p><h2 id="基本思想">基本思想</h2><p>  我们在之前已经介绍了感知机模型，它的模型形式是：</p><p><span class="math display">\[f(x) = sign(w^Tx+b)\]</span></p><p>  其中，<span class="math inline">\(x \in \mathbb{R}^n; w \in\mathbb{R}^n, b \in \mathbb{R}\)</span>为模型参数。通过设置初始参数，利用梯度下降算法，我们可以得到模型参数<span class="math inline">\(\hat{w},\hat{b}\)</span>.当我们设置不同的初始参数时，我们会得到不同的模型参数，即不同的分离超平面。现假设在二维情形下，我们得到了如图1所示的两个分离超平面<span class="math inline">\(S_1,S_2\)</span>。</p><center><img src="https://s2.loli.net/2023/10/24/Xe3R6PnIalTABcb.jpg" width="60%" height="60%"><div data-align="center">Image1: 感知机中分离超平面</div></center><p>  在感知机学习中，超平面<span class="math inline">\(S_1\)</span>与<span class="math inline">\(S_2\)</span>均可以将训练数据集完全正确分类，这两个分离超平面并没有优劣可分，实际上，感知机学习得到的分离超平面为无数个。<strong>现在的问题是：感知机学习得到的众多分离超平面中，哪一个分离超平面是最优的</strong>？为了回答这个问题，我们先来思考一下图1中分离超平面<span class="math inline">\(S_1\)</span>与<span class="math inline">\(S_2\)</span>哪一个更好。相信大多数读者在直觉上都会认为分离超平面<span class="math inline">\(S_2\)</span>要优于<span class="math inline">\(S_1\)</span>，这个结论是正确的。通过进一步地分析，我们可以发现，训练数据集中的实例点到分离超平面<span class="math inline">\(S_1\)</span>的最小距离要大于其到分离超平面<span class="math inline">\(S_2\)</span>的最小距离，这使得分离超平面<span class="math inline">\(S_1\)</span>的泛化能力要弱于分离超平面<span class="math inline">\(S_2\)</span>，一些轻微的噪声扰动便有可能使得实例点越过分离超平面<span class="math inline">\(S_1\)</span>，造成模型分类错误，如图1中的红色实例点所示，然而由于实例点距离分离超平面<span class="math inline">\(S_2\)</span>的距离相对较远，噪声扰动并不容易使得实例点越过超平面<span class="math inline">\(S_2\)</span>。因此，我们认为分离超平面<span class="math inline">\(S_2\)</span>要优于分离超平面<span class="math inline">\(S_1\)</span>。<br>  硬间隔支持向量机的基本思想便来源于以上的思考，其基本思想为：<strong>当训练数据集线性可分时，在特征空间中寻找一个超平面，其能够将训练数据中的实例点完全正确分类，同时最大程度远离训练数据中的示例点。</strong></p><h2 id="模型">模型</h2><p><strong>输入</strong></p><ul><li><strong>输入空间:</strong> <span class="math inline">\(\mathcal{X}\in \mathbb{R}^n\)</span>.<br></li><li><strong>输入实例:</strong> <span class="math inline">\(x =\begin{bmatrix}  x^{(1)},x^{(2)},\dots,x^{(n)} \\ \end{bmatrix}^T \in\mathcal{X}\)</span>.</li></ul><p>  其中，输入空间<span class="math inline">\(\mathcal{X}\)</span>为<span class="math inline">\(n\)</span>维实数空间的子集，输入实例<span class="math inline">\(x\)</span>为<span class="math inline">\(n\)</span>维特征向量。</p><p><strong>输出</strong><br>  由于我们只考虑二分类问题，因此实例点的类别只有正类与负类两种。</p><ul><li><strong>输出空间:</strong> <span class="math inline">\(\mathcal{Y} =\{ -1,+1 \}\)</span><br></li><li><strong>输出实例:</strong> <span class="math inline">\(y \in\mathcal{Y}\)</span>.</li></ul><p>  其中，输出空间<span class="math inline">\(\mathcal{Y}\)</span>为只包含+1，-1两个元素的集合，+1与-1分别代表二分类问题中的正类与负类。输出实例<span class="math inline">\(y\)</span>代表相对应的输出实例<span class="math inline">\(x\)</span>的类别。</p><p><strong>数据集</strong><br>  支持向量机的训练数据集<span class="math inline">\(T_{train}\)</span>为：</p><p><span class="math display">\[T_{train}=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N) \}\]</span></p><p>  其中，<span class="math inline">\(x_i \in\mathcal{X}=\mathbb{R}^{n},y_i \in\mathcal{Y}=\{+1,-1\},i=1,2,\dots,N\)</span>，<span class="math inline">\(N\)</span>表示训练数据的样本容量。<strong>需要注意的是，硬间隔线性支持向量机模型的前提假设为训练数据集<span class="math inline">\(T_{train}\)</span>是线性可分的</strong>。</p><p><strong>模型</strong><br>  支持向量机的模型形式与感知机相同，其模型形式为：</p><p><span class="math display">\[f(x) = sign(w^{T}x+b)\]</span></p><p>  其中，<span class="math inline">\(w\)</span>和<span class="math inline">\(b\)</span>为感知机模型的参数，<span class="math inline">\(w \in \mathbb{R}^{n}\)</span>为权重向量，<span class="math inline">\(b \in \mathbb{R}\)</span>为偏置。<span class="math inline">\(sign(\cdot)\)</span>为符号函数。硬间隔线性支持向量机为线性分类模型，属于判别模型。</p><p><strong>假设空间</strong><br>  硬间隔线性支持向量机模型的假设空间为：</p><p><span class="math display">\[\mathcal{H} = \{f \vert f(x)=w^{T}x+b\}\]</span></p><p>  与感知机模型相同，硬间隔线性支持向量机模型的假设空间实际上是特征空间中所有超平面的集合。</p><p><strong>参数空间</strong><br>  令<span class="math inline">\(\theta=(w,b)\)</span>，则模型的假设空间<span class="math inline">\(\mathcal{H}\)</span>等价于参数空间<span class="math inline">\(\Theta\)</span>：</p><p><span class="math display">\[\Theta=\{ \theta \vert \theta \in\mathbb{R}^{n+1} \}\]</span></p><h2 id="策略">策略</h2><p>  我们的目标是找到所有分离超平面中最优的一个超平面。首先，假设分离超平面为<span class="math inline">\(S=\{x \vert w^T x + b =0\}\)</span>，则实例点<span class="math inline">\(x_i\)</span>到超平面<span class="math inline">\(S\)</span>的距离为：</p><p><span class="math display">\[d(S,x_i)=\frac{|w^Tx_i+b|}{||w||}\]</span></p><p>  由于硬间隔线性支持向量机假设训练数据集是线性可分的，因此分离超平面<span class="math inline">\(S\)</span>能够将实例点完全正确分类。因此，当 <span class="math inline">\(w^Tx_i+b &gt; 0\)</span>时，有<span class="math inline">\(y_i=+1\)</span>；当<span class="math inline">\(w^Tx_i+b&lt;0\)</span>时，有<span class="math inline">\(y_i=-1\)</span>。则<span class="math inline">\(d(S,x_i)\)</span>同样可以表示为：</p><p><span class="math display">\[\gamma_i=\frac{y_i(w^Tx_i+b)}{||w||}\]</span></p><p>  我们称<span class="math inline">\(\gamma_i\)</span>为实例点<span class="math inline">\(x_i\)</span>到超平面<span class="math inline">\(S\)</span>的<strong>几何间隔</strong>。<br>  我们希望能够找到与训练数据集中的实例点距离最大的超平面，记 <span class="math inline">\(margin(T,S)\)</span> 表示数据集<span class="math inline">\(T\)</span>中的实例点到超平面<span class="math inline">\(S\)</span>的最小距离：</p><p><span class="math display">\[margin(T_{train},S)=\min_{i=1,\dots,N}d(S,x_i)=\min_{i=1,\dots,N} \gamma_i=\min_{i=1,\dots,N}\frac{y_i(w^Tx_i+b)}{||w||}\]</span></p><p>  我们使用 <span class="math inline">\(margin(T_{train},S)\)</span>来衡量训练数据集到超平面的距离，这种距离也称为<span class="math inline">\(hard-margin\)</span>，即硬间隔。由于训练数据集<span class="math inline">\(T_{train}\)</span>是给定的，因此 <span class="math inline">\(margin(T_{train},S)\)</span>实际上是由超平面的参数<span class="math inline">\(w,b\)</span>决定的，不同的分离超平面对应着不同的<span class="math inline">\(margin\)</span>距离，我们希望能够在所有分离超平面中找到与训练数据集距离最远的超平面，这个问题可以被描述为：</p><p><span class="math display">\[\max_{w,b}margin(T_{train},S)=\max_{w,b} \min_{i=1,\dots,N}\frac{y_i(w^Tx_i+b)}{||w||}\]</span></p><p>  综上所述，寻找最优分离超平面的优化问题可以描述为<strong>最小几何间隔最大化问题(1)</strong>：</p><p><span class="math display">\[\begin{equation}    \begin{split}    &amp; \max_{w,b} \min_{i=1,2,\dots,N} \quad\frac{1}{||w||}y_i(w^Tx_i+b) \\    &amp; \space s.t. \quad y_i(w^Tx_i+b) &gt; 0, \quad i=1,2,\dots,N\\    \end{split}\end{equation}\]</span></p><p>  其中称<span class="math inline">\(y_i(w^Tx_i+b)\)</span>为实例点<span class="math inline">\(x_i\)</span>到超平面<span class="math inline">\(S\)</span>的<strong>函数间隔</strong>；约束条件<span class="math inline">\(y_i(w^Tx_i+b) &gt;0\)</span>保证了该优化问题的可行解集为能够将训练数据集正确分类的超平面的集合。确定最优分离超平面需要找到最小几何间隔对应的实例点，这些实例点被称为<strong>支持向量</strong>。<br>  下面我们来对该优化问题做一些简化，通过分析我们可以得到：</p><p><span class="math display">\[\exists r &gt; 0, \space s.t. \space\min_{i=1,\dots,N} y_i(w^Tx_i+b)=r\]</span></p><p>  由于我们没有对<span class="math inline">\(w\)</span>的长度进行限制，则同一个超平面可能对应这不同的参数值<span class="math inline">\(w,b\)</span>，例如超平面<span class="math inline">\(S_1=\{x \vert w^Tx+b=0\}\)</span>与超平面<span class="math inline">\(S_2=\{x \vert(2w)^Tx+(2b)=0\}\)</span>在特征空间中表示同一个超平面。这样会造成，即使是同一个分离超平面，不同的参数值<span class="math inline">\(w,b\)</span>会对应不同的<span class="math inline">\(r\)</span>值。我们想要优化问题能够得到唯一一组确定的参数值<span class="math inline">\(w,b\)</span>，我们可以给定<span class="math inline">\(w\)</span>的模长，也可以固定<span class="math inline">\(r\)</span>值，为了简化优化问题，我们固定<span class="math inline">\(r=1\)</span>，此时优化问题(1)的约束条件可以转化为：</p><p><span class="math display">\[\min_{i=1,\dots,N} y_i(w^Tx_i+b)=1\Leftrightarrow y_i(w^Tx_i+b) \ge 1, i=1,2,\dots,N\]</span></p><p>  优化问题(1)中的目标函数可以转化为：</p><p><span class="math display">\[\max_{w,b} \min_{i=1,\dots,N}\frac{1}{||w||}y_i(w^Tx_i+b)=\max_{w,b}\frac{1}{||w||}\min_{i=1,\dots,N} y_i(w^Tx_i+b)=\max_{w,b} \frac{1}{||w||}\]</span></p><p>  再简最大化问题转化为常见的最小化问题：</p><p><span class="math display">\[\max_{w,b} \frac{1}{||w||}\Leftrightarrow \min_{w,b} ||w|| \Leftrightarrow \min_{w,b}\frac{1}{2}w^Tw\]</span></p><p>  综上所述，优化问题(1)可以转化为以下的优化问题(2)：</p><p><span class="math display">\[\begin{equation}    \begin{split}        &amp; \min_{w,b} \quad \frac{1}{2}w^Tw  \\        &amp; \space s.t. \quad 1-y_i(w^Tx_i+b) \leq 0, \quadi=1,2,\dots,N    \end{split}\end{equation}\]</span></p><p>  通过求解优化问题(2)，我们便可以得到最优的分离超平面参数<span class="math inline">\((w^{*},b^{*})\)</span>。可以证明在训练数据集线性可分的条件下，通过最小间隔最大化问题的求解，最优的分离超平面是唯一的。这部分证明将放在附录，有兴趣的读者可自行阅读。</p><center><img src="https://s2.loli.net/2023/10/25/DCZeJ5TOM93vtLS.jpg" width="60%" height="60%"><div data-align="center">Image2: 最优分离超平面</div></center><h2 id="算法">算法</h2><p>  通过前文的推导，我们得到的优化问题为问题(2)：</p><p><span class="math display">\[\begin{align*}        &amp; \min_{w,b} \frac{1}{2}w^Tw  \\        &amp; \space s.t. \space 1-y_i(w^Tx_i+b) \leq 0, \quadi=1,2,\dots,N \\    \end{align*}\]</span></p><p>  该优化问题为<strong>凸二次规划问题</strong>，有<span class="math inline">\(N\)</span>个约束条件。首先写出优化问题(2)的拉格朗日函数：</p><p><span class="math display">\[L(w,b,\lambda)=\frac{1}{2}w^Tw+\sum_{i=1}^{N}\lambda_i\left[ 1-y_i(w^Tx_i+b) \right]\]</span></p><p><span class="math display">\[\lambda = \begin{bmatrix}    \lambda_1,\lambda_2,\dots,\lambda_N\end{bmatrix}^T,\lambda_i \ge 0, i=1,2,\dots,N\]</span></p><p>  则优化问题(2)的无约束形式(3)为：</p><p><span class="math display">\[\begin{equation}    \begin{split}        &amp; \min_{w,b} \max_{\lambda} \quad L(w,b,\lambda)  \\        &amp; \space s.t. \quad \lambda_i \ge 0, \quad i=1,2,\dots,N \\    \end{split}\end{equation}\]</span></p><p>  则优化问题(2)的<strong>对偶问题</strong>(4)为：</p><p><span class="math display">\[\begin{equation}    \begin{split}        &amp; \max_{\lambda} \min_{w,b} \quad L(w,b,\lambda)  \\        &amp; \space s.t. \quad \lambda_i \ge 0, \quad i=1,2,\dots,N    \end{split}\end{equation}\]</span></p><p>  因为原始问题(2)为凸二次规划问题，其满足<strong>强对偶关系</strong>，即：</p><p><span class="math display">\[\min_{w,b} \max_{\lambda}L(w,b,\lambda)=\max_{\lambda} \max_{w,b} L(w,b,\lambda)\]</span></p><p>  首先来解决内部极大化问题：</p><p><span class="math display">\[\max_{w,b}L(w,b,\lambda)=\frac{1}{2}w^Tw+\sum_{i=1}^{N}\lambda_i \left[1-y_i(w^Tx_i+b) \right]\]</span></p><p>  由费马定理可得：</p><p><span class="math display">\[\left \{\begin{array}{lr}    \frac{\partial L(w,b,\lambda)}{\partial w}=0  \Rightarrow w =\sum_{i=1}^{N}\lambda_{i}y_ix_i  \\    \frac{\partial L(w,b,\lambda)}{\partial b}=0  \Rightarrow\sum_{i=1}^{N}\lambda_iy_i=0\end{array} \right.\]</span></p><p>  将其代入拉格朗日函数<span class="math inline">\(L(w,b,\lambda)\)</span>得：</p><p><span class="math display">\[\begin{align*}    L(w,b,\lambda) &amp;= \frac{1}{2} \left(\sum_{i=1}^{N}\lambda_{i}y_ix_i \right)^{T}\left(\sum_{i=1}^{N}\lambda_{i}y_ix_i \right)-\sum_{i=1}^{N}\lambda_iy_i\left( \sum_{i=1}^{N}\lambda_{i}y_ix_i\right)^{T}x_i-b\sum_{i=1}^{N}\lambda_iy_i+\sum_{i=1}^{N}\lambda_i  \\    &amp;=\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\lambda_i \lambda_jy_iy_j (x_i^{T}x_j)-\sum_{i=1}^{N}\sum_{j=1}^{N}\lambda_i \lambda_jy_iy_j (x_i^{T}x_j)+\sum_{i=1}^{N}\lambda_i  \\    &amp;= -\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\lambda_i \lambda_jy_iy_j (x_i^{T}x_j)+\sum_{i=1}^{N}\lambda_i  \\\end{align*}\]</span></p><p>  则对偶问题(4)可以化为以下优化问题(5)：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{\lambda} \quad &amp;\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\lambda_i \lambda_j y_iy_j(x_i^{T}x_j)-\sum_{i=1}^{N}\lambda_i \\        s.t. \quad &amp; \lambda_i \ge 0, \quad i=1,2,\space,N  \\        &amp; \sum_{i=1}^{N}\lambda_i y_i=0    \end{split}\end{equation}\]</span></p><p>  记 <span class="math inline">\(w^{*},b^{*}\)</span>为原始问题(2)的最优解，<span class="math inline">\(\lambda^{*}\)</span>为对偶问题(4)的最优解，其同时也是优化问题(5)的最优解。根据定理：<strong>原始问题及其对偶问题具备强对偶关系是原始问题与其对偶问题满足KKT条件的充要条件可知</strong>：</p><p><span class="math display">\[KKT条件:  \left \{\begin{array}{l}1-y_i(w^{*} \cdot x_i+b^{*}) \leq 0  \\\\\lambda_i^{*}(1-y_i(w^{*} \cdot x_i+b^{*}))=0 \\\\\frac{\partial L(w,b,\lambda^{*})}{\partial w}|_{w=w^{*}}=0  \Rightarrow w^{*}=\sum_{i=1}^{N}\lambda^{*}_{i}y_ix_i \\\\\frac{\partial L(w,b,\lambda^{*})}{\partial b} |_{b=b^{*}}=0 \Rightarrow\sum_{i=1}^{N}\lambda^{*}_{i}y_i=0 \\\\\lambda^{*} \ge 0 \\  \end{array} \right.\]</span></p><p>  其中，<span class="math inline">\(i=1,2,\dots,N\)</span>.以上关于凸二次规划、对偶关系、KKT条件等最优化知识会在附录中介绍，这里不多家阐述。<br>  由<span class="math inline">\(KKT\)</span>条件得到 <span class="math inline">\(w^{*}=\sum_{i=1}^{N}\lambda^{*}_{i}y_ix_i\)</span>，现在来思考<span class="math inline">\(b^{*}\)</span>如何用<span class="math inline">\(\lambda^{*}\)</span>来表示。由于 <span class="math inline">\(\lambda_i^{*}(1-y_i(w^{*} \cdotx_i+b^{*}))=0\)</span>，若 <span class="math inline">\(y_i(w^{*} \cdotx_i+b^{*}) &gt; 1\)</span>，则 <span class="math inline">\(\lambda_i^{*}=0\)</span>，即在图2中位于间隔边界<span class="math inline">\(H_1,H_2\)</span>两侧的实例点<span class="math inline">\(x_i\)</span>对确定最优超平面<span class="math inline">\(S\)</span>的参数<span class="math inline">\(w^{*},b^{*}\)</span>不起作用；若 <span class="math inline">\(y_i(w^{*} \cdot x_i+b^{*}) = 1\)</span>，则<span class="math inline">\(\lambda_i^{*}\)</span>不一定为0，即在图2中位于间隔边界上的实例点<span class="math inline">\(x_i\)</span>，也就是支持向量，对确定最优超平面<span class="math inline">\(S\)</span>的参数<span class="math inline">\(w^{*},b^{*}\)</span>起作用。此时可以解得：</p><p><span class="math display">\[b^{*}=y_j-\sum_{i=1}^{N}\lambda_i^{*}y_i(x_i^{T}\cdot x_j)\]</span></p><p>  其中实例点<span class="math inline">\(x_j\)</span>为某一个支持向量，由于<span class="math inline">\(b^{*}\)</span>的值是固定的，因此代入不同的支持向量，得到的<span class="math inline">\(b^{*}\)</span>的值是相同的，在实际计算中，我们一般在求解优化问题(5)得到的<span class="math inline">\(\lambda^{*}\)</span>中选择一个正值分量<span class="math inline">\(\lambda_j &gt;0\)</span>，使用其所对应的支持向量<span class="math inline">\(x_j\)</span>来求解<span class="math inline">\(b^{*}\)</span>。</p><h3 id="硬间隔支持向量机算法">硬间隔支持向量机算法</h3><p>  输入：训练数据集<span class="math inline">\(T_{train}=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}\)</span>，其中<span class="math inline">\(x_i \in \mathcal{X}=\mathbb{R}^{n}，y \in\mathcal{Y}=\{-1,+1\},i=1,2,\dots,N\)</span>.<br>  输出：最优分离超平面的参数<span class="math inline">\(w^{*},b^{*}\)</span>，以及相应的分类模型<span class="math inline">\(f(x)=sign(w^{*} \cdot x+b^{*})\)</span>.<br>  (1) 基于训练数据集<span class="math inline">\(T_{train}\)</span>构造凸二次规划问题(6)：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{\lambda} \quad &amp;\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\lambda_i \lambda_j y_iy_j(x_i^{T}x_j)-\sum_{i=1}^{N}\lambda_i \\        s.t. \quad &amp; \lambda_i \ge 0, \quad i=1,2,\space,N  \\        &amp; \sum_{i=1}^{N}\lambda_i y_i=0    \end{split}\end{equation}\]</span></p><p>  解得该优化问题的最优解为：<span class="math inline">\(\lambda^{*}=\begin{bmatrix}  \lambda_1^{*},\lambda_2^{*},\dots,\lambda_N^{*}\\ \end{bmatrix}^{T}\)</span></p><p>  (2) 在最优解<span class="math inline">\(\lambda^{*}\)</span>中选择一个正值分量<span class="math inline">\(\lambda_j &gt; 0\)</span>，取出其下标<span class="math inline">\(j\)</span>对应的数据<span class="math inline">\((x_j,y_j)\)</span>，基于<span class="math inline">\(KKT\)</span>条件求解最优超平面的参数<span class="math inline">\(w^{*},b^{*}\)</span>：</p><p><span class="math display">\[w^{*}=\sum_{i=1}^{N}\lambda_i^{*}y_ix_i\quadb^{*}=y_j-\sum_{i=1}^{N}\lambda_i^{*}y_i(x_i^{T} \cdot x_j)\]</span></p><p>  (3) 得到相应的分类模型：</p><p><span class="math display">\[f(x) = sign(w^{*} \cdot x +b^{*})\]</span></p><h2 id="基于python的算法实现">基于Python的算法实现</h2><p>  导入所需要的Python包：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> cvxopt<br><span class="hljs-keyword">from</span> sklearn.utils <span class="hljs-keyword">import</span> shuffle<br><span class="hljs-keyword">from</span> sklearn.svm <span class="hljs-keyword">import</span> LinearSVC<br></code></pre></td></tr></tbody></table></figure><h3 id="基于numpypandascvxopt等的算法实现">基于numpy,pandas,cvxopt等的算法实现</h3><p>  首先我们来生成我们的训练数据集，为了便于进行可视化，我们依然选择在二维情况下构建数据集。由于硬间隔线性支持向量机假设训练数据集是线性可分的，我利用均匀分布在区域<span class="math inline">\(S_{p}=\{ (x_1,x_2) \vert x_1 \in (1,2), x_2 \in(3,4)\}\)</span>产生了30个正类的实例点，在区域<span class="math inline">\(S_{n}= \{ (x_1,x_2) \vert x_1 \in (3,4), x_2 \in(1,2)\}\)</span>产生了20个负类的实例点。以下是数据生成的代码：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># data generation</span><br>np.random.seed(<span class="hljs-number">520</span>)<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">data_generation</span>(<span class="hljs-params">n,basepoint,ylabel</span>):<br>    x1 = np.random.random((n,))+np.array([basepoint[<span class="hljs-number">0</span>]]*n)<br>    x2 = np.random.random((n,))+np.array([basepoint[<span class="hljs-number">1</span>]]*n)<br>    y = np.array([ylabel]*n)<br>    data = pd.DataFrame({<span class="hljs-string">"x1"</span>:x1,<span class="hljs-string">"x2"</span>:x2,<span class="hljs-string">"y"</span>:y})<br>    <span class="hljs-keyword">return</span> data<br><br><span class="hljs-comment"># positive data</span><br>positivedata = data_generation(n=<span class="hljs-number">30</span>,basepoint=(<span class="hljs-number">1</span>,<span class="hljs-number">3</span>),ylabel=<span class="hljs-number">1</span>)<br><span class="hljs-comment"># negative data</span><br>negativedata = data_generation(n=<span class="hljs-number">20</span>,basepoint=(<span class="hljs-number">3</span>,<span class="hljs-number">1</span>),ylabel=-<span class="hljs-number">1</span>)<br><span class="hljs-comment"># train data</span><br>train_data = pd.concat([positivedata,negativedata])<br>train_data = shuffle(train_data)<br>train_data.index = <span class="hljs-built_in">range</span>(train_data.shape[<span class="hljs-number">0</span>])<br>train_data.head(<span class="hljs-number">5</span>)<br></code></pre></td></tr></tbody></table></figure><p>  这50个实例点构成训练数据集，使用以下代码画出训练数据的散点图:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># train data scatter plot</span><br>plt.scatter(x=positivedata[<span class="hljs-string">"x1"</span>],y=positivedata[<span class="hljs-string">"x2"</span>],marker=<span class="hljs-string">"o"</span>,c=<span class="hljs-string">"green"</span>,label=<span class="hljs-string">"positive data"</span>)<br>plt.scatter(x=negativedata[<span class="hljs-string">"x1"</span>],y=negativedata[<span class="hljs-string">"x2"</span>],marker=<span class="hljs-string">"x"</span>,c=<span class="hljs-string">"red"</span>,label=<span class="hljs-string">"negative data"</span>)<br>plt.xlabel(<span class="hljs-string">"x1"</span>)<br>plt.ylabel(<span class="hljs-string">"x2"</span>)<br>plt.xlim((<span class="hljs-number">0</span>,<span class="hljs-number">5</span>))<br>plt.ylim((<span class="hljs-number">0</span>,<span class="hljs-number">5</span>))<br>plt.xticks(np.arange(<span class="hljs-number">0</span>,<span class="hljs-number">5</span>,<span class="hljs-number">0.5</span>))<br>plt.yticks(np.arange(<span class="hljs-number">0</span>,<span class="hljs-number">5</span>,<span class="hljs-number">0.5</span>))<br>plt.legend()<br></code></pre></td></tr></tbody></table></figure><p>  图3为训练数据集的可视化结果：</p><center><img src="https://s2.loli.net/2023/10/31/eNkl2GVYi4mPy1v.png" width="60%" height="60%"><div data-align="center">Image3: 训练数据集</div></center><p>  从图3中可以很容易看出，训练数据集是线性可分的，满足硬间隔线性支持向量机的前提条件。下面我们来利用硬间隔线性支持向量机算法来求解最优超平面的参数<span class="math inline">\(w^{*},b^{*}\)</span>.<br>  为了使用Python的cvxopt包来求救凸二次规划问题(6)，需要将问题(6)转化为矩阵形式，记：</p><p><span class="math display">\[\lambda = \begin{bmatrix}    \lambda_1 \\    \lambda_2 \\    \vdots \\    \lambda_N \\\end{bmatrix},\quad y = \begin{bmatrix}    y_1 \\    y_2 \\    \vdots \\    y_N \\\end{bmatrix},\quad x = \begin{bmatrix}    x_1 \\    x_2 \\    \vdots \\    x_N \\\end{bmatrix}\]</span></p><p>  则：</p><p><span class="math display">\[x \odot y = \begin{bmatrix}    x_1y_1 \\    x_2y_2 \\    \vdots \\    x_Ny_N \\\end{bmatrix},\qquad Gram(x \odot y) = \begin{bmatrix}    (x_1y_1)^{T}(x_1y_1) &amp; (x_1y_1)^{T}(x_2y_2) &amp; \dots &amp;(x_1y_1)^{T}(x_Ny_N)  \\    (x_2y_2)^{T}(x_1y_1) &amp; (x_2y_2)^{T}(x_2y_2) &amp; \dots &amp;(x_2y_2)^{T}(x_Ny_N)  \\    \vdots &amp; \vdots &amp;  &amp; \vdots \\    (x_Ny_N)^{T}(x_1y_1) &amp; (x_Ny_N)^{T}(x_2y_2) &amp; \dots &amp;(x_Ny_N)^{T}(x_Ny_N)  \\\end{bmatrix}\]</span></p><p>  设：</p><p><span class="math display">\[P=Gram(x \odot y), \quadq=-1_{N}=\begin{bmatrix}    -1 \\    -1 \\    \vdots \\    -1\end{bmatrix}_{N \times 1}, \quad G = -I_{N}=\begin{bmatrix}    -1 &amp; 0 &amp; \dots &amp; 0 \\    0 &amp; -1 &amp; \dots &amp; 0 \\    \vdots &amp; \vdots &amp; &amp; \vdots \\    0 &amp; 0 &amp; \dots &amp; -1 \\\end{bmatrix}_{N \times N}\]</span></p><p><span class="math display">\[h = \begin{bmatrix}    0 \\    0 \\    \vdots \\    0\end{bmatrix}_{N \times 1}, \quad A = y^{T}, \quad b=0\]</span></p><p>  则凸二次规划问题(6)的矩阵形式为(7)：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{\lambda} \quad &amp; \frac{1}{2}\lambda^{T}P\lambda+q^{T}\lambda \\        s.t. \quad &amp;  G\lambda \leq h \\        &amp; A\lambda=b  \\    \end{split}\end{equation}\]</span></p><p>  利用硬间隔支持向量机算法来求解最优超平面的参数<span class="math inline">\(w^{*},b^{*}\)</span>，以下是相应的代码：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># data matrix</span><br>train_X = train_data.iloc[:,[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]].values<br>train_y = train_data.iloc[:,<span class="hljs-number">2</span>].values<br><br><span class="hljs-comment"># parameters solving</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">HardMargin_SVM</span>(<span class="hljs-params">X,y</span>):<br>    Y = np.array([y]*X.shape[<span class="hljs-number">1</span>]).T<br>    XdotY = X * Y<br>    n_samples = X.shape[<span class="hljs-number">0</span>]<br>    Gram_Matrix = np.zeros((n_samples,n_samples))<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_samples):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_samples):<br>            Gram_Matrix[i,j] = np.dot(XdotY[i],XdotY[j])<br>    <br>    P = matrix(Gram_Matrix).T<br>    q = matrix(-np.ones(n_samples))<br>    G = matrix(-np.eye(n_samples))<br>    h = matrix(np.zeros(n_samples))<br>    A = matrix(y.astype(<span class="hljs-built_in">float</span>)).T<br>    b = matrix([<span class="hljs-number">0.0</span>])<br><br>    lamda = np.array(solvers.qp(P,q,G,h,A,b)[<span class="hljs-string">'x'</span>])<br>    threshold = <span class="hljs-number">1e-5</span><br>    lamda[lamda &lt; threshold] = <span class="hljs-number">0</span><br>    <br>    w_hat = np.<span class="hljs-built_in">round</span>(np.<span class="hljs-built_in">sum</span>(lamda*XdotY,axis=<span class="hljs-number">0</span>),<span class="hljs-number">4</span>)<br><br>    positive_index = np.where(lamda&gt;<span class="hljs-number">0</span>)[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]<br>    SV_x = X[positive_index]<br>    SV_y = y[positive_index]<br>    GM_xwithSVx = np.zeros(n_samples)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_samples):<br>        GM_xwithSVx[i] = np.dot(X[i],SV_x)<br><br>    lamda = lamda.ravel()<br>    b_hat = <span class="hljs-built_in">round</span>(SV_y - np.dot(lamda,y * GM_xwithSVx),<span class="hljs-number">4</span>)<br><br>    <span class="hljs-keyword">return</span> [w_hat,b_hat],lamda<br><br>theta,lamda = HardMargin_SVM(X=train_X,y=train_y)<br></code></pre></td></tr></tbody></table></figure><p>  解得最优超平面的参数为：</p><p><span class="math display">\[w^{*}=\begin{bmatrix}    -0.8985 \\    0.6889 \\\end{bmatrix}, \quad b^{*}=0.4643\]</span></p><p>  画出最优分离超平面<span class="math inline">\(S^{*}=\{x \vert w^{*}\cdot x + b^{*}=0\}\)</span>：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># draw the Optimal Classification Hyperplane</span><br>w,b = theta[<span class="hljs-number">0</span>],theta[<span class="hljs-number">1</span>]<br>x1 = np.linspace(<span class="hljs-number">0</span>,<span class="hljs-number">5</span>,<span class="hljs-number">1000</span>)<br>x2 = -(w[<span class="hljs-number">0</span>]*x1+b)/w[<span class="hljs-number">1</span>]<br>plt.figure(figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">6</span>))<br>plt.scatter(x=positivedata[<span class="hljs-string">"x1"</span>],y=positivedata[<span class="hljs-string">"x2"</span>],marker=<span class="hljs-string">"o"</span>,c=<span class="hljs-string">"green"</span>,label=<span class="hljs-string">"positive data"</span>)<br>plt.scatter(x=negativedata[<span class="hljs-string">"x1"</span>],y=negativedata[<span class="hljs-string">"x2"</span>],marker=<span class="hljs-string">"x"</span>,c=<span class="hljs-string">"red"</span>,label=<span class="hljs-string">"negative data"</span>)<br>plt.plot(x1,x2,c=<span class="hljs-string">"blue"</span>,label=<span class="hljs-string">"Optimal Classification Hyperplane"</span>)<br>plt.xlabel(<span class="hljs-string">"x1"</span>)<br>plt.ylabel(<span class="hljs-string">"x2"</span>)<br>plt.xlim((<span class="hljs-number">0</span>,<span class="hljs-number">5</span>))<br>plt.ylim((<span class="hljs-number">0</span>,<span class="hljs-number">5</span>))<br>plt.xticks(np.arange(<span class="hljs-number">0</span>,<span class="hljs-number">5</span>,<span class="hljs-number">0.5</span>))<br>plt.yticks(np.arange(<span class="hljs-number">0</span>,<span class="hljs-number">5</span>,<span class="hljs-number">0.5</span>))<br>plt.legend()<br></code></pre></td></tr></tbody></table></figure><center><img src="https://s2.loli.net/2023/10/31/Pgsh97eGjJTlXfS.png" width="60%" height="60%"><div data-align="center">Image4: 最优分离超平面</div></center><p>  分类模型：</p><p><span class="math display">\[f(x)=sign(w^{*} \cdot x +b^{*})\]</span></p><h3 id="基于sklearn的算法实现">基于sklearn的算法实现</h3><p>  下面我们使用<code>sklearn</code>库来完成这个分类任务，我们依然使用前文生成的由30个正类实例点与20个负类实例点构成的训练数据集，使用<code>sklearn.svm</code>中的<code>LinearSVC</code>进行分类的代码为：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">svm = LinearSVC(penalty=<span class="hljs-string">"l2"</span>,C=<span class="hljs-number">1.0</span>,loss=<span class="hljs-string">'hinge'</span>)<br>svm.fit(train_X,train_y)<br>w,b = svm.coef_[<span class="hljs-number">0</span>],svm.intercept_[<span class="hljs-number">0</span>]<br></code></pre></td></tr></tbody></table></figure><p>  <code>LinearSVC</code>的一些主要参数为：</p><ul><li><code>penalty</code>:{'l1','l2'},default='l2'。设定惩罚项，SVC中默认为'l2'惩罚项，'l1'会导致稀疏的<code>coef_</code>向量。<br></li><li><code>loss</code>: {'hinge', 'squared_hinge'},default='squared_hinge'。设定损失函数，hinge是标准的SVM损失(如SVC类使用的)，而squared_hinge是hinge损失的平方。<br></li><li><code>dual</code>: bool,default=True。选择算法来解决对偶或原始优化问题。 当n_samples&gt;n_features时，首选dual = False。<br></li><li><code>tol</code>: float, default=1e-4。设置停止的条件。</li><li><code>C</code>: float, default=1.0。正则化参数。正则化的强度与C成反比。必须严格设置为正的。</li></ul><p>  得到的最优分类超平面的参数为：</p><p><span class="math display">\[w^{*}=\begin{bmatrix}    -0.8240 \\    0.7860 \\\end{bmatrix}, \quad b^{*}=0.0349\]</span></p><p>  画出最优分离超平面<span class="math inline">\(S^{*}=\{x \vert w^{*}\cdot x + b^{*}=0\}\)</span>：</p><center><img src="https://s2.loli.net/2023/11/01/j2cTdOPCIbWKHEV.png" width="60%" height="60%"><div data-align="center">Image4: 最优分离超平面</div></center><h2 id="附录">附录</h2><h3 id="关于凸优化的相关知识">关于凸优化的相关知识</h3><h4 id="基础概念">基础概念</h4><p><strong>仿射集</strong><br>  设<span class="math inline">\(C\)</span>为某一集合，若 <span class="math inline">\(\forall x_1,x_2 \in C, \theta \in \mathbb{R},\theta x_1+(1-\theta)x_2 \in C\)</span>，则称集合<span class="math inline">\(C\)</span>为仿射集。</p><p><strong>仿射函数</strong><br>  设有映射 <span class="math inline">\(f: \mathbb{R}^{n} \rightarrow\mathbb{R}^{m}\)</span>，若 <span class="math inline">\(f(x)=Ax+b, A \in\mathbb{R}^{m \times n}, b \in \mathbb{R}^{m}\)</span>，则称映射<span class="math inline">\(f\)</span>为仿射函数。</p><p><strong>凸集</strong><br>  设<span class="math inline">\(C\)</span>为某一集合，若 <span class="math inline">\(\forall x_1,x_2 \in C, \theta \in [0,1], \thetax_1+(1-\theta)x_2 \in C\)</span>，则称集合<span class="math inline">\(C\)</span>为凸集。</p><p><strong>凸函数</strong><br>  一个函数<span class="math inline">\(f(x)\)</span>被称为凸函数，如果它的定义域 <span class="math inline">\(dom f\)</span> 为凸集，并且对 <span class="math inline">\(\forall x_1,x_2 \in dom f, \alpha \in[0,1]\)</span>，有以下不等式成立：</p><p><span class="math display">\[f[\alpha x_1+(1-\alpha) x_2] \leq \alphaf(x_1)+(1-\alpha)f(x_2)\]</span></p><p>  <strong>凸函数的一阶条件</strong><br>  设函数<span class="math inline">\(f(x)\)</span><strong>一阶可微</strong>，<span class="math inline">\(x \in \mathbb{R}^{n}\)</span>，则<span class="math inline">\(f(x)\)</span>为凸函数的<strong>充要条件</strong>为：<span class="math inline">\(dom \space f\)</span>为凸集，且对<span class="math inline">\(\forall x,y \in dom \spacef\)</span>，有以下不等式成立：</p><p><span class="math display">\[f(y) \ge f(x)+\nablaf^{T}(x)(y-x)\]</span></p><p>  <strong>凸函数的二阶条件</strong><br>  设函数<span class="math inline">\(f(x)\)</span><strong>二阶可微</strong>，<span class="math inline">\(x \in \mathbb{R}^{n}\)</span>，则<span class="math inline">\(f(x)\)</span>为凸函数的<strong>充要条件</strong>为：<span class="math inline">\(dom \space f\)</span>为凸集，且对<span class="math inline">\(\forall x \in dom \space f\)</span>，有<span class="math inline">\(\nabla^{2}f(x) \succeq 0\)</span>，即<span class="math inline">\(Hessain\)</span>矩阵半正定。</p><p><strong>最优化问题</strong><br>  最优化问题的基本形式(7)：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{x} \quad &amp; f(x) \\        s.t. \quad &amp; m_{i}(x) \leq 0,\quad i=1,2,\dots,M \\        &amp; n_{j}(x) = 0,\quad j=1,2,\dots,N    \end{split}\end{equation}\]</span></p><ul><li><span class="math inline">\(x=\begin{bmatrix}  x_1,x_2,\dots,x_n\end{bmatrix}^{T},x \in \mathbb{R}^{n}\)</span>称为<strong>优化变量(Optimization Variable)</strong>.<br></li><li><span class="math inline">\(f: \mathbb{R}^{n} \rightarrow\mathbb{R}\)</span>，称为<strong>目标函数(ObjectiveFunction)</strong>.<br></li><li><span class="math inline">\(m_i: \mathbb{R}^{n} \rightarrow\mathbb{R}\)</span>，称为<strong>不等式约束(InequalityConstraint)</strong>.<br></li><li><span class="math inline">\(n_j: \mathbb{R}^{n} \rightarrow\mathbb{R}\)</span>，称为<strong>等式约束(EqualityConstraint)</strong>.<br></li><li><span class="math inline">\(D = \left( dom \space f \right) \bigcap\{x \vert m_{i}(x) \leq 0,i=1,2,\dots,M\} \bigcap \{x \vert h_{j}(x) =0,j=1,2,\dots,N\}\)</span>，称为最优化问题的<strong>域(Domain)</strong>，<span class="math inline">\(x \in D\)</span> 称为<strong>可行解(FeasibleSolution)</strong>.</li><li><span class="math inline">\(p^{*} = \inf \{ f(x) \vert x \in D\}\)</span>，称为最优化问题的<strong>最优值(Optimum)</strong>.<br></li><li><span class="math inline">\(f(x^{*})=p^{*}\)</span>，称<span class="math inline">\(x^{*}\)</span>为最优化问题的<strong>最优解(OptimalSolution)</strong>.<br></li><li><span class="math inline">\(X_{opt} = \{ x \vert x \in D,f(x)=p^{*}\}\)</span>，称为最优化问题的<strong>最优解集(Optima Set)</strong>.</li></ul><p><strong>凸优化问题</strong><br>  若在优化问题(7)中，目标函数<span class="math inline">\(f(x)\)</span>为凸函数，不等式约束<span class="math inline">\(m_i(x)\)</span>为凸函数，等式约束<span class="math inline">\(n_j(x)\)</span>为仿射函数，则称该优化问题为凸优化问题。</p><h4 id="对偶关系">对偶关系</h4><p><strong>拉格朗日函数</strong><br>  原问题(7)的拉格朗日函数为：</p><p><span class="math display">\[L(x,\lambda,\eta)=f(x)+\sum_{i=1}^{M}\lambda_im_i(x)+\sum_{j=1}^{N}\eta_j n_j(x)\]</span></p><p><span class="math display">\[\lambda_i \ge 0, \quadi=1,2,\dots,M\]</span></p><p><strong>原问题的无约束形式</strong><br>  原问题(7)的无约束形式(8)为：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{x} \max_{\lambda,\eta} \quad &amp; L(x,\lambda,\eta)  \\        s.t. \quad &amp; \lambda_i \ge 0, \quad i=1,2,\dots,M \\    \end{split}\end{equation}\]</span></p><p>  原问题(7)与其无约束形式(8)是等价的，现证明这个结论。<br><strong>证明:</strong><br>  令<span class="math inline">\(h(x) = \max_{\lambda,\eta}L(x,\lambda,\eta)\)</span>,<br>  若优化变量<span class="math inline">\(x\)</span>不满足某个不等式约束<span class="math inline">\(m_i(x)\)</span>，即 <span class="math inline">\(\exists i, m_i(x) &gt; 0\)</span>,则有：</p><p><span class="math display">\[h(x)=\max_{\lambda,\eta}L(x,\lambda,\eta) \rightarrow +\infty\]</span></p><p>  若优化变量<span class="math inline">\(x\)</span>不满足某个等式约束<span class="math inline">\(n_j(x)\)</span>，即 <span class="math inline">\(\exists j,n_j(x) \ne 0\)</span>，则有：</p><p><span class="math display">\[h(x)=\max_{\lambda,\eta}L(x,\lambda,\eta) \rightarrow +\infty\]</span></p><p>  若优化变量<span class="math inline">\(x\)</span>满足所有的不等式约束<span class="math inline">\(m_i(x)\)</span>与等式约束<span class="math inline">\(n_j(x)\)</span>,即 <span class="math inline">\(\forall i,j, m_i(x) \leq 0,n_j(x)=0\)</span>，则有:</p><p><span class="math display">\[h(x) = \max_{\lambda,\eta}L(x,\lambda,\eta) &lt; +\infty\]</span></p><p><span class="math display">\[\lambda_i = 0, \quadi=1,2,\dots,M\]</span></p><p>  设集合<span class="math inline">\(S_1,S_2\)</span>分别为：</p><p><span class="math display">\[S_1 = \{ x \vert \exists i,m_i(x) &gt; 0\} \cup \{ x \vert \exists j, n_j(x) \ne 0 \}\]</span></p><p><span class="math display">\[S_2 = \{ x \vert \forall i,j, m_i(x)\leq 0,n_j(x) = 0 \}\]</span></p><p>  有:</p><p><span class="math display">\[S_1 \cup S_2 = \mathbb{R}^{n}, S_1 \capS_2 = \emptyset\]</span></p><p>  则无约束问题(8)可以写为</p><p><span class="math display">\[\min_{x} \max_{\lambda,\eta}L(x,\lambda,\eta) \Leftrightarrow \min_{x} h(x) = \left \{\begin{array}{l}+\infty, &amp; {x \in S_1}\\c(x)&lt;+\infty,&amp; {x \in S_2}\\\end{array} \right.\]</span></p><p><span class="math display">\[\Rightarrow \min_{x} \max_{\lambda,\eta}L(x,\lambda,\eta) \Leftrightarrow \min_{x \in S_2} h(x)=\min_{x \in S_2}f(x)\]</span></p><p>  故原问题(7)与其无约束形式(8)等价。<br>  证毕.</p><p><strong>对偶问题</strong><br>  原问题(7)的拉格朗日对偶问题(9)为：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \max_{\lambda,\eta} \min_{x} \quad &amp; L(x,\lambda,\eta) \\        s.t. \quad &amp; \lambda_i \ge 0, \quad i = 1,2,\dots,M    \end{split}\end{equation}\]</span></p><p><strong>弱对偶关系</strong><br>  原问题(7)与其对偶问题(9)满足弱对偶关系：</p><p><span class="math display">\[\max_{\lambda,\eta} \min_{x}L(x,\lambda,\eta) \leq \min_{x} \max_{\lambda,\eta}L(x,\lambda,\eta)\]</span></p><p><strong>证明:</strong><br>  令：</p><p><span class="math display">\[A(\lambda,\eta)=\min_{x}L(x,\lambda,\eta),\quad B(x)=\max_{\lambda,\eta}L(x,\lambda,\eta)\]</span></p><p><span class="math display">\[\because \min_{x}L(x,\lambda,\eta) \leqL(x,\lambda,\eta) \leq \max_{\lambda,\eta}L(x,\lambda,\eta)\]</span></p><p><span class="math display">\[\Rightarrow A(\lambda,\eta) \leqB(x),\quad \forall \lambda,\eta, \forall x\]</span></p><p><span class="math display">\[\Rightarrow \max_{\lambda,\eta}A(\lambda,\eta) \leq \min_{x} B(x)\]</span></p><p>  即：</p><p><span class="math display">\[\max_{\lambda,\eta} \min_{x}L(x,\lambda,\eta) \leq \min_{x} \max_{\lambda,\eta}L(x,\lambda,\eta)\]</span></p><p>  证毕.</p><p><strong>强对偶关系</strong><br>  若原问题(7)与其对偶问题(9)满足：</p><p><span class="math display">\[\max_{\lambda,\eta} \min_{x}L(x,\lambda,\eta) = \min_{x} \max_{\lambda,\eta}L(x,\lambda,\eta)\]</span></p><p>  则称原问题(7)与其对偶问题(9)满足强对偶关系。</p><p><strong>强对偶关系的几何理解</strong><br>  设原问题为仅有不等式约束的优化问题(10)：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{x} \quad &amp; f(x) \\        s.t. \quad &amp; m_{i}(x) \leq 0, \quad i=1,2,\dots,M \\    \end{split}\end{equation}\]</span></p><p>  原问题(10)的拉格朗日函数为：</p><p><span class="math display">\[L(x,\lambda)=f(x)+\sum_{i=1}^{M}\lambda_im_i(x)\]</span></p><p>  若原问题与其对偶问题满足强对偶关系，则有：</p><p><span class="math display">\[\min_{x} \max_{\lambda} L(x,\lambda) =\max_{\lambda} \min_{x} L(x,\lambda)\]</span></p><p>  <strong>鞍点的定义:</strong> 若 <span class="math inline">\(\exists(\hat{w},\hat{z})\)</span>，使得：</p><p><span class="math display">\[\sup_{z} \inf_{w} f(w,z)=f(\hat{w},\hat{z})= \inf_{w} \sup_{z} f(w,z)\]</span></p><p>  则称 <span class="math inline">\((\hat{w},\hat{z})\)</span> 为<span class="math inline">\(f(w,z)\)</span> 的鞍点。由于 <span class="math inline">\(f(\hat{w},z)= \inf_{w} f(w,z),f(w,\hat{z})=sup_{z} f(w,z)\)</span>，则以上等式也可以写成：</p><p><span class="math display">\[\sup_{z}f(\hat{w},z)=f(\hat{w},\hat{z})=\inf_{w} f(w,\hat{z})\]</span></p><center><img src="https://s2.loli.net/2023/10/29/lH3OtvE7UN9A15m.jpg" width="60%" height="60%"><div data-align="center">Image: 鞍点</div></center><p>  结合鞍点以及强对偶关系的概率，我们可以得出结论：<strong>原问题的拉格朗日函数存在鞍点是原问题与对偶问题满足强对偶关系的充要条件，且鞍点即为原问题与对偶问题的最优解。</strong></p><h4 id="kkt条件">KKT条件</h4><p>  下面来介绍如何判断原问题与对偶问题是否满足强对偶关系，以及如何求出相应的最优解。首先来介绍<span class="math inline">\(Slater\)</span>条件。<br><strong><span class="math inline">\(Slater\)</span>条件</strong><br>  若原问题(7)是凸问题，同时 <span class="math inline">\(\exists x \inrelint(D)\)</span>，使得约束满足：</p><p><span class="math display">\[\begin{split}    &amp; m_{i}(x) &lt; 0, \quad i=1,2,\dots,M \\    &amp; n_{j}(x) = 0, \quad j=1,2,\dots,N \\\end{split}\]</span></p><p>  则原问题与对偶问题满足强对偶关系。</p><ul><li>注：<span class="math inline">\(relint(D)\)</span>表示原始凸问题的域的相对内部，即域内除了边界点以外的所有点。</li></ul><p>  <span class="math inline">\(Slater\)</span>条件是一个<strong>充分不必要条件</strong>，若满足<span class="math inline">\(Slater\)</span>条件，则强对偶一定成立，不满足<span class="math inline">\(Slater\)</span>条件，强对偶也可能成立。大多数凸优化问题均满足<span class="math inline">\(Slater\)</span>条件，即有强对偶性。<br>  若我们已知原问题与对偶问题满足强对偶关系，如何求解出原问题以及对偶问题的最优解？下面我们来介绍凸优化中一个非常经典的理论——KKT条件。</p><p><strong>KKT条件</strong><br>  设<span class="math inline">\(x^{*}\)</span>为原始问题(7)的最优解，<span class="math inline">\(\lambda^{*},\eta^{*}\)</span>为对偶问题(9)的最优解，且原始问题与对偶问题满足强对偶关系，则有以下四组条件成立：</p><p><span class="math display">\[KKT条件:  \left \{\begin{array}{l}m_{i}(x^{*}) \leq 0, h_{j}(x^{*}) = 0 &amp;&amp;(primal \spacefeasibility)  \\\\\lambda^{*} \ge 0 &amp;&amp;(dual \space feasibility)\\\\\lambda^{*}m_{i}(x^{*})=0 &amp;&amp;(complementary \space slackness)\\\\\frac{\partial L(x,\lambda^{*},\eta^{*})}{\partial x} \vert_{x=x^{*}}=0&amp;&amp;(stationarity)\\\end{array} \right.\]</span></p><p>  原问题、对偶问题的可行性条件，稳定性条件都很好理解，我们主要来推导一下互补松弛条件是如何得到的。<br>  <strong>证明:</strong><br>  记 <span class="math inline">\(p^{*}\)</span>为原问题(7)的最优值，<span class="math inline">\(d^{*}\)</span>为对偶问题(9)的最优值，即：</p><p><span class="math display">\[p^{*}= \min_{x}f(x)=f(x^{*})\]</span></p><p><span class="math display">\[d^{*}=\max_{\lambda,\eta} \min_{x}L(x,\lambda,\eta) \triangleq \max_{\lambda,\eta}g(\lambda,\eta)=g(\lambda^{*},\eta^{*})\]</span></p><p>  通过分析可得：</p><p><span class="math display">\[\begin{align*}    d^{*} &amp;= \max_{\lambda,\eta} \min_{x} L(x,\lambda,\eta) =\min_{x} \max_{\lambda,\eta} L(x,\lambda,\eta) = \min_{x}L(x,\lambda^{*},\eta^{*}) \\    &amp;\leq L(x^{*},\lambda^{*},\eta^{*}) =f(x^{*})+\sum_{i=1}^{M}\lambda_{i}^{*}m_{i}(x^{*}) +\sum_{j=1}^{N}\eta_{j}^{*}n_{j}(x^{*}) \\    &amp;\leq f(x^{*}) = p^{*}\end{align*}\]</span></p><p>  由原问题与对偶问题满足强对偶关系可知：<span class="math inline">\(p^{*}=d^{*}\)</span>，则以上式子中的小于等于号均取等号，故有：</p><p><span class="math display">\[\sum_{i=1}^{M}\lambda_{i}^{*}m_{i}(x^{*})=0\Rightarrow \lambda^{*}m_{i}(x^{*})=0, \forall i\]</span></p><p>  互补松弛条件成立，证毕.</p><p>  对于<strong>一般的原问题</strong>，KKT条件是 <span class="math inline">\(x^{*},\lambda^{*},\eta^{*}\)</span>为最优解的<strong>必要条件</strong>，即只要 <span class="math inline">\(x^{*},\lambda^{*},\eta^{*}\)</span>为原问题及对偶问题的最优解，则一定满足KKT条件。<br>  对于<strong>原问题为凸问题</strong>，KKT条件是 <span class="math inline">\(x^{*},\lambda^{*},\eta^{*}\)</span>为最优解的<strong>充要条件</strong>，即只要 <span class="math inline">\(x^{*},\lambda^{*},\eta^{*}\)</span>满足KKT条件，则其一定为原问题及对偶问题的最优解，反过来，只要 <span class="math inline">\(x^{*},\lambda^{*},\eta^{*}\)</span>为原问题及对偶问题的最优解，则其一定满足KKT条件。</p><h4 id="凸二次规划">凸二次规划</h4><p>  凸二次规划问题的基本形式为：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{x} \quad &amp; \frac{1}{2} x^{T}Px+q^{T}x+r \\        s.t. \quad &amp;  Gx \leq h \\        &amp; Ax=b  \\    \end{split}\end{equation}\]</span></p><p>  其中，<span class="math inline">\(x \in \mathbb{R}^{n}, P \inS_{+}^{n}\)</span>，为对称正定矩阵；<span class="math inline">\(q,r \in\mathbb{R}^{n}; G \in \mathbb{R}^{M \times n}, h \in \mathbb{R}^{M}; A\in \mathbb{R}^{N \times n}, b \in \mathbb{R}^{N}\)</span>.<br>  结论：<strong>凸二次规划问题满足强对偶关系。</strong></p><h3 id="硬间隔线性支持向量机的解是唯一的">硬间隔线性支持向量机的解是唯一的</h3><p><strong>结论:</strong> 如果训练数据集<span class="math inline">\(T_{train}\)</span>完全线性可分，通过最小间隔最大化问题的求解，存在唯一的分离超平面可以将所有样本点完全分开。<br><strong>证明:</strong><br>  (1) 存在性的证明：<br>  由于训练数据集<span class="math inline">\(T_{train}\)</span>完全线性可分，由线性可分的定义可知存在分离超平面<span class="math inline">\(S = \{x \vert w \cdot x + b = 0\}\)</span>可将所有样本点完全正确分类。存在性得证。<br>  (2) 唯一性的证明：<br>  利用反证法来进行证明。假设优化问题(2)存在两个不同的最优解，分别记为<span class="math inline">\(w_{1}^{*},b_{1}^{*}\)</span>和<span class="math inline">\(w_{2}^{*},b_{2}^{*}\)</span>，意味着两个权值向量所对应的模都是最小值，记为<span class="math inline">\(a\)</span>：</p><p><span class="math display">\[||w_{1}^{*}||=||w_{2}^{*}||=a\]</span></p><p>  根据这两组参数可以构造一组新的参数：</p><p><span class="math display">\[w=\frac{w_{1}^{*}+w_{2}^{*}}{2}, \quadb=\frac{b_{1}^{*}+b_{2}^{*}}{2}\]</span></p><p>  新构造的参数<span class="math inline">\(w\)</span>的模长一定满足：</p><p><span class="math display">\[||w||= || \frac{w_{1}^{*}+w_{2}^{*}}{2}|| \ge a\]</span></p><p>  另一方面，由模长的三角不等式可得：</p><p><span class="math display">\[||\frac{w_{1}^{*}+w_{2}^{*}}{2}|| =||\frac{1}{2}w_{1}^{*}+\frac{1}{2}w_{2}^{*}|| \leq\frac{1}{2}||w_{1}^{*}||+\frac{1}{2}||w_{2}^{*}||=a\]</span></p><p>  从而有：</p><p><span class="math display">\[|| \frac{w_{1}^{*}+w_{2}^{*}}{2}||=a\]</span></p><p>  即：</p><p><span class="math display">\[||w||=\frac{1}{2}||w_{1}^{*}||+\frac{1}{2}||w_{2}^{*}||\]</span></p><p>  由此发现，<span class="math inline">\(w_{1}^{*}\)</span>和<span class="math inline">\(w_{2}^{*}\)</span>在同一直线上：</p><p><span class="math display">\[w_{1}^{*}=\pm w_{2}^{*}\]</span></p><p>  当<span class="math inline">\(w_{1}^{*}=w_{2}^{*}\)</span>时，与假设矛盾；当<span class="math inline">\(w_{1}^{*}=-w_{2}^{*}\)</span>时，违背了权重向量时非零向量的前提。唯一性得证。<br>  证毕.</p><h2 id="参考">参考</h2><p><strong>[1] Book: 李航,统计学习方法(第2版)</strong><br><strong>[2] Book: 董平,机器学习中的统计思维(Python实现)</strong><br><strong>[3] Video: bilibili,简博士,支持向量机系列</strong><br><strong>[4] Video: bilibili,shuhuai008,支持向量机系列</strong><br><strong>[5] Video:bilibili,欧拉的费米子,凸优化理论-中科大凌青</strong><br><strong>[6] Blog:知乎,Lauer,[凸优化笔记6]-拉格朗日对偶、KKT条件</strong></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>机器学习-2.感知机</title>
      <link href="/2023/10/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
      <url>/2023/10/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-%E6%84%9F%E7%9F%A5%E6%9C%BA/</url>
      
        <content type="html"><![CDATA[<h1 id="感知机与线性判别分析">感知机与线性判别分析</h1><h2 id="感知机">感知机</h2><p>  感知机(perception)是一个处理二分类问题的线性分类模型。感知机模型旨在寻找一个合适的超平面，将特征空间中的示例进行正确分类。为了找到这个超平面，需要设定基于误分类的损失函数，并利用梯度下降方法对损失函数进行极小化，求得感知机模型。感知机学习算法具有简单与易于实现的优点，分为原始形式和对偶形式。<br>  感知机模型由<span class="math inline">\(Rosenblatt\)</span>于1957年提出，尽管感知机在处理复杂问题方面存在局限性，但它为神经网络和深度学习的发展铺平了道路。感知机模型的思想和学习算法为后来更复杂的神经网络提供了基础，尤其是多层感知机和其他深度学习模型。感知机的历史地位在机器学习领域被广泛认可，被视为神经网络的早期里程碑。</p><h3 id="基本思想">基本思想</h3><p>  感知机的基本思想为：在<span class="math inline">\(n\)</span>维特征空间中，寻找一个能够使训练数据集中的正实例点与负实例点完全分开的超平面，对于新的实例，利用该超平面进行分类。为了找到这个超平面，感知机设置了以误分类样本数为基础的损失函数，通过梯度下降算法最小化损失函数，更新超平面，最终使得所有的训练实例均被正确分类，此时便找到了分类所需的超平面。感知机的基本思想可以用如下图1来描述：</p><center><img src="https://s2.loli.net/2023/10/08/r9CDXyRLPI5kdN6.jpg" width="60%" height="60%"><div data-align="center">Image1: 感知机的基本思想</div></center><h3 id="模型">模型</h3><p><strong>输入</strong></p><ul><li><strong>输入空间:</strong> <span class="math inline">\(\mathcal{X} =\mathbb{R}^{n}\)</span><br></li><li><strong>输入实例:</strong> <span class="math inline">\(x =\begin{bmatrix}  x^{(1)},x^{(2)},\dots,x^{(n)} \\ \end{bmatrix} \in\mathcal{X}\)</span></li></ul><p>  其中<span class="math inline">\(\mathcal{X}\)</span>代表<span class="math inline">\(n\)</span>维实数空间，输入实例<span class="math inline">\(x\)</span>为<span class="math inline">\(n\)</span>维特征向量。</p><p><strong>输出</strong></p><ul><li><strong>输出空间:</strong> <span class="math inline">\(\mathcal{Y} =\{-1,+1\}\)</span><br></li><li><strong>输出实例:</strong> <span class="math inline">\(y \in\mathcal{Y}\)</span></li></ul><p>  其中输出空间<span class="math inline">\(\mathcal{Y}\)</span>只包含+1和-1的一个集合，+1与-1分别代表二分类问题中的正类与负类。输出实例<span class="math inline">\(y\)</span>代表输出实例<span class="math inline">\(x\)</span>的类别。</p><p><strong>数据集</strong><br>  感知机模型的训练数据集<span class="math inline">\(T_{train}\)</span>为：</p><p><span class="math display">\[T_{train}=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}\]</span></p><p>  其中，<span class="math inline">\(x_i \in\mathcal{X}=\mathbb{R}^{n},y_i \in\mathcal{Y}=\{+1,-1\},i=1,2,\dots,N\)</span>，<span class="math inline">\(N\)</span>表示训练数据的样本容量。</p><p><strong>模型</strong><br>  设输入空间<span class="math inline">\(\mathcal{X}\)</span>到输出空间<span class="math inline">\(\mathcal{Y}\)</span>的映射为：</p><p><span class="math display">\[f(x)=sign(w^{T}x+b)\]</span></p><p>  称模型<span class="math inline">\(f\)</span>为感知机，其中，<span class="math inline">\(w\)</span>和<span class="math inline">\(b\)</span>为感知机模型的参数，<span class="math inline">\(w \in \mathbb{R}^{n}\)</span>为权重向量，<span class="math inline">\(b \in \mathbb{R}\)</span>为偏置。<span class="math inline">\(sign(\cdot)\)</span>为符号函数，即：</p><p><span class="math display">\[sign(x)= \left \{\begin{array}{rcl}+1, &amp; x \ge 0\\-1, &amp; x &lt; 0  \\\end{array} \right.\]</span></p><p>  感知机是一种线性分类模型，属于判别模型。</p><p><strong>假设空间</strong><br>  感知机模型的假设空间<span class="math inline">\(\mathcal{H}\)</span>为：</p><p><span class="math display">\[\mathcal{H}=\{f \vertf(x)=w^{T}x+b\}\]</span></p><p>  感知机模型的假设空间实际上特征空间中超平面的集合。</p><p><strong>参数空间</strong><br>  令<span class="math inline">\(\theta=(w,b)\)</span>，则模型的假设空间<span class="math inline">\(\mathcal{H}\)</span>等价于参数空间<span class="math inline">\(\Theta\)</span>：</p><p><span class="math display">\[\Theta=\{\theta \vert \theta \in\mathbb{R}^{n+1}\}\]</span></p><h3 id="策略">策略</h3><p>  感知机学习的基本假设是训练数据集是线性可分的，这里先给出数据集线性可分性的定义。</p><p><strong>数据集的线性可分性</strong><br>  给定一个数据集<span class="math inline">\(T\)</span>：</p><p><span class="math display">\[T=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}\]</span></p><p>其中，<span class="math inline">\(x_i \in\mathcal{X}=\mathbb{R}^{n},y_i \in\mathcal{Y}=\{+1,-1\},i=1,2,\dots,N\)</span>，如果存在某个超平面<span class="math inline">\(S\)</span>能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，即<span class="math inline">\(\exists \theta=(w,b) \in\mathbb{R}^{n+1}\)</span>，对所有 <span class="math inline">\(y_i=+1\)</span> 的实例<span class="math inline">\(i\)</span>，有 <span class="math inline">\(w^{T}x_i+b &gt; 0\)</span> ；对所有 <span class="math inline">\(y_i=-1\)</span> 的实例<span class="math inline">\(i\)</span>，有 <span class="math inline">\(w^{T}x_i+b &lt; 0\)</span>，则称数据集<span class="math inline">\(T\)</span>为线性可分数据集(linearly separable dataset)；否则，称数据集<span class="math inline">\(T\)</span>线性不可分。</p><p><strong>损失函数</strong>  为了寻找到合适的超平面，需要设置损失函数，一个很自然的想法是将损失函数设置为误分类点的个数，对于误分类数据<span class="math inline">\((x_i,y_i)\)</span>有：</p><p><span class="math display">\[-y_i(w^{T}x_i+b) &gt; 0\]</span></p><p>这是因为误分类的两种情况:<span class="math inline">\(w^{T}x_i+b &gt;0,y_i=-1\)</span>或<span class="math inline">\(w^{T}x_i+b &lt;0,y_i=+1\)</span>均可由上式表示，则损失函数可以表示为：</p><p><span class="math display">\[L(x,y)=\sum_{i=1}^{N}I_{\{-y_i(w^{T}x_i+b)&gt;0\}}\]</span></p><p><span class="math display">\[I_{\{-y_i(w^{T}x_i+b)&gt;0\}}=\left \{\begin{array}{rcl}1, &amp; {-y_i(w^{T}x_i+b)&gt;0}\\0, &amp; {-y_i(w^{T}x_i+b) \leq 0}\\\end{array} \right.\]</span></p><p>  这样设置损失函数的思路非常直观，但是，这样的损失函数不是参数<span class="math inline">\(w,b\)</span>的连续可导函数，不易进行优化，因此我们一般不将损失函数设置为这种形式。<br>  <strong>感知机所采用的损失函数是误分类点到超平面的总距离</strong>，其与误分类点的个数相关，当损失函数降至0时，意味着没有误分类点了，我们也就找到了可以将训练数据集完全正确分类的超平面了。  为此，首先给出输入空间<span class="math inline">\(\mathcal{X}\)</span>中任意一点<span class="math inline">\(x_0\)</span>到超平面<span class="math inline">\(S=\{x \vert w^{T}x+b=0\}\)</span>的距离：</p><p><span class="math display">\[d(x_0,S)=\frac{|w^{T}x_0+b|}{|| w ||_{2}}\]</span></p><p>  关于<span class="math inline">\(n\)</span>维空间中点到超平面的距离公式的证明可参见附录，这里不多做赘述。对于误分类点<span class="math inline">\(x_i\)</span>，其到初始超平面<span class="math inline">\(S\)</span>的距离为：</p><p><span class="math display">\[d(x_i,S)=\frac{-y_i(w^{T}x_i+b)}{||w||_2}\]</span></p><p>  假设在初始超平面<span class="math inline">\(S\)</span>的分类下，误分类点的集合为<span class="math inline">\(M\)</span>，则误分类点到超平面的总距离为：</p><p><span class="math display">\[d(M,S)=-\frac{1}{||w||_2}\sum_{x_i \inM}y_i(w^{T}x_i+b)\]</span></p><p>  在设定损失函数时，我们可以不考虑<span class="math inline">\(\frac{1}{||w||_2}\)</span>，因为其既不影响损失函数的正负，也不影响感知机算法的最终结果。这样，对于线性可分的训练数据集：</p><p><span class="math display">\[T_{train}=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}\]</span></p><p>  感知机学习的损失函数一般设定为：</p><p><span class="math display">\[L(w,b)=-\sum_{x_i \inM}y_i(w^{T}x_i+b)\]</span></p><p>  损失函数<span class="math inline">\(L(w,b)\)</span>是参数<span class="math inline">\(w,b\)</span>的连续可导函数，便于进行优化。当损失函数减小时，误分类点的数量也会减小，当<span class="math inline">\(L(w,b)=0\)</span>时，意味着训练数据集中所有的实例都被正确分类了，此时得到的超平面即是我们要寻找的可以完全正确分类的超平面。</p><h3 id="算法">算法</h3><p>  感知机算法分为原始形似与对偶形式，这里分别来介绍这两种形式。</p><h4 id="原始形式">原始形式</h4><p>  在策略里我们已经确定了感知机学习的损失函数，我们现在要做的便是最小化损失函数，即：</p><p><span class="math display">\[\min_{w,b} -\sum_{x_i \inM}y_i(w^{T}x_i+b)\]</span></p><p>  我们采用随机梯度下降法(stochastic gradientdescent)来优化损失函数。<br>  首先，随机选取一个初始超平面<span class="math inline">\(S_0=\{x \vertw_0^{T}x+b_0=0\}\)</span>，此时误分类点的集合为<span class="math inline">\(M_0\)</span>，则损失函数<span class="math inline">\(L(w,b)\)</span>关于参数<span class="math inline">\(w_0,b_0\)</span>的梯度为：</p><p><span class="math display">\[\nabla_{w_0}L(w_0,b_0)=-\sum_{x_i \inM_0}y_ix_i\]</span></p><p><span class="math display">\[\nabla_{b_0}L(w_0,b_0)=-\sum_{x_i \inM_0}y_i\]</span></p><p>  在<span class="math inline">\(M_0\)</span>随机选取一个误分类点<span class="math inline">\((x_0,y_0)\)</span>，对<span class="math inline">\(w_0,b_0\)</span>进行更新：</p><p><span class="math display">\[w_1 \leftarrow w_0+\etay_ix_i\]</span></p><p><span class="math display">\[b_1 \leftarrow b_0+\eta y_i\]</span></p><p>  其中，<span class="math inline">\(\eta\)</span>为梯度下降的步长，在机器学习中也称为学习率(learningrate)。更新后我们会得到一个新的超平面<span class="math inline">\(S_1=\{x\vert w_1^{T} x +b_1=0\}\)</span>，此时损失函数的值会减小。这样通过迭代可以期待损失函数<span class="math inline">\(L(w,b)\)</span>不断减小，直到为0，此时误分类集合<span class="math inline">\(M=\varnothing\)</span>，我们便找到了可以对训练集完全正确分类的超平面<span class="math inline">\(S\)</span>.<br>  综上所述，我们写出感知机算法的原始形式。</p><h5 id="感知机学习算法的原始形式">感知机学习算法的原始形式</h5><p>  输入：训练数据集<span class="math inline">\(T_{train}=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}\)</span>，其中<span class="math inline">\(x_i \in \mathcal{X}=\mathbb{R}^{n}，y \in\mathcal{Y}=\{-1,+1\},i=1,2,\dots,N\)</span>；学习率<span class="math inline">\(\eta (0 &lt; \eta \leq 1)\)</span>.<br>  输出：<span class="math inline">\(w,b\)</span>：感知机模型 <span class="math inline">\(f(x)=sign(w^{T}x+b)\)</span><br>  (1) 随机选取初始参数<span class="math inline">\(w_0,b_0\)</span>；<br>  (2) 在训练数据集中选取一个数据<span class="math inline">\((x_i,y_i)\)</span>；<br>  (3) <span class="math inline">\(if \space y_i(w_0^{T}x_i+b_0) \leq0:\)</span></p><p><span class="math display">\[w_1 \leftarrow w_0+\etay_ix_i\]</span></p><p><span class="math display">\[b_1 \leftarrow b_0+\eta y_i\]</span></p><p>  (4) 转至 (2)，直至训练集中没有误分类点。</p><p>  这种学习算法直观上有如下解释：<strong>当一个实例点被误分类，即位于分离超平面的错误一侧时，则调整<span class="math inline">\(w,b\)</span>的值，使超平面向该误分类点的一侧移动，以减小误分类点与超平面间的距离，直至超平面越过该误分类点使其被正确分类。</strong></p><h4 id="对偶形式">对偶形式</h4><p>  在感知机算法的原始形式中，如果实例点<span class="math inline">\((x_i,y_i)\)</span>是误分类点，则可以用该点更新参数，即：</p><p><span class="math display">\[w \leftarrow w + \etay_ix_i\]</span></p><p><span class="math display">\[b \leftarrow b + \eta y_i\]</span></p><p>  现在设想一下，如果在参数更新的过程中，点<span class="math inline">\((x_i,y_i)\)</span>被误分类了<span class="math inline">\(n_i\)</span>次，则在从初始参数<span class="math inline">\(w_0,b_0\)</span>到最终参数<span class="math inline">\(w,b\)</span>中，点<span class="math inline">\((x_i,y_i)\)</span>贡献的增量为<span class="math inline">\(n_i \eta y_ix_i\)</span>和<span class="math inline">\(n_i \eta y_i\)</span>.<br>  假设在迭代过程中，训练集中的每一个实例点<span class="math inline">\((x_i,y_i)\)</span>被误分类的次数为<span class="math inline">\(n_i,i=1,2,\dots,N\)</span>，取初始的参数向量为零向量，则通过迭代最终学习到的次数可以表示为：</p><p><span class="math display">\[w=\sum_{i=1}^{N}\alpha_iy_ix_i\]</span></p><p><span class="math display">\[b =\sum_{i=1}^{N}\alpha_iy_i\]</span></p><p>  其中，<span class="math inline">\(\alpha_i=n_i\eta\)</span>，若<span class="math inline">\(\eta=1\)</span>，则<span class="math inline">\(\alpha_i\)</span>表示训练集中的实例点<span class="math inline">\((x_i,y_i)\)</span>由于被误分类而用于更新参数的次数。<br>  在使用对偶算法进行迭代时，若<span class="math inline">\((x_i,y_i)\)</span>被误分类，则我们便在其对应的<span class="math inline">\(\alpha_i\)</span>上加上增量<span class="math inline">\(\eta\)</span>，从而得到新的超平面，循环迭代过程，直至没有误分类点为止。  综上所述，我们可以写出感知机算法的对偶形式。</p><h5 id="感知机学习算法的对偶形式">感知机学习算法的对偶形式</h5><p>  输入：训练数据集<span class="math inline">\(T_{train}=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}\)</span>，其中<span class="math inline">\(x_i \in \mathcal{X}=\mathbb{R}^{n}，y \in\mathcal{Y}=\{-1,+1\},i=1,2,\dots,N\)</span>；学习率<span class="math inline">\(\eta (0 &lt; \eta \leq 1)\)</span>.<br>  输出：<span class="math inline">\(\alpha\)</span>：感知机模型 <span class="math inline">\(f(x)=sign \left( (\sum_{i=1}^{N}\alpha_iy_ix_i)\cdot x+(\sum_{i=1}^{N}\alpha_iy_i) \right),\alpha=\begin{bmatrix}  \alpha_1, \alpha_2, \dots, \alpha_N \\\end{bmatrix}^T\)</span><br>  (1) <span class="math inline">\(\alpha \leftarrow 0\)</span>;<br>  (2) 在训练数据集<span class="math inline">\(T_{train}\)</span>中选取实例点<span class="math inline">\((x_j,y_j)\)</span>;<br>  (3) <span class="math inline">\(if \space y_i \left(\sum_{i=1}^{N}\alpha_iy_ix_i \cdot x_j+\sum_{i=1}^{N}\alpha_iy_i \right)\leq 0：\)</span></p><p><span class="math display">\[\alpha_i \leftarrow \alpha_i +\eta\]</span></p><p>  (4) 转至(2)直到没有误分类数据。</p><p>  值得注意的是，在对偶算法的每次迭代中，训练实例仅以内积的形式出现。为了方便计算，可以预先将训练数据中的实例间的内积计算出来，并以矩阵的形式储存，这个矩阵便是<span class="math inline">\(Gram\)</span>矩阵：</p><p><span class="math display">\[G=[x_i \cdot x_j]_{N \timesN}\]</span></p><p>  可以证明感知机算法是收敛的，即经过有限次迭代可以得到一个能将训练数据集完全正确分类的分离超平面。这部分证明放在附录里，感兴趣的读者可自行阅读。</p><h3 id="感知机算法的实例及python实现">感知机算法的实例及Python实现</h3><p>  首先，我们需要创造一个线性可分的二分类数据。为了便于可视化，设输入实例<span class="math inline">\(x \in \mathcal{X}=\mathbb{R}^2，y \in \mathcal{Y}=\{-1,1\}\)</span>，以下是用于产生数据的Python代码：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Data Generation</span><br>np.random.seed(<span class="hljs-number">1314</span>)<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">data_generation</span>(<span class="hljs-params">n,basepoint,ylabel</span>):<br>    x1 = np.random.random((n,))+np.array([basepoint[<span class="hljs-number">0</span>]]*n)<br>    x2 = np.random.random((n,))+np.array([basepoint[<span class="hljs-number">1</span>]]*n)<br>    y = np.array([ylabel]*n)<br>    data = pd.DataFrame({<span class="hljs-string">"x1"</span>:x1,<span class="hljs-string">"x2"</span>:x2,<span class="hljs-string">"y"</span>:y})<br>    <span class="hljs-keyword">return</span> data<br><br><span class="hljs-comment"># positive data</span><br>positivedata = data_generation(n=<span class="hljs-number">30</span>,basepoint=(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>),ylabel=<span class="hljs-number">1</span>)<br><span class="hljs-comment"># negative data</span><br>negativedata = data_generation(n=<span class="hljs-number">20</span>,basepoint=(<span class="hljs-number">2</span>,<span class="hljs-number">1</span>),ylabel=-<span class="hljs-number">1</span>)<br><span class="hljs-comment"># train data</span><br>train_data = pd.concat([positivedata,negativedata])<br>train_data = shuffle(train_data)<br>train_data.index = <span class="hljs-built_in">range</span>(train_data.shape[<span class="hljs-number">0</span>])<br>train_data.head(<span class="hljs-number">5</span>)<br></code></pre></td></tr></tbody></table></figure><p>  在这段代码里，我利用均匀分布在<span class="math inline">\(\{x \vertx_1 \in (1,2);x_2 \in (2,3)\}\)</span>里产生了30个正类实例，在<span class="math inline">\(\{x \vert x_1 \in (2,3);x_2 \in(1,2)\}\)</span>里产生了20个负类实例。得到的实例数据如下表1：</p><style>.center {  width: auto;  display: table;  margin-left: auto;  margin-right: auto;}</style><p align="center"><font face="黑体" size="2.">table1 训练数据(部分)</font></p><div class="center"><table><thead><tr class="header"><th style="text-align: center;">index</th><th style="text-align: center;">x1</th><th style="text-align: center;">x2</th><th style="text-align: center;">y</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">0</td><td style="text-align: center;">1.749421</td><td style="text-align: center;">2.193847</td><td style="text-align: center;">1</td></tr><tr class="even"><td style="text-align: center;">1</td><td style="text-align: center;">2.057071</td><td style="text-align: center;">1.983485</td><td style="text-align: center;">-1</td></tr><tr class="odd"><td style="text-align: center;">2</td><td style="text-align: center;">2.830457</td><td style="text-align: center;">1.026819</td><td style="text-align: center;">-1</td></tr><tr class="even"><td style="text-align: center;">3</td><td style="text-align: center;">1.437378</td><td style="text-align: center;">2.345612</td><td style="text-align: center;">1</td></tr><tr class="odd"><td style="text-align: center;">4</td><td style="text-align: center;">2.730692</td><td style="text-align: center;">1.493602</td><td style="text-align: center;">-1</td></tr></tbody></table></div><p>  利用以下代码将训练数据进行可视化：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># train data scatter plot</span><br>plt.scatter(x=positivedata[<span class="hljs-string">"x1"</span>],y=positivedata[<span class="hljs-string">"x2"</span>],marker=<span class="hljs-string">"o"</span>,c=<span class="hljs-string">"green"</span>,label=<span class="hljs-string">"positive data"</span>)<br>plt.scatter(x=negativedata[<span class="hljs-string">"x1"</span>],y=negativedata[<span class="hljs-string">"x2"</span>],marker=<span class="hljs-string">"x"</span>,c=<span class="hljs-string">"red"</span>,label=<span class="hljs-string">"negative data"</span>)<br>plt.xlabel(<span class="hljs-string">"x1"</span>)<br>plt.ylabel(<span class="hljs-string">"x2"</span>)<br>plt.xlim((<span class="hljs-number">0</span>,<span class="hljs-number">4</span>))<br>plt.ylim((<span class="hljs-number">0</span>,<span class="hljs-number">4</span>))<br>plt.xticks(np.arange(<span class="hljs-number">0</span>,<span class="hljs-number">4</span>,<span class="hljs-number">0.5</span>))<br>plt.yticks(np.arange(<span class="hljs-number">0</span>,<span class="hljs-number">4</span>,<span class="hljs-number">0.5</span>))<br>plt.legend()<br></code></pre></td></tr></tbody></table></figure><p>  得到的训练数据散点图为下图2：</p><center><img src="https://s2.loli.net/2023/10/12/b3KfhBve5VYl6Sw.png" width="60%" height="60%"><div data-align="center">Image2: 训练数据散点图</div></center><p>  从图中我们可以看出，训练数据是线性可分的，满足感知机学习的假设。<br>  之后我们可以根据感知机学习的原始算法来求解分离超平面的参数<span class="math inline">\(w,b\)</span>，参数求解的代码如下：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">param_solving</span>(<span class="hljs-params">train_data,lr,start_w</span>):<br>    <span class="hljs-comment"># initial parameter</span><br>    w = np.array(start_w) <span class="hljs-comment"># w = [w1,w2,b]</span><br>    x = train_data[[<span class="hljs-string">"x1"</span>,<span class="hljs-string">"x2"</span>]].values<br>    onecolumn = np.ones((train_data.shape[<span class="hljs-number">0</span>],))<br>    X = np.column_stack((x,onecolumn))<br>    y = train_data[<span class="hljs-string">"y"</span>].values<br>    k = <span class="hljs-number">0</span><br>    inter_data = {<span class="hljs-string">"k"</span>:[],<span class="hljs-string">"Parameter"</span>:[],<span class="hljs-string">"Loss Function"</span>:[],<span class="hljs-string">"Misclassifications"</span>:[],<span class="hljs-string">"Point Used for Update"</span>:[]}<br>    <span class="hljs-comment"># stochastic gradient descent</span><br>    <span class="hljs-keyword">while</span> np.<span class="hljs-built_in">any</span>((np.dot(X,w)*y&lt;<span class="hljs-number">0</span>) | (np.dot(X,w)*y==<span class="hljs-number">0</span>)):<br>        k += <span class="hljs-number">1</span><br>        falseindex = np.where((np.dot(X,w)*y&lt;<span class="hljs-number">0</span>) | (np.dot(X,w)*y==<span class="hljs-number">0</span>))<br><br>        <span class="hljs-comment"># storing information about the iterative process</span><br>        <span class="hljs-keyword">if</span> k%<span class="hljs-number">5</span> == <span class="hljs-number">0</span>:<br>            inter_data[<span class="hljs-string">"k"</span>].append(k)<br>            inter_data[<span class="hljs-string">"Parameter"</span>].append(w.<span class="hljs-built_in">round</span>(<span class="hljs-number">4</span>))<br>            falsedata = train_data.iloc[falseindex[<span class="hljs-number">0</span>],:]<br>            x_falseclassified = falsedata[[<span class="hljs-string">"x1"</span>,<span class="hljs-string">"x2"</span>]].values<br>            X_falseclassified = np.column_stack((x_falseclassified,np.ones((falsedata.shape[<span class="hljs-number">0</span>],))))<br>            y_falseclassified = falsedata[<span class="hljs-string">"y"</span>].values<br>            loss = -<span class="hljs-built_in">sum</span>(np.dot(X_falseclassified,w)*y_falseclassified)<br>            inter_data[<span class="hljs-string">"Loss Function"</span>].append(loss.<span class="hljs-built_in">round</span>(<span class="hljs-number">4</span>))<br>            inter_data[<span class="hljs-string">"Misclassifications"</span>].append(falseindex[<span class="hljs-number">0</span>])<br>            inter_data[<span class="hljs-string">"Point Used for Update"</span>].append(falseindex[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>])<br>        <br>        x_forupdate = train_data.iloc[falseindex[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>],[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]].values<br>        y_forupdate = train_data.iloc[falseindex[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>],[<span class="hljs-number">2</span>]].values<br>        delta = np.append(lr*y_forupdate*x_forupdate,lr*y_forupdate)<br>        w = w + delta<br>    inter_information = pd.DataFrame(inter_data)<br><br>    result_data = {}<br>    result_data[<span class="hljs-string">"final parameters"</span>] = w.<span class="hljs-built_in">round</span>(<span class="hljs-number">4</span>)<br>    result_data[<span class="hljs-string">"interative information"</span>] = inter_information<br>    result_data[<span class="hljs-string">"number of iterations"</span>] = k<br> <br>    <span class="hljs-keyword">return</span> result_data<br></code></pre></td></tr></tbody></table></figure><p>  若我们将初始的参数均设置为0，则我们最终得到的分离超平面参数为：</p><p><span class="math display">\[w,b = \begin{bmatrix}    -0.1117,0.1103 \\\end{bmatrix}^T,0.01\]</span></p><p>  总共的迭代次数为<span class="math inline">\(k=85\)</span>，以下表是迭代过程中的一些信息：</p><style>.center {  width: auto;  display: table;  margin-left: auto;  margin-right: auto;}</style><p align="center"><font face="黑体" size="2.">table2 迭代过程信息(部分)</font></p><div class="center"><table style="width:100%;"><thead><tr class="header"><th style="text-align: center;">k</th><th style="text-align: center;">Parameter</th><th style="text-align: center;">Loss Function</th><th style="text-align: center;">Misclassifications</th><th style="text-align: center;">Point Used for Update</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">5</td><td style="text-align: center;">[-0.0062, 0.0042, 0.0]</td><td style="text-align: center;">0.0161</td><td style="text-align: center;">[0, 5, 7, 9, 24, 28, 36, 43, 45,49]</td><td style="text-align: center;">0</td></tr><tr class="even"><td style="text-align: center;">10</td><td style="text-align: center;">[0.0052, 0.0304, 0.01]</td><td style="text-align: center;">1.3912</td><td style="text-align: center;">[ 1, 2, 4, 10, 12, 13, 15, 18, 22, 29,30, 32, 33, 37, 38, 39, 40,42, 44, 46]</td><td style="text-align: center;">1</td></tr><tr class="odd"><td style="text-align: center;">15</td><td style="text-align: center;">[-0.0215, 0.0147, 0.0]</td><td style="text-align: center;">0.0564</td><td style="text-align: center;">[0, 5, 7, 9, 24, 28, 36, 43, 45,49]</td><td style="text-align: center;">0</td></tr><tr class="even"><td style="text-align: center;">20</td><td style="text-align: center;">[-0.0102, 0.0409, 0.01]</td><td style="text-align: center;">0.9020</td><td style="text-align: center;">[ 1, 2, 4, 10, 12, 13, 15, 18, 22, 29,30, 32, 33, 37, 38, 39, 40,42, 44, 46]</td><td style="text-align: center;">1</td></tr><tr class="odd"><td style="text-align: center;">25</td><td style="text-align: center;">[-0.0369, 0.0252, 0.0]</td><td style="text-align: center;">0.0967</td><td style="text-align: center;">[0, 5, 7, 9, 24, 28, 36, 43, 45,49]</td><td style="text-align: center;">0</td></tr></tbody></table></div><p>  表中每一列的含义为：</p><ul><li><strong>k:</strong> 表示第<span class="math inline">\(k\)</span>次更新参数前。<br></li><li><strong>Parameter:</strong> 表示第<span class="math inline">\(k\)</span>次更新参数前参数向量<span class="math inline">\(\hat{w}=(w^T,b)^T\)</span>.<br></li><li><strong>Loss Function:</strong> 表示第<span class="math inline">\(k\)</span>次更新参数前损失函数<span class="math inline">\(L(w,b)\)</span>的值。<br></li><li><strong>Misclassifications:</strong> 表示第<span class="math inline">\(k\)</span>次更新参数前训练数据中被误分类的示例点的索引。<br></li><li><strong>Point Used for Update:</strong> 表示第<span class="math inline">\(k\)</span>次更新所用的误分类示例点的索引。</li></ul><p>  利用以下代码画出分类超平面：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">result = param_solving(train_data=train_data,lr=<span class="hljs-number">0.01</span>,start_w=[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>])<br>w = result[<span class="hljs-string">"final parameters"</span>]<br>x1_line = np.linspace(<span class="hljs-number">0</span>,<span class="hljs-number">4</span>,<span class="hljs-number">1000</span>)<br>x2_line = (-w[<span class="hljs-number">0</span>]*x1_line - w[<span class="hljs-number">2</span>])/w[<span class="hljs-number">1</span>]<br>plt.scatter(x=positivedata[<span class="hljs-string">"x1"</span>],y=positivedata[<span class="hljs-string">"x2"</span>],marker=<span class="hljs-string">"o"</span>,c=<span class="hljs-string">"green"</span>,label=<span class="hljs-string">"positive data"</span>)<br>plt.scatter(x=negativedata[<span class="hljs-string">"x1"</span>],y=negativedata[<span class="hljs-string">"x2"</span>],marker=<span class="hljs-string">"x"</span>,c=<span class="hljs-string">"red"</span>,label=<span class="hljs-string">"negative data"</span>)<br>plt.plot(x1_line,x2_line,c=<span class="hljs-string">"blue"</span>,label=<span class="hljs-string">"Classification Hyperplane"</span>)<br>plt.xlim((<span class="hljs-number">0</span>,<span class="hljs-number">4</span>))<br>plt.ylim((<span class="hljs-number">0</span>,<span class="hljs-number">4</span>))<br>plt.xticks(np.arange(<span class="hljs-number">0</span>,<span class="hljs-number">4</span>,<span class="hljs-number">0.5</span>))<br>plt.yticks(np.arange(<span class="hljs-number">0</span>,<span class="hljs-number">4</span>,<span class="hljs-number">0.5</span>))<br>plt.xlabel(<span class="hljs-string">"x1"</span>)<br>plt.ylabel(<span class="hljs-string">"x2"</span>)<br>plt.legend()<br></code></pre></td></tr></tbody></table></figure><p>  得到的分类超平面如下图所示：</p><center><img src="https://s2.loli.net/2023/10/12/8tjLUPw4aqFr29O.png" width="60%" height="60%"><div data-align="center">Image3: 分类超平面</div></center><p>  如果我们改变初始参数的值，我们可能会得到不同的分类超平面，比如我们将初始参数设置为<span class="math inline">\(w=\begin{bmatrix}  1,1 \\\end{bmatrix}^T,b=1\)</span>，则我们最终得到的分类超平面如下图4所示：</p><center><img src="https://s2.loli.net/2023/10/12/Hmcf6eJ13yoFWwh.png" width="60%" height="60%"><div data-align="center">Image4: 分类超平面</div></center><p>  此时，分类超平面的参数为：</p><p><span class="math display">\[w,b = \begin{bmatrix}    -0.2049,-0.0153 \\\end{bmatrix}^T,0.45\]</span></p><h2 id="附录">附录</h2><h3 id="空间中点到超平面的距离">空间中点到超平面的距离</h3><p><strong>结论</strong>   设<span class="math inline">\(n\)</span>维空间中存在某超平面<span class="math inline">\(S=\{x \vert w^{T}x+b=0;x,w \in \mathbb{R}^n,b \in\mathbb{R}\}\)</span>，已知点<span class="math inline">\(x_0 \in\mathbb{R}^{n},x_0 \notin S\)</span>，则点<span class="math inline">\(x_0\)</span>到超平面<span class="math inline">\(S\)</span>的距离为：</p><p><span class="math display">\[d(x_0,S)=\frac{|w^{T}x_0+b|}{||w||}\]</span></p><p><strong>证明:</strong></p><center><img src="https://s2.loli.net/2023/10/11/mKVMIlyDUipd45k.jpg" width="60%" height="60%"><div data-align="center">Image6: 点到超平面的距离</div></center><p>  设<span class="math inline">\(x_1\)</span>为<span class="math inline">\(x_0\)</span>在超平面<span class="math inline">\(S\)</span>上的投影，<span class="math inline">\(x_2\)</span>为<span class="math inline">\(S\)</span>上另外任意一点，向量<span class="math inline">\(\overrightarrow{x_0x_1}\)</span>与向量<span class="math inline">\(\overrightarrow{x_0x_2}\)</span>的夹角为<span class="math inline">\(\theta\)</span>，则有：</p><p><span class="math display">\[d(x_0,S)=||\overrightarrow{x_0x_1}||=||\overrightarrow{x_0x_2}||\cos{\theta}\]</span></p><p>  由向量夹角公式可知：</p><p><span class="math display">\[\cos{\theta}=\frac{\overrightarrow{x_0x_1} \cdot\overrightarrow{x_0x_2}}{||\overrightarrow{x_0x_1}|| \cdot||\overrightarrow{x_0x_2}||}\]</span></p><p>  将其代入到<span class="math inline">\(d(x_0,S)\)</span>表达式中可得：</p><p><span class="math display">\[d(x_0,S)=\frac{\overrightarrow{x_0x_1}\cdot \overrightarrow{x_0x_2}}{||\overrightarrow{x_0x_1}||}\]</span></p><p>  由于<span class="math inline">\(x_1,x_2\)</span>是超平面<span class="math inline">\(S\)</span>上的点，故有：</p><p><span class="math display">\[w^{T}x_1+b=0\]</span></p><p><span class="math display">\[w^{T}x_2+b=0\]</span></p><p><span class="math display">\[\Rightarrow w^{T}(x_1-x_2)=0 \Rightarroww \cdot \overrightarrow{x_2x_1}=0\]</span></p><p>  由于<span class="math inline">\(\overrightarrow{x_0x_1}\)</span>与<span class="math inline">\(\overrightarrow{x_2x_1}\)</span>垂直，因而有：</p><p><span class="math display">\[\overrightarrow{x_0x_1} \cdot\overrightarrow{x_2x_1}=0\]</span></p><p>  由于法向量<span class="math inline">\(w\)</span>与向量<span class="math inline">\(\overrightarrow{x_0x_1}\)</span>均垂直于超平面<span class="math inline">\(S\)</span>，故可设<span class="math inline">\(\overrightarrow{x_0x_1}=kw,k \in\mathbb{R}\)</span>，将其代入<span class="math inline">\(d(x_0,S)\)</span>可得：</p><p><span class="math display">\[d(x_0,S)=\frac{|k| \cdot |w \cdot\overrightarrow{x_0x_2}|}{|k| \cdot ||w||}=\frac{|w \cdot(x_2-x_0)|}{||w||}\]</span></p><p>  由于<span class="math inline">\(w \cdot x_2 + b=0\)</span>可得<span class="math inline">\(w \cdot x_2 = -b\)</span>，代入上式可得：</p><p><span class="math display">\[d(x_0,S)=\frac{|-b-w \cdotx_0|}{||w||}=\frac{|w^{T}x_0+b|}{||w||}\]</span></p><p>  证毕.</p><h3 id="感知机算法收敛性证明">感知机算法收敛性证明</h3><p>  现在证明，对于线性可分数据集，感知机学习算法原始形式收敛，即经过有限次迭代可以得到一个将训练数据集完全正确划分的分离超平面及感知机模型。<br>  为了便于叙述和推导，将偏置<span class="math inline">\(b\)</span>并入权重向量<span class="math inline">\(w\)</span>，记作<span class="math inline">\(\hat{w}=(w^T,b)^T\)</span>，同样也将输入向量<span class="math inline">\(x\)</span>进行扩充，加进常数1，记作<span class="math inline">\(\hat{x}=(x^T,1)^T\)</span>。这样，<span class="math inline">\(\hat{x} \in \mathbb{R}^{n+1},\hat{w} \in\mathbb{R}^{n+1}\)</span>。显然，<span class="math inline">\(\hat{w}^T\hat{x}=w^{T}x+b\)</span>。</p><h4 id="novikoff定理"><span class="math inline">\(Novikoff\)</span>定理</h4><p><strong>结论</strong><br>  设训练数据集<span class="math inline">\(T_{train}=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}\)</span>是线性可分的，其中<span class="math inline">\(x_i \in \mathcal{X} = \mathbb{R}^{n},y_i \in\mathcal{Y} = \{-1,+1\},i=1,2,\dots,N\)</span>，则有：<br>  (1) 存在超平面<span class="math inline">\(S=\{x \vert\hat{w}_{opt}^Tx=w_{opt}^Tx+b_{opt}=0,||\hat{w}_{opt}||=1\}\)</span>可将训练数据集<span class="math inline">\(T_{train}\)</span>完全正确分类；且<span class="math inline">\(\exists \gamma &gt;0\)</span>，对所有的<span class="math inline">\(x_i,i=1,2,\dots,N\)</span>，有</p><p><span class="math display">\[y_i(\hat{w}_{opt}^{T}\hat{x}_i)=y_i(w_{opt}^{T}x_i+b_{opt})\ge \gamma\]</span></p><p>  (2) 令 <span class="math inline">\(R=\max \{ ||\hat{x_i}|| \space\vert i=1,2,\dots,N\}\)</span>，则感知机学习的原始算法在训练数据集<span class="math inline">\(T_{train}\)</span>上的误分类次数<span class="math inline">\(k\)</span>满足不等式：</p><p><span class="math display">\[k \leq \left( \frac{R}{\gamma}\right)^2\]</span></p><p>  <span class="math inline">\(Novikoff\)</span>定理说明感知机学习算法能够经过有限次迭代得到一个可以将训练数据集完全正确分类的超平面，即算法收敛。下面，给出这个定理的证明。</p><p><strong>证明:</strong><br>  首先来证明(1):<br>  由于训练数据集<span class="math inline">\(T_{train}\)</span>是线性可分的，由数据集线性可分的定义可知，一定存在某个超平面能够将训练数据集完全正确分类，不妨设这个超平面为<span class="math inline">\(S=\{x \vert \hat{w}_{opt}^Tx=w_{opt}^Tx+b_{opt}=0,||\hat{w}_{opt}||=1\}\)</span>，则对于训练数据集中的每个实例，若<span class="math inline">\(y_i = +1\)</span>,则<span class="math inline">\(\hat{w}_{opt}^Tx_i &gt; 0\)</span>；若<span class="math inline">\(y_i=-1\)</span>，则<span class="math inline">\(\hat{w}_{opt}^Tx_i &lt;0\)</span>，因此有以下不等式成立：</p><p><span class="math display">\[y_i(\hat{w}_{opt}^{T}\hat{x}_i)=y_i(w_{opt}^{T}x_i+b_{opt})&gt; 0\]</span></p><p>  故存在：</p><p><span class="math display">\[\gamma = \min_{i}\{y_i(w_{opt}^{T}x_i+b_{opt})\}\]</span></p><p>  使得：</p><p><span class="math display">\[y_i(\hat{w}_{opt}^{T}\hat{x}_i)=y_i(w_{opt}^{T}x_i+b_{opt})\ge \gamma\]</span></p><p>  再来证明(2):</p><p>  不妨设感知机算法的初始权重向量<span class="math inline">\(\hat{w}_{0}=0\)</span>，如果发现一个实例被误分类，则更新一次权重。令<span class="math inline">\(\hat{w}_{k-1}\)</span>是发现的第<span class="math inline">\(i\)</span>个误分类实例之前的权重向量，即：</p><p><span class="math display">\[\hat{w}_{k-1}=(w_{k-1}^{T},b_{k-1})^T\]</span></p><p>  判断训练集中的实例<span class="math inline">\((x_i,y_i)\)</span>被误分类的条件为：</p><p><span class="math display">\[y_i(\hat{w}_{opt}^{T}\hat{x}_i)=y_i(w_{opt}^{T}x_i+b_{opt})\leq 0\]</span></p><p>  若实例<span class="math inline">\((x_i,y_i)\)</span>是被<span class="math inline">\(\hat{w}_{k-1}=(w_{k-1}^{T},b_{k-1})^T\)</span>误分类的数据，则<span class="math inline">\(w\)</span>和<span class="math inline">\(b\)</span>的更新过程为：</p><p><span class="math display">\[w_{k} \leftarrow w_{k-1} + \etay_ix_i\]</span></p><p><span class="math display">\[b_{k} \leftarrow b_{k-1} + \etay_i\]</span></p><p>  则：</p><p><span class="math display">\[\hat{w}_{k}= \hat{w}_{k-1}+\eta y_i\hat{x}_i\]</span></p><p>  由此可得：</p><p><span class="math display">\[\begin{align*}    \hat{w}_{k} \cdot \hat{w}_{opt} &amp;= \hat{w}_{k-1} \cdot\hat{w}_{opt}+\eta (y_i \hat{w}_{opt}^T \hat{x}_i) \\    &amp; \ge \hat{w}_{k-1} \cdot \hat{w}_{opt} + \eta \gamma  \\    &amp; \ge \hat{w}_{k-2} \cdot \hat{w}_{opt} + 2\eta \gamma  \\      &amp; \space \vdots  \\    &amp; \ge k \eta \gamma \\\end{align*}\]</span></p><p>  同时：</p><p><span class="math display">\[\begin{align*}    ||\hat{w}_{k}||^{2} &amp;= ||\hat{w}_{k-1}+\eta y_i \hat{x}_i||^2 \\    &amp;= ||\hat{w}_{k-1}||^2 + 2 \eta (y_i \hat{w}_{k-1}^T\hat{x}_i)+\eta^2 ||\hat{x}_i||^2 \\    &amp; \leq ||\hat{w}_{k-1}||^{2} + \eta^2||\hat{x_i}||^2 \\    &amp; \leq ||\hat{w}_{k-1}||^{2} + \eta^2R^2 \\    &amp; \leq ||\hat{w}_{k-2}||^{2} + 2\eta^2R^2 \\      &amp; \space \vdots \\    &amp; \leq k \eta^2R^2\end{align*}\]</span></p><p>  由内积不等式可知：</p><p><span class="math display">\[k \eta \gamma \leq \hat{w}_k \cdot\hat{w}_{opt} \leq ||\hat{w}_k||\space ||\hat{w}_{opt}|| \leq \sqrt{k}\eta R\]</span></p><p><span class="math display">\[\Rightarrow k \leq \left(\frac{R}{\gamma} \right)^2\]</span></p><p>  证毕.</p><h2 id="参考">参考</h2><p><strong>[1] Book: 李航.(2019).统计学习方法</strong><br><strong>[2] Video: bilibili,简博士,感知机系列</strong><br><strong>[3] Blog: 知乎,夜魔刀,点到超平面距离的公式推导</strong></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>矩阵分析-5.线性映射</title>
      <link href="/2023/10/04/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-5-%E7%BA%BF%E6%80%A7%E6%98%A0%E5%B0%84/"/>
      <url>/2023/10/04/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-5-%E7%BA%BF%E6%80%A7%E6%98%A0%E5%B0%84/</url>
      
        <content type="html"><![CDATA[<h1 id="线性映射">线性映射</h1><h2 id="线性映射的定义">线性映射的定义</h2><p>  设<span class="math inline">\(V_1,V_2\)</span>是数域<span class="math inline">\(\mathbb{F}\)</span>上的线性空间，有映射 <span class="math inline">\(\sigma: V_1 \rightarrow V_2\)</span>，如果<span class="math inline">\(\sigma\)</span>满足：<br>(1) <strong>加法关系:</strong> 对<span class="math inline">\(\foralle_1,e_2 \in V_1, \sigma(e_1+e_2) = \sigma(e_1)+\sigma(e_2) \inV_2\)</span>.<br>(2) <strong>数乘关系:</strong> d对<span class="math inline">\(\forall e\in V_1, k \in \mathbb{F}, \sigma(ek)=\sigma(e)k\)</span><br>则称映射<span class="math inline">\(\sigma\)</span>是<span class="math inline">\(V_1\)</span>到<span class="math inline">\(V_2\)</span>的<strong>线性映射</strong>。特别地，若有<span class="math inline">\(V_1=V_2=V\)</span>，则称映射<span class="math inline">\(\sigma\)</span>为<span class="math inline">\(V\)</span>上的<strong>线性变换</strong>。</p><p><strong>注:</strong> 若线性映射<span class="math inline">\(\sigma:V_1 \rightarrow V_2\)</span> 是可逆映射(一一映射)，则称<span class="math inline">\(\sigma\)</span>为<strong>线性同构</strong>。</p><h2 id="线性映射的实例">线性映射的实例</h2><h3 id="例1-线性与非线性映射">例1 线性与非线性映射</h3><p><strong>非线性映射的实例</strong>   设线性空间<span class="math inline">\(V_1,V_2=\mathbb{R}^2\)</span>，有映射：</p><p><span class="math display">\[\mathcal{A}: \begin{bmatrix}    x_1 \\    x_2 \\\end{bmatrix} \mapsto \begin{bmatrix}    x_1+x_2 \\    x_1x_2 \\\end{bmatrix}\]</span></p><p>则映射<span class="math inline">\(\mathcal{A}\)</span>为<span class="math inline">\(V_1\)</span>到<span class="math inline">\(V_2\)</span>的非线性映射。<br><strong>证明:</strong><br>  取<span class="math inline">\(e_1,e_2 \in V_1\)</span>，其中:</p><p><span class="math display">\[e_1=e_2=\begin{bmatrix}    1 \\    1 \\\end{bmatrix},e_1+e_2=\begin{bmatrix}    2 \\    2 \\\end{bmatrix}\]</span></p><p><span class="math display">\[\mathcal{A}(e_1+e_2)=\begin{bmatrix}    2+2 \\    2 \times 2 \\\end{bmatrix}=\begin{bmatrix}    4 \\    4 \\\end{bmatrix}\]</span></p><p><span class="math display">\[\mathcal{A}(e_1)+\mathcal{A}(e_1)=\begin{bmatrix}    1+1 \\    1 \times 1 \\\end{bmatrix}+\begin{bmatrix}    1+1 \\    1 \times 1 \\\end{bmatrix}=\begin{bmatrix}    4 \\    2 \\\end{bmatrix}\]</span></p><p>  <span class="math inline">\(\because \mathcal{A}(e_1+e_2) \ne\mathcal{A}(e_1)+\mathcal{A}(e_1)\)</span>，故映射<span class="math inline">\(\mathcal{A}\)</span>不满足加法关系，其不是<span class="math inline">\(V_1\)</span>到<span class="math inline">\(V_2\)</span>上的线性映射。</p><p><strong>线性映射的实例</strong><br>  设线性空间<span class="math inline">\(V_1=\mathbb{R}^3,V_2=\mathbb{R}^2\)</span>，有映射：</p><p><span class="math display">\[\mathcal{B}: \begin{bmatrix}    x_1 \\    x_2 \\    x_3 \\\end{bmatrix} \mapsto \begin{bmatrix}    x_1+x_2-x_3 \\    \frac{1}{2}x_1-3x_2\end{bmatrix}\]</span></p><p>则线性映射<span class="math inline">\(\mathcal{B}\)</span>为<span class="math inline">\(V_1\)</span>到<span class="math inline">\(V_2\)</span>上的线性映射。<br><strong>证明:</strong><br>  (1) 先验证映射<span class="math inline">\(\mathcal{B}\)</span>满足线性映射的加法关系:<br>  设<span class="math inline">\(\forall \alpha,\beta \in V_1, \alpha =\begin{bmatrix}  \alpha_1 ,\alpha_2,\alpha_3 \end{bmatrix}^T,\beta =\begin{bmatrix}  \beta_1,\beta_2,\beta_3 \end{bmatrix}^T\)</span></p><p><span class="math display">\[\mathcal{B}(\alpha)=\begin{bmatrix}    \alpha_1+\alpha_2-\alpha_3 \\    \frac{1}{2}\alpha_1-3\alpha_2 \\\end{bmatrix},\mathcal{B}(\beta)=\begin{bmatrix}    \beta_1+\beta_2-\beta_3 \\    \frac{1}{2}\beta_1-3\beta_2 \\\end{bmatrix}\]</span></p><p><span class="math display">\[\mathcal{B}(\alpha)+\mathcal{B}(\beta)=\begin{bmatrix}    \alpha_1+\beta_1+\alpha_2+\beta_2-(\alpha_3+\beta_3) \\    \frac{1}{2}(\alpha_1+\beta_1)-3(\alpha_2+\beta_2) \\\end{bmatrix}\]</span></p><p><span class="math display">\[\alpha+\beta=\begin{bmatrix}    \alpha_1+\beta_1 \\    \alpha_2+\beta_2 \\    \alpha_3+\beta_3 \\\end{bmatrix},\mathcal{B}(\alpha+\beta)=\begin{bmatrix}    \alpha_1+\beta_1+\alpha_2+\beta_2-(\alpha_3+\beta_3) \\    \frac{1}{2}(\alpha_1+\beta_1)-3(\alpha_2+\beta_2) \\\end{bmatrix}\]</span></p><p><span class="math display">\[\Rightarrow\mathcal{B}(\alpha+\beta)=\mathcal{B}(\alpha)+\mathcal{B}(\beta)\]</span></p><p>  故映射<span class="math inline">\(\mathcal{B}\)</span>满足线性映射的加法关系。<br>  (2) 再验证映射<span class="math inline">\(\mathcal{B}\)</span>满足线性映射的数乘关系:<br>  设<span class="math inline">\(\forall \alpha \in V_1,k \in\mathbb{F},\alpha = \begin{bmatrix}  \alpha_1 ,\alpha_2,\alpha_3 \\\end{bmatrix}^T\)</span>,有:</p><p><span class="math display">\[\alpha k = \begin{bmatrix}    \alpha_1 k \\    \alpha_2 k \\    \alpha_3 k \\\end{bmatrix}, \mathcal{B}(\alpha k)=\begin{bmatrix}    \alpha_1 k+\alpha_2 k-\alpha_3 k \\    \frac{1}{2}\alpha_1 k-3\alpha_2 k \\\end{bmatrix}=\begin{bmatrix}    (\alpha_1+\alpha_2-\alpha_3)k \\    (\frac{1}{2}\alpha_1-3\alpha_2) k \\\end{bmatrix}\]</span></p><p><span class="math display">\[\mathcal{B}(\alpha)=\begin{bmatrix}    \alpha_1+\alpha_2-\alpha_3 \\    \frac{1}{2}\alpha_1-3\alpha_2 \\\end{bmatrix},\mathcal{B}(\alpha)k=\begin{bmatrix}    (\alpha_1+\alpha_2-\alpha_3)k \\    (\frac{1}{2}\alpha_1-3\alpha_2) k \\\end{bmatrix}\]</span></p><p><span class="math display">\[\Rightarrow \mathcal{B}(\alphak)=\mathcal{B}(\alpha)k\]</span></p><p>  故映射<span class="math inline">\(\mathcal{B}\)</span>满足线性映射的数乘关系。<br>  综上所述，映射<span class="math inline">\(\mathcal{B}\)</span>为<span class="math inline">\(V_1\)</span>到<span class="math inline">\(V_2\)</span>上的线性映射。</p><h3 id="例2-矩阵与标准线性空间之间的线性映射的等同性">例2矩阵与标准线性空间之间的线性映射的等同性</h3><p>  给定矩阵 <span class="math inline">\(A \in \mathbb{F}^{m \times n},x \in \mathbb{F}^n\)</span>，则矩阵<span class="math inline">\(A\)</span>可以作为线性映射<span class="math inline">\(\sigma_{A}\)</span>：</p><p><span class="math display">\[\begin{align*}    \sigma_{A}: &amp;\mathbb{F}^{n} \rightarrow \mathbb{F}^{m} \\    &amp;x \mapsto y=Ax\end{align*}\]</span></p><p>  若我们已知有线性映射 <span class="math inline">\(\sigma:\mathbb{F}^{n} \rightarrow\mathbb{F}^{m}\)</span>，如例1中的线性映射<span class="math inline">\(\mathcal{B}\)</span>，能否求得相应的矩阵<span class="math inline">\(A\)</span>?<br><strong>解:</strong><br>  记标准线性空间<span class="math inline">\(\mathbb{F}^n\)</span>的标准基为：<span class="math inline">\(e_1,e_2,\dots,e_n\)</span>，可以构造矩阵：</p><p><span class="math display">\[\sigma(\begin{bmatrix}    e_1,e_2,\dots,e_n \\\end{bmatrix})=\begin{bmatrix}    \sigma(e_1),\sigma(e_2),\dots,\sigma(e_n) \\\end{bmatrix} \triangleq A \in \mathbb{F}^{n \times m}\]</span></p><p>  对<span class="math inline">\(\forall x \in\mathbb{F}^{n}\)</span>，将<span class="math inline">\(x\)</span>沿着标准基展开：</p><p><span class="math display">\[x = \begin{bmatrix}    e_1,e_2,\dots,e_n \\\end{bmatrix}x=e_1x_1+e_2x_2,\dots,e_nx_n\]</span></p><p><span class="math display">\[\begin{align*}    \sigma(x)&amp;=\sigma(e_1x_1+e_2x_2,\dots,e_nx_n)=\sigma(e_1)x_1+\sigma(e_2)x_2+\dots+\sigma(e_n)x_n\\    &amp;= \begin{bmatrix}    \sigma(e_1),\sigma(e_2),\dots,\sigma(e_n) \\\end{bmatrix}x=Ax\end{align*}\]</span></p><p>  因此矩阵<span class="math inline">\(A\)</span>与线性映射<span class="math inline">\(\sigma\)</span>具有等同性.</p><h2 id="线性映射的矩阵表示">线性映射的矩阵表示</h2><h3 id="定义">定义</h3><p>  设有标准线性空间<span class="math inline">\(V=\mathbb{F}^n,W=\mathbb{F}^m\)</span>，给定线性映射：</p><p><span class="math display">\[\begin{align*}    \sigma: &amp;V \rightarrow W \\    &amp; v \mapsto w\end{align*}\]</span></p><p>  选取<span class="math inline">\(V\)</span>的基向量<span class="math inline">\(v_1,v_2,\dots,v_n\)</span>，作为<strong>入口基</strong>，<span class="math inline">\(W\)</span>的基向量<span class="math inline">\(w_1,w_2,\dots,w_m\)</span>作为<strong>出口基</strong>，记<span class="math inline">\(V\)</span>中第<span class="math inline">\(j\)</span>个入口基向量<span class="math inline">\(v_j\)</span>在<span class="math inline">\(W\)</span>中的象<span class="math inline">\(\sigma(v_j)\)</span>在出口基下的坐标表示为<span class="math inline">\(a_j = \begin{bmatrix}  a_{1j},a_{2j},\dots,a_{mj}\\ \end{bmatrix}^T\)</span>，即：</p><p><span class="math display">\[\sigma(v_j)=\begin{bmatrix}    w_1,w_2,\dots,w_n \\\end{bmatrix}\begin{bmatrix}    a_{1j} \\    a_{2j} \\    \vdots \\    a_{mj} \\\end{bmatrix}\]</span></p><p>  将所有入口基的象在出口基下的坐标拼成矩阵<span class="math inline">\(A\)</span>:</p><p><span class="math display">\[A=\begin{bmatrix}    \sigma(v_1),\sigma(v_2),\dots,\sigma(v_n) \\\end{bmatrix}=\begin{bmatrix}    a_{11} &amp; a_{21} &amp; \dots &amp; a_{1n} \\    a_{21} &amp; a_{22} &amp; \dots &amp; a_{2n} \\    \vdots &amp; \vdots &amp;  &amp; \vdots \\    a_{m1} &amp; a_{m2} &amp; \dots &amp; a_{mn} \\\end{bmatrix}\]</span></p><p>  则有：</p><p><span class="math display">\[\sigma(\begin{bmatrix}    v_1,v_2,\dots,v_n\end{bmatrix})=\begin{bmatrix}    w_1,w_2,\dots,w_m\end{bmatrix}A\]</span></p><p>  则称矩阵<span class="math inline">\(A\)</span>为线性映射<span class="math inline">\(\sigma\)</span>在入口基<span class="math inline">\(v_i,i=1,\dots,n\)</span>和出口基<span class="math inline">\(w_j,j=1,\dots,m\)</span>下的矩阵表示。</p><p><strong>注:</strong></p><p><span class="math display">\[\color{green} \begin{bmatrix}    线性 \\    映射 \\\end{bmatrix}\begin{bmatrix}    入口基 \\    矩阵  \\\end{bmatrix}=\begin{bmatrix}    出口基 \\    矩阵  \\\end{bmatrix}\begin{bmatrix}    线性映射 \\    矩阵表示  \\\end{bmatrix}\]</span></p><h3 id="定理线性映射的坐标计算">定理(线性映射的坐标计算)</h3><p>  设有<span class="math inline">\(n\)</span>维线性空间<span class="math inline">\(V\)</span>和<span class="math inline">\(m\)</span>维线性空间<span class="math inline">\(W\)</span>，<span class="math inline">\(v_1,v_2,\dots,v_n\)</span>为<span class="math inline">\(V\)</span>的一组基向量，<span class="math inline">\(w_1,w_2,\dots,w_m\)</span>为<span class="math inline">\(W\)</span>的一组基向量，映射<span class="math inline">\(\sigma\)</span>为<span class="math inline">\(V\)</span>到<span class="math inline">\(W\)</span>上的线性映射，矩阵<span class="math inline">\(A\)</span>为线性映射<span class="math inline">\(\sigma\)</span>在入口基<span class="math inline">\(\{v_i \vert i=1,2,\dots,n\}\)</span>与出口基<span class="math inline">\(\{w_i \verti=1,2,\dots,m\}\)</span>下的矩阵表示。<br>  给定<span class="math inline">\(\forall v \inV\)</span>，其沿着基向量组展开的坐标为<span class="math inline">\(x\)</span>，即：</p><p><span class="math display">\[v=\begin{bmatrix}    v_1,v_2,\dots,v_n \\\end{bmatrix}x=\begin{bmatrix}    v_1,v_2,\dots,v_n \\\end{bmatrix}\begin{bmatrix}    x_1 \\    x_2 \\    \vdots \\    x_m \\\end{bmatrix}\]</span></p><p>则经过线性映射<span class="math inline">\(\sigma\)</span>后，<span class="math inline">\(\sigma(v) \inW\)</span>在沿着基向量组展开的坐标为<span class="math inline">\(Ax\)</span>，即：</p><p><span class="math display">\[\sigma(v) = \begin{bmatrix}    w_1,w_2,\dots,w_m \\\end{bmatrix}Ax\]</span></p><p><strong>证明:</strong><br>  由题意可知：</p><p><span class="math display">\[v=\begin{bmatrix}    v_1,v_2,\dots,v_n \\\end{bmatrix}\begin{bmatrix}    x_1 \\    x_2 \\    \vdots \\    x_m \\\end{bmatrix}=v_1x_1+v_2x_2+\dots+v_nx_n\]</span></p><p>  则有</p><p><span class="math display">\[\begin{align*}    \sigma(v)&amp;=\sigma(v_1x_1+v_2x_2+\dots+v_nx_n)=\sigma(v_1)x_1+\sigma(v_2)x_2+\dots+\sigma(v_n)x_n\\    &amp;=\begin{bmatrix}        \sigma(v_1),\sigma(v_2),\dots,\sigma(v_n) \\    \end{bmatrix}=(\begin{bmatrix}        w_1,w_2,\dots,w_m    \end{bmatrix}A)x \\    &amp;=\begin{bmatrix}        w_1,w_2,\dots,w_m    \end{bmatrix}(Ax)\end{align*}\]</span></p><p>  故<span class="math inline">\(\sigma(v)\)</span>沿着基向量组<span class="math inline">\(\{w_i \verti=1,2,\dots,m\}\)</span>展开后的坐标为<span class="math inline">\(Ax\)</span>.</p><p><span style="color: green;">结论：基向量组将抽象的线性空间映射为标准线性空间，在基向量组的表示下，原本抽象线性空间之间的线性映射可以被表示为具体的矩阵。</span></p><center><img src="https://s2.loli.net/2023/10/05/GTHiFnjpR4v2CQr.jpg    " width="60%" height="60%"><div data-align="center">Image1: 线性映射的矩阵表示</div></center><h2 id="线性映射矩阵表示的实例">线性映射矩阵表示的实例</h2><h3 id="例1-微分算子的矩阵表示">例1 微分算子的矩阵表示</h3><p>  设线性空间<span class="math inline">\(V=\mathbb{R}_{4}[x],W=\mathbb{R}_{3}[x]\)</span>(<span class="math inline">\(\mathbb{R}_{n}[x]\)</span>表示<span class="math inline">\(n\)</span>维多项式函数空间)，微分算子<span class="math inline">\(\sigma\)</span>为<span class="math inline">\(V\)</span>到<span class="math inline">\(W\)</span>上的线性映射，求<span class="math inline">\(\sigma\)</span>在标准基向量组下的矩阵表示.</p><p><strong>解:</strong><br>  <span class="math inline">\(V\)</span>的标准基向量组为：<span class="math inline">\(\{1,x,x^2,x^3\}\)</span>(入口基)，<span class="math inline">\(W\)</span>的标准基向量组为<span class="math inline">\(\{1,x,x^2\}\)</span>，则有：</p><p><span class="math display">\[\begin{align*}    \sigma(\begin{bmatrix}        1,x,x^2,x^3 \\    \end{bmatrix})&amp;=\begin{bmatrix}        \sigma(1),\sigma(x),\sigma(x^2),\sigma(x^3) \\    \end{bmatrix} \\    &amp;=\begin{bmatrix}        0,1,2x,3x^2 \\    \end{bmatrix}=\begin{bmatrix}        1,x,x^2 \\    \end{bmatrix}A\end{align*}\]</span></p><p><span class="math display">\[A=\begin{bmatrix}    0 &amp; 1 &amp; 0 &amp; 0 \\    0 &amp; 0 &amp; 2 &amp; 0 \\    0 &amp; 0 &amp; 0 &amp; 3 \\\end{bmatrix}\]</span></p><p>  故矩阵<span class="math inline">\(A\)</span>即为微分算子<span class="math inline">\(\sigma\)</span>在<span class="math inline">\(V\)</span>与<span class="math inline">\(W\)</span>的标准基向量组下的矩阵表示.</p><p><strong>应用:</strong></p><p>  <span class="math inline">\(v = \frac{1}{2}x^3+5x \inV\)</span>，将<span class="math inline">\(v\)</span>沿着<span class="math inline">\(V\)</span>的标准基向量组<span class="math inline">\(\{1,x,x^2,x^3\}\)</span>展开得其坐标:</p><p><span class="math display">\[v=\frac{1}{2}x^3+5x=\begin{bmatrix}    1,x,x^2,x^3 \\\end{bmatrix}\begin{bmatrix}    0 \\    5 \\    0 \\    \frac{1}{2} \\\end{bmatrix}\]</span></p><p>  则<span class="math inline">\(\sigma(v)\)</span>在<span class="math inline">\(W\)</span>的标准基向量组<span class="math inline">\(\{1,x,x^2\}\)</span>下的坐标表示为:</p><p><span class="math display">\[A\begin{bmatrix}    0 \\    5 \\    0 \\    \frac{1}{2} \\\end{bmatrix}=\begin{bmatrix}    0 &amp; 1 &amp; 0 &amp; 0 \\    0 &amp; 0 &amp; 2 &amp; 0 \\    0 &amp; 0 &amp; 0 &amp; 3 \\\end{bmatrix}\begin{bmatrix}    0 \\    5 \\    0 \\    \frac{1}{2} \\\end{bmatrix}=\begin{bmatrix}    5 \\    0 \\    \frac{3}{2} \\\end{bmatrix}\]</span></p><p>  故对<span class="math inline">\(v\)</span>求微分的结果为：</p><p><span class="math display">\[\sigma(v)=\begin{bmatrix}    1,x,x^2,x^3 \\\end{bmatrix}\begin{bmatrix}    5 \\    0 \\    \frac{3}{2} \\\end{bmatrix}=\frac{3}{2}x^2+5\]</span></p><h3 id="例2-旋转变换的矩阵表示">例2 旋转变换的矩阵表示</h3><p>  欧几里得空间中的某一物体绕固定轴旋转<span class="math inline">\(\theta\)</span>，求该变换的矩阵表示。<br><strong>解:</strong><br>  设<span class="math inline">\(V=W\)</span>为欧几里得空间，映射<span class="math inline">\(\sigma\)</span>为旋转变换，易知<span class="math inline">\(\sigma\)</span>为线性变换.  设欧几里得空间中的一组标准正交基向量为<span class="math inline">\(e_1,e_2,e_3\)</span>，</p><p><span class="math display">\[e_1=\begin{bmatrix}    1 \\    0 \\    0 \\\end{bmatrix}, e_2=\begin{bmatrix}    0 \\    1 \\    0 \\\end{bmatrix}, e_3=\begin{bmatrix}    0 \\    0 \\    1 \\\end{bmatrix}\]</span></p><p>  其中<span class="math inline">\(e_3\)</span>为旋转变换所固定的轴，则<span class="math inline">\(e_1,e_2\)</span>为与旋转轴所垂直的平面的一组正交基，可以<span class="math inline">\(e_1,e_2,e_3\)</span>的方向为<span class="math inline">\(x,y,z\)</span>轴建立坐标系. 向量组<span class="math inline">\(e_1,e_2,e_3\)</span>既为入口基也为出口基.  旋转变换<span class="math inline">\(\sigma\)</span>作用于基向量组<span class="math inline">\(e_1,e_2,e_3\)</span>时，<span class="math inline">\(e_3\)</span>并不会改变，<span class="math inline">\(e_1,e_2\)</span>在其所在的平面上旋转<span class="math inline">\(\theta\)</span>，旋转变换可用图2表示。</p><center><img src="https://s2.loli.net/2023/10/05/hU9ixmNgfXRtWze.jpg    " width="40%" height="40%"><div data-align="center">Image2: 旋转变换</div></center><p>  由几何知识可得：</p><p><span class="math display">\[\sigma(e_1)=\begin{bmatrix}    cos\theta \\    sin\theta \\    0 \\\end{bmatrix},\sigma(e_2)=\begin{bmatrix}    -sin\theta \\    cos\theta \\    0 \\\end{bmatrix},\sigma(e_3)=\begin{bmatrix}    0 \\    0 \\    1 \\\end{bmatrix}\]</span></p><p>  故有:</p><p><span class="math display">\[\begin{align*}    \sigma(\begin{bmatrix}    e_1,e_2,e_3 \\\end{bmatrix})&amp;=\begin{bmatrix}    \sigma(e_1),\sigma(e_2),\sigma(e_3)\end{bmatrix}=\begin{bmatrix}    cos\theta &amp; -sin\theta &amp; 0 \\    sin\theta &amp; cos\theta &amp; 0 \\    0 &amp; 0 &amp; 1 \\\end{bmatrix} \\    &amp;=\begin{bmatrix}        1 &amp; 0 &amp; 0 \\        0 &amp; 1 &amp; 0 \\        0 &amp; 0 &amp; 1 \\    \end{bmatrix}\begin{bmatrix}    cos\theta &amp; -sin\theta &amp; 0 \\    sin\theta &amp; cos\theta &amp; 0 \\    0 &amp; 0 &amp; 1 \\\end{bmatrix}=\begin{bmatrix}    e_1,e_2,e_3 \\\end{bmatrix}A\end{align*}\]</span></p><p><span class="math display">\[A=\begin{bmatrix}    cos\theta &amp; -sin\theta &amp; 0 \\    sin\theta &amp; cos\theta &amp; 0 \\    0 &amp; 0 &amp; 1 \\\end{bmatrix}\]</span></p><p>  矩阵<span class="math inline">\(A\)</span>即为欧几里得空间中旋转变换<span class="math inline">\(\sigma\)</span>在标准正交基下的矩阵表示.</p><h3 id="例3-镜面反射的矩阵表示">例3 镜面反射的矩阵表示</h3><p>  欧几里得空间中的某一物体对固定平面进行镜面反射，求该变换的矩阵表示.<br><strong>解:</strong><br>  设<span class="math inline">\(V=M\)</span>为欧几里得空间，映射<span class="math inline">\(\sigma\)</span>为镜面反射，易知<span class="math inline">\(\sigma\)</span>为线性变换.  设欧几里得空间中的一组标准正交基向量为<span class="math inline">\(e_1,e_2,e_3\)</span>，</p><p><span class="math display">\[e_1=\begin{bmatrix}    1 \\    0 \\    0 \\\end{bmatrix}, e_2=\begin{bmatrix}    0 \\    1 \\    0 \\\end{bmatrix}, e_3=\begin{bmatrix}    0 \\    0 \\    1 \\\end{bmatrix}\]</span></p><p>  其中，<span class="math inline">\(e_1,e_2\)</span>所在的平面即为进行镜面反射所依赖的平面，<span class="math inline">\(e_3\)</span>为垂直于该平面的一个基向量。可以<span class="math inline">\(e_1,e_2,e_3\)</span>的方向为<span class="math inline">\(x,y,z\)</span>轴建立坐标系. 向量组<span class="math inline">\(e_1,e_2,e_3\)</span>既为入口基也为出口基.<br>  镜面反射<span class="math inline">\(\sigma\)</span>作用于基向量组<span class="math inline">\(e_1,e_2,e_3\)</span>时，<span class="math inline">\(e_1,e_2\)</span>并不会改变，<span class="math inline">\(e_3\)</span>变为相反方向，镜面反射可用图3表示。</p><center><img src="https://s2.loli.net/2023/10/05/oSeUqtnj5I74Kyf.jpg    " width="40%" height="40%"><div data-align="center">Image3: 镜面反射</div></center><p>  由几何知识可知：</p><p><span class="math display">\[\sigma(e_1)=\begin{bmatrix}    1 \\    0 \\    0 \\\end{bmatrix},\sigma(e_2)=\begin{bmatrix}    0 \\    1 \\    0 \\\end{bmatrix},\sigma(e_3)=\begin{bmatrix}    0 \\    0 \\    -1 \\\end{bmatrix}\]</span></p><p>  故有:</p><p><span class="math display">\[\begin{align*}    \sigma(\begin{bmatrix}    e_1,e_2,e_3 \\\end{bmatrix})&amp;=\begin{bmatrix}    \sigma(e_1),\sigma(e_2),\sigma(e_3)\end{bmatrix}=\begin{bmatrix}    1 &amp; 0 &amp; 0 \\    0 &amp; 1 &amp; 0 \\    0 &amp; 0 &amp; -1 \\\end{bmatrix} \\    &amp;=\begin{bmatrix}        1 &amp; 0 &amp; 0 \\        0 &amp; 1 &amp; 0 \\        0 &amp; 0 &amp; 1 \\    \end{bmatrix}\begin{bmatrix}    1 &amp; 0 &amp; 0 \\    0 &amp; 1 &amp; 0 \\    0 &amp; 0 &amp; -1 \\\end{bmatrix}=\begin{bmatrix}    e_1,e_2,e_3 \\\end{bmatrix}A\end{align*}\]</span></p><p><span class="math display">\[A=\begin{bmatrix}    1 &amp; 0 &amp; 0 \\    0 &amp; 1 &amp; 0 \\    0 &amp; 0 &amp; -1 \\\end{bmatrix}\]</span></p><p>  矩阵<span class="math inline">\(A\)</span>即为欧几里得空间中镜面反射<span class="math inline">\(\sigma\)</span>在标准正交基下的矩阵表示.</p>]]></content>
      
      
      <categories>
          
          <category> 矩阵分析 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>机器学习-1.概论</title>
      <link href="/2023/09/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-1-%E6%A6%82%E8%AE%BA/"/>
      <url>/2023/09/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-1-%E6%A6%82%E8%AE%BA/</url>
      
        <content type="html"><![CDATA[<h1 id="概论">概论</h1><h2 id="机器学习">机器学习</h2><p><strong>定义</strong><br>  机器学习(MachineLearning)是关于计算机基于数据构建模型并运用模型对数据进行预测与分析的一门学科。</p><p><strong>特点</strong><br>  机器学习的主要特点是:<br>  (1)机器学习以计算机及网络为<strong>平台</strong>，是建立在计算机网络上的。<br>  (2)机器学习以数据为<strong>研究对象</strong>，是数据驱动的学科。<br>  (3) 机器学习的<strong>目的</strong>是对数据进行预测与分析。<br>  (4)机器学习以<strong>方法</strong>为中心，机器学习方法构建模型并应用模型进行预测与分析。<br>  (5)机器学习是微积分、线性代数、概率论、统计学、信息论、数值计算、最优化及计算机科学等多个领域的交叉<strong>学科</strong>，并且在发展中逐步形成独自的理论体系与方法论。</p><p><strong>步骤</strong><br>  实现机器学习方法的步骤：<br>  <strong>(1) 得到一个有限的训练数据集合；</strong><br>  <strong>(2)确定包含所有可能的模型的假设空间，即学习模型的集合；</strong><br>  <strong>(3) 确定模型选择的准则，即学习的策略；</strong><br>  <strong>(4) 实现求解最优模型的算法，即学习的算法；</strong><br>  <strong>(5) 通过学习方法选择最优模型；</strong><br>  <strong>(6) 利用学习的最优模型对新数据进行预测与分析。</strong></p><h2 id="机器学习的分类">机器学习的分类</h2><p>  我们可以从多个方面对机器学习方法进行分类。</p><p>  (1) 基本分类</p><ul><li><strong>监督学习(Supervised Learning):</strong>从标注数据中学习预测模型的机器学习问题。标注数据表示输入到输出的对应关系，预测模型对给定的输入产生相应的输出。监督学习的本质是学习输入到输出的映射的统计规律。<ul><li><strong>监督学习的实例:</strong>线性回归、逻辑回归、决策树、支持向量机、神经网络等。<br></li></ul></li><li><strong>无监督学习(Unsupervised Learning):</strong>从无标注数据中学习预测模型的机器学习问题。无标注数据是自然得到的数据，预测模型表示数据的类别、转换或概率。无监督学习的本质是学习数据中的统计规律或潜在结构。<ul><li><strong>无监督学习的实例:</strong>聚类分析、主成分分析、高斯混合模型、自编码等。</li></ul></li><li><strong>强化学习(Reinforcement Learning):</strong>指智能系统在与环境的连续互动中学习最优行为策略的机器学习问题。假设智能系统与环境的互动基于马尔可夫决策过程(Markovdecisionprocess)，智能系统能观测到的是与环境互动得到的数据序列。强化学习的本质是学习最优的序贯决策。<ul><li><strong>强化学习的实例:</strong>Q-学习、深度Q网络、蒙特卡洛树搜索等。</li></ul></li></ul><p>  (2) 按模型分类</p><ul><li><strong>概率模型与非概率模型</strong><ul><li><strong>概率模型:</strong> 模型的形式为条件概率分布<span class="math inline">\(P(y\vert x)\)</span>.<ul><li><strong>概率模型的实例:</strong>决策树、朴素贝叶斯、隐马尔可夫模型、条件随机场、概率图模型等。</li></ul></li><li><strong>非概率模型:</strong> 模型的形式为决策函数<span class="math inline">\(y = f(x)\)</span>.<ul><li><strong>非概率模型的实例:</strong>感知机、支持向量机、AdaBoost、神经网络等。</li></ul></li></ul></li><li><strong>线性模型与非线性模型</strong><ul><li><strong>线性模型:</strong> 非概率模型中，决策函数<span class="math inline">\(y=f(x)\)</span>是线性函数的模型。<ul><li><strong>线性模型的实例:</strong>感知机、线性支持向量机、k近邻、潜在语义分析等。<br></li></ul></li><li><strong>非线性模型:</strong> 非概率模型中，决策函数<span class="math inline">\(y=f(x)\)</span>是非线性函数的模型。<ul><li><strong>非线性模型的实例:</strong>核函数支持向量机、AdaBoost、神经网络等。<br></li></ul></li></ul></li><li><strong>参数化模型与非参数化模型</strong><ul><li><strong>参数化模型:</strong>假设模型参数的维数是固定的，模型可以由有限维参数完全刻画。<ul><li><strong>参数化模型的实例:</strong>感知机、朴素贝叶斯、逻辑回归、高斯混合模型等。</li></ul></li><li><strong>非参数化模型:</strong>假设模型参数的维度不固定或者说是无穷大，随着训练数据量的增加而不断增大。<ul><li><strong>非参数化模型的实例:</strong>决策树、支持向量机、AdaBoost、k近邻等。</li></ul></li></ul></li></ul><p>  (3) 按算法分类</p><ul><li><strong>在线学习(Online Learning):</strong>指每次接受一个样本，进行预测，之后学习模型，并不断重复该操作的机器学习。<ul><li><strong>在线学习的实例:</strong>在线线性回归、在线逻辑回归、在线决策树、在线神经网络等。<br></li></ul></li><li><strong>批量学习(Batch Learning):</strong>一次性接受所有数据，学习模型，之后进行预测。<ul><li><strong>批量学习的实例:</strong>线性回归、逻辑回归、决策树、神经网络、支持向量机等。</li></ul></li></ul><p>  在监督学习方法又可以生成方法与判别方法：</p><ul><li><strong>生成方法:</strong> 原理上由数据学习联合概率分布<span class="math inline">\(P(X,Y)\)</span>，然后再求出条件概率分布<span class="math inline">\(P(Y \vert X)\)</span>作为预测的模型，即生成模型。<ul><li><strong>生成方法的特点:</strong> 生成方法可以还原出联合概率分布<span class="math inline">\(P(X,Y)\)</span>，而判别方法则不能；生成方法的学习收敛速度更快，即当样本容量增加时，学到的模型可以更块地收敛于真实模型；当存在隐变量时，仍可以用生成方法学习，此时判别方法就不能用；生成模型的输入和输出变量均为随机变量；生成方法所需的数据量较大。<br></li><li><strong>生成模型的实例:</strong>朴素贝叶斯模型、隐马尔可夫模型等。<br></li></ul></li><li><strong>判别方法:</strong> 由数据直接学习决策函数<span class="math inline">\(f(X)\)</span>或条件概率分布<span class="math inline">\(P(Y \vert X)\)</span>作为预测模型，即判别模型。<ul><li><strong>判别方法的特点:</strong>判别方法直接学习的是条件概率分布<span class="math inline">\(P(Y \vertX)\)</span>或决策函数<span class="math inline">\(f(X)\)</span>，直接面对预测，往往学习的准确率更高；由于直接学习<span class="math inline">\(P(Y \vert X)\)</span>和<span class="math inline">\(f(X)\)</span>，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题；判别方法不需要输入和输出变量均为随机变量；判别方法所需的样本量少于生成方法。<br></li><li><strong>判别模型的实例:</strong>感知机、Logistic回归模型、支持向量机、条件随机场等。</li></ul></li></ul><h3 id="基本分类的相关概念">基本分类的相关概念</h3><h4 id="监督学习">监督学习</h4><ul><li><strong>输入空间:</strong> 输入的所有可能取值的集合，记为<span class="math inline">\(\mathcal{X}\)</span>.<br></li><li><strong>输出空间:</strong> 输出的所有可能取值的集合，记为<span class="math inline">\(\mathcal{Y}\)</span>.<br></li><li><strong>实例:</strong> 每一个具体的输入，通常由特征向量表示：<br><span class="math display">\[x_i = \begin{bmatrix}  x_{i}^{(1)}, x_{i}^{(2)}, \dots, x_{i}^{(n)}\end{bmatrix}^T\]</span></li></ul><p>  其中<span class="math inline">\(x_{i}^{(j)}\)</span>表示第<span class="math inline">\(i\)</span>个实例的第<span class="math inline">\(j\)</span>个特征。</p><ul><li><strong>特征空间:</strong>所有特征向量存在的空间称为特征空间，记为<span class="math inline">\(\mathcal{F}\)</span>。<br></li><li><strong>训练集:</strong> 模型用于学习的数据集合，样本容量为<span class="math inline">\(N\)</span>的训练集记为：<br><span class="math display">\[T=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}\]</span></li></ul><p>  输入变量<span class="math inline">\(X\)</span>和输出变量<span class="math inline">\(Y\)</span>有不同的类型，可以是连续的，也可以是离散的。人们根据输入输出变量的不同类型，对预测任务给予不同的名称：</p><ul><li><strong>回归问题:</strong>输入变量与输出变量均为连续变量的预测问题。<br></li><li><strong>分类问题:</strong>输出变量为有限个离散变量的预测问题。<br></li><li><strong>标注问题:</strong>输入变量与输出变量均为变量序列的预测问题。</li></ul><p>  对于监督学习，主要就是研究输入到输出之间的统计规律，所以要有关于输入和输出和模型的基本假设。</p><ul><li><p><strong>监督学习的基本假设:</strong> 输入变量<span class="math inline">\(X\)</span>与输出变量<span class="math inline">\(Y\)</span>具有联合概率分布<span class="math inline">\(P(X,Y)\)</span>。训练数据与测试数据被看作是依联合概率分布<span class="math inline">\(P(X,Y)\)</span>独立同分布产生的。<br></p></li><li><p><strong>监督学习的目的:</strong>学习一个输入空间到输出空间的映射，这一映射以模型的形式表示。<br></p></li><li><p><strong>模型形式:</strong>监督学习的模型可以是概率模型或非概率模型，由条件概率分布<span class="math inline">\(P(Y\vert X)\)</span>或决策函数<span class="math inline">\(Y=f(X)\)</span>表示，随具体学习方法而定。对具体的输入实例进行相应的输出预测时写作<span class="math inline">\(P(y \vert x)\)</span>或决策函数<span class="math inline">\(y=f(x)\)</span>.<br></p></li><li><p><strong>假设空间:</strong> 所有可能的模型的集合，记为<span class="math inline">\(\mathcal{H}\)</span>.</p></li><li><p><strong>监督学习的过程:</strong>监督学习分为学习和预测两个过程，由学习系统和预测系统完成。在学习过程中，学习系统利用给定的训练数据集，通过学习(或训练)得到一个模型，表示为条件概率分布<span class="math inline">\(\hat{P}(Y \vert X)\)</span>或决策函数<span class="math inline">\(Y=\hat{f}(X)\)</span>。条件概率分布<span class="math inline">\(\hat{P}(Y \vert X)\)</span>或决策函数<span class="math inline">\(Y=\hat{f}(X)\)</span>描述输入与输出变量之间的映射关系。在预测过程中，预测系统对于给定的测试样本集中的输入<span class="math inline">\(X_{N+1}\)</span>，由模型<span class="math inline">\(\hat{y}_{N+1}=arg \space max \space \hat{P}(y\vert x_{N+1})\)</span>或<span class="math inline">\(\hat{y}_{N+1}=\hat{f}(x_{N+1})\)</span>给出相应的输出<span class="math inline">\(\hat{y}_{N+1}\)</span>。监督学习的过程可以用图1来描述。</p></li></ul><center><img src="https://s2.loli.net/2023/09/25/6FAu9srNYcwDSkh.jpg" width="60%" height="60%"><div data-align="center">Image1: 监督学习的过程</div></center><h4 id="无监督学习">无监督学习</h4><p>  无监督学习中的大部分概率与监督学习中的一致，不同点主要在于以下几个方面：</p><style>.center {  width: auto;  display: table;  margin-left: auto;  margin-right: auto;}</style><p align="center"><font face="黑体" size="2.">表1 监督学习与无监督学习的区别</font></p><div class="center"><table><thead><tr class="header"><th style="text-align: center;"></th><th style="text-align: center;">监督学习</th><th style="text-align: center;">无监督学习</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">数据</td><td style="text-align: center;">标注数据</td><td style="text-align: center;">无标注数据</td></tr><tr class="even"><td style="text-align: center;">输出</td><td style="text-align: center;">输出空间<span class="math inline">\(\mathcal{Y}\)</span></td><td style="text-align: center;">隐式结构空间<span class="math inline">\(\mathcal{Z}\)</span></td></tr><tr class="odd"><td style="text-align: center;">预测模型</td><td style="text-align: center;">对给定输入产生相应的输出</td><td style="text-align: center;">表示数据的类别、转换或概率</td></tr><tr class="even"><td style="text-align: center;">本质</td><td style="text-align: center;">学习输入到输出的映射的统计规律</td><td style="text-align: center;">学习数据中的统计规律或潜在结构</td></tr></tbody></table></div><ul><li><strong>无监督学习的过程:</strong>和监督学习类似，无监督学习由学习系统与预测系统构成。在学习过程中，学习系统从训练数据集学习，得到一个最优模型，表示为<span class="math inline">\(z=\hat{g}(x)\)</span>，条件概率分布<span class="math inline">\(\hat{P}(z\vert x)\)</span>或者条件概率分布<span class="math inline">\(\hat{P}(x \vertz)\)</span>。在预测过程中，预测系统对于给定的输入<span class="math inline">\(x_{N+1}\)</span>，由模型<span class="math inline">\(\hat{z}_{N+1}=\hat{g}(x_{N+1})\)</span>或<span class="math inline">\(\hat{z}_{N+1}=\arg\max\limits_{z} \hat{P}(z \vertx_{N+1})\)</span>给出相应的输出<span class="math inline">\(\hat{z}_{N+1}\)</span>，进行聚类或降维，或者由模型<span class="math inline">\(\hat{P}(x \vertz)\)</span>给出输入的概率<span class="math inline">\(\hat{P}(x_{N+1}\vertz_{N+1})\)</span>，进行概率估计。无监督学习的过程可以用图2来描述。</li></ul><center><img src="https://s2.loli.net/2023/09/27/itEGIlOMS9mWbhQ.jpg" width="60%" height="60%"><div data-align="center">Image2: 无监督学习的过程</div></center><h4 id="强化学习">强化学习</h4><p>  强化学习其实要强调一个互动，互动是指的是智能系统与环境之间的一个连续互动，通过这个互动，学习一个最优的行为策略。</p><ul><li><strong>强化学习的过程:</strong> 在每一步<span class="math inline">\(t\)</span>，智能系统从环境中观测到一个状态<span class="math inline">\((state)s_t\)</span>，与一个奖励<span class="math inline">\((reward)r_t\)</span>，采取一个动作<span class="math inline">\((action)a_t\)</span>。环境根据智能系统选择的动作，决定下一步<span class="math inline">\(t+1\)</span>的状态<span class="math inline">\(s_{t+1}\)</span>与奖励<span class="math inline">\(r_{t+1}\)</span>。要学习的策略表示为给定的状态下采取的动作。智能系统的目标不是短期奖励的最大化，而是长期累积奖励的最大化。强化学习过程中，智能系统不断试错<span class="math inline">\((trial \space\ and \spaceerror)\)</span>，以达到学习最优策略的目的。强化学习的过程可以用图3来描述。</li></ul><center><img src="https://s2.loli.net/2023/09/27/x4InRw56OMVubzt.jpg" width="60%" height="60%"><div data-align="center">Image3: 强化学习的过程</div></center><h2 id="机器学习方法的三要素">机器学习方法的三要素</h2><p>  机器学习方法都是由模型、策略和算法构成，即机器学习方法由三要素构成，可以简单地表示为：<br><span class="math display">\[方法(Method)=模型(Model)+策略(Strategy)+算法(Algorithm)\]</span></p><p>  下面主要介绍监督学习的三要素。</p><h3 id="监督学习的三要素">监督学习的三要素</h3><h4 id="模型">模型</h4><p>  对于监督学习，模型主要可以表示成两种形式，一种是条件概率形式<span class="math inline">\(P(y \vert x)\)</span>，另一种是决策函数的形式<span class="math inline">\(y=f(x)\)</span>。<br>  如果模型是由参数向量<span class="math inline">\(\theta\)</span>决定的，我们称所有可能的参数向量组成的空间为参数空间<span class="math inline">\(\Theta\)</span>，那么这个假设空间<span class="math inline">\(\mathcal{H}\)</span>就应该是由参数空间<span class="math inline">\(\Theta\)</span>决定的了。<br>  (1) 当用决策函数的形式表示模型时，定义：</p><ul><li><strong>决策函数:</strong> <span class="math inline">\(Y =f(X)\)</span></li><li><strong>假设空间:</strong> 决策函数的集合<span class="math inline">\(\mathcal{H} = \{f \vert Y=f(X) \}\)</span>.<br></li><li><strong>函数族:</strong> 假设空间<span class="math inline">\(\mathcal{H}\)</span>是由一个参数向量<span class="math inline">\(\theta\)</span>决定的函数族构成: <span class="math inline">\(\mathcal{H}=\{f \vert Y=f_{\theta}(X),\theta \in\mathbb{R}^{n}\}\)</span>.<br></li><li><strong>参数空间:</strong> <span class="math inline">\(\Theta =\{\theta \vert \theta \in \mathbb{R}^{n} \}\)</span>.</li></ul><p>  以线性回归问题为例，这个问题的模型可以表示为决策函数的形式。</p><ul><li><strong>决策函数:</strong> <span class="math inline">\(f(x) =w^{T}x+b, x \in \mathbb{R}^{n},w \in \mathbb{R}^{n}, b \in\mathbb{R}\)</span>.<br></li><li><strong>假设空间:</strong> <span class="math inline">\(\mathcal{H}=\{f \vert y = f_{\theta}(x),\theta=(w,b), \theta \in \mathbb{R}^{n+1}\}\)</span><br></li><li><strong>参数空间:</strong> <span class="math inline">\(\Theta =\{\theta \vert \theta = (w,b),\theta \in\mathbb{R}^{n+1}\}\)</span>.</li></ul><p>  (2) 当用条件概率的形式表示模型时，定义:</p><ul><li><strong>条件概率分布:</strong> <span class="math inline">\(P(Y \vertX)\)</span>.<br></li><li><strong>假设空间:</strong> 条件概率分布的集合<span class="math inline">\(\mathcal{H}=\{P \vert P(Y \vertX)\}\)</span>.<br></li><li><strong>分布族:</strong> 假设空间<span class="math inline">\(\mathcal{H}\)</span>是由一个参数向量<span class="math inline">\(\theta\)</span>决定的分布族构成: <span class="math inline">\(\mathcal{H}=\{P\vert P_{\theta}(Y\vert X), \theta\in \mathbb{R}^{n}\}\)</span>.<br></li><li><strong>参数空间:</strong> <span class="math inline">\(\Theta =\{\theta \vert \theta \in \mathbb{R}^{n}\}\)</span>.</li></ul><p>  以二项Logistic回归为例，这个问题的模型可以用条件概率分布的形式表示。</p><ul><li><strong>条件概率分布:</strong></li></ul><p><span class="math display">\[P(Y=1 \vert x) =\frac{exp(wx+b)}{1+exp(wx+b)}\]</span></p><p><span class="math display">\[P(Y=0 \vert x) =\frac{1}{1+exp(wx+b)}\]</span></p><p>  其中，<span class="math inline">\(x \in \mathbb{R}^{n}, Y \in\{0,1\}, w \in \mathbb{R}^{n}, b \in \mathbb{R}\)</span>.</p><ul><li><strong>假设空间:</strong> <span class="math inline">\(\mathcal{H} =\{P \vert P_{\theta}(Y\vert X), \theta = (w,b), \theta \in\mathbb{R}^{n+1} \}\)</span>.<br></li><li><strong>参数空间:</strong> <span class="math inline">\(\Theta =\{\theta \vert \theta = (w,b), \theta \in \mathbb{R}^{n+1}\}\)</span>.</li></ul><h4 id="策略">策略</h4><p>  有了模型的假设空间，机器学习接着需要考虑的是按照什么样的准则学习或者选择最优模型。机器学习的目标在于从假设空间中选取最优模型。所谓策略其实就是一种学习准则，用来选择最优模型的。想要选择模型，那么一定要知道如何度量模型的好坏。所以，这里先要引入几个概念。</p><ul><li><strong>损失函数(Loss function):</strong>度量模型一次预测的好坏，记作<span class="math inline">\(L(Y,f(X))\)</span>.几种常用的损失函数如下：</li></ul><p>  (1)<strong>0-1损失函数:</strong> 主要用于分类问题。</p><p><span class="math display">\[L(Y,f(X))= \left \{\begin{array}{rcl}1, &amp; {Y \neq f(X)}\\0,&amp; {Y = f(X)}\\\end{array} \right.\]</span></p><p>  (2)<strong>平方损失函数:</strong> 主要用于回归问题。</p><p><span class="math display">\[L(Y,f(X))=(Y-f(X))^2\]</span></p><p>  (3)<strong>绝对损失函数:</strong> 主要用于回归问题。</p><p><span class="math display">\[L(Y,f(X))= \vert Y-f(X)\vert\]</span></p><p>  (4) <strong>对数损失函数:</strong> 主要针对概率模型。</p><p><span class="math display">\[L(Y,P(Y \vert X))= - \log P(Y \vertX)\]</span></p><p>  损失函数越小说明模型的预测值与真实值越接近，模型的预测效果越好。但仅衡量一次预测的好坏是不够的，我们更想要知道模型在整个联合分布上的表现，这时就需要风险函数。</p><ul><li><strong>期望风险(Expexted risk):</strong>度量平均意义下模型预测的好坏。</li></ul><p><span class="math display">\[R_{exp}(f)=E_{P}[L(Y,f(X))]=\int_{\mathcal{X}\times \mathcal{Y}}L(y,f(x))P(x,y)dxdy\]</span></p><p>  理论上我们需要计算期望风险来评估模型的好坏，但在实际中联合概率分布<span class="math inline">\(P(X,Y)\)</span>一般是未知的，期望风险无法直接计算，因此我们引入经验风险用于估计风险函数.</p><ul><li><strong>经验风险(Empirical risk):</strong>模型关于数据集的平均损失.</li></ul><p><span class="math display">\[R_{emp}(f)=\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))\]</span></p><p>  其中，训练集<span class="math inline">\(T=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}\)</span>.</p><p>  期望风险<span class="math inline">\(R_{exp}(f)\)</span>是模型关于联合分布的期望损失，经验风险<span class="math inline">\(R_{emp}(f)\)</span>是模型关于训练集的平均损失。我们知道训练集中的样本<span class="math inline">\((x_i,y_i)\)</span>都是独立同分布的，其损失函数的期望<span class="math inline">\(L(Y,f(X))\)</span>存在，由大数定律可知，当<span class="math inline">\(n \rightarrow \infty\)</span>时，有：</p><p><span class="math display">\[R_{emp}(f)=\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))\rightarrow R_{exp}(f)=E_{P}[L(Y,f(X))]\]</span></p><p>  可是在现实生活中，样本容量<span class="math inline">\(N\)</span>一般是有限的，有的时候甚至会很小。因此，仅仅用经验风险来估计期望风险效果并不理想，需要对其进行一定的矫正。这里就涉及到监督学习的两个基本策略，一个是经验风险最小化策略，一个是结构风险最小化策略。</p><ul><li><strong>经验风险最小化策略:</strong> 求解使得经验风险<span class="math inline">\(R_{emp}(f)\)</span>最小的模型<span class="math inline">\(f^{*}\)</span>，模型<span class="math inline">\(f^{*}\)</span>即为最优的模型。根据这一策略，求解最优模型的问题就转化为求解最优化问题：</li></ul><p><span class="math display">\[\min_{f \in \mathcal{H}}\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))\]</span></p><p>  当样本容量足够大时，经验风险最小化能保证有很好的学习效果。但是，当样本容量很小时，经验风险最小化学习的效果就未必很好，会产生“过拟合”(over-fitting)现象。<br>  结构风险最小化策略是为了防止过拟合现象而提出的策略。结构风险在经验风险上加上表示模型复杂度的正则化项。</p><ul><li><strong>结构风险(Structural risk):</strong></li></ul><p><span class="math display">\[R_{srm}(f)=\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))+\lambdaJ(f)\]</span></p><p>  其中，正则化项<span class="math inline">\(J(f)\)</span>为模型复杂度，是定义在假设空间<span class="math inline">\(\mathcal{H}\)</span>上的泛函。模型<span class="math inline">\(f\)</span>越复杂，正则化项<span class="math inline">\(J(f)\)</span>就越大，反之，模型<span class="math inline">\(f\)</span>越简单，正则化项<span class="math inline">\(J(f)\)</span>就越小。<span class="math inline">\(\lambda &gt;0\)</span>是用于权衡经验风险与模型复杂度的系数。</p><ul><li><strong>结构风险最小化策略:</strong> 求解使得结构风险<span class="math inline">\(R_{srm}(f)\)</span>最小的模型<span class="math inline">\(f^{*}\)</span>，模型<span class="math inline">\(f^{*}\)</span>即为最优模型。这一策略同样可以转化为求解最优化问题：</li></ul><p><span class="math display">\[\min_{f \in \mathcal{H}}\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))+\lambda J(f)\]</span></p><p>  关于监督学习的策略，追根究底，就是选取一个目标函数，可以是经验风险，或者是结构风险，然后通过优化这个目标函数，达到学习模型的目的。</p><h4 id="算法">算法</h4><p>  在假设空间里面，根据策略去选择最优模型，需要一个具体的操作方案，操作方案也就是算法，是用来求解最优模型的。如果这个最优模型存在显式解析解，那么简单了，直接把这个结果写出来即可。但是往往这个显式解是不存在的，所以需要一定的数值计算方法，比如梯度下降法。</p><h2 id="模型评估与模型选择">模型评估与模型选择</h2><h3 id="模型评估">模型评估</h3><p>  在通过策略训练完模型后，我们需要评估得到的模型在已知数据和未知数据上的预测效果。我们称模型在训练集上的预测误差为训练误差，在测试集上的误差为测试误差。</p><h4 id="训练误差">训练误差</h4><p>  假设我们定义完假设空间<span class="math inline">\(\mathcal{H}\)</span>后，通过策略与算法学习到的模型为:<br><span class="math display">\[Y = \hat{f(X)}\]</span></p><p>  训练数据集为：</p><p><span class="math display">\[T_{train}=\{(x_1,y_1),(x_2,y_2),\dots,(x_{N_{1}}.y_{N_{1}})\}\]</span></p><p>  其中<span class="math inline">\(N_1\)</span>表示训练样本容量。由前文的概念可知，我们可以使用模型在测试集上的经验风险<span class="math inline">\(R_{emp}\)</span>来表示训练误差：</p><p><span class="math display">\[e_{train}(\hat{f})=\frac{1}{N_1}\sum_{i=1}^{N_1}L(y_i,\hat{f}(x_i))\]</span></p><p>  训练误差<span class="math inline">\(e_{train}\)</span>表示模型<span class="math inline">\(\hat{f}\)</span>在训练集上预测效果的好坏，训练误差越小，说明模型在训练集上的预测效果越好。然而，仅仅获得模型对已知数据的预测效果并不足以能够正确评价模型性能，我们还需要评估模型对未知数据的预测效果，这就需要计算模型在测试集上的预测误差，即测试误差。</p><h4 id="测试误差">测试误差</h4><p>  若测试数据集为：</p><p><span class="math display">\[T_{test}=\{(x_1,y_1),(x_2,y_2),\dots,(x_{N_{2}}.y_{N_{2}})\}\]</span></p><p>  其中<span class="math inline">\(N_2\)</span>表示测试样本容量。同样使用模型在测试集上的经验风险<span class="math inline">\(R_{emp}\)</span>来表示测试误差：</p><p><span class="math display">\[e_{test}(\hat{f})=\frac{1}{N_2}\sum_{i=1}^{N_2}L(y_i,\hat{f}(x_i))\]</span></p><p>  测试误差<span class="math inline">\(e_{test}\)</span>表示模型<span class="math inline">\(\hat{f}\)</span>在测试集上的预测效果的好坏，测试误差越小，说明模型在测试集上的预测效果越好。训练误差反映了学习方法对未知的测试数据集的预测能力，是评估模型性能的重要指标。通常，我们将学习方法对未知数据的预测能力称为<strong>泛化能力(Generalizationability)</strong>。</p><h3 id="模型选择">模型选择</h3><p>  在得到模型的训练误差和测试误差后，我们便需要据此评估模型的性能，选择对实际问题性能最出众的模型。最理想的情况是，我们得到的模型在测试集和训练集上的误差都较小，这样我们可以认为模型对于该问题的效果是较好的。但在实际情况中，我们经常会发现模型在训练集上的误差较小，而在测试集上的误差较大，这表明模型的泛化能力交叉，我们称这样的现象为<strong>过拟合(Over-fitting)</strong>。</p><h4 id="过拟合">过拟合</h4><p>  过拟合是指学习时选择的模型所包含的参数过多，模型复杂度过高，以至于出现这一模型对已知数据预测得很好，但对未知数据预测得很差的现象。<br>  模型选择的目的实际上就是避免过拟合现象并提高模型的泛化能力。这里我们通过一个多项式函数拟合的问题来说明过拟合与模型选择。</p><h4 id="关于模型选择的实例多项式拟合问题">关于模型选择的实例——多项式拟合问题</h4><p>  假设输入空间<span class="math inline">\(\mathcal{X}=(0,1)\)</span>到输出空间<span class="math inline">\(\mathcal{Y}=[-1,1]\)</span>的真实映射为:</p><p><span class="math display">\[Y = \sin(2\pi X)\]</span></p><p>  由于误差的影响，抽取的样本为真实值加上一个随机误差，我们假设这个随机误差满足均值为0，方差为0.1的正态分布：</p><p><span class="math display">\[y=\sin(2 \pi x)+\varepsilon, \varepsilon\sim N(0,0.1)\]</span></p><p>  我们的样本数据集为：</p><p><span class="math display">\[T=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}\]</span></p><p>  其中，样本容量<span class="math inline">\(N=30\)</span>.将样本数据集划分为样本容量为20的训练数据集与样本容量为10的测试数据集。以下给出了生成数成以及测试数据的散点图的代码。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> math<br>%matplotlib inline<br><br><span class="hljs-comment"># Data Generation</span><br>np.random.seed(<span class="hljs-number">1314</span>) <span class="hljs-comment"># set random seed</span><br>x = np.random.random((<span class="hljs-number">30</span>,)) <span class="hljs-comment"># x belong to (0,1)</span><br>r = np.random.normal(<span class="hljs-number">0</span>,<span class="hljs-number">0.1</span>,<span class="hljs-number">30</span>) <span class="hljs-comment"># r ~ N(0,0.1)</span><br>y = np.sin(<span class="hljs-number">2</span>*(math.pi)*x)+r<br>data = {<span class="hljs-string">"y"</span>:y,<span class="hljs-string">"x"</span>:x}<br>Data = pd.DataFrame(data,columns=[<span class="hljs-string">"y"</span>,<span class="hljs-string">"x"</span>]) <span class="hljs-comment"># data set</span><br>train_data = Data[:<span class="hljs-number">20</span>] <span class="hljs-comment"># Train data</span><br>test_data = Data[<span class="hljs-number">20</span>:] <span class="hljs-comment"># Test data</span><br><br><span class="hljs-comment"># Tradin data scatter plot</span><br>plt.scatter(train_data[<span class="hljs-string">"x"</span>],train_data[<span class="hljs-string">"y"</span>],marker=<span class="hljs-string">"o"</span>,c=<span class="hljs-string">"black"</span>,label=<span class="hljs-string">"noise"</span>)<br><span class="hljs-comment"># Real line</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">real_func</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> np.sin(<span class="hljs-number">2</span>*np.pi*x)<br>x_points = np.linspace(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1000</span>)<br>plt.plot(x_points,real_func(x_points),label=<span class="hljs-string">"real curve"</span>)<br>plt.xlabel(<span class="hljs-string">"x"</span>)<br>plt.ylabel(<span class="hljs-string">"y"</span>)<br>plt.legend()<br></code></pre></td></tr></tbody></table></figure>  得到的散点图如下图4所示：<br><center><img src="https://s2.loli.net/2023/09/30/TlHEjSPh5BRZOzN.png" width="60%" height="60%"><div data-align="center">Image4: 训练数据散点图</div></center><p>  图4中的黑色散点代表训练数据，蓝色曲线为输入空间到输出空间的真实映射。下面，我们便要建立模型去拟合数据。我们使用多项式函数进行拟合，模型形式为：</p><p><span class="math display">\[f_{M}(x,w)=w_0+w_{1}x+w_{2}x^2+\dots+w_{M}x^{M}=\sum_{j=1}^{M}w_{j}x_{j}= w^{T}x\]</span></p><p>  其中，<span class="math inline">\(x=\begin{bmatrix}  1,x,x^2,\dots,x^M\end{bmatrix}^{T}\)</span>为输入变量的不同次方，且<span class="math inline">\(x \in (0,1)\)</span>，<span class="math inline">\(w=\begin{bmatrix}  w_0,w_1,\dots,w_M\end{bmatrix}^{T}\)</span>为多项式的参数向量。则该问题的假设空间为：</p><p><span class="math display">\[\mathcal{H}=\{f \verty=f_{M}(x,w)\}\]</span></p><p>  由于多项式函数<span class="math inline">\(f_{M}(x,w)\)</span>是由参数向量<span class="math inline">\(w\)</span>决定的，因此假设空间<span class="math inline">\(\mathcal{H}\)</span>等价于想要的参数空间<span class="math inline">\(\Theta\)</span>：</p><p><span class="math display">\[\Theta = \{w \vert w \in\mathbb{R}^{M+1} \}\]</span></p><p>  在拟合的时候，我们采取使经验风险最小化的策略。由于这个问题为回归问题，经验风险中的损失函数可以选用平方损失函数，即预测值与真实值平方差。此时模型的经验风险可以表示为训练误差：</p><p><span class="math display">\[L(w)=\frac{1}{2}\sum_{i=1}^{N_1}(f_{M}(x_i,w)-y_i)^2=\frac{1}{2}\sum_{i=1}^{N_1}(w^{T}x_i-y_i)^2=\frac{1}{2}(Xw-y)^{T}(Xw-y)\]</span></p><p><span class="math display">\[X=\begin{bmatrix}    1 &amp; x_1 &amp; x_1^2 &amp; \dots &amp; x_1^M \\    1 &amp; x_2 &amp; x_2^2 &amp; \dots &amp; x_2^M \\    \vdots &amp; \vdots &amp; \vdots &amp; &amp; \vdots \\    1 &amp; x_{N_1} &amp; x_{N_1}^2 &amp; \dots &amp; x_{N_1}^M \\\end{bmatrix},w = \begin{bmatrix}    w_0 \\    w_1 \\    \vdots \\    w_M \\\end{bmatrix}, y = \begin{bmatrix}    y_1 \\    y_2 \\    \vdots \\    y_{N_1} \\\end{bmatrix}\]</span></p><p>  采取使得经验风险最小化的策略，则从假设空间中求解最优模型的问题就转化成了一个最优化问题：</p><p><span class="math display">\[\min_{w} \frac{1}{2}(Xw-y)^{T}(Xw-y)\]</span></p><p>  要求<span class="math inline">\(L(w)\)</span>的最小值点，我们需要对<span class="math inline">\(w\)</span>求偏导，并令偏导为<span class="math inline">\(0\)</span>：</p><p><span class="math display">\[\frac{\partial L(w)}{\partialw}=X^TXw-X^Ty=0 \]</span></p><p><span class="math display">\[\Rightarrow \hat{w} =(X^TX)^{-1}X^Ty\]</span></p><p>  其中<span class="math inline">\(N_1 = 20\)</span>。<br>  在估计完参数后，我们便得到了该问题的预测模型：</p><p><span class="math display">\[\hat{f}_{M}(x,\hat{w})=\hat{w}^Tx\]</span></p><p>  对于测试集中的实例<span class="math inline">\(x_i\)</span>，可以利用模型计算<span class="math inline">\(y_i\)</span>的预测值：</p><p><span class="math display">\[\hat{y_i} =\hat{f}_M(x_i,\hat{w})=\hat{w}^Tx_i=y^TX(X^TX)^{-1}x_i\]</span></p><p>  要评估模型的性能我们可以计算模型在训练集与测试机上的误差：</p><p><span class="math display">\[e_{train}(\hat{f}_M)=\frac{1}{N_1}\sum_{i=1}^{N_1}L(y_i,\hat{f}_M(x_i,\hat{w}))\]</span></p><p><span class="math display">\[e_{test}(\hat{f}_M)=\frac{1}{N_2}\sum_{i=1}^{N_2}L(y_i,\hat{f}_M(x_i,\hat{w}))\]</span></p><p>  其中，<span class="math inline">\(N_2=10\)</span>，改变多项式的次数<span class="math inline">\(M\)</span>可以得到不同的模型形式，进而得到不同的<span class="math inline">\(e_{train}\)</span>和<span class="math inline">\(e_{test}\)</span>，我们可以通过比较这两个指标选择最优的多项式次数。下面是多项式回归的代码实现。</p><p>  通过以下这段代码中的<code>ployreg()</code>函数，我们可以计算多项式回归的参数向量：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Ploy-regression</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">ployreg</span>(<span class="hljs-params">M</span>):  <br>    <span class="hljs-keyword">global</span> train_data<br>    x = train_data[<span class="hljs-string">"x"</span>].values<br>    X = np.ones((<span class="hljs-built_in">len</span>(x),M+<span class="hljs-number">1</span>))  <br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(M):  <br>        X[:,i+<span class="hljs-number">1</span>]=np.power(x,i+<span class="hljs-number">1</span>)<br>    y = train_data[<span class="hljs-string">"y"</span>].values<br>    X_T = np.transpose(X)<br>    X_TX = np.dot(X_T,X)<br>    w = np.dot(np.linalg.inv(X_TX),np.dot(X_T,y))<br>    <span class="hljs-keyword">return</span> w.<span class="hljs-built_in">round</span>(<span class="hljs-number">4</span>)<br></code></pre></td></tr></tbody></table></figure><p>  当我们设置多项式的次数<span class="math inline">\(M=2\)</span>时，得到的最优多项式函数为：</p><p><span class="math display">\[\hat{f}(x)=0.6325-1.0391x-0.5826x^2\]</span></p><p>  在得到最优模型后，我们可以计算模型在训练集与测试集上的误差。通过设置不同的多项式次数<span class="math inline">\(M\)</span>，我们可以比较不同模型的性能，从而选择最优的模型形式。以下代码中的<code>calerror()</code>函数便是用于计算模型在训练集与测试集上的误差。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># calculate error</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">calerror</span>(<span class="hljs-params">M</span>):<br>    <span class="hljs-keyword">global</span> train_data,test_data<br>    error = {}<br>    w = ployreg(M)<br>    <span class="hljs-comment"># train error</span><br>    x = train_data[<span class="hljs-string">"x"</span>].values<br>    X = np.ones((<span class="hljs-built_in">len</span>(x),M+<span class="hljs-number">1</span>))  <br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(M):  <br>        X[:,i+<span class="hljs-number">1</span>]=np.power(x,i+<span class="hljs-number">1</span>)<br>    y = train_data[<span class="hljs-string">"y"</span>].values<br>    e = np.dot(X,w)-y<br>    train_error = (np.dot(np.transpose(e),e))/<span class="hljs-built_in">len</span>(x)<br>    <span class="hljs-comment"># test error</span><br>    x = test_data[<span class="hljs-string">"x"</span>].values<br>    X = np.ones((<span class="hljs-built_in">len</span>(x),M+<span class="hljs-number">1</span>))  <br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(M):  <br>        X[:,i+<span class="hljs-number">1</span>]=np.power(x,i+<span class="hljs-number">1</span>)<br>    y = test_data[<span class="hljs-string">"y"</span>].values<br>    e = np.dot(X,w)-y<br>    test_error = (np.dot(np.transpose(e),e))/<span class="hljs-built_in">len</span>(x)<br>    error[<span class="hljs-string">"train error"</span>] = train_error.<span class="hljs-built_in">round</span>(<span class="hljs-number">4</span>)<br>    error[<span class="hljs-string">"test error"</span>] = test_error.<span class="hljs-built_in">round</span>(<span class="hljs-number">4</span>)<br>    <span class="hljs-keyword">return</span> error<br></code></pre></td></tr></tbody></table></figure><p>  设置多项式次数为<span class="math inline">\(M \in[0,9]\)</span>，我们可以得到下表2所示的计算结果：</p><style>.center {  width: auto;  display: table;  margin-left: auto;  margin-right: auto;}</style><p align="center"><font face="黑体" size="2.">表2 模型误差</font></p><div class="center"><table><thead><tr class="header"><th style="text-align: center;">M</th><th style="text-align: center;">Train error</th><th style="text-align: center;">Test error</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">0</td><td style="text-align: center;">0.413</td><td style="text-align: center;">0.562</td></tr><tr class="even"><td style="text-align: center;">1</td><td style="text-align: center;">0.1392</td><td style="text-align: center;">0.2668</td></tr><tr class="odd"><td style="text-align: center;">2</td><td style="text-align: center;">0.1372</td><td style="text-align: center;">0.2398</td></tr><tr class="even"><td style="text-align: center;">3</td><td style="text-align: center;">0.0144</td><td style="text-align: center;">0.0182</td></tr><tr class="odd"><td style="text-align: center;">4</td><td style="text-align: center;">0.0137</td><td style="text-align: center;">0.0151</td></tr><tr class="even"><td style="text-align: center;">5</td><td style="text-align: center;">0.0098</td><td style="text-align: center;">0.0065</td></tr><tr class="odd"><td style="text-align: center;">6</td><td style="text-align: center;">0.0098</td><td style="text-align: center;">0.0065</td></tr><tr class="even"><td style="text-align: center;">7</td><td style="text-align: center;">0.0087</td><td style="text-align: center;">0.0074</td></tr><tr class="odd"><td style="text-align: center;">8</td><td style="text-align: center;">0.0084</td><td style="text-align: center;">0.0121</td></tr><tr class="even"><td style="text-align: center;">9</td><td style="text-align: center;">0.0049</td><td style="text-align: center;">0.0388</td></tr></tbody></table></div><p>  为了更直观的观察模型误差的变化，我们可以利用上表的数据绘制误差曲线图。通过以下代码中的<code>errorplot()</code>函数来绘制图像：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># learning curve</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">errorplot</span>():<br>    train_error = []<br>    test_error = []<br>    <span class="hljs-keyword">for</span> M <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>        error = calerror(M)<br>        train_error.append(error[<span class="hljs-string">'train error'</span>])<br>        test_error.append(error[<span class="hljs-string">'test error'</span>])<br>    M = <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>)<br>    <span class="hljs-comment"># train error curve</span><br>    plt.plot(M,train_error,c=<span class="hljs-string">"blue"</span>,label=<span class="hljs-string">"train error"</span>)<br>    <span class="hljs-comment"># test error curve</span><br>    plt.plot(M,test_error,c=<span class="hljs-string">"green"</span>,label=<span class="hljs-string">"test error"</span>)<br>    plt.xlabel(<span class="hljs-string">"M"</span>)<br>    plt.ylabel(<span class="hljs-string">"error"</span>)<br>    plt.legend()<br></code></pre></td></tr></tbody></table></figure><p>  误差图如下图5所示：</p><center><img src="https://s2.loli.net/2023/10/01/yCiIKm39GYcWXFO.png" width="60%" height="60%"><div data-align="center">Image5: 模型误差</div></center><p>  结合表1和图5，我们可以发现随着多项式次数<span class="math inline">\(M\)</span>的增加，训练误差是逐渐减小的，而测试误差会先减小后增大。我们知道模型的复杂度随着<span class="math inline">\(M\)</span>的增加而增加，实际上，当<span class="math inline">\(M\)</span>增加到7之后，虽然训练误差仍在下降，但测试误差不降反升，此时模型已经出现了过拟合现象，模型的泛化能力下降。当<span class="math inline">\(M=5\)</span>或<span class="math inline">\(M=6\)</span>时，模型的训练误差与测试误差均达到较低水平，此时模型达到最优。在同样的误差下，我们应当选择复杂度较小的模型，以降低预测时的计算量，因此我们选择<span class="math inline">\(M=5\)</span>时的模型作为该问题的最优模型：</p><p><span class="math display">\[\hat{f}(x)=0.0320+4.6026x+15.8081x^2-107.3805x^3+142.5954x^4-55.7287x^5\]</span></p><p>  通过以下代码中的<code>regcurve()</code>函数我们可以画出相应的多项式函数的曲线：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># draw regression curve</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">regcurve</span>(<span class="hljs-params">M</span>):<br>    <span class="hljs-keyword">global</span> train_data<br>    <span class="hljs-comment"># Tradin data scatter plot</span><br>    plt.scatter(train_data[<span class="hljs-string">"x"</span>],train_data[<span class="hljs-string">"y"</span>],marker=<span class="hljs-string">"o"</span>,c=<span class="hljs-string">"black"</span>,label=<span class="hljs-string">"train data"</span>)<br>    <span class="hljs-comment"># real line</span><br>    x_points = np.linspace(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1000</span>)<br>    plt.plot(x_points,real_func(x_points),label=<span class="hljs-string">"real curve"</span>)<br>    <span class="hljs-comment"># regression curve</span><br>    w = ployreg(M)<br>    X_points = np.ones((<span class="hljs-built_in">len</span>(x_points),M+<span class="hljs-number">1</span>))<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(M):<br>        X_points[:,i+<span class="hljs-number">1</span>] = np.power(x_points,i+<span class="hljs-number">1</span>)<br>    y_points = np.dot(X_points,w)<br>    plt.plot(x_points,y_points,c=<span class="hljs-string">"red"</span>,label=<span class="hljs-string">"regression curve"</span>)<br>    plt.xlabel(<span class="hljs-string">"x"</span>)<br>    plt.ylabel(<span class="hljs-string">"y"</span>)<br>    plt.title(<span class="hljs-string">f"M=<span class="hljs-subst">{M}</span>"</span>)<br>    plt.legend()<br></code></pre></td></tr></tbody></table></figure><p>  画出<span class="math inline">\(M=0,1,5,9\)</span>时的拟合曲线的图像：</p><center><img src="https://s2.loli.net/2023/10/01/3zGbMkHDVx75t91.png" width="80%" height="80%"><div data-align="center">Image6: 拟合曲线</div></center><p>  从图6可以看出，当<span class="math inline">\(M=0,1\)</span>时，模型过于简单，与真实曲线相差很大，模型误差也可以看出此时模型在训练集与测试集上的误差均较大，我们称模型出现了<strong>欠拟合</strong>现象。当<span class="math inline">\(M=5\)</span>时，拟合曲线与真实曲线几乎一致，且模型在训练集上的误差也很小，模型效果很好。当<span class="math inline">\(M=9\)</span>时，模型几乎能够预测所有的训练数据，模型的训练误差非常小，但拟合曲线与真实曲线相差较大，模型在测试集上的误差较大，泛化能力较差，此时模型出现了<strong>过拟合</strong>现象。</p><h2 id="正则化与交叉验证">正则化与交叉验证</h2><h3 id="正则化">正则化</h3><p>  上文中我们介绍了进行模型选择时主要评估模型在已知数据与未知数据上的预测能力，即训练误差与测试误差。在进行模型选择时，我们需要综合考虑这两个指标，以避免过拟合与欠拟合现象。<br>  为了综合考虑这两个指标，模型选择的一种经典方法是<strong>正则化(regularization)</strong>。正则化是结构风险最小化策略的实现，是在经验风险上加上一个正则化项(regularizer)或罚项(penaltyterm)。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值越大。比如，正则化项可以是模型参数向量的范数。<br>  正则化一般具有如下形式：</p><p><span class="math display">\[\min_{f \in \mathcal{H}}\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))+\lambda J(f)\]</span></p><p>  其中，第1项是经验风险，第2项是正则化项，<span class="math inline">\(\lambda \geq0\)</span>为调整两者关系的系数。<br>  当模型处于欠拟合状态时，虽然模型复杂度较低，正则化值较低，但模型在训练集上的误差会较大，即经验风险较大；当模型处于过拟合状态时，虽然模型在训练集上的误差会较小，即经验风险较小，但模型复杂度会较高，即正则化值较大。正则化的作用是选择经验风险与模型复杂度同时较小的模型，通过优化正则化表达式，能同时减弱模型的欠拟合与过拟合现象，找到最优模型所处的中间状态。<br>  正则化项可以取不同的形式。在回归问题中，损失函数是平方损失，正则化项可以是参数向量的<span class="math inline">\(L_2\)</span>范数，以减轻模型的过拟合现象：</p><p><span class="math display">\[L(w)=\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))+\frac{\lambda}{2}\Vert w \Vert_{2}^{2}\]</span></p><p>  正则化项也可以是参数向量的<span class="math inline">\(L_1\)</span>范数，以得到一个较为稀疏的参数向量：</p><p><span class="math display">\[L(w)=\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))+\lambda \Vert w \Vert_{1}\]</span></p><p><strong>注: 奥卡姆剃刀原理</strong><br>  正则化符合奥卡姆剃刀原理。奥卡姆剃刀原理应用于模型选择时便为以下想法：在所有可能选择的模型中，能够很好解释已知数据并且十分简单的模型才是最好的模型。</p><h3 id="交叉验证">交叉验证</h3><p>  另一种常用的模型选择方法是交叉验证(crossvalidation).如果给定的样本数据充足，进行模型选择的一种简单方法是随机地将数据集切分成三部分，分别为<strong>训练集、验证集、测试集</strong>。训练集用于训练模型，验证集用于模型的选择，而测试集用于最终对学习方法的评估。我们最终选择对验证集有最小预测误差的模型。<br>  但是，在许多实际应用中数据是不充足的，为了选择好的模型，可以采用交叉验证方法。<strong>交叉验证的基本想法是重复地使用数据；把给定的数据进行切分，将切分的数据组合为训练集与测试集，在此基础上反复地进行训练、测试以及模型选择。</strong><br>  简单介绍几种主要的交叉验证的方法：</p><ul><li><strong>简单交叉验证:</strong>首先随机地将已给数据分为两部分，一部分作为训练集，另一部分作为测试集(例如，70%的数据为训练集，30%的数据为测试集)；然后用训练集在各种条件下(例如，不同的参数个数)训练模型，从而得到不同的模型；在测试集上评价各个模型的测试误差，选出测试误差最小的模型。<br></li><li><strong>S折交叉验证:</strong>首先随机地将已给数据切分为S个互不相交、大小相同的子集；然后利用S-1个子集的数据训练模型，利用余下的子集测试模型；将这一过程对可能的S种选择重复进行；最后选出在S次评测中平均测试误差最小的模型。<br></li><li><strong>留一交叉验证:</strong>令S折交叉验证中的S=N，称为留一交叉验证，往往在数据缺乏的情况下使用。这里，N是给定数据集的容量。</li></ul><h2 id="泛化能力">泛化能力</h2><p>  学习方法发泛化能力是指由该方法学习到的模型对未知数据的预测能力。通常，测试数据集用以评价训练所得模型的泛化能力。由于测试数据集包含的样本有限，仅仅通过测试数据集去评价泛化能力，有时并不可靠，此时需要我们从机器学习理论出发，对模型的泛化能力进行评价。</p><h3 id="泛化误差">泛化误差</h3><p>  如果学习到的模型是<span class="math inline">\(\hat{f}\)</span>，那么用这个模型对未知数据预测的期望风险即为泛化误差(generalizationability)：</p><p><span class="math display">\[R_{exp}(\hat{f})=E_{P}[L(Y,\hat{f}(X))]=\int_{\mathcal{X}\times \mathcal{Y}}L(y,\hat{f}(x))P(x,y)dxdy\]</span></p><p>  计算测试误差所使用的样本为测试集，而泛化误差则是基于整个样本空间的。所以泛化误差是度量对未知数据预测能力的理论值，测试误差则是经验值。如果一种学习方法的模型比另一种方法所学习的模型具有更小的泛化误差，那么这种方法就更有效。</p><h3 id="泛化误差上界">泛化误差上界</h3><p>  学习方法的泛化能力分析往往是通过研究泛化误差的概率上界进行的，简称泛化误差上界(generalizationerror bound)。泛化误差上界通常具有以下性质：</p><ul><li><strong>泛化误差上界是样本容量<span class="math inline">\(N\)</span>的函数，当样本容量<span class="math inline">\(N\)</span>增加时，泛化误差上界趋于0</strong>；</li><li><strong>泛化误差上界是假设空间容量(capacity)的函数，假设空间容量越大，模型就越难学，泛化误差上界越大</strong>。</li></ul><p>  第一条性质，可以从泛化误差的概念出发辅助理解。泛化误差定义为一个期望值，那么经验值就是以全样本空间作为测试集计算所得的测试误差，它表示为一个平均值。在平均值中，样本量位于分母的位置，那么随着样本量的增加，平均值则趋于零。<br>  第二条性质，泛化误差上界是假设空间容量的函数，不同的模型，对应不同的泛化误差上界。假设空间是所有可能的模型的集合，那么假设空间容量越大，所有可能的模型类型就会越多，模型也就愈加难以学习，与之对应的泛化误差上界就会越大。<br>  下面给出一个简单的泛化误差上界的例子：二分类问题的泛化误差上界。</p><h4 id="二分类问题的泛化误差上界">二分类问题的泛化误差上界</h4><p>  已知二分类问题的数据集为<span class="math inline">\(T\)</span>：</p><p><span class="math display">\[T=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}\]</span></p><p>  其中，<span class="math inline">\(N\)</span>为样本容量。假设数据集<span class="math inline">\(T\)</span>是从联合概率分布<span class="math inline">\(P(X,Y)\)</span>独立同分布产生的，<span class="math inline">\(X \in \mathbb{R}^N, Y \in\{-1,1\}\)</span>。模型的假设空间<span class="math inline">\(\mathcal{H}\)</span>是函数的集合：</p><p><span class="math display">\[\mathcal{H}=\{f_1,f_2,\dots,f_d\},f_i:\mathbb{R}^N \rightarrow \{-1,1\}\]</span></p><p>  由于是分类问题，我们可以设置损失函数为0-1损失函数:</p><p><span class="math display">\[L(Y,f(X))= \left \{\begin{array}{rcl}1, &amp; {Y \neq f(X)}\\0,&amp; {Y = f(X)}\\\end{array} \right.\]</span></p><p>  设<span class="math inline">\(f\)</span>是从假设空间<span class="math inline">\(\mathcal{H}\)</span>中选取的函数，则<span class="math inline">\(f\)</span>的期望风险<span class="math inline">\(R(f)\)</span>与经验风险<span class="math inline">\(\hat{R}(f)\)</span>分别为：</p><p><span class="math display">\[R(f)=E[L(Y,f(X))]\]</span></p><p><span class="math display">\[\hat{R}(f)=\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))\]</span></p><p>  为了从假设空间<span class="math inline">\(\mathcal{H}\)</span>中选取最优模型，我们可以采取经验风险最小化策略，设得到的最优模型为<span class="math inline">\(f_N\)</span>:</p><p><span class="math display">\[f_N = \arg \min_{f \in \mathcal{H}}\hat{R}(f)\]</span></p><p>  <span class="math inline">\(f_N\)</span>依赖训练数据集的样本容量<span class="math inline">\(N\)</span>，<span class="math inline">\(f_N\)</span>的泛化能力即其在整个样本空间上的期望风险：</p><p><span class="math display">\[R(f_N)=E[L(Y,f_{N}(X))]\]</span></p><p>  下面，首先给出关于假设空间<span class="math inline">\(\mathcal{H}\)</span>中任意函数<span class="math inline">\(f\)</span>的泛化误差上界的定理，再对定理进行证明。</p><p><strong>定理</strong></p><p>  对二分类问题，当假设空间是有限个函数的集合<span class="math inline">\(\mathcal{H}=\{f_1,f_2,\dots,f_d\}\)</span>时，对<span class="math inline">\(\forall f \in\mathcal{H}\)</span>，其泛化误差<span class="math inline">\(R(f)\)</span>有以下不等式成立：</p><p><span class="math display">\[P(R(f) \leq\hat{R}(f)+\varepsilon(d,N,\delta)) \geq 1-\delta, \delta \in(0,1)\]</span></p><p><span class="math display">\[\varepsilon(d,N,\delta)=\sqrt{\frac{1}{2N}(\log{d}+\log{\frac{1}{\delta}})}\]</span></p><p>  该不等式的左端<span class="math inline">\(R(f)\)</span>是泛化误差，右端即为泛化误差的上界。从泛化误差上界的表达时中，我们可以发现：</p><ul><li>当模型<span class="math inline">\(f\)</span>的经验风险，即训练误差<span class="math inline">\(\hat{R}(f)\)</span>越小时，泛化误差上界也越小。<br></li><li>当样本容量<span class="math inline">\(N \rightarrow\infty\)</span>时，<span class="math inline">\(\varepsilon(d,N,\delta)\rightarrow 0\)</span>，泛化误差上界下降。<br></li><li>当假设空间<span class="math inline">\(\mathcal{H}\)</span>中的函数越多，即<span class="math inline">\(d\)</span>越大时，<span class="math inline">\(\varepsilon(d,N,\delta)\)</span>越大，泛化误差上界越大。</li></ul><p>  下面对该定理进行证明，证明中需要用到 <span class="math inline">\(Hoeffiding\)</span> 不等式，这里先给出其结论。</p><p><strong><span class="math inline">\(Hoeffding\)</span>不等式</strong></p><p>  设 <span class="math inline">\(X_1,X_2,\dots,X_N\)</span>是独立随机变量，且 <span class="math inline">\(X_i \in [a_i,b_i], i =1,2,\dots,N\)</span>；<span class="math inline">\(\bar{X}\)</span> 是<span class="math inline">\(X_1,X_2,\dots,X_N\)</span>的均值，即<span class="math inline">\(\bar{X}=\frac{1}{N}\sum_{i=1}^{N}X_i\)</span>，则对<span class="math inline">\(\forall t &gt;0\)</span>，有以下不等式成立：</p><p><span class="math display">\[P \left(  \bar{X}-E(\bar{X})  \ge t\right) \leq exp \left( -\frac{2N^2t^2}{\sum_{i=1}^{N}(b_i-a_i)^2}\right)\]</span></p><p><span class="math display">\[P \left( | \bar{X}-E(\bar{X}) | \ge t\right) \leq 2exp \left( -\frac{2N^2t^2}{\sum_{i=1}^{N}(b_i-a_i)^2}\right)\]</span></p><p>  <span class="math inline">\(Hoeffding\)</span>不等式的证明放在附录中，感兴趣的读者可以自行阅读，下面我们来正式证明关于二分类问题的误差上界的定理。</p><p><strong>证明</strong><br>  令<span class="math inline">\(X_i =L(y_i,f(x_i))\)</span>，由于样本<span class="math inline">\((x_i,y_i)\)</span>是从联合概率分布<span class="math inline">\(P(X,Y)\)</span>中产生的，故有<span class="math inline">\(X_1,X_2,\dotsb,X_N\)</span>独立同分布，且对 <span class="math inline">\(\forall i, [a_i,b_i]=[0,1]\)</span>,</p><p><span class="math display">\[\bar{X}=\frac{1}{N}\sum_{i=1}^{N}X_i=\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))=\hat{R}(f)\]</span></p><p><span class="math display">\[\begin{align*}    E(\bar{X}) &amp;= E \left( \frac{1}{N} \sum_{i=1}^{N}X_i \right)=\frac{1}{N}\sum_{i=1}^{N}E(X_i) =E(X)\\    &amp;= E \left( L(Y,f(X)) \right) = R(f) \\\end{align*}\]</span></p><p>  根据 <span class="math inline">\(Hoeffding\)</span>不等式可知：对<span class="math inline">\(\forall \varepsilon &gt; 0\)</span>，有：</p><p><span class="math display">\[P(\bar{X}-E(\bar{X}) \ge\varepsilon)=P(R(f)-\hat{R}(f) \ge \varepsilon) \leq exp(-2N\varepsilon^2)\]</span></p><p>   <span class="math inline">\(\because\)</span>事件 <span class="math inline">\(\{R{f}-\hat{R}{f} \ge \varepsilon\}\Leftrightarrow\)</span> 事件 <span class="math inline">\(\{ \exists f\in \mathcal{H}: R(f)-\hat{R}(f) \ge \varepsilon \}\)</span>，故有：</p><p><span class="math display">\[\begin{align*}    P(R(f)-\hat{R}(f) \ge \varepsilon) &amp;= P(\exists f \in\mathcal{H}: R(f)-\hat{R}(f) \ge \varepsilon)=P \left( \bigcup_{f \in\mathcal{H}}\{ R(f)-\hat{R}(f) \ge \varepsilon \} \right) \\    &amp;\leq \sum_{f \in \mathcal{H}} P(R(f)-\hat{R}(f) \ge\varepsilon) \leq dexp(-2N\varepsilon^2)\end{align*}\]</span></p><p>   <span class="math inline">\(\because\)</span> 事件<span class="math inline">\(\{ \exists f \in \mathcal{H}: R(f)-\hat{R}(f) \ge\varepsilon \}\)</span>与事件<span class="math inline">\(\{\forall f \in\mathcal{H}: R(f)-\hat{R}(f) &lt; \varepsilon\}\)</span>互补，故对<span class="math inline">\(\forall f \in \mathcal{H}\)</span>有：</p><p><span class="math display">\[P(R(f)-\hat{R}(f) &lt;\varepsilon)=P(R(f)-\hat{R}(f) \leq \varepsilon) \ge1-dexp(-2N\varepsilon^2)\]</span></p><p>  令<span class="math inline">\(\delta = dexp(-2N\varepsilon^2) \in(0,1)\)</span>，即：</p><p><span class="math display">\[P(R(f)-\hat{R}(f) \leq \varepsilon) \ge1-\delta\]</span></p><p><span class="math display">\[\delta = dexp(-2N\varepsilon^2)\Rightarrow \varepsilon =\sqrt{\frac{1}{2N}(\log{d}+\log{\frac{1}{\delta}})}\]</span></p><p>  证毕.</p><h2 id="附录">附录</h2><h3 id="关于-hoeffding-不等式的证明">关于 <span class="math inline">\(Hoeffding\)</span> 不等式的证明</h3><p>  <span class="math inline">\(Hoeffding\)</span> 不等式的证明需要使用<span class="math inline">\(Markov\)</span> 不等式与 <span class="math inline">\(Hoeffding\)</span>不等式引理，这里我们首先证明这两个结论。</p><h4 id="markov-不等式"><span class="math inline">\(Markov\)</span>不等式</h4><p><strong>结论</strong><br>  对任意非负随变量<span class="math inline">\(X\)</span>，<span class="math inline">\(\varepsilon &gt; 0\)</span>，有：</p><p><span class="math display">\[P(X \ge \varepsilon) \leq\frac{E(X)}{\varepsilon}\]</span></p><p><strong>证明:</strong><br>  设随机变量<span class="math inline">\(X\)</span>的概率密度函数为<span class="math inline">\(f(x)\)</span>，则有：</p><p><span class="math display">\[E(X)=\int_{0}^{\infty}xf(x) \leq\int_{\varepsilon}^{\infty}xf(x)dx \leq \varepsilon\int_{\varepsilon}^{\infty}f(x)dx=\varepsilon P(X \ge\varepsilon)\]</span></p><p><span class="math display">\[\Rightarrow P(X \ge \varepsilon) \leq\frac{E(X)}{\varepsilon}\]</span></p><p>  证毕.</p><h4 id="hoeffding-不等式引理"><span class="math inline">\(Hoeffding\)</span> 不等式引理</h4><p><strong>结论</strong><br>  对任意随机变量<span class="math inline">\(X\)</span>，若其满足<span class="math inline">\(P(X \in [a,b])=1\)</span>和<span class="math inline">\(E(X)=0\)</span>，则有以下不等式成立：</p><p><span class="math display">\[E(e^{sX}) \leqe^{\frac{s^2(b-a)^2}{8}}\]</span></p><p><strong>证明:</strong><br>  设<span class="math inline">\(f(x)=e^{sx}\)</span>，且<span class="math inline">\(dom f = [a,b]\)</span>,<br>  <span class="math inline">\(\because f(x)\)</span>二阶可微，<span class="math inline">\(dom f\)</span>为凸集，且<span class="math inline">\(f^{''}(x)=s^2e^{sx} \ge 0\)</span>,<br>  由凸函数的二阶条件可知：<span class="math inline">\(f(x)\)</span>为凸函数,<br>  由凸函数的性质可知：对<span class="math inline">\(\forall X_1,X_2 \in[a,b]\)</span>，有以下不等式成立：</p><p><span class="math display">\[f(\alpha X_1+\beta X_2) \leq \alphaf(X_1)+\beta f(X_2),\space \forall \alpha,\beta &gt; 0,\alpha+\beta=1\]</span></p><p>  令<span class="math inline">\(\alpha=\frac{b-X}{b-a},\beta=\frac{X-a}{b-a}, X_1=a, X_2=b\)</span>，则有：</p><p><span class="math display">\[f(\alpha X_1+\betaX_2)=f(X)=e^{sX}\]</span></p><p><span class="math display">\[\alpha f(X_1)+\betaf(X_2)=\frac{b-X}{b-a}e^{sa}+\frac{X-a}{b-a}e^{sb}\]</span></p><p>  由凸函数的性质的得：</p><p><span class="math display">\[e^{sX} \leq\frac{b-X}{b-a}e^{sa}+\frac{X-a}{b-a}e^{sb}\]</span></p><p>  两边同时对<span class="math inline">\(X\)</span>取期望，由<span class="math inline">\(E(X)=0\)</span>，得：</p><p><span class="math display">\[0 \leq E(e^{sX}) \leq\frac{be^{sa}-ae^{sb}}{b-a}\]</span></p><p>  现要证明以下不等式成立：</p><p><span class="math display">\[\frac{be^{sa}-ae^{sb}}{b-a} \leqe^{\frac{s^2(b-a)^2}{8}}\]</span></p><p>  两边同时取对数，则等价于证明以下不等式成立：</p><p><span class="math display">\[\log \left( \frac{be^{sa}-ae^{sb}}{b-a}\right) \leq \frac{s^2(b-a)^2}{8}\]</span></p><p>  令<span class="math inline">\(t=b-a\)</span>，则<span class="math inline">\(b=t+a\)</span>，则不等式可作如下变换：</p><p><span class="math display">\[\begin{align*}    \log \left( \frac{be^{sa}-ae^{sb}}{b-a} \right) &amp;=\log(be^{sa}-ae^{sb})-\log(b-a) \\    &amp;= \log(te^{sa}+ae^{sa}-ae^{s(t+a)})-\log t  \\    &amp;= sa + \log(t+a-ae^{st}) - \log t \\    &amp;\leq \frac{s^2t^2}{8} \\\end{align*}\]</span></p><p>  令<span class="math inline">\(u=st, g(u)=sa +\log(t+a-ae^{st})-\log t\)</span>，则以上不等式等价于：</p><p><span class="math display">\[g(u) \leq \frac{1}{8}u^2\]</span></p><p>  对<span class="math inline">\(g(u)\)</span>求一阶导数和二阶导数得：</p><p><span class="math display">\[g^{'}(u)=\frac{-ae^u}{t+a-ae^u},\spaceg^{''}(x)=\frac{-ae^u(t+a)}{(t+a-ae^u)^2}\]</span></p><p>  将<span class="math inline">\(g(u)\)</span>在<span class="math inline">\(u=0\)</span>处进行泰勒展开：</p><p><span class="math display">\[g(u)=g(0)+g^{'}(0)u+\frac{1}{2}g^{''}(\varepsilon)u^2=\frac{1}{2}g^{''}(\varepsilon)u^2\]</span></p><p><span class="math display">\[\begin{align*}    g^{''}(\varepsilon) &amp;=\frac{-ae^{\varepsilon}(t+a)}{(t+a-ae^{\varepsilon})^2}=\frac{t+a}{t+a-ae^{\varepsilon}}\cdot \frac{-ae^{\varepsilon}}{t+a-ae^{\varepsilon}}  \\    &amp;= \left( \frac{t+a}{t+a-ae^{\varepsilon}} \right) \cdot \left(1- \frac{t+a}{t+a-ae^{\varepsilon}} \right)  = m(1-m) \leq \frac{1}{4}\\\end{align*}\]</span></p><p>  故可证明以下不等式成立：</p><p><span class="math display">\[g(u) =\frac{1}{2}g^{''}(\varepsilon)u^2 \leq\frac{1}{8}u^2\]</span></p><p>  证毕.</p><p>  有了<span class="math inline">\(Markov\)</span>不等式与 <span class="math inline">\(Hodeffding\)</span>不等式引理作为基础，下面我们开始正式证明 <span class="math inline">\(Hoeffding\)</span> 不等式。</p><p><strong>证明:</strong><br>  取 <span class="math inline">\(s &gt; 0, t &gt; 0\)</span>，由<span class="math inline">\(Markov\)</span>不等式得：</p><p><span class="math display">\[P(\bar{X}-E(\bar{X}) \ge t)=P \left(e^{s(\bar{X}-E(\bar{X}))} \ge e^{st} \right) \leq e^{-st}E \left(e^{s(\bar{X}-E(\bar{X}))} \right)\]</span></p><p>   <span class="math inline">\(\because\bar{X}=\frac{1}{N}\sum_{i=1}^{N}X_i, \spaceE(\bar{X})=\frac{1}{N}\sum_{i=1}^{N}E(X_i)\)</span>，故有：</p><p><span class="math display">\[E \left( e^{s(\bar{X}-E(\bar{X}))}\right) = E \left( e^{s \sum_{i=1}^{N} \frac{X_i-E(X_i)}{N}} \right) =\prod_{i=1}^{N}E \left( e^{s \frac{X_i-E(X_i)}{N}} \right)\]</span></p><p>  令<span class="math inline">\(Y_i =\frac{X_i-E(X_i)}{N}\)</span>，则<span class="math inline">\(Y_i\)</span>相互独立，且有<span class="math inline">\(E(Y_i)=0, Y_i \in[\frac{a_i-E(X_i)}{N},\frac{b_i-E(X_i)}{N}]\)</span>，由 <span class="math inline">\(Hoeffding\)</span> 不等式引理得：</p><p><span class="math display">\[E \left( e^{sY_i} \right) \leq e^{s\frac{(b_i-a_i)^2}{8N^2}}, i=1,2,\dots,N\]</span></p><p>  由以上不等式可得：</p><p><span class="math display">\[E \left( e^{s(\bar{X}-E(\bar{X}))}\right) = \prod_{i=1}^{N}E \left( e^{sY_i} \right) \leq \prod_{i=1}^{N}e^{s^2 \frac{(b_i-a_i)^2}{8N^2}} = e^{s^2 \sum_{i=1}^{N}\frac{(b_i-a_i)^2}{N}}\]</span></p><p>  则有：</p><p><span class="math display">\[P(\bar{X}-E(\bar{X}) \ge t) \leqe^{-st}E \left( e^{s(\bar{X}-E(\bar{X}))} \right) \leq e^{-st+s^2\sum_{i=1}^{N} \frac{(b_i-a_i)^2}{N}}\]</span></p><p>  对 <span class="math inline">\(\forall s &gt; 0\)</span>均有以上不等式成立，故不等式右端对<span class="math inline">\(s\)</span>取最小值，不等式仍然成立：<br>  令<span class="math inline">\(h(s) = -st+s^2 \sum_{i=1}^{N}\frac{(b_i-a_i)^2}{N}\)</span>，对其求一阶导数并令其为零：</p><p><span class="math display">\[h^{'}(s)=-t+s\frac{\sum_{i=1}^{N}(b_i-a_i)^2}{4N^2}=0 \Rightarrows^{'}=\frac{4N^2t}{\sum_{i=1}^{N}(b_i-a_i)^2}\]</span></p><p>  代入<span class="math inline">\(s^{'}\)</span>可以得到<span class="math inline">\(h(s)\)</span>的最小值：</p><p><span class="math display">\[h_{min}(s)=h(s^{'})=\frac{-2N^2t^2}{\sum_{i=1}^{N}(b_i-a_i)^2}\]</span></p><p>  由于<span class="math inline">\(e^x\)</span>在<span class="math inline">\(\mathbb{R}\)</span>上的单调性可知：</p><p><span class="math display">\[e^{-st+s^2 \sum_{i=1}^{N}\frac{(b_i-a_i)^2}{N}}=e^{h(s)} \gee^{h_{min}(s)}=e^{\frac{-2N^2t^2}{\sum_{i=1}^{N}(b_i-a_i)^2}}\]</span></p><p>  故有以下不等式成立：</p><p><span class="math display">\[P(\bar{X}-E(\bar{X}) \ge t) \leq exp\left( \frac{-2N^2t^2}{\sum_{i=1}^{N}(b_i-a_i)^2} \right)\]</span></p><p>  同时：</p><p><span class="math display">\[P(E(\bar{X})-\bar{X} \ge t) =P(-\bar{X}-E(-\bar{X}) \ge t)\]</span></p><p>  令<span class="math inline">\(\bar{Z}=-\bar{X},E(\bar{Z})=E(-\bar{X})\)</span>，同上可证：</p><p><span class="math display">\[P(E(\bar{X})-\bar{X} \ge t) =P(\bar{Z}-E(\bar{Z}) \ge t) \leq exp \left(\frac{-2N^2t^2}{\sum_{i=1}^{N}(b_i-a_i)^2} \right)\]</span></p><p>  综上所述，有以下不等式成立：</p><p><span class="math display">\[P \left( | \bar{X}-E(\bar{X}) | \ge t\right) \leq 2exp \left( -\frac{2N^2t^2}{\sum_{i=1}^{N}(b_i-a_i)^2}\right)\]</span></p><p>  证毕.</p><h2 id="参考">参考</h2><p><strong>[1] Book: 李航.(2019).统计学习方法</strong><br><strong>[2] Video: B站数学up主——简博士</strong><br><strong>[3] Github:Dod-o/Statistical-Learning-Method_Code</strong><br><strong>[4] Blog: 知乎,ziyoufeixiang,Hoeffding 不等式证明</strong><br><strong>[5] Blog:CSDN,吃吃今天努力学习了吗,[Python]多项式曲线拟合(Polynomial CurveFitting)</strong></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>矩阵分析-4.线性子空间</title>
      <link href="/2023/09/07/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-4-%E7%BA%BF%E6%80%A7%E5%AD%90%E7%A9%BA%E9%97%B4/"/>
      <url>/2023/09/07/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-4-%E7%BA%BF%E6%80%A7%E5%AD%90%E7%A9%BA%E9%97%B4/</url>
      
        <content type="html"><![CDATA[<h1 id="线性子空间">线性子空间</h1><h2 id="线性子空间的定义">线性子空间的定义</h2><p>  设<span class="math inline">\(V\)</span>是数域<span class="math inline">\(\mathbb{F}\)</span>上的线性空间，<span class="math inline">\(W \in V\)</span>是非空子集，若<span class="math inline">\(W\)</span>有以下两个属性:<br>(1) <strong>对加法封闭</strong>：<span class="math inline">\(\forall\alpha,\beta \in W, \alpha+\beta \in W\)</span>.<br>(2) <strong>对数乘封闭</strong>：<span class="math inline">\(\forall k\in \mathbb{F}, \alpha \in W, \alpha k \in W\)</span>.<br>则称集合<span class="math inline">\(W\)</span>是线性空间<span class="math inline">\(V\)</span>的线性子空间.</p><p>注：线性子空间<span class="math inline">\(W\)</span>本身按<span class="math inline">\(V\)</span>中定义的加法与数乘也构成线性空间.</p><h2 id="线性子空间的实例">线性子空间的实例</h2><h3 id="例1-二维空间的线性子空间">例1 二维空间的线性子空间</h3><p>  <span class="math inline">\(\color{green}{结论:二维空间的线性子空间是任意经过原点的直线.}\)</span><br>  <strong>证明:</strong><br>  设<span class="math inline">\(V = \mathbb{R}^{2}\)</span>，<span class="math inline">\(V\)</span>表示二维平面.<br>  二维平面中经过原点的某一直线：<span class="math inline">\(W = \{x\vert \theta^{T} x=0, x \in V\}, \theta \in\mathbb{R}^{2}\)</span>.<br>  验证直线<span class="math inline">\(W\)</span>是二维平面<span class="math inline">\(V\)</span>的线性子空间：<br>  (1) 对<span class="math inline">\(\forall x_1,x_2 \in W,\theta^{T}(x_1+x_2)= \theta^{T}x_1+\theta^{T}x_2=0\)</span>,且<span class="math inline">\(x_1+x_2 \in V,\)</span><br>  <span class="math inline">\(\Rightarrow x_1+x_2 \inW\)</span>，即<span class="math inline">\(W\)</span>对加法封闭.<br>  (2) 对<span class="math inline">\(\forall \beta \in \mathbb{R},\forall x \in W, \theta^{T}(\betax)=\beta(\theta^{T}x)=0\)</span>，且<span class="math inline">\(\beta x\in V,\)</span><br>  <span class="math inline">\(\Rightarrow \beta x \inW\)</span>，即<span class="math inline">\(W\)</span>对数乘封闭.<br>  综上所述：直线<span class="math inline">\(W\)</span>是二维空间<span class="math inline">\(V\)</span>的线性子空间.</p><p>  <strong>注：二维空间中不经过原点的直线不是其线性子空间。</strong><br>  <strong>证明:</strong><br>  设<span class="math inline">\(V=\mathbb{R}^2\)</span>，<span class="math inline">\(V\)</span>表示二维平面.<br>  二维平面中不经过原点的某一直线：<span class="math inline">\(W=\{x\vert \theta^{T}x+b=0,b \neq 0,x \in V\}, \theta \in\mathbb{R}^{2}.\)</span><br>  验证直线<span class="math inline">\(W\)</span>不是二维平面<span class="math inline">\(V\)</span>的线性子空间：<br>  (1) 对<span class="math inline">\(\forall x_1,x_2 \in W, x_1,x_2 \neq0, \theta^{T}(x_1+x_2)+b = (\theta^{T}x_1+b)+\theta^{T}x_2=\theta^{T}x_2\neq 0.\)</span><br>  又<span class="math inline">\(\because x_1+x_2 \in V \Rightarrowx_1+x_2 \notin W \Rightarrow\)</span> 直线<span class="math inline">\(W\)</span>不满足对加法封闭，故其不是二维空间<span class="math inline">\(V\)</span>的线性子空间.</p><p>  对于二维空间的线性子空间，我们也可以借助图像来直观理解：</p><center><img src="https://s2.loli.net/2023/09/17/6eWufsxXb3FpVYh.jpg" width="40%" height="40%"><div data-align="center">Image1: 二维空间的子空间</div></center><p>  从图中我们可以发现，经过原点的直线<span class="math inline">\(W_1\)</span>上任意两个向量<span class="math inline">\(x_1,x_2\)</span>的和仍位于直线<span class="math inline">\(W_1\)</span>上，说明<span class="math inline">\(W_1\)</span>对加法封闭，同时<span class="math inline">\(W_1\)</span>上的向量伸缩后仍位于直线<span class="math inline">\(W_1\)</span>上，说明<span class="math inline">\(W_1\)</span>对数乘封闭。故经过原点的直线<span class="math inline">\(W_1\)</span>是二维空间<span class="math inline">\(V\)</span>的线性子空间.<br>  不经过原点的直线<span class="math inline">\(W_2\)</span>上的任意两个向量<span class="math inline">\(y_1,y_2\)</span>，这两个向量的和<span class="math inline">\(y_3\)</span>不位于直线<span class="math inline">\(W_2\)</span>上，故直线<span class="math inline">\(W_2\)</span>对加法不封闭，其不是二维空间<span class="math inline">\(V\)</span>的线性子空间.</p><h3 id="例2-向量组生成的线性子空间及线性子空间的生成组">例2向量组生成的线性子空间及线性子空间的生成组</h3><p>  设有向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>，定义：<br><span class="math display">\[W = span\{\alpha_1,\alpha_2,\dots,\alpha_p\}=\{\alpha_{1}c_{1}+\alpha_{2}c_{2}+\dots+\alpha_{p}c_{p} \vert c_{i} \in\mathbb{F},i=1,2,\dots,p\} =\{向量组\alpha_i的线性组合的全体\}\]</span></p><p>  称<span class="math inline">\(W\)</span>为向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>所生成的线性子空间,向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>称为线性子空间的生成组.</p><h3 id="例3-矩阵的核与象">例3 矩阵的核与象</h3><p>  设矩阵<span class="math inline">\(A \in \mathbb{F}^{m\timesn}\)</span>，定义集合<span class="math inline">\(\{x \vert x \in\mathbb{F}^{n},Ax=0 \}\)</span>为矩阵<span class="math inline">\(A\)</span>的核，记为<span class="math inline">\(ker A\)</span>，集合<span class="math inline">\(\{y \vert y \in \mathbb{F}^{m}, \exists x \in \mathbb{F}^{n},y=Ax\}\)</span>为矩阵<span class="math inline">\(A\)</span>的象，记为<span class="math inline">\(imA\)</span>. <span class="math inline">\(ker A\)</span>与<span class="math inline">\(im A\)</span>分别是<span class="math inline">\(\mathbb{F}^n\)</span>与<span class="math inline">\(\mathbb{F}^{m}\)</span>的线性子空间.</p><p><strong>证明:</strong><br>  设矩阵<span class="math inline">\(A \in \mathbb{F}^{m \timesn}\)</span>，记矩阵<span class="math inline">\(A\)</span>的核为集合<span class="math inline">\(ker A = \{x \vert x \in \mathbb{F}^{n},Ax=0\}\)</span>，<br>  矩阵<span class="math inline">\(A\)</span>的象为集合<span class="math inline">\(im A = \{ y \vert y \in \mathbb{F}^{m}, \exists x\in \mathbb{F}^{n}, y=Ax\}\)</span>.<br>  (1) 验证<span class="math inline">\(ker A\)</span>是线性空间<span class="math inline">\(\mathbb{F}^{n}\)</span>的线性子空间:<br>  对 <span class="math inline">\(\forall x_1,x_2 \in kerA\)</span>，有<span class="math inline">\(Ax_1=Ax_2=0 \RightarrowA(x_1+x_2)=0\)</span>，故有：<br>  <span class="math inline">\(x_1+x_2 \in ker A\)</span>，即<span class="math inline">\(ker A\)</span>对加法封闭.<br>  对<span class="math inline">\(\forall k \in \mathbb{F}, A(x_1 k) =(Ax_1)k = 0\)</span>，故有<span class="math inline">\(x_1 k \in kerA\)</span>，即<span class="math inline">\(kerA\)</span>对数乘封闭.<br>  棕上所述，<span class="math inline">\(ker A\)</span>是线性空间<span class="math inline">\(\mathbb{F}^{n}\)</span>的线性子空间.<br>  (2) 验证<span class="math inline">\(im A\)</span>是线性空间<span class="math inline">\(\mathbb{F}^{m}\)</span>的线性子空间:<br>   对<span class="math inline">\(\forall y_1,y_2 \in im A, \existsx_1,x_2 \in \mathbb{F}^n\)</span>，使得<span class="math inline">\(y_1 =Ax_1, y_2 = Ax_2\)</span>，<br>  <span class="math inline">\(\Rightarrow y_1+y_2 = Ax_1 + Ax_2 =A(x_1+x_2)\)</span>，令<span class="math inline">\(y_3=y_1+y_2,x_3 =x_1+x_2\)</span>，<br>  <span class="math inline">\(\because y_1,y_2 \in\mathbb{F}^{m}\)</span>，且线性空间<span class="math inline">\(\mathbb{F}^{m}\)</span>对加法封闭，故有<span class="math inline">\(y_3 \in \mathbb{F}^{m}\)</span>，同理可得<span class="math inline">\(x_3 \in \mathbb{F}^{n}\)</span>，<br>  <span class="math inline">\(y_3 = Ax_3 \Rightarrow y_3 \in imA\)</span>，即<span class="math inline">\(im A\)</span>对加法封闭.<br>  对<span class="math inline">\(\forall k \in \mathbb{F}, y_1 k =(Ax_1)k = A(x_1 k)\)</span>，<br>  <span class="math inline">\(\because x_1 \in\mathbb{F}^{n}\)</span>，且线性空间<span class="math inline">\(\mathbb{F}^{n}\)</span>对数乘封闭, 故有<span class="math inline">\(x_1 k \in \mathbb{F}^{n}\)</span>,<br>  则有<span class="math inline">\(y_1 k \in im A\)</span>，即<span class="math inline">\(im A\)</span>对数乘封闭.<br>  综上所述，<span class="math inline">\(im A\)</span>是线性空间<span class="math inline">\(\mathbb{F}^{m}\)</span>的线性子空间.</p><p><strong>注:</strong> <span class="math inline">\(Ax\)</span>为矩阵<span class="math inline">\(A\)</span>的列向量组<span class="math inline">\(\{\alpha_1, \alpha_2, \dots, \alpha_n\}\)</span>以<span class="math inline">\(x\)</span>为系数的线性组合.<span class="math inline">\(im A\)</span>也就是向量组<span class="math inline">\(\{\alpha_1, \alpha_2, \dots, \alpha_n\}\)</span>所张成的线性子空间.</p><h2 id="线性子空间的交与和">线性子空间的交与和</h2><p>  设<span class="math inline">\(U,W\)</span>是<span class="math inline">\(V\)</span>的线性子空间，则有：<br>(1) <span class="math inline">\(U \cap W = \{v \vert v \in U, v \inW\}\)</span>，<span class="math inline">\(U \cap W\)</span>也是<span class="math inline">\(V\)</span>的线性子空间，称为<span class="math inline">\(U,W\)</span>的交.<br>(2) <span class="math inline">\(U+W = span \{U,W\} = \{u+w \vert u \inU, w \in W\}\)</span>，<span class="math inline">\(U+W\)</span>也是<span class="math inline">\(V\)</span>的线性子空间，称为<span class="math inline">\(U,W\)</span>的和.</p><p><strong>证明:</strong><br>  设<span class="math inline">\(V\)</span>是线性空间,<span class="math inline">\(U,W\)</span>是<span class="math inline">\(V\)</span>的线性子空间,<br>  (1) 验证<span class="math inline">\(U \cap W\)</span>是<span class="math inline">\(V\)</span>的线性子空间<br>  对<span class="math inline">\(\forall v_1,v_2 \in U \capW\)</span>，有<span class="math inline">\(v_1,v_2 \in U, v_1,v_2 \inW\)</span><br>  <span class="math inline">\(\because U,W\)</span>是<span class="math inline">\(V\)</span>的线性子空间，故<span class="math inline">\(U,W\)</span>对加法封闭 <span class="math inline">\(\Rightarrow v_1+v_2 \in U, v_1+v_2 \in W\Rightarrow v_1+v_2 \in U \cap W\)</span><br>  即<span class="math inline">\(U \cap W\)</span>对加法封闭.<br>  对<span class="math inline">\(\forall k \in \mathbb{F}, v_1 \in U \capW\)</span>，<span class="math inline">\(U,W\)</span>对数乘封闭，故有:<br>  <span class="math inline">\(v_1k \in U, v_1k \in W \Rightarrow v_1k\in U \cap W\)</span>，即<span class="math inline">\(U \capW\)</span>对数乘封闭.<br>  (2) 验证<span class="math inline">\(U+W\)</span>是<span class="math inline">\(V\)</span>的线性子空间<br>  对<span class="math inline">\(\forall v_1,v_2 \in U+W, \exists u_1,u_2\in U, w_1,w_2 \in W\)</span>，使得<span class="math inline">\(v_1 =u_1+w_1,v_2 = u_2+w_2\)</span><br>  <span class="math inline">\(v_1+v_2=u_1+w_1+u_2+w_2=(u_1+u_2)+(w_1+w_2)\)</span>，<br>  令<span class="math inline">\(v_3=v_1+v_2, u_3 = u_1+u_2, w_3 =w_1+w_2 \Rightarrow v_3=u_3+w_3\)</span>，由<span class="math inline">\(U,W\)</span>对加法的封闭性可知：<br>  <span class="math inline">\(u_3 \in U, w_3 \in W \Rightarrow v_3 \inU+W\)</span>，即<span class="math inline">\(U+W\)</span>对加法封闭.<br>  对<span class="math inline">\(\forall k \in \mathbb{F}, v_1 \in U+W,v_1k = (u_1+w_1)k=u_1k+w_1k\)</span>,<br>  <span class="math inline">\(\because U,W\)</span>对数乘封闭，故有<span class="math inline">\(u_1k \in U, w_1k \in W \Rightarrow v_1 k \inU+W\)</span>，即<span class="math inline">\(U+W\)</span>对数乘封闭.<br>  综上所述，<span class="math inline">\(U \Cap w, U+W\)</span>均为<span class="math inline">\(V\)</span>的线性子空间.</p>]]></content>
      
      
      <categories>
          
          <category> 矩阵分析 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>矩阵分析-3.基与坐标</title>
      <link href="/2023/08/27/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-3-%E5%9F%BA%E4%B8%8E%E5%9D%90%E6%A0%87/"/>
      <url>/2023/08/27/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-3-%E5%9F%BA%E4%B8%8E%E5%9D%90%E6%A0%87/</url>
      
        <content type="html"><![CDATA[<h1 id="基与坐标">基与坐标</h1><p>  在前面的章节，我们介绍了线性空间的概念，线性空间是一个抽象的概念，但在实际应用中，出于对向量运算的需求，我们通常更需要标准线性空间中的向量。为了将抽象线性空间中的向量映射到标准线性空间，我们引入了基向量的概念。抽象向量沿着基向量展开后得到坐标向量，我们用坐标向量来表示映射后的抽象向量。</p><h2 id="有限维线性空间基坐标">有限维线性空间基坐标</h2><p>  设<span class="math inline">\(V\)</span>是数域<span class="math inline">\(\mathbb{F}\)</span>上的线性空间，若有正整数<span class="math inline">\(n\)</span>，及<span class="math inline">\(V\)</span>中的向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_n\}\)</span>，使得:</p><ol type="1"><li><strong>线性无关性</strong>: 向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_n \}\)</span>为线性无关向量组.<br></li><li><strong>线性生成性</strong>: <span class="math inline">\(\forall\alpha \in V\)</span>，均可由向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_n \}\)</span>线性表示.</li></ol><p><span class="math display">\[\alpha = \alpha_{1} k_1 + \alpha_{2} k_2+ \dots + \alpha_{n} k_n = \begin{bmatrix}    \alpha_1,\alpha_2,\dots,\alpha_n \\\end{bmatrix} \begin{bmatrix}    k_1 \\    k_2 \\    \vdots \\    k_n \\\end{bmatrix} = Ak\]</span></p><p>  则称<span class="math inline">\(V\)</span>为<span class="math inline">\(n\)</span>维线性空间，向量组<span class="math inline">\(\{ \alpha_1,\alpha_2,\dots,\alpha_n\}\)</span>称为<span class="math inline">\(V\)</span>中的一个基(坐标系)，<span class="math inline">\(k \in \mathbb{F}^n\)</span>称为<span class="math inline">\(\alpha \in V\)</span>，沿着该基的坐标向量.</p><p>注：</p><p><span class="math display">\[\color{green} \begin{bmatrix}    抽 \\    象 \\    向 \\    量 \\\end{bmatrix} = \begin{bmatrix}    基矩阵 \\\end{bmatrix} \begin{bmatrix}    坐 \\    标 \\    向 \\    量 \\\end{bmatrix}\]</span></p><h2 id="关于基向量组的定理">关于基向量组的定理</h2><p><strong>定理1(基向量个数的唯一性)</strong> 设向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_m\}\)</span>及<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_s\}\)</span>分别是线性空间<span class="math inline">\(V\)</span>的两个基，则有<span class="math inline">\(m=s\)</span>.<br><strong>证明:</strong><br>  从线性空间中任意取<span class="math inline">\(p\)</span>个向量组成一个向量组<span class="math inline">\(\{v_1,v_2,\dots,v_p\}\)</span>，要求<span class="math inline">\(m \leq p, s \leq p\)</span>.<br>  由基向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_m\}\)</span>的定义可知：<br>  1. 向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_m\}\)</span>为线性无关向量组.<br>  2. 对<span class="math inline">\(\forall v \in \{v_1,v_2,\dots,v_p\},v\)</span>均可由向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_m\}\)</span>线性表示.<br>  <span class="math inline">\(\Rightarrow\)</span>向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_m\}\)</span>是向量组<span class="math inline">\(\{v_1,v_2,\dots,v_p\}\)</span>的极大线性无关组.<br>  同理可知：向量组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_s\}\)</span>也为向量组<span class="math inline">\(\{v_1,v_2,\dots,v_p\}\)</span>的极大线性无关组.<br>  由向量组的极大线性无关组中向量个数的唯一性可知: <span class="math inline">\(m=s\)</span>.</p><p><strong>定理2</strong> <span class="math inline">\(\color{green}{基实现了抽象线性空间到标准线性空间的一一映射.}\)</span><br><strong>证明:</strong><br>  设<span class="math inline">\(V\)</span>为<span class="math inline">\(n\)</span>维线性空间，向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_n\}\)</span>是<span class="math inline">\(V\)</span>的一个基.<br>  设由基向量组实现的映射为:<br><span class="math display">\[\sigma: V \longrightarrow\mathbb{F}^{n}\]</span></p><p><span class="math display">\[v \longmapsto k = \begin{bmatrix}    k_1 \\    k_2 \\    \vdots \\    k_n \\\end{bmatrix}\]</span></p><p>  <span class="math inline">\(k \in \mathbb{F}^{n}\)</span>是<span class="math inline">\(v\)</span>在基下的坐标向量.<br>  现需要验证映射<span class="math inline">\(\sigma\)</span>满足一一映射的两个条件.<br>  (1) 验证对<span class="math inline">\(\forall k \in \mathbb{F}^n,\exists v \in V\)</span>，使得<span class="math inline">\(v=\alpha_1k_1+\alpha_2k_2+\dots+\alpha_nk_n\)</span>.<br>  <span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_n\} \inV\)</span>. 由线性空间<span class="math inline">\(V\)</span>对加法与数乘封闭的性质可知:<br>  <span class="math inline">\(\exists v \in V\)</span>使得<span class="math inline">\(v =\alpha_1k_1+\alpha_2k_2+\dots+\alpha_nk_n\)</span>.<br>  (2) 验证若<span class="math inline">\(\sigma(v)=\sigma(v_0)=k\)</span>，则有<span class="math inline">\(v=v_0\)</span>.<br>  <span class="math inline">\(\sigma(v)=k \Leftrightarrowv=\alpha_1k_1+\alpha_2k_2+\dots+\alpha_nk_n\)</span>.<br>  <span class="math inline">\(\sigma(v_0)=k \Leftrightarrowv_0=\alpha_1k_1+\alpha_2k_2+\dots+\alpha_nk_n\)</span>.<br>  <span class="math inline">\(\Rightarrow v_0 = v\)</span>.<br>  综上所述映射<span class="math inline">\(\sigma\)</span>为一一映射.</p><p><strong>注：一一映射的定义</strong><br>  设映射<span class="math inline">\(\sigma: S_1 \rightarrowS_2\)</span>满足:<br>  (1)满射：对<span class="math inline">\(\forall s_2 \in S_2, \existss_1 \in S_1, \sigma(s_1)=s_2 .\)</span><br>  (2)单射：若<span class="math inline">\(\sigma(s_1)=\sigma(s^{*}_{1})\)</span>,则<span class="math inline">\(s_1=s^{*}_{1}.\)</span><br>  则称映射<span class="math inline">\(\sigma\)</span>为集合<span class="math inline">\(S_1\)</span>与<span class="math inline">\(S_2\)</span>之间的一一映射.</p><h2 id="标准线性空间mathbbrn的标准基与一般基">标准线性空间<span class="math inline">\(\mathbb{R}^n\)</span>的标准基与一般基</h2><h3 id="标准基">标准基</h3><p>  标准线性空间<span class="math inline">\(\mathbb{R}^n\)</span>的标准基：</p><p><span class="math display">\[e_1=\begin{bmatrix}    1 \\    0 \\    \vdots \\    0 \\\end{bmatrix}, e_2 = \begin{bmatrix}    0 \\    1 \\    \vdots \\    0\end{bmatrix}, \dots, e_n = \begin{bmatrix}    0 \\    0 \\    \vdots \\    1\end{bmatrix}\]</span></p><p><strong>证明:</strong><br>  (1)先证明标准基向量组的线性无关性：<br>  令 <span class="math inline">\(I_n =\begin{bmatrix}  e_1,e_2,\dots,e_n\end{bmatrix}\)</span>，有线性方程组<span class="math inline">\(I_n x =0\)</span>.<br>  该方程组仅有<span class="math inline">\(x=0\)</span>唯一解，故标准基向量组<span class="math inline">\(\{e_1,e_2,\dots,e_n\}\)</span>线性无关.<br>  (2)再证标准基向量组的线性生成性：<br>  对<span class="math inline">\(\forall v \in\mathbb{R}^n\)</span>，判断线性方程组<span class="math inline">\(I_n x =v\)</span>是否有解.<br>  <span class="math inline">\(I_n x = v \Rightarrow x = v\Rightarrow\)</span>方程有解<span class="math inline">\(\Rightarrowv\)</span>可由标准基向量组<span class="math inline">\(\{e_1,e_2,\dots,e_n\}\)</span>线性表示.<br>  综上所述，向量组<span class="math inline">\(\{e_1,e_2,\dots,e_n\}\)</span>可作为标准线性空间<span class="math inline">\(\mathbb{R}^n\)</span>的基向量组.</p><h3 id="一般基">一般基</h3><p>  <span class="math inline">\(\alpha_1,\alpha_2,\dots,\alpha_n \in\mathbb{R}^n\)</span>，向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_n\}\)</span>构成标准线性空间<span class="math inline">\(\mathbb{R}^n\)</span>的一组基的充要条件为：向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_n\}\)</span>的秩为<span class="math inline">\(n\)</span>.</p><p><strong>证明:</strong><br>  向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_n\}\)</span>的秩为<span class="math inline">\(n\)</span> <span class="math inline">\(\Rightarrow\)</span> 向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_n\}\)</span>线性无关<br>  令<span class="math inline">\(A=\begin{bmatrix}  \alpha_1,\alpha_2,\dots,\alpha_n\end{bmatrix}\)</span>，有<span class="math inline">\(rank(A) =n\)</span>.<br>  对<span class="math inline">\(\forall \beta \in\mathbb{R}^n\)</span>，判断矩阵方程<span class="math inline">\(Ax=\beta\)</span>是否有解.<br>  <span class="math inline">\(\becauserank(A)=rank([A,b])=n\)</span>，<span class="math inline">\(\therefore\)</span>方程<span class="math inline">\(Ax=\beta\)</span>有解.</p><p><span class="math inline">\(\color{green}{注：线性方程组 Ax=\beta的几何语言：在n维线性空间中，将向量\beta沿着矩阵A的列向量组所构成的基展开.}\)</span></p><h2 id="多项式函数空间作为线性空间的基">多项式函数空间作为线性空间的基</h2><p>  在第一节我们已经说明函数空间<span class="math inline">\(V=\mathcal{F}(I,\mathbb{F}^n)\)</span>可以作为线性空间。多项式是函数的一种形式，我们可以定义以多项式为元素的线性空间：</p><p><span class="math display">\[\mathbb{F}[x]=\{以x为自变量，以数域\mathbb{F}中的数为系数的多项式\}=\{f=a_0+a_1x+a_2x^2+\dots\vert a_i \in \mathbb{F}, i =1,2,\dots\}\]</span></p><p>  通过分析我们可以得知这是一个无限维的线性空间，这里我们不讨论无限维线性空间的基，通过对<span class="math inline">\(x\)</span>的次数添加限制，我们可以将这个线性空间变为有限维：</p><p><span class="math display">\[\mathbb{F}_n[x]=\{以x为自变量，以数域\mathbb{F}中的数为系数,次数小于n的多项式\}=\{f=a_0+a_1x++\dots+a_{n-1}x^{n-1}\vert a_i \in \mathbb{F}, i =1,2,\dots,n-1\}\]</span></p><p>  <span class="math inline">\(\mathbb{F}_n[x]\)</span>是一个<span class="math inline">\(n\)</span>维线性空间，取<span class="math inline">\(\mathbb{F}=\mathbb{R}\)</span>，接下来我们来讨论<span class="math inline">\(\mathbb{R}_n[x]\)</span>的基向量组。<br>  在高等代数中，我们知道多项式函数空间中的元素与标准线性空间中的元素一一对应，对<span class="math inline">\(\forall f \in \mathbb{R}_n[x]\)</span>，有：</p><p><span class="math display">\[f=a_0+a_1x++\dots+a_{n-1}x^{n-1}=\begin{bmatrix}    1,x,\dots,x^{n-1} \\\end{bmatrix}\begin{bmatrix}    a_0 \\    a_1 \\    \vdots \\    a_{n-1} \\\end{bmatrix}, f \rightarrow \begin{bmatrix}    a_0 \\    a_1 \\    \vdots \\    a_{n-1} \\\end{bmatrix}\]</span></p><p>  可以取<span class="math inline">\(\{1,x,\dots,x^{n-1}\}\)</span>为<span class="math inline">\(\mathbb{R}_n[x]\)</span>的一组基，以下是证明<span class="math inline">\(\{1,x,\dots,x^{n-1}\}\)</span>可以作为<span class="math inline">\(\mathbb{R}_n[x]\)</span>的基.<br><strong>证明:</strong><br>  (1)线性无关性<br>  若<span class="math inline">\(a_0+a_1x+\dots+a_{n-1}x^{n-1}=0\)</span>，带入<span class="math inline">\(x=1,2,\dots,n\)</span>，得：</p><p><span class="math display">\[\begin{bmatrix}    1^{0}&amp;1^{1}&amp;\dots&amp;1^{n-1} \\    2^{0}&amp;2^{1}&amp;\dots&amp;2^{n-1} \\    \vdots&amp;\vdots&amp;&amp;\vdots \\    n^{0}&amp;n^{1}&amp;\dots&amp;n^{n-1} \\\end{bmatrix}\begin{bmatrix}    a_0 \\    a_1 \\    \vdots \\    a_{n-1} \\\end{bmatrix}=\begin{bmatrix}    0 \\    0 \\    \vdots \\    0\end{bmatrix}\]</span></p><p>  令<span class="math inline">\(A=\begin{bmatrix}  1^{0}&amp;1^{1}&amp;\dots&amp;1^{n-1}\\  2^{0}&amp;2^{1}&amp;\dots&amp;2^{n-1}\\  \vdots&amp;\vdots&amp;&amp;\vdots\\  n^{0}&amp;n^{1}&amp;\dots&amp;n^{n-1} \\\end{bmatrix},a=\begin{bmatrix}  a_0 \\  a_1 \\  \vdots \\  a_{n-1} \\\end{bmatrix}\)</span>，<span class="math inline">\(det(A)\)</span>为范德蒙行列式,且<span class="math inline">\(det(A)\neq 0\)</span>，故<span class="math inline">\(rank(A)=n\)</span>.<br>  <span class="math inline">\(rank(A)=n \Rightarrow\)</span> 方程<span class="math inline">\(Aa=0\)</span>只有零解,即 <span class="math inline">\(a_i=0, i=1,2,\dots,n-1\)</span>.<br>  故<span class="math inline">\(\{1,x,\dots,x^{n-1}\}\)</span>线性无关.<br>  (2)线性生成性<br>  由多项式的定义可知，<span class="math inline">\(\mathbb{R}_n[x]\)</span>中的元素均可由<span class="math inline">\(\{1,x,\dots,x^{n-1}\}\)</span>线性表示.<br>  综上所述：<span class="math inline">\(\{1,x,\dots,x^{n-1}\}\)</span>可以作为<span class="math inline">\(\mathbb{R}_n[x]\)</span>的一组基.</p><p>注：<span class="math inline">\(\{1,x,\dots,x^{n-1}\}\)</span>对应于<span class="math inline">\(\mathbb{R}^n\)</span>中的标准基<br>  设<span class="math inline">\(\sigma\)</span>为以<span class="math inline">\(\{1,x,\dots,x^{n-1}\}\)</span>为基时，从<span class="math inline">\(\mathbb{R}_n[x]\)</span>到<span class="math inline">\(\mathbb{R}^n\)</span>的映射，有：</p><p><span class="math display">\[1=\begin{bmatrix}    1,x,\dots,x^{n-1} \\\end{bmatrix}\begin{bmatrix}    1 \\    0 \\    \vdots \\    0 \\\end{bmatrix}, \sigma: 1 \rightarrow \begin{bmatrix}    1 \\    0 \\    \vdots \\    0 \\\end{bmatrix}\]</span></p><p><span class="math display">\[x=\begin{bmatrix}    1,x,\dots,x^{n-1} \\\end{bmatrix}\begin{bmatrix}    0 \\    1 \\    \vdots \\    0 \\\end{bmatrix}, \sigma: x \rightarrow \begin{bmatrix}    0 \\    1 \\    \vdots \\    0 \\\end{bmatrix}\]</span></p><p><span class="math display">\[\vdots\]</span></p><p><span class="math display">\[x^{n-1}=\begin{bmatrix}    1,x,\dots,x^{n-1} \\\end{bmatrix}\begin{bmatrix}    0 \\    0 \\    \vdots \\    1 \\\end{bmatrix}, \sigma: x^{n-1} \rightarrow \begin{bmatrix}    0 \\    0 \\    \vdots \\    1 \\\end{bmatrix}\]</span></p>]]></content>
      
      
      <categories>
          
          <category> 矩阵分析 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>深度学习-1-Code:使用Pytorch搭建深度学习模型的基本框架——以COVID-19 Cases Prediction为例</title>
      <link href="/2023/08/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-1-Code-%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A1%86%E6%9E%B6%E2%80%94%E2%80%94%E4%BB%A5COVID-19%20Cases%20Prediction%E4%B8%BA%E4%BE%8B/"/>
      <url>/2023/08/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-1-Code-%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A1%86%E6%9E%B6%E2%80%94%E2%80%94%E4%BB%A5COVID-19%20Cases%20Prediction%E4%B8%BA%E4%BE%8B/</url>
      
        <content type="html"><![CDATA[<h1>使用Pytorch搭建深度学习模型的基本框架</h1><p>  自2012年Alexnet在Imagenet图像分类竞赛中一鸣惊人，以神经网络算法为主体的深度学习技术在人工智能领域兴起，而后诸如卷积神经网络(CNN)，残差网络(Resnet)，生成式对抗网络(GAN)，自注意力模型(Transformer)等众多性能强大的算法模型被提出，使得人工智能领域的研究与应用进入了一个蓬勃发展的阶段。2016年，DeepMind推出围棋人工智能AlphaGo，其以4:1战胜围棋世界冠军李世石，让世人认识到了深度学习技术的强大。2023年，基于Transformer的LLM模型ChatGPT横空出世，这场来自AI领域的技术革命第一次距离我们如此之近。无人知晓深度学习未来能给人类世界带来多大的变革，它能否最终实现强人工智能，带领我们进入科幻电影中的世界，但至少目前，深度学习仍牢牢占据学术与工业界研究的主流。<br>  在众多用于实现深度学习技术的框架中，Pytorch与TensorFlow目前被使用得最为广泛，而Pyotrch以其简洁的语法与强大的生态颇受学者的青睐，成为目前学术研究最流行的深度学习框架，本节会以COVID-19 Cases为例，使用Pytorch搭建一个深度学习模型。这是一个非常简单的回归任务，在完成这个任务的过程中，我们将会了解使用Pytorch搭建深度学习模型的流程、各种函数的作用以及训练模型的步骤。</p><h2 id="Task-Description">Task Description</h2><ul><li><p><strong>Objectives</strong></p><ul><li>Solve a regression problem with deep neural networks(DNN)</li><li>Understand basic DNN training steps.</li><li>Get familiar with PyTorch.</li></ul></li><li><p><strong>Task</strong><br>  <strong>COVID-19 Cases Prediction:</strong> Given survey results in the past 5 days in a specific states in U.S., then predict the percentage of new tested positive cases in the 5th day.</p></li><li><p><strong>Data</strong></p><ul><li>Training Data: 2699 samples</li><li>Testing Data: 1078 samples</li><li>Feature Infactors(117):<ul><li>States(37, encoded to one-hot vector)</li><li>COVID-like illness(4*5)<ul><li>cli、ili …</li></ul></li><li>Behavior Indicators(8*5)<ul><li>wearing_mask、travel_outside_state …</li></ul></li><li>Mental Health Indicators(3*5)<ul><li>anxious、depressed …</li></ul></li><li>Tested Positive Cases(1*5)<ul><li>tested_positive(<strong>this is what we want to predict</strong>)</li></ul></li></ul></li></ul></li></ul><h2 id="Download-Data">Download Data</h2><p>  本案例的数据集来自于Kaggle，可以访问以下网站下载数据集.</p><ul><li>Data Source: <a href="https://www.kaggle.com/competitions/ml2022spring-hw1/overview">https://www.kaggle.com/competitions/ml2022spring-hw1/overview</a></li></ul><h2 id="Import-Packages">Import Packages</h2><p>  在开始任务前首先导入我吗需要使用到的Python库</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Numerical Operations</span><br><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># Reading/Writing Data</span><br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> csv<br><br><span class="hljs-comment"># For Progress Bar</span><br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><br><span class="hljs-comment"># Pytorch</span><br><span class="hljs-keyword">import</span> torch <br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset, DataLoader, random_split<br><br><span class="hljs-comment"># For plotting learning curve</span><br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br></code></pre></td></tr></tbody></table></figure><p>  一些库的主要功能：</p><ul><li><code>tqdm</code>: 用于在循环中显示进度条，以增强用户对程序运行进度的可视化体验。主要功能有<strong>进度条显示、时间估算、定制输出、支持多种数据结构、并发安全.</strong></li><li><code>torch.nn</code>: 用于构建神经网络模型。它提供了各种用于构建神经网络层、损失函数、优化器等的类和函数，使用户能够方便地创建、训练和部署各种类型的神经网络模型。主要功能有<strong>神经网络层的构建、损失函数的定义、优化器、自定义模型、数据转换层、模型的保存和加载.</strong></li><li><code>torch.utils.data</code>: 用于处理和管理数据加载、预处理以及批量处理等任务。它提供了一组工具和类，帮助用户有效地加载、处理和传输数据到神经网络模型中，从而方便地进行训练、验证和测试。主要功能有<strong>数据加载与管理、数据预处理、数据批处理、并行加载、迭代加载.</strong></li><li><code>torch.utils.tensorboard</code>: 用于在训练过程中可视化模型训练和性能指标，可以帮助深度学习研究人员和工程师实时监视、分析和优化他们的模型训练过程。主要功能有<strong>训练过程的可视化、模型结构的可视化、嵌入向量的可视化、分布的可视化、图像和音频的可视化.</strong></li></ul><h2 id="Set-Random-Seed">Set Random Seed</h2><p>  由于深度学习的训练过程包含一定的随机性，例如网络参数初始化、随机梯度下降、Dropout等，在学术研究中为了使得结果可以复现，我们通常需要事先设置随机数种子以固定模型的随机性，其代码如下:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">same_seed</span>(<span class="hljs-params">seed</span>): <br>    <span class="hljs-comment"># Fixes random number generator seeds for reproducibility.</span><br>    torch.backends.cudnn.deterministic = <span class="hljs-literal">True</span><br>    torch.backends.cudnn.benchmark = <span class="hljs-literal">False</span><br>    np.random.seed(seed)<br>    torch.manual_seed(seed)<br>    <span class="hljs-keyword">if</span> torch.cuda.is_available():<br>        torch.cuda.manual_seed(seed)<br>        torch.cuda.manual_seed_all(seed)<br></code></pre></td></tr></tbody></table></figure><p>  一些主要函数的功能：</p><ul><li><code>torch.bankends.cudnn.deterministic</code>: 用于控制在使用CUDA进行深度学习训练时是否启用确定性计算。在深度学习中，训练过程中使用CUDA可以显著加速计算，但由于浮点数的不精确性和优化算法的随机性，相同的代码在不同的运行环境中可能会产生不同的结果。但需要注意的是，启用确定性计算会带来一定的计算效率损失，因为一些优化策略可能会被禁用，从而降低了性能。</li><li><code>torch.backends.cudnn.benchmark</code>: 用于控制CuDNN（CUDA Deep Neural Network library，NVIDIA深度神经网络库）在使用CUDA加速时是否自动寻找最适合当前硬件环境的优化算法。当设置为True时，将会让程序在开始时花费一点额外时间，为整个网络的每个卷积层搜索最适合它的卷积实现算法，进而实现网络的加速，然而，不同的卷积算法可能在计算精度和数值稳定性方面有微小差异，这可能导致每次前向传播的结果略微不同。当设置为False时，CuDNN不再搜寻最佳算法，而是选择一个固定的卷积算法。这个固定的算法在相同的输入数据和参数情况下产生相同的输出，因为它不受运行时的微小变化影响。</li><li><code>torch.manual_seed()</code>: 用于为CPU设置随机数生成器的种子，从而控制生成的随机数序列。</li><li><code>torch.cuda.is_available()</code>: 用于检查当前系统是否支持CUDA，以及是否安装了可用的GPU设备。</li><li><code>torch.cuda.manual_seed()</code>: 为GPU设置随机数种子，从而控制生成的随机数序列。</li><li><code>torch.cuda.manual_seed_all()</code>: 如果使用的是多GPU模型，其可以设置用于在所有GPU上生成随机数的种子。</li></ul><h2 id="Dataset">Dataset</h2><p>  在PyTorch中，我们需要将原始数据集定义为一个Dataset实例，定义Dataset实例的作用是将数据加载和预处理封装成一个可迭代的对象，以便在训练过程中有效地加载和使用数据。其代码如下：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">COVID19Dataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-string">'''</span><br><span class="hljs-string">    x: Features.</span><br><span class="hljs-string">    y: Targets, if none, do prediction.</span><br><span class="hljs-string">    '''</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, x, y=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-keyword">if</span> y <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            self.y = y<br>        <span class="hljs-keyword">else</span>:<br>            self.y = torch.FloatTensor(y)<br>        self.x = torch.FloatTensor(x)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        <span class="hljs-keyword">if</span> self.y <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">return</span> self.x[idx]<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">return</span> self.x[idx], self.y[idx]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.x)<br></code></pre></td></tr></tbody></table></figure><p>  Dataset类中一些函数的功能：</p><ul><li><code>__init__()</code>: Dataset类的构造函数，用于初始化数据集的属性和参数。</li><li><code>__getitem__()</code>: 用于根据索引获取数据集中的一个样本。在训练过程中，DataLoader会通过迭代访问数据集，调用__getitem__来获取样本。</li><li><code>__len__()</code>: 返回数据集中样本的数量，通常通过获取数据的长度来实现。</li></ul><h2 id="Feature-Selection">Feature Selection</h2><p>  原始数据集包含众多的特征，但如果将所有的特征作为训练数据，可能会造成训练时间过长，模型过拟合等问题。实际上特征之间往往存在多重共线性，对于这个任务，我们并不需要数量非常庞大的特征，这时我们需要进行特征工程方面的工作，这里不展开解释特征工程的方法，有兴趣可以自行查阅相关资料。总得来说，当我们的数据集包含众多特征时，我们需要保有进行特征工程的意识。在本案例为了简单起见，省略掉了特征筛选的过程，以人为设置的特征作为筛选结果。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">select_feat</span>(<span class="hljs-params">train_data, valid_data, test_data, select_all=<span class="hljs-literal">True</span></span>):<br>    <span class="hljs-string">'''Selects useful features to perform regression'''</span><br>    y_train, y_valid = train_data[:,-<span class="hljs-number">1</span>], valid_data[:,-<span class="hljs-number">1</span>]<br>    raw_x_train, raw_x_valid, raw_x_test = train_data[:,:-<span class="hljs-number">1</span>], valid_data[:,:-<span class="hljs-number">1</span>], test_data<br><br>    <span class="hljs-keyword">if</span> select_all:<br>        feat_idx = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(raw_x_train.shape[<span class="hljs-number">1</span>]))<br>    <span class="hljs-keyword">else</span>:<br>        feat_idx = [<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>] <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> Select suitable feature columns.</span><br>        <br>    <span class="hljs-keyword">return</span> raw_x_train[:,feat_idx], raw_x_valid[:,feat_idx], raw_x_test[:,feat_idx], y_train, y_valid<br></code></pre></td></tr></tbody></table></figure><h2 id="Dataloader">Dataloader</h2><p>  在定义了Dataset实例后，我们通常需要将Dataset实例传递给Dataloader类。Dataloader是一个迭代器，用于加载和处理训练数据，其可以将数据集划分成小批量的数据，并可以自动进行数据预处理、洗牌和GPU加速等操作。在定义Dataloader时，我们需要确定Training Data、Testing Data、代码如下：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Set seed for reproducibility</span><br>same_seed(config[<span class="hljs-string">'seed'</span>])<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_valid_split</span>(<span class="hljs-params">data_set, valid_ratio, seed</span>):<br>    <span class="hljs-comment">#Split provided training data into training set and validation set</span><br>    valid_set_size = <span class="hljs-built_in">int</span>(valid_ratio * <span class="hljs-built_in">len</span>(data_set)) <br>    train_set_size = <span class="hljs-built_in">len</span>(data_set) - valid_set_size<br>    train_set, valid_set = random_split(data_set, [train_set_size, valid_set_size], generator=torch.Generator().manual_seed(seed))<br>    <span class="hljs-keyword">return</span> np.array(train_set), np.array(valid_set)<br><br><span class="hljs-comment"># train_data size: 2699 x 118 (id + 37 states + 16 features x 5 days) </span><br><span class="hljs-comment"># test_data size: 1078 x 117 (without last day's positive rate)</span><br>train_data, test_data = pd.read_csv(<span class="hljs-string">'./covid.train.csv'</span>).values, pd.read_csv(<span class="hljs-string">'./covid.test.csv'</span>).values<br>train_data, valid_data = train_valid_split(train_data, config[<span class="hljs-string">'valid_ratio'</span>], config[<span class="hljs-string">'seed'</span>])<br><br><span class="hljs-comment"># Print out the data size.</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f"""train_data size: <span class="hljs-subst">{train_data.shape}</span> </span><br><span class="hljs-string">valid_data size: <span class="hljs-subst">{valid_data.shape}</span> </span><br><span class="hljs-string">test_data size: <span class="hljs-subst">{test_data.shape}</span>"""</span>)<br><br><span class="hljs-comment"># Select features</span><br>x_train, x_valid, x_test, y_train, y_valid = select_feat(train_data, valid_data, test_data, config[<span class="hljs-string">'select_all'</span>])<br><br><span class="hljs-comment"># Print out the number of features.</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f'number of features: <span class="hljs-subst">{x_train.shape[<span class="hljs-number">1</span>]}</span>'</span>)<br><br>train_dataset, valid_dataset, test_dataset = COVID19Dataset(x_train, y_train),COVID19Dataset(x_valid, y_valid), COVID19Dataset(x_test)<br><br><span class="hljs-comment"># Pytorch data loader loads pytorch dataset into batches.</span><br>train_loader = DataLoader(train_dataset, batch_size=config[<span class="hljs-string">'batch_size'</span>], shuffle=<span class="hljs-literal">True</span>, pin_memory=<span class="hljs-literal">True</span>)<br>valid_loader = DataLoader(valid_dataset, batch_size=config[<span class="hljs-string">'batch_size'</span>], shuffle=<span class="hljs-literal">True</span>, pin_memory=<span class="hljs-literal">True</span>)<br>test_loader = DataLoader(test_dataset, batch_size=config[<span class="hljs-string">'batch_size'</span>], shuffle=<span class="hljs-literal">False</span>, pin_memory=<span class="hljs-literal">True</span>)  <br></code></pre></td></tr></tbody></table></figure><p>  在这段代码中，我们首先通常前文定义的<code>same_seed()</code>函数固定随机数种子，使得之后的一系列操作的结果可复现。然后我们定义了<code>train_valid_split()</code>函数，这个函数用于将Training Data进行再次划分，分为真正用于训练的数据与用于检验模型泛化能力的数据，通过<code>valid_ratio</code>参数，我们可以设置train data和valid data的划分比例。定义好函数后，我们读取原始csv文件，得到原始的Training Data和Testing Data，利用<code>train_valid_split()</code>函数得到划分好的train data与valid data。此时，这些数据集仍包含着所有的特征，我们需要通过前文定义的<code>select_feat()</code>函数进行特征筛选，得到正在用于本次任务的数据。最后我们使用这些数据定义Dataset实例，并将定义好的Dataset传递给<code>Dataloader</code>用于之后训练模型。</p><p>  <code>Dataloader</code>的一些主要参数及其作用：</p><ul><li><code>dataset</code>: 指定要加载的Dataset实例，这是DataLoader的必需参数，用于提供数据样本。</li><li><code>batch_size</code>: 指定每个批次中包含的样本数量。将数据划分成小批次可以在训练时提高内存的使用效率。</li><li><code>shuffle</code>: 设置为True时，在每次返回一批次前会对数据进行随机洗牌。这有助于提高模型的泛化能力。默认值为False。</li><li><code>pin_memory</code>：如果在GPU上训练，可以将此参数设置为True，以便在加载数据时将数据置于CUDA固定内存中，从而加速数据传输。</li><li><code>drop_last</code>：如果数据样本数量不能整除批次大小，设置为True时，会丢弃最后一个不完整的批次。默认值为False。</li><li><code>num_workers</code>：指定用于数据加载的并行线程数。通过并行加载数据可以加快速度，特别是在数据集较大时。</li></ul><h2 id="Neural-Network-Model">Neural Network Model</h2><p>  准备好数据后，我们便需要构建本次任务所用的深度学习模型，这里我构建了一个简单的三层前馈神经网络模型，这个神经网络包含一个输入层，一个隐藏层，一个输出层，其代码如下:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">My_Model</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_dim</span>):<br>        <span class="hljs-built_in">super</span>(My_Model, self).__init__()<br>        <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> modify model's structure, be aware of dimensions. </span><br>        self.layers = nn.Sequential(<br>            nn.Linear(input_dim, <span class="hljs-number">16</span>),<br>            nn.ReLU(),<br>            nn.Linear(<span class="hljs-number">16</span>, <span class="hljs-number">8</span>),<br>            nn.ReLU(),<br>            nn.Linear(<span class="hljs-number">8</span>, <span class="hljs-number">1</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.layers(x)<br>        x = x.squeeze(<span class="hljs-number">1</span>) <span class="hljs-comment"># (B, 1) -&gt; (B)</span><br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></tbody></table></figure><p>  一些函数及类的主要功能：</p><ul><li><p><code>nn.Module</code>: 提供了一种组织和管理神经网络组件的方式，使得模型的构建、参数管理和前向传播等过程更加简洁和可控。我们在实际应用中定义的模型<code>My_Model</code>需要继承<code>nn.Module</code>类。</p></li><li><p><code>nn.Sequrntial()</code>: PyTorch中的一个模型容器，用于按顺序组合多个神经网络模块，从而构建一个序列式的神经网络模型。它可以在不需要自定义模型类的情况下，方便地定义简单的神经网络结构。</p></li><li><p><code>nn.Linear()</code>: 用于定义线性变换（全连接层）。它将输入数据与权重矩阵相乘，并添加一个偏置，从而实现线性变换。nn.Linear()主要用于神经网络中的全连接层，将输入特征映射到输出特征。<br>  除了使用<code>nn.Linear()</code>进行线性神经网络层，我们还可以根据任务的不同使用诸如卷积神经网络层<code>nn.convXd</code>、循环神经网络层<code>nn.RNN()</code>等。</p></li><li><p><code>nn.ReLU()</code>: 用于实现激活函数 ReLU（Rectified Linear Activation）。ReLU 是深度学习中常用的激活函数之一，它对输入进行非线性变换，将负值变为零，保持正值不变。ReLU 激活函数在神经网络中引入非线性性质，有助于模型学习复杂的特征和表示。<br>  除了ReLU之外，还有一些常用的激活函数，例如<code>nn.Sigmoid()</code>(Sigmoid函数)、<code>nn.Tanh()</code>(双曲正切激活函数)。我们可以根据训练数据的特点，选择合适的激活函数，或者通过实验确定最佳激活函数。</p></li><li><p><code>forward()</code>: 用于将训练数据通过网络进行前向传播。</p></li></ul><h2 id="Training-Loop">Training Loop</h2><p>  在准备好数据与模型后，下一步就是训练模型，训练模型的主要流程如下：</p><figure class="highlight markdown"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs markdown">设置损失函数<br>定义优化器<br>定义tensorboard<br>总训练轮次、当前训练轮次、当前最佳误差、无效训练次数等参数初始化<br><br>for 每一训练轮次 in 总训练轮次：<br><span class="hljs-code">    设置模型为训练模式</span><br><span class="hljs-code">    定义一个空的训练误差列表</span><br><span class="hljs-code">    将训练数据传递给tqdm</span><br><span class="hljs-code">    for 每一批次特征数据、标签数据 in tqdm(训练数据):</span><br><span class="hljs-code">        初始化优化器梯度值</span><br><span class="hljs-code">        将数据移动到GPU中</span><br><span class="hljs-code">        将特征数据输入模型，通过正向传播得到标签数据的预测值</span><br><span class="hljs-code">        通过标签数据的预测值与真实值得到误差</span><br><span class="hljs-code">        误差反向传播，得到误差对于网络参数的梯度</span><br><span class="hljs-code">        利用梯度下降算法更新网络参数</span><br><span class="hljs-code">        step += 1</span><br><span class="hljs-code">        将本批次的误差添加到训练误差列表中</span><br><span class="hljs-code">        自定义训练进度条内容</span><br><span class="hljs-code">    通过对训练误差列表求平均得到这一轮次的平均训练误差</span><br><span class="hljs-code">    将setp与平均训练误差添加到tensorboard中</span><br><span class="hljs-code">    将模型设置为评估模式</span><br><span class="hljs-code">    定义一个空的评估误差列表</span><br><span class="hljs-code">    for 每一批次特征数据、标签数据 in 评估数据:</span><br><span class="hljs-code">        将数据移动到GPU中</span><br><span class="hljs-code">        with 初始化优化器梯度值:</span><br><span class="hljs-code">            将特征数据输入当前模型，通过正向传播得到标签数据的预测值</span><br><span class="hljs-code">            通过标签数据的预测值与真实值得到误差</span><br><span class="hljs-code">        将本批次的误差添加到评估误差列表</span><br><span class="hljs-code">    通过对评估误差列表求平均得到这一轮次的平均评估误差</span><br><span class="hljs-code">    print(当前训练轮次/平均训练误差/平均评估误差)</span><br><span class="hljs-code">    将setp与平均评估误差添加到tensorboard中</span><br><span class="hljs-code">    if 平均评估误差 &lt; 当前最佳误差:</span><br><span class="hljs-code">        将当前最佳误差设置为平均评估误差</span><br><span class="hljs-code">        保存当前模型</span><br><span class="hljs-code">        初始化无效训练次数</span><br><span class="hljs-code">    else:</span><br><span class="hljs-code">        无效训练次数 += 1</span><br><span class="hljs-code">    if 无效训练次数 &gt; 所设置的无效训练次数上限:</span><br><span class="hljs-code">        停止训练</span><br></code></pre></td></tr></tbody></table></figure><p>  这一流程的代码如下：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">trainer</span>(<span class="hljs-params">train_loader, valid_loader, model, config, device</span>):<br><br>    criterion = nn.MSELoss(reduction=<span class="hljs-string">'mean'</span>) <span class="hljs-comment"># Define your loss function, do not modify this.</span><br><br>    <span class="hljs-comment"># Define your optimization algorithm. </span><br>    optimizer = torch.optim.SGD(model.parameters(), lr=config[<span class="hljs-string">'learning_rate'</span>], momentum=<span class="hljs-number">0.9</span>) <br><br>    writer = SummaryWriter() <span class="hljs-comment"># Writer of tensoboard.</span><br><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.isdir(<span class="hljs-string">'./models'</span>):<br>        os.mkdir(<span class="hljs-string">'./models'</span>) <span class="hljs-comment"># Create directory of saving models.</span><br><br>    n_epochs, best_loss, step, early_stop_count = config[<span class="hljs-string">'n_epochs'</span>], math.inf, <span class="hljs-number">0</span>, <span class="hljs-number">0</span><br><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_epochs):<br>        model.train() <span class="hljs-comment"># Set your model to train mode.</span><br>        loss_record = []<br><br>        <span class="hljs-comment"># tqdm is a package to visualize your training progress.</span><br>        train_pbar = tqdm(train_loader, position=<span class="hljs-number">0</span>, leave=<span class="hljs-literal">True</span>)<br><br>        <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> train_pbar:<br>            optimizer.zero_grad()               <span class="hljs-comment"># Set gradient to zero.</span><br>            x, y = x.to(device), y.to(device)   <span class="hljs-comment"># Move your data to device. </span><br>            pred = model(x)             <br>            loss = criterion(pred, y)<br>            loss.backward()                     <span class="hljs-comment"># Compute gradient(backpropagation).</span><br>            optimizer.step()                    <span class="hljs-comment"># Update parameters.</span><br>            step += <span class="hljs-number">1</span><br>            loss_record.append(loss.detach().item())<br>            <br>            <span class="hljs-comment"># Display current epoch number and loss on tqdm progress bar.</span><br>            train_pbar.set_description(<span class="hljs-string">f'Epoch [<span class="hljs-subst">{epoch+<span class="hljs-number">1</span>}</span>/<span class="hljs-subst">{n_epochs}</span>]'</span>)<br>            train_pbar.set_postfix({<span class="hljs-string">'loss'</span>: loss.detach().item()})<br><br>        mean_train_loss = <span class="hljs-built_in">sum</span>(loss_record)/<span class="hljs-built_in">len</span>(loss_record)<br>        writer.add_scalar(<span class="hljs-string">'Loss/train'</span>, mean_train_loss, step)<br><br>        model.<span class="hljs-built_in">eval</span>() <span class="hljs-comment"># Set your model to evaluation mode.</span><br>        loss_record = []<br>        <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> valid_loader:<br>            x, y = x.to(device), y.to(device)<br>            <span class="hljs-keyword">with</span> torch.no_grad():<br>                pred = model(x)<br>                loss = criterion(pred, y)<br><br>            loss_record.append(loss.item())<br>            <br>        mean_valid_loss = <span class="hljs-built_in">sum</span>(loss_record)/<span class="hljs-built_in">len</span>(loss_record)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f'Epoch [<span class="hljs-subst">{epoch+<span class="hljs-number">1</span>}</span>/<span class="hljs-subst">{n_epochs}</span>]: Train loss: <span class="hljs-subst">{mean_train_loss:<span class="hljs-number">.4</span>f}</span>, Valid loss: <span class="hljs-subst">{mean_valid_loss:<span class="hljs-number">.4</span>f}</span>'</span>)<br>        writer.add_scalar(<span class="hljs-string">'Loss/valid'</span>, mean_valid_loss, step)<br><br>        <span class="hljs-keyword">if</span> mean_valid_loss &lt; best_loss:<br>            best_loss = mean_valid_loss<br>            torch.save(model.state_dict(), config[<span class="hljs-string">'save_path'</span>]) <span class="hljs-comment"># Save your best model</span><br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">'Saving model with loss {:.3f}...'</span>.<span class="hljs-built_in">format</span>(best_loss))<br>            early_stop_count = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">else</span>: <br>            early_stop_count += <span class="hljs-number">1</span><br><br>        <span class="hljs-keyword">if</span> early_stop_count &gt;= config[<span class="hljs-string">'early_stop'</span>]:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">'\nModel is not improving, so we halt the training session.'</span>)<br>            <span class="hljs-keyword">return</span><br></code></pre></td></tr></tbody></table></figure><p>  训练流程中一些函数的作用及参数:</p><ul><li><code>nn.MSELoss()</code>: PyTorch 中的一个损失函数模块，用于计算均方误差损失。<br>  常用的一些损失函数: 交叉熵损失<code>nn.CrossEntropyLoss()</code>，负对数似然损失<code>nn.NLLLoss()</code>，KL散度损失<code>nn.KLDivLoss()</code>等，可以更具任务特点与所用模型选择合适的损失函数，例如均方误差损失多用于回归任务，交叉熵损失多用于分类任务，KL散度损失一般用于GAN模型。</li><li><code>torch.optim.SGD()</code>: 用于实现随机梯度下降（Stochastic Gradient Descent，SGD）优化算法。SGD 是深度学习中最基本和常用的优化算法之一，用于调整模型的参数以最小化损失函数。其重要参数:<ul><li><code>params</code>：这是一个模型参数的可迭代对象，指定了需要进行优化的参数。一般通过<code>model.parameters()</code>来获取模型中的参数列表。</li><li><code>lr</code>：学习率，控制参数更新的步长。它决定了每次参数更新的幅度，过大可能导致不稳定的训练，过小可能导致收敛速度缓慢。</li><li><code>momentum</code>: 动量，用于加速梯度下降过程。设置一个介于 0 到 1 之间的值，代表在更新参数时考虑前一次的动量。较大的动量值可以帮助跳出局部最小值。</li></ul></li></ul><p>  除开基础的SGD优化方法，深度学习中还有Adam<code>torch.optim.Adam()</code>，RMSprop<code>torch.optim.RMSprop()</code>，Adagrad<code>torch.optim.Adagrad()</code>等优化算法，它们各种适应不同的数据特点。想进一步了解深度学习中的优化算法可以查阅相关资料。</p><p>  完成模型训练的流程后，下一步便是设置训练步骤中所需要的超参数。</p><h2 id="Configurations">Configurations</h2><p>  设置我们在整个任务中所需要用到的参数：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">device = <span class="hljs-string">'cuda'</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">'cpu'</span><br>config = {<br>    <span class="hljs-string">'seed'</span>: <span class="hljs-number">5201314</span>,      <span class="hljs-comment"># random seed</span><br>    <span class="hljs-string">'select_all'</span>: <span class="hljs-literal">True</span>,   <span class="hljs-comment"># Whether to use all features.</span><br>    <span class="hljs-string">'valid_ratio'</span>: <span class="hljs-number">0.2</span>,   <span class="hljs-comment"># validation_size = train_size * valid_ratio</span><br>    <span class="hljs-string">'n_epochs'</span>: <span class="hljs-number">3000</span>,     <span class="hljs-comment"># Number of epochs.            </span><br>    <span class="hljs-string">'batch_size'</span>: <span class="hljs-number">256</span>, <br>    <span class="hljs-string">'learning_rate'</span>: <span class="hljs-number">1e-5</span>,              <br>    <span class="hljs-string">'early_stop'</span>: <span class="hljs-number">400</span>,    <span class="hljs-comment"># If model has not improved for this many consecutive epochs, stop training.     </span><br>    <span class="hljs-string">'save_path'</span>: <span class="hljs-string">'./models/model.ckpt'</span>  <span class="hljs-comment"># Your model will be saved here.</span><br>}<br></code></pre></td></tr></tbody></table></figure><h2 id="Start-training">Start training!</h2><p>  万事俱备，开始训练我们的模型吧！</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">model = My_Model(input_dim=x_train.shape[<span class="hljs-number">1</span>]).to(device) <span class="hljs-comment"># put your model and data on the same computation device.</span><br>trainer(train_loader, valid_loader, model, config, device)<br></code></pre></td></tr></tbody></table></figure><p>  由于深度学习模型的参数量一般较大，训练可能会花费一定的时间，待训练完成后我们便得到了已更新好参数的神经网络模型。</p><h2 id="Plot-learning-curves-with-tensorboard">Plot learning curves with tensorboard</h2><p>  通过使用<code>tensorboard</code>，我们可以得到损失曲线与学习曲线，便于我们更好地理解模型训练的过程。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">%reload_ext tensorboard<br>%tensorboard --logdir=./runs/<br></code></pre></td></tr></tbody></table></figure><h2 id="Testing">Testing</h2><p>  最后，我们可以将Testing Data输入到已经更新好参数的模型，得到相应标签数据的预测值，我们可以通过比较真实值与预测值之间的差异评价模型训练的结果。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">test_loader, model, device</span>):<br>    model.<span class="hljs-built_in">eval</span>() <span class="hljs-comment"># Set your model to evaluation mode.</span><br>    preds = []<br>    <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> tqdm(test_loader):<br>        x = x.to(device)                        <br>        <span class="hljs-keyword">with</span> torch.no_grad():                   <br>            pred = model(x)                     <br>            preds.append(pred.detach().cpu())   <br>    preds = torch.cat(preds, dim=<span class="hljs-number">0</span>).numpy()  <br>    <span class="hljs-keyword">return</span> preds<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">save_pred</span>(<span class="hljs-params">preds, file</span>):<br>    <span class="hljs-string">''' Save predictions to specified file '''</span><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file, <span class="hljs-string">'w'</span>) <span class="hljs-keyword">as</span> fp:<br>        writer = csv.writer(fp)<br>        writer.writerow([<span class="hljs-string">'id'</span>, <span class="hljs-string">'tested_positive'</span>])<br>        <span class="hljs-keyword">for</span> i, p <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(preds):<br>            writer.writerow([i, p])<br><br>model = My_Model(input_dim=x_train.shape[<span class="hljs-number">1</span>]).to(device)<br>model.load_state_dict(torch.load(config[<span class="hljs-string">'save_path'</span>]))<br>preds = predict(test_loader, model, device) <br>save_pred(preds, <span class="hljs-string">'pred.csv'</span>)      <br></code></pre></td></tr></tbody></table></figure><p>  预测的结果保存在<code>pred.csv</code>文件中。</p><h2 id="Reference">Reference</h2><p><a href="https://www.bilibili.com/video/BV1Wv411h7kN?p=11&amp;vd_source=234cf2ac075a1558881a6956450ddf89">https://www.bilibili.com/video/BV1Wv411h7kN?p=11&amp;vd_source=234cf2ac075a1558881a6956450ddf89</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>最优化理论-2.仿射集与仿射包</title>
      <link href="/2023/08/08/%E6%9C%80%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA-2-%E4%BB%BF%E5%B0%84%E9%9B%86%E4%B8%8E%E4%BB%BF%E5%B0%84%E5%8C%85/"/>
      <url>/2023/08/08/%E6%9C%80%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA-2-%E4%BB%BF%E5%B0%84%E9%9B%86%E4%B8%8E%E4%BB%BF%E5%B0%84%E5%8C%85/</url>
      
        <content type="html"><![CDATA[<h1 id="仿射集与仿射包">仿射集与仿射包</h1><p>  上节讨论了最优化问题的基本形式，其数学表达为：<br><span class="math display">\[min \space\space f_0(x)\]</span></p><p><span class="math display">\[s.t. \space\space f_i(x) \leq b_i,i=1,\dots,m\]</span></p><p>  其中 <span class="math inline">\(x \in \mathbb{R}^n, x =\begin{bmatrix}  x_1,x_2,\dots,x_n \\ \end{bmatrix}^T\)</span>.<br>  最优化问题可以分为凸优化与非凸优化，本博客会首先从凸优化开始讨论。要准确认识何为"凸"，我们需要学习仿射集、凸集、凸函数等概念，接下来我们先从仿射集的概念开始学习。</p><h2 id="仿射集">仿射集</h2><p>  设<span class="math inline">\(x_1,x_2 \in\mathbb{R}^n\)</span>，且<span class="math inline">\(x_1 \neqx_2\)</span>，<br>  经过<span class="math inline">\(x_1,x_2\)</span>两点的直线<span class="math inline">\(y\)</span>的表达式为：<span class="math inline">\(y=\theta x_1 + (1-\theta)x_2, \theta \in\mathbb{R}.\)</span><br>  经过<span class="math inline">\(x_1,x_2\)</span>两点的线段<span class="math inline">\(y\)</span>的表达式为：<span class="math inline">\(y=\theta x_1 + (1-\theta)x_2, \theta \in[0,1].\)</span></p><h3 id="仿射集的定义与性质">仿射集的定义与性质</h3><p><strong>定义</strong><br>  一个集合<span class="math inline">\(C\)</span>是仿射集，若<span class="math inline">\(\forall x_1,x_2 \in C ,\theta \in \mathbb{R},y=\theta x_1 + (1-\theta)x_2 \in C.\)</span></p><ul><li>注1：从定义可以得知，若经过集合<span class="math inline">\(C\)</span>内任意两点<span class="math inline">\(x_1,x_2\)</span>的直线为在集合<span class="math inline">\(C\)</span>内，则集合<span class="math inline">\(C\)</span>为仿射集。<br></li><li>注2：由仿射集的定义可知：在二维空间中，整个二维空间构成一个仿射集，但二维空间中的某一区域则不是仿射集；二维空间中的直线是一个仿射集，但线段不是仿射集。</li></ul><p><strong>性质1：仿射集对仿射组合封闭.</strong><br>  设<span class="math inline">\(\forall x_1,x_2,\dots,x_k \in C,\theta_1,\theta_2,\dots,\theta_k \in \mathbb{R}, \theta_1 + \theta_2 +\dots + \theta_k = 1\)</span>，定义<strong>仿射组合:</strong><br><span class="math display">\[\theta_1x_1+\theta_2x_2+\dots+\theta_kx_k\]</span></p><p>若集合<span class="math inline">\(C\)</span>为仿射集，则仿射组合<span class="math inline">\(\theta_1x_1+\theta_2x_2+\dots+\theta_kx_k\in C\)</span>.</p><p><strong>证明:</strong><br>  借助定义，使用数学归纳法来证明该性质：<br>  当<span class="math inline">\(k=2\)</span>时，由定义1可知：<br><span class="math display">\[集合C是仿射集 \Rightarrow \forall x_1,x_2\in C,\theta \in \mathbb{R},\theta x_1+(1-\theta)x_2 \in C\]</span></p><p>  当<span class="math inline">\(k=t\)</span>时，假设该结论仍然成立，即：</p><p><span class="math display">\[集合C是仿射集 \Rightarrow \forallx_1,x_2,\dots,x_t \in C,\theta_1,\theta_2,\dots,\theta_t \in \mathbb{R},\sum_{i=1}^{t}\theta_i=1, 有\sum_{i=1}^{t}\theta_ix_i \in C\]</span></p><p>  当<span class="math inline">\(k=t+1\)</span>时，需要证明以下结论成立：</p><p><span class="math display">\[集合C是仿射集 \Rightarrow \forallx_1,x_2,\dots,x_{t+1} \in C,\theta_1,\theta_2,\dots,\theta_{t+1} \in\mathbb{R}, \sum_{i=1}^{t+1}\theta_i=1, 有\sum_{i=1}^{t+1}\theta_ix_i\in C\]</span></p><p>  已知集合<span class="math inline">\(C\)</span>是仿射集，<span class="math inline">\(\forall x_1,x_2,\dots,x_{t+1} \inC,\theta_1,\theta_2,\dots,\theta_{t+1} \in \mathbb{R},\sum_{i=1}^{t+1}\theta_i=1\)</span>,<br>  令 <span class="math inline">\(\beta_i =\theta_i/\sum_{i=1}^{t}\theta_i, i=1,\dots t\)</span>，则有<span class="math inline">\(\sum_{i=1}^{t}\beta_i = 1\)</span>.<br>  由前文的结论可知：<span class="math inline">\(\sum_{i=1}^{t}\beta_ix_i\in C\)</span>.<br>  令<span class="math inline">\(y = \sum_{i=1}^{t}{\beta_ix_i} \in C,x_{t+1} \in C\)</span>，由前文的结论可知: <span class="math inline">\((\sum_{i=1}^{t}\theta_i)y +(1-\sum_{i=1}^{t}\theta_i)x_{t+1} \in C\)</span>.<br>  即: <span class="math inline">\(\sum_{i=1}^{t+1}\theta_ix_i \inC\)</span>，证毕.</p><p><strong>性质2：与仿射集相关的子空间也是仿射集.</strong><br>  设集合<span class="math inline">\(C\)</span>是仿射集，对<span class="math inline">\(\forall x_0 \in C\)</span>，令<span class="math inline">\(V=C-x_0= \{ x-x_0 \vert x \in C\}\)</span>，称集合<span class="math inline">\(V\)</span>为与<span class="math inline">\(C\)</span>相关的子空间.<br>  集合<span class="math inline">\(V\)</span>有以下性质：<br>  (1) <span class="math inline">\(V\)</span>为仿射集.<br>  (2) 记<span class="math inline">\(v_1,v_2 \in V\)</span>，则有<span class="math inline">\(\alpha v_1+\beta v_2 \in V, \alpha,\beta \in\mathbb{R}.\)</span><br>  (3) 集合V一定包含原点<span class="math inline">\(\mathbf{0}\)</span>.</p><p><strong>证明:</strong><br>  记<span class="math inline">\(v_1 ,v_2 \in V, \theta_1,\theta_2 \in\mathbb{R},\theta_1+\theta_2 = 1.\)</span><br>  设<span class="math inline">\(v_1 = x_1 - x_0, v_2 = x_2 - x_0,x_1,x_2 \in C.\)</span><br>  <span class="math inline">\(\theta_1v_1 + \theta_2v_2 =\theta_1(x_1-x_0)+\theta_2(x_2-x_0) = \theta_1x_1 + \theta_2x_2 -x_0\)</span><br>  <span class="math inline">\(\because C\)</span>是仿射集，且<span class="math inline">\(\theta_1+\theta_2=1\)</span>，<span class="math inline">\(\therefore \theta_1x_1+\theta_2x_2 \inC\)</span>，令<span class="math inline">\(x_3 = \theta_1x_1+\theta_2x_2,x_3 \in C\)</span>.<br>  则 <span class="math inline">\(\theta_1v_1+\theta_2v_2 =\theta_1x_1+\theta_2x_2 - x_0 = x_3 - x_0 \in V, \theta_1 + \theta_2 =1.\)</span><br>  故集合<span class="math inline">\(V\)</span>是仿射集，性质(1)证毕.<br>  设<span class="math inline">\(\alpha, \beta \in\mathbb{R},\)</span><br>  <span class="math inline">\(\alpha v_1 + \beta v_2 \in V\Leftrightarrow \alpha v_1 + \beta v_2 +x_0 = x, x \in C \Leftrightarrow\alpha v_1 + \beta v_2 + x_0 \in C.\)</span><br>  <span class="math inline">\(\alpha v_1 + \beta v_2 +x_0 =\alpha(v_1+x_0)+\beta(v_2+x_0)+(1-\alpha-\beta)x_0=\alpha x_1 + \betax_2 + (1-\alpha-\beta)x_0.\)</span><br>  由仿射集的性质1可知: 仿射组合 <span class="math inline">\(\alpha x_1 +\beta x_2 + (1-\alpha-\beta)x_0 \in C.\)</span><br>  故有<span class="math inline">\(\alpha v_1 + \beta v_2 \inV\)</span>，性质(2)证毕.<br>  由集合V的定义易证性质(3)成立.</p><h3 id="仿射集的实例">仿射集的实例</h3><p><strong>例：线性方程组的解集是仿射集</strong></p><p>  设有线性方程组 <span class="math inline">\(Ax=b, A \in\mathbb{R}^{m\times n}, b \in \mathbb{R}^m, x\in \mathbb{R}^n. 其解集C=\{ x \vert Ax=b \}\)</span>，则集合<span class="math inline">\(C\)</span>是仿射集.</p><p><strong>证明:</strong><br>  对<span class="math inline">\(\forall x_1,x_2 \in C\)</span>，有 <span class="math inline">\(Ax_1=b, Ax_2 = b.\)</span><br>  若 <span class="math inline">\(\theta x_1 + (1-\theta)x_2 \in C,\theta \in \mathbb{R}\)</span>，则集合<span class="math inline">\(C\)</span>是仿射集.<br>  <span class="math inline">\(\theta x_1 + (1-\theta)x_2 \in C\Leftrightarrow \theta x_1 + (1-\theta)x_2 = x, x \in C \LeftrightarrowA(\theta x_1 + (1-\theta)x_2) = Ax = b.\)</span><br>  <span class="math inline">\(A(\theta x_1 + (1-\theta)x_2) = \thetaAx_1 + (1-\theta)Ax_2 = \theta b + (1-\theta)b = b\)</span><br>  故有<span class="math inline">\(\theta x_1 + (1-\theta)x_2 \inC\)</span>，解集<span class="math inline">\(C\)</span>是仿射集.</p><ul><li><strong>注：与线性方程组解集C相关的子空间V是系数矩阵A的核</strong></li></ul><p><strong>证明:</strong><br>  设集合 <span class="math inline">\(V= \{ x-x_0 \vert x \in C \},\forall x_0 \in C\)</span>，则集合V是与C相关的子空间.<br>  由<span class="math inline">\(Ax = b \Rightarrow V= \{ x-x_0 \vertAx=b \}\)</span>.<br>  由<span class="math inline">\(Ax_0 = b \Rightarrow V = \{ x-x_0 \vertAx=Ax_0 \} = \{ x-x_0 \vert A(x-x_0)=0 \}.\)</span><br>  令<span class="math inline">\(y = x-x_0， V= \{ y \vert Ay = 0\}\)</span>，即<span class="math inline">\(V= ker \spaceA\)</span>，证毕.</p><h2 id="仿射包">仿射包</h2><h3 id="仿射包的定义">仿射包的定义</h3><p>  对任意集合<span class="math inline">\(C\)</span>，记：</p><p><span class="math display">\[aff \space C = \{\theta_1x_1+\dots+\theta_kx_k \vert \forall x_1,\dots,x_k \in C,\theta_i \in \mathbb{R}, \sum_{i=1}^{k}\theta_i=1 \}\]</span></p><p>则集合<span class="math inline">\(aff \space C\)</span>称为集合<span class="math inline">\(C\)</span>的仿射包.</p><p><font color="green">注：仿射包<span class="math inline">\(aff\spaceC\)</span>是集合<span class="math inline">\(C\)</span>所能构造的最小的仿射集，是集合<span class="math inline">\(C\)</span>中元素的仿射组合的集合.</font></p><h3 id="仿射包的实例">仿射包的实例</h3><p><strong>例：二维空间中的仿射包</strong></p><p>  设集合<span class="math inline">\(V\)</span>为二维平面，<span class="math inline">\(x_1,x_2,x_3\)</span>为二维平面中不在同一条直线上的三个点，<span class="math inline">\(L\)</span>为经过点<span class="math inline">\(x_1,x_2\)</span>的直线，如下图所示：</p><center><img src="https://s2.loli.net/2023/08/13/mh4IpMxoERZ3sy9.png" width="75%"><div data-align="center">Image1: 二维平面及其元素</div></center><p><br></p><p>  设集合<span class="math inline">\(C_1 = \{ x_1,x_2\}\)</span>，则集合<span class="math inline">\(C_1\)</span>的仿射包<span class="math inline">\(aff \space C_1 = \{ \theta_1x_1+\theta_2x_2 \vertx_1,x_2 \in C_1, \theta_1,\theta_2 \in \mathbb{R}, \theta_1+\theta_2=1\}\)</span>，仿射包<span class="math inline">\(aff \spaceC_1\)</span>即为直线<span class="math inline">\(L\)</span>.<br>  设集合<span class="math inline">\(C_2 = \{ x_1,x_2,x_3\}\)</span>，则集合<span class="math inline">\(C_2\)</span>的仿射包<span class="math inline">\(aff \space C_2 = \{\theta_1x_1+\theta_2x_2+\theta_3x_3 \vert x_1,x_2,x_3 \in C_2,\theta_1,\theta_2,\theta_3 \in \mathbb{R}, \theta_1+\theta_2+\theta_3 =1 \}\)</span>，仿射包<span class="math inline">\(aff \spaceC_2\)</span>即为二维平面<span class="math inline">\(V\)</span>.</p>]]></content>
      
      
      <categories>
          
          <category> 最优化理论 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>最优化理论-1.最优化问题的基本形式</title>
      <link href="/2023/08/03/%E6%9C%80%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA-1-%E6%9C%80%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%BD%A2%E5%BC%8F/"/>
      <url>/2023/08/03/%E6%9C%80%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA-1-%E6%9C%80%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%BD%A2%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<h1 id="最优化问题的基本形式">最优化问题的基本形式</h1><h2 id="最优化问题的定义">最优化问题的定义</h2><p>  从一个可行解的集合<span class="math inline">\(S\)</span>中，求出针对某一问题的最优的解<span class="math inline">\(X^*\)</span>。</p><h2 id="最优化问题的基本形式-1">最优化问题的基本形式</h2><p>  最优化问题的基本形式(1)：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{x} \quad &amp; f(x) \\        s.t. \quad &amp; m_{i}(x) \leq 0,\quad i=1,2,\dots,M \\        &amp; n_{j}(x) = 0,\quad j=1,2,\dots,N    \end{split}\end{equation}\]</span></p><ul><li><p><span class="math inline">\(x=\begin{bmatrix}  x_1,x_2,\dots,x_n\end{bmatrix}^{T},x \in \mathbb{R}^{n}\)</span>称为<strong>优化变量(Optimization Variable)</strong>.<br></p></li><li><p><span class="math inline">\(f: \mathbb{R}^{n} \rightarrow\mathbb{R}\)</span>，称为<strong>目标函数(ObjectiveFunction)</strong>.<br></p></li><li><p><span class="math inline">\(m_i: \mathbb{R}^{n} \rightarrow\mathbb{R}\)</span>，称为<strong>不等式约束(InequalityConstraint)</strong>.<br></p></li><li><p><span class="math inline">\(n_j: \mathbb{R}^{n} \rightarrow\mathbb{R}\)</span>，称为<strong>等式约束(EqualityConstraint)</strong>.<br></p></li><li><p><span class="math inline">\(D = \left( dom \space f \right)\bigcap \{x \vert m_{i}(x) \leq 0,i=1,2,\dots,M\} \bigcap \{x \verth_{j}(x) =0,j=1,2,\dots,N\}\)</span>，称为最优化问题的<strong>域(Domain)</strong>，<span class="math inline">\(x \in D\)</span> 称为<strong>可行解(FeasibleSolution)</strong>.</p></li><li><p><span class="math inline">\(p^{*} = \inf \{ f(x) \vert x \in D\}\)</span>，称为最优化问题的<strong>最优值(Optimum)</strong>.<br></p></li><li><p><span class="math inline">\(f(x^{*})=p^{*}\)</span>，称<span class="math inline">\(x^{*}\)</span>为最优化问题的<strong>最优解(OptimalSolution)</strong>.<br></p></li><li><p><span class="math inline">\(X_{opt} = \{ x \vert x \inD,f(x)=p^{*} \}\)</span>，称为最优化问题的<strong>最优解集(OptimaSet)</strong>.</p></li><li><p>注：有时最优解不止一个，最优解的集合称为最优解集。</p></li></ul><h2 id="最优化问题的分类">最优化问题的分类</h2><p>  根据最优化问题的目标函数、可行解的不同，可以将最优化问题分为多种类别，以下是一些比较常见的分类：</p><ul><li><p><strong>线性规划/非线性规划</strong><br>  若目标函数<span class="math inline">\(f_0\)</span>与不等式约束<span class="math inline">\(f_i\)</span>均为线性函数，则该最优化问题称为线性规划问题，否则即为非线性规划问题。<br>  函数<span class="math inline">\(f\)</span>为线性函数的充要条件：<span class="math inline">\(f(\alpha x + \beta y) = \alpha f(x) + \betaf(y)\)</span>.</p></li><li><p><strong>凸优化/非凸优化</strong><br>  若目标函数<span class="math inline">\(f_0\)</span>为凸函数，且可行解<span class="math inline">\(S\)</span>为凸集，则该最优化问题称为凸优化问题。<br>  集合<span class="math inline">\(C\)</span>为凸集的充要条件：<span class="math inline">\(\forall x_1,x_2 \in C, \forall \theta \in\mathbb{R}, \theta x_1 + (1-\theta)x_2 \in C\)</span>.<br>  函数<span class="math inline">\(f\)</span>为凸函数的充要条件: <span class="math inline">\(dom f\)</span>为凸集，且<span class="math inline">\(\forall x_1,x_2 \in dom f, 0 \leq \theta \leq1\)</span>，有以下不等式成立：<br><span class="math display">\[f(\theta x_1 + (1-\theta)x_2) \leq \thetaf(x_1) + (1-\theta)f(x_2)\]</span></p></li><li><p><strong>连续变量优化/离散变量优化</strong><br>  若可行解<span class="math inline">\(S\)</span>是连续的，则该最优化问题为连续变量优化问题；若可行解<span class="math inline">\(S\)</span>是离散的，则该最优化问题称为离散变量优化问题。<br>  离散变量优化问题中比较常见的为可行解均为整数的整数变量优化。</p></li><li><p><strong>单目标优化/多目标优化</strong><br>  若目标函数<span class="math inline">\(f_0\)</span>是唯一的，则为单目标优化问题；若有多个目标函数<span class="math inline">\(f_{0}^{1},f_{0}^{2},\dots,f_{0}^{r}\)</span>，则为多目标优化问题。</p></li><li><p><strong>无约束问题/等式约束问题/不等式约束问题</strong><br>  若最优化问题没有约束条件，只有目标函数，则称为无约束问题；若最优化问题的约束条件是一些等式，则称为等式约束问题；若最优化问题的约束条件是一些不等式，则称为不等式约束问题。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 最优化理论 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>矩阵分析-2.向量组与线性相关性</title>
      <link href="/2023/08/01/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-2-%E5%90%91%E9%87%8F%E7%BB%84%E4%B8%8E%E7%BA%BF%E6%80%A7%E7%9B%B8%E5%85%B3%E6%80%A7/"/>
      <url>/2023/08/01/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-2-%E5%90%91%E9%87%8F%E7%BB%84%E4%B8%8E%E7%BA%BF%E6%80%A7%E7%9B%B8%E5%85%B3%E6%80%A7/</url>
      
        <content type="html"><![CDATA[<h1 id="向量组与线性相关性">向量组与线性相关性</h1><h2 id="向量组">向量组</h2><h3 id="向量组的定义">向量组的定义</h3><p>  设<span class="math inline">\(V\)</span>是数域<span class="math inline">\(\mathbb{F}\)</span>上的线性空间，<span class="math inline">\(V\)</span>中有限序列<span class="math inline">\(\alpha_1,\alpha_2,\dots,\alpha_n\)</span>称为<span class="math inline">\(V\)</span>中的一个向量组，记为向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_n\}\)</span>。</p><h3 id="向量组所拼成的抽象矩阵">向量组所拼成的抽象矩阵</h3><p>  若矩阵<span class="math inline">\(A\)</span>中的每一列是由向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_n\}\)</span>所构成的，则称矩阵<span class="math inline">\(A\)</span>是由向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_n\}\)</span>所拼成的抽象矩阵，记为：</p><p><span class="math display">\[A = \begin{bmatrix}    \alpha_1,\alpha_2,\dots,\alpha_n \\\end{bmatrix}\]</span></p><h2 id="向量组的线性相关性">向量组的线性相关性</h2><h3 id="线性相关与线性无关的定义">线性相关与线性无关的定义</h3><p>  向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>称为线性相关，如果存在不全为零的<span class="math inline">\(P\)</span>个数<span class="math inline">\(k_i \in\mathbb{F}, i=1,2,\dots,p\)</span>，使得：</p><p><span class="math display">\[\begin{equation}\\alpha_1k_1+\alpha_2k_2+\dots+\alpha_pk_p=0\end{equation}\]</span></p><p>  向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>称为线性无关，如果任意不全为零的<span class="math inline">\(P\)</span>个数<span class="math inline">\(k_i \in\mathbb{F}, i=1,2,\dots,p\)</span>，使得：</p><p><span class="math display">\[\begin{equation}\\alpha_1k_1+\alpha_2k_2+\dots+\alpha_pk_p \ne 0\end{equation}\]</span></p><ul><li><strong>注1</strong> 线性无关的另外一种表述：<br>  向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>称为线性无关，若<span class="math inline">\(\alpha_1k_1+\alpha_2k_2+\dots+\alpha_pk_p =0\)</span> 当且仅当 <span class="math inline">\(k_i=0,i=1,\dots,p\)</span> 时成立。</li></ul><h3 id="线性相关性的矩阵表述">线性相关性的矩阵表述</h3><p>  设向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_n\}\)</span>所拼成的抽象矩阵为<span class="math inline">\(A\)</span>，向量<span class="math inline">\(x =\begin{bmatrix}  x_1,x_2,\dots,x_p\end{bmatrix}^T\)</span>，有齐次线性方程：</p><p><span class="math display">\[\begin{equation}    Ax=\begin{bmatrix}    \alpha_1,\alpha_2,\dots,\alpha_p \\\end{bmatrix}\begin{bmatrix}    x_1 \\    x_2 \\    \vdots \\    x_p\end{bmatrix}=0\end{equation}\]</span></p><p><span style="color: green;">向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>线性相关<span class="math inline">\(\Leftrightarrow\)</span>齐次线性方程(3)有非零解。</span><br><span style="color: green;">向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>线性无关<span class="math inline">\(\Leftrightarrow\)</span>齐次线性方程(3)只有零解。</span></p><h2 id="向量组之间的线性表示">向量组之间的线性表示</h2><h3 id="线性表示的定义">线性表示的定义</h3><p>  设有向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>，<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_q\}\)</span>以及向量<span class="math inline">\(\beta\)</span>。<br>  称向量<span class="math inline">\(\beta\)</span>可由向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>线性表示，如果<span class="math inline">\(\exists k_1,k_2,\dots,k_p \in\mathbb{F}\)</span>，使得：<br><span class="math display">\[\begin{equation}   \beta = \alpha_1k_1+\alpha_2k_2+\dots+\alpha_pk_p\end{equation}\]</span></p><p>  称向量组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_q\}\)</span>可由向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>线性表示，如果每个<span class="math inline">\(\beta_i\)</span>均可由向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>线性表示。</p><h3 id="线性表示的矩阵表述">线性表示的矩阵表述</h3><p>  设向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>所拼成的抽象矩阵为<span class="math inline">\(A\)</span>，向量组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_q\}\)</span>所拼成的抽象矩阵为<span class="math inline">\(B\)</span>,有非齐次线性方程：<br><span class="math display">\[\begin{equation}    Ax=\begin{bmatrix}        \alpha_1,\alpha_2,\dots,\alpha_p    \end{bmatrix}\begin{bmatrix}        x_1 \\        x_2 \\        \vdots \\        x_p    \end{bmatrix} = \beta\end{equation}\]</span></p><p><span class="math display">\[\begin{equation}    AX= \begin{bmatrix}        \alpha_1,\alpha_2,\dots,\alpha_p    \end{bmatrix}\begin{bmatrix}        x_{11} &amp; x_{12} &amp; \dotsb &amp; x_{1q} \\        x_{21} &amp; x_{22} &amp; \dotsb &amp; x_{2q} \\        \vdots &amp; \vdots &amp;        &amp; \vdots \\        x_{p1} &amp; x_{p2} &amp; \dotsb &amp; x_{pq} \\    \end{bmatrix} = \begin{bmatrix}        \beta_1,\beta_2,\dots,\beta_q    \end{bmatrix}=B\end{equation}\]</span></p><p><span style="color: green;">向量<span class="math inline">\(\beta\)</span>可由向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>线性表示<span class="math inline">\(\Leftrightarrow\)</span>非齐次线性方程(5)有解。</span><br><span style="color: green;">向量组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_q\}\)</span>可由向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>线性表示<span class="math inline">\(\Leftrightarrow\)</span>非齐次线性方程组(6)有解。</span></p><h3 id="线性表示的传递性">线性表示的传递性</h3><p>  设有三个向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>、<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_q\}\)</span>、<span class="math inline">\(\{v_1,v_2,\dots,v_t\}\)</span>，其所拼成抽象矩阵分别为<span class="math inline">\(A、B、C\)</span>。<br>  若向量组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_q\}\)</span>可由向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>线性表示，而向量组<span class="math inline">\(\{v_1,v_2,\dots,v_t\}\)</span>可由向量组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_q\}\)</span>线性表示，则向量组<span class="math inline">\(\{v_1,v_2,\dots,v_t\}\)</span>可由向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>线性表示。</p><p><strong>证明：</strong><br>向量组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_q\}\)</span>可由向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>线性表示<span class="math inline">\(\Rightarrow\)</span> 矩阵方程 <span class="math inline">\(AX_{p\times q}=B\)</span> 有解<br>向量组<span class="math inline">\(\{v_1,v_2,\dots,v_t\}\)</span>可由向量组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_q\}\)</span>线性表示<span class="math inline">\(\Rightarrow\)</span> 矩阵方程 <span class="math inline">\(BY_{q\times t}=C\)</span> 有解</p><p><span class="math display">\[\left\{             \begin{array}{lr}             AX_{p\times q}=B \\             BY_{q \times t}=C             \end{array}\right. \Rightarrow AXY=C \Rightarrow 矩阵方程AZ_{p \times t} = C有解，Z_{p \times t}=XY\]</span></p><p>故向量组<span class="math inline">\(\{v_1,v_2,\dots,v_t\}\)</span>可由向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>线性表示。</p><h2 id="向量组的极大线性无关组">向量组的极大线性无关组</h2><h3 id="极大线性无关组的定义">极大线性无关组的定义</h3><p>  设向量组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_s\}\)</span>是向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>的子组，即<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_s\} \subseteq\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>。子组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_s\}\)</span>称为向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>的极大线性无关组，若其满足：<br>1. 子组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_s\}\)</span>线性无关。<br>2. 若向量组<span class="math inline">\(\{v_1,v_2,\dots,v_t\}\)</span>也是向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>的子组，且<span class="math inline">\(s &lt; t\)</span>，则子组<span class="math inline">\(\{v_1,v_2,\dots,v_t\}\)</span>线性相关。</p><ul><li><strong>注：“极大性”的另一种说法：“生成性”。</strong></li></ul><p>  若子组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_s\}\)</span>线性无关，且<span class="math inline">\(\forall \alpha_i \in\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>，<span class="math inline">\(\alpha_i\)</span>都可由子组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_s\}\)</span>线性表示，则子组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_s\}\)</span>是向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>的极大线性无关组。</p><p><span class="math display">\[\color{green} 极大线性无关组\Leftrightarrow 线性无关生成组\]</span></p><h3 id="向量个数的唯一性">向量个数的唯一性</h3><p><strong>定理：</strong>向量组的极大线性无关组所含向量的个数是唯一的。<br><strong>证明：</strong><br>  设有向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>，向量组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_s\}\)</span>与<span class="math inline">\(\{v_1,v_2,\dots,v_t\}\)</span>均是其极大线性无关组，<br>  要证明 $ s=t $，可以利用反证法，假设 <span class="math inline">\(s&lt; t\)</span>，<br>  设 <span class="math inline">\(A =\begin{bmatrix}  \alpha_1,\alpha_2,\dots,\alpha_p\end{bmatrix}\)</span>，<span class="math inline">\(B=\begin{bmatrix}  \beta_1,\beta_2,\dots,\beta_s\end{bmatrix}\)</span>，<span class="math inline">\(C =\begin{bmatrix}  v_1,v_2,\dots,v_t \end{bmatrix}\)</span>，<br>  <span class="math inline">\(\because\)</span> 向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>可由其极大线性无关组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_s\}\)</span>、<span class="math inline">\(\{v_1,v_2,\dots,v_t\}\)</span>线性表示，<br>  <span class="math inline">\(\therefore\)</span> 矩阵方程 <span class="math inline">\(BX=A, AY=C\)</span>均有解，<br>  <span class="math inline">\(\Rightarrow\)</span> 矩阵方程 <span class="math inline">\(BZ=C\)</span> 有解，其中<span class="math inline">\(Z_{s \times t}=XY\)</span>，<br>  $ s &lt; t$ <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(rank(Z) &lt; t\)</span> <span class="math inline">\(\Rightarrow\)</span> 矩阵方程<span class="math inline">\(ZW= \textbf{0}\)</span>有无穷多解 <span class="math inline">\(\Rightarrow\)</span> 矩阵方程<span class="math inline">\(ZW= \textbf{0}\)</span>必有非零解，<br>  设矩阵方程 <span class="math inline">\(ZW= \textbf{0}\)</span>的非零解为<span class="math inline">\(W\)</span>，将矩阵方程 <span class="math inline">\(BZ=C\)</span> 的两边同时乘以<span class="math inline">\(W\)</span>，<br>  有 <span class="math inline">\(B(ZW) = CW, \becauseZW=\textbf{0}\)</span>， <span class="math inline">\(\therefore CW=\textbf{0}\)</span>，<br>  与向量组<span class="math inline">\(\{v_1,v_2,\dots,v_t\}\)</span>线性无关矛盾，<br>  <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(s \nless t\)</span>，同理可证 <span class="math inline">\(s \ngtr t\)</span>，故有 <span class="math inline">\(s = t\)</span></p><h2 id="向量组的秩">向量组的秩</h2><p>  <span style="color: green;">向量组的秩等于向量组的极大线性无关组所含向量的个数。</span></p>]]></content>
      
      
      <categories>
          
          <category> 矩阵分析 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>最优化理论-0.引言</title>
      <link href="/2023/07/28/%E6%9C%80%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA-0-%E5%BC%95%E8%A8%80/"/>
      <url>/2023/07/28/%E6%9C%80%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA-0-%E5%BC%95%E8%A8%80/</url>
      
        <content type="html"><![CDATA[<h1 id="引言">引言</h1><p>  最优化理论(OptimizationTheory)是一门研究在给定条件下如何寻找最优解的学科。这个学科涵盖了广泛的数学方法和算法，用于解决机器学习、经济学、工程学等领域的各种实际问题。<br>  如今，人工智能与机器学习领域的研究十分火热，而最优化理论是在其中起着至关重要的作用的一门数学学科。在统计机器学习中，很多问题的实质都能用一个最优化模型表示，例如线性判别分析(LinearDiscriminantAnalysis)的思想是将样本从高维空间投影到低维空间，使得不同类别样本之间的距离尽可能大，同一类别样本之间的距离尽可能小，从而完成样本分类的目标。支持向量机(SupportVectorMachine)的思想是在特征空间中找到一个超平面，使得不同类别的样本尽可能远离该超平面，从而实现分类。这些思想都可以用最优化模型表示，且都是一个凸优化问题，可以利用凸优化的方法得到最优解。在深度学习中，我们要通过最小化损失函数得到神经网络的权重参数，这实际上也是一个最优化问题。由于损失函数大多是非凸函数，我们通常使用梯度下降算法来求得近似最优解。<br>  总得来说，最优化理论在当今是一门非常有用的学科，笔者认为对于统计与数据科学领域的学生来说，学好最优化理论是未来开展研究的基础。</p><h2 id="最优化理论的主要内容">最优化理论的主要内容</h2><p>  最优化理论的主要内容包括以下几个方面：</p><ul><li><strong>最优化问题的表述</strong>:最优化问题通常由目标函数和约束条件构成。目标函数是需要最大化或最小化的函数，而约束条件是问题的限制条件。<br></li><li><strong>最优解的定义</strong>:最优解是指满足约束条件的使得目标函数取得最大值或最小值的变量值。<br></li><li><strong>凸优化</strong>:凸优化是一类特殊的最优化问题，其中目标函数是凸函数，约束条件是凸集。凸优化问题具有良好的性质，可以高效地求解。<br></li><li><strong>等式约束与不等式约束</strong>:最优化问题的约束条件可以是等式约束，也可以是不等式约束。等式约束将变量限制在一个子空间内，而不等式约束则将变量限制在一个半空间内。<br></li><li><strong>无约束优化</strong>:在无约束优化问题中，只需考虑目标函数的最大化或最小化，没有额外的约束条件。<br></li><li><strong>数值优化方法</strong>:为了求解最优化问题，需要使用各种数值优化方法。常见的数值优化方法包括梯度下降法、牛顿法、共轭梯度法、拟牛顿法等。<br></li><li><strong>条件极值与全局极值</strong>:最优化问题可能存在多个极值点，包括局部极值和全局极值。局部极值是在某个特定区域内的最优解，而全局极值是在整个定义域内的最优解。<br></li><li><strong>敏感性分析</strong>:最优化理论还涉及敏感性分析，即研究目标函数和约束条件中参数的微小变化对最优解的影响。<br></li><li><strong>最优化理论在实际问题中的应用</strong>:最优化理论广泛应用于各个领域，如机器学习中的模型训练、工程中的设计优化、经济学中的资源分配问题等。</li></ul><p>  最优化问题可分为凸优化问题与非凸优化问题，本系列首先会以凸优化为主展开讨论，之后再补充非凸优化的相关方法。</p><h2 id="相关学习资料">相关学习资料</h2><p>  本系列主要参考了以下学习资料:</p><ul><li><a href="https://www.bilibili.com/video/BV19M411T7S7/?vd_source=234cf2ac075a1558881a6956450ddf89">Video:《凸优化》- 凌青 中科大</a><br></li><li><a href="https://github.com/SXUSongJH/Book">Book1:《最优化：建模、算法与理论》- 刘浩洋等</a><br></li><li><a href="https://github.com/SXUSongJH/Book">Book2：《ConvexAnalysis》- Stephen Boyd</a></li></ul><p>  本系列首先会参考凌青老师的课程，介绍凸优化的基础知识，然后以另外两本书作为补充，介绍一些课程中没有提及的知识以及非凸优化的方法。</p>]]></content>
      
      
      <categories>
          
          <category> 最优化理论 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>矩阵分析-1.线性空间</title>
      <link href="/2023/07/26/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-1-%E7%BA%BF%E6%80%A7%E7%A9%BA%E9%97%B4/"/>
      <url>/2023/07/26/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-1-%E7%BA%BF%E6%80%A7%E7%A9%BA%E9%97%B4/</url>
      
        <content type="html"><![CDATA[<h1 id="线性空间">线性空间</h1><h2 id="加法与数乘">加法与数乘</h2><p><strong>加法的定义</strong><br>  给定非空集合V和数域<span class="math inline">\(\mathbb{F}\)</span>，若存在映射：<br><span class="math display">\[\sigma: V \times V \rightarrowV\]</span></p><p><span class="math display">\[(\alpha,\beta) \mapsto\sigma(\alpha,\beta)\]</span></p><p>即对V中任意元素<span class="math inline">\(\alpha\)</span>，<span class="math inline">\(\beta\)</span>，在集合V中都存在唯一元素<span class="math inline">\(\gamma\)</span>,使得<span class="math inline">\(\gamma=\alpha+\beta \in V\)</span>，则称映射<span class="math inline">\(\sigma\)</span>为集合V上的加法。</p><p><strong>数乘的定义</strong><br>  给定非空集合V和数域<span class="math inline">\(\mathbb{F}\)</span>，若存在映射： <span class="math display">\[\sigma: V \times \mathbb{F} \rightarrowV\]</span></p><p><span class="math display">\[(\alpha,k) \mapsto\sigma(\alpha,k)\]</span></p><p>即对集合V中的任意元素<span class="math inline">\(\alpha\)</span>和数域<span class="math inline">\(\mathbb{F}\)</span>中任意元素k，在集合V中都存在唯一元素<span class="math inline">\(\gamma\)</span>，使得<span class="math inline">\(\gamma=\alpha k \in V\)</span>，则称映射<span class="math inline">\(\sigma\)</span>为集合V上的数乘。</p><ul><li><p><strong>注1：关于映射的符号</strong><br>  设映射<span class="math inline">\(\sigma\)</span>为正弦函数<span class="math inline">\(sin(*)\)</span><br>  “<span class="math inline">\(\rightarrow\)</span>”表示将集合映射到集合。例如<span class="math inline">\(\sigma: R \rightarrow[-1,1]\)</span>，表示将实数集R映射到集合[-1,1]。<br>  “<span class="math inline">\(\mapsto\)</span>”表示将元素映射到元素。例如 <span class="math inline">\(\sigma: \pi \mapsto 0\)</span>，表示将元素<span class="math inline">\(\pi\)</span>映射到元素0。</p></li><li><p><strong>注2：集合的笛卡尔积</strong><br>  集合<span class="math inline">\(S_1\)</span>和<span class="math inline">\(S_2\)</span>的笛卡尔积的数学表示为： <span class="math display">\[S_1 \times S_1 = \{\begin{bmatrix}s_1\\s_2\\\end{bmatrix} \mid s_1 \in S_1,s_1 \in S_2\}\]</span></p></li><li><p><strong>注3：域的定义及常用的域</strong><br>  域的定义：包含加法与乘法的，满足通常运算规则的代数结构称为域。<br>  常用的域：<span class="math inline">\(\mathbb{Q}\)</span>(有理数域)、<span class="math inline">\(\mathbb{R}\)</span>(实数域)、<span class="math inline">\(\mathbb{C}\)</span>(复数域)等。</p></li></ul><h2 id="通常的运算规则">通常的运算规则</h2><ol type="1"><li><strong>加法交换律</strong><br>  已知非空集合V，对 <span class="math inline">\(\forall v_1,v_2 \inV\)</span>，有 <span class="math inline">\(v_1+v_2=v_2+v_1\)</span>。<br></li><li><strong>加法结合律</strong><br>  已知非空集合V，对 <span class="math inline">\(\forall v_1,v_2,v_3 \inV\)</span>，有 <span class="math inline">\((v_1+v_2)+v_3=v_1+(v_2+v_3)\)</span>。<br></li><li><strong>加法零元素</strong><br>  已知非空集合V，对 <span class="math inline">\(\forall v \in V, \existse \in V\)</span>，使得 <span class="math inline">\(e+v=v\)</span>。<br></li><li><strong>加法逆元素</strong><br>  已知非空集合V，对 <span class="math inline">\(\forall v \in V, \existsa \in V\)</span>，使得 <span class="math inline">\(v+a=e\)</span>，记<span class="math inline">\(a=-v\)</span>。<br></li><li><strong>数乘分配律</strong><br>  已知非空集合V和数域<span class="math inline">\(\mathbb{F}\)</span>，对<span class="math inline">\(\forall v_1,v_2 \in V, k \in\mathbb{F}\)</span>，有 <span class="math inline">\((v_1+v_2)k=v_1k+v_2k\)</span>。<br>  已知非空集合V和数域<span class="math inline">\(\mathbb{F}\)</span>，对<span class="math inline">\(\forall v \in V;k_1,k_2 \in\mathbb{F}\)</span>，有 <span class="math inline">\(v(k_1+k_2)=vk_1+vk_2\)</span>。<br></li><li><strong>数乘结合律</strong><br>  已知非空集合V和数域<span class="math inline">\(\mathbb{F}\)</span>，对<span class="math inline">\(\forall v \in V;k,l \in\mathbb{F}\)</span>，有 <span class="math inline">\((vk)l=v(kl)\)</span>。<br></li><li><strong>数乘单位元素</strong><br>  已知非空集合V和数域<span class="math inline">\(\mathbb{F}\)</span>，对<span class="math inline">\(\forall v \in V,\exists 1 \in\mathbb{F}\)</span>，使得 <span class="math inline">\(v1=v\)</span>。</li></ol><h2 id="线性空间的定义">线性空间的定义</h2><p>  <font color="#008000">若集合V满足<b>加法</b>与<b>数乘</b>两种运算，且这两种运算满足<b>通常的运算规则</b>，则称<b>集合V</b>关于此加法和数乘是<b>数域<span class="math inline">\(\mathbb{F}\)</span></b>上的线性空间。</font>一般也把这种线性空间称为向量空间，集合V中的元素称为向量。</p><h2 id="线性空间的具体实例">线性空间的具体实例</h2><h3 id="例1数域mathbbf上的标准线性空间mathbbfn">例1：数域<span class="math inline">\(\mathbb{F}\)</span>上的标准线性空间<span class="math inline">\(\mathbb{F}^n\)</span></h3><p>  已知数域<span class="math inline">\(\mathbb{F}\)</span>,设<br><span class="math display">\[V := \mathbb{F}^n=\mathbb{F} \times\mathbb{F} \times \dots \times \mathbb{F}=\{\begin{bmatrix}  v_1\\  v_2\\  \vdots\\  v_n\\\end{bmatrix} \mid v_i \in \mathbb{F},i=1, \dots,n\}\]</span></p><p>  对 <span class="math inline">\(\forall \alpha,\beta \in V, k \in\mathbb{F}\)</span>，<br>  定义集合V上的加法运算：<br><span class="math display">\[\alpha + \beta=\begin{bmatrix}  \alpha_1\\  \alpha_2\\  \vdots\\  \alpha_n\\\end{bmatrix}+\begin{bmatrix}  \beta_1\\  \beta_2\\  \vdots\\  \beta_n\\\end{bmatrix}=\begin{bmatrix}  \alpha_1+\beta_1\\  \alpha_2+\beta_2\\  \vdots\\  \alpha_n+\beta_n\\\end{bmatrix} \in V\]</span></p><p>  定义集合V上的数乘运算：<br><span class="math display">\[\alpha \cdot k=\begin{bmatrix}  \alpha_1\\  \alpha_2\\  \vdots\\  \alpha_n\\\end{bmatrix} \cdot k = \begin{bmatrix}  \alpha_1 k \\  \alpha_2 k \\  \vdots \\  \alpha_n k \\\end{bmatrix} \in V\]</span></p><p>  易证此加法与数乘满足通常的运算法则，此时称集合V为数域<span class="math inline">\(\mathbb{F}\)</span>上的n维标准线性空间，记为<span class="math inline">\(\mathbb{F}^n\)</span>。<br>  一些常用数域上的n维标准线性空间：<span class="math inline">\(\mathbb{R}^n\)</span>(实数域)、<span class="math inline">\(\mathbb{C}^n\)</span>(复数域)等。</p><h3 id="例2欧几里得空间作为线性空间">例2：欧几里得空间作为线性空间</h3>  令数域<span class="math inline">\(\mathbb{F}=\mathbb{R}\)</span>，集合<span class="math inline">\(V=\{欧几里得空间中的全体有向线段\}\)</span>。(当两条有向线段经过平移能够重叠时，则把这两条线段算做一条线段)<br>  定义集合V上的加法运算：向量运算的平行四边形法则。<br>  定义集合V上的数乘运算：向量同向或反向伸缩。<br><center class="half"><img src="https://s2.loli.net/2023/07/27/GNvpJQHgdrOLMRA.png" width="50%"><img src="https://s2.loli.net/2023/07/27/lFhM6DNEmVkRW7c.png" width="50%"><p>Image 1 向量的平行四边形法则            Image 2 向量的伸缩</p></center><p>  易证此加法与数乘满足通常的运算法则，则集合V是线性空间，说明欧几里得空间可以作为线性空间。</p><h3 id="例3函数空间作为线性空间">例3：函数空间作为线性空间</h3><p>  已知数域<span class="math inline">\(\mathbb{F}\)</span>，集合<span class="math inline">\(V=\mathcal{F}(I,\mathbb{F}^n)\)</span>。集合V为函数空间，V中的元素是以数域<span class="math inline">\(\mathbb{F}\)</span>中的区间<span class="math inline">\(I\)</span>为定义域，具有n个分量的n维向量值函数。例如：<br><span class="math display">\[f=\begin{bmatrix}  f_1(x) \\  f_2(x) \\\end{bmatrix}=\begin{bmatrix}  sin(x) \\  \frac{1}{2}x^3 \\\end{bmatrix}, f \in \mathcal{F}([-1,1],\mathbb{R}^2) \]</span></p><p>  对 <span class="math inline">\(\forall f,g \in\mathcal{F}(I,\mathbb{F}^n),k \in \mathbb{F}\)</span>，<br>  定义集合V上的加法运算：<br><span class="math display">\[f+g = \begin{bmatrix}  f_1(x) \\  f_2(x) \\  \vdots \\  f_n(x) \\\end{bmatrix}+\begin{bmatrix}  g_1(x) \\  g_2(x) \\  \vdots \\  g_n(x) \\\end{bmatrix} = \begin{bmatrix}  f_1(x)+g_1(x) \\  f_2(x)+g_2(x) \\  \vdots \\  f_n(x)+g_n(x) \\\end{bmatrix} \in \mathcal{F}(I,\mathbb{F}^n)\]</span></p><p>  定义集合V上的数乘运算：<br><span class="math display">\[f \cdot k = \begin{bmatrix}  f_1(x) \\  f_2(x) \\  \vdots \\  f_n(x) \\\end{bmatrix} \cdot k = \begin{bmatrix}  kf_1(x) \\  kf_2(x) \\  \vdots \\  kf_n(x) \\\end{bmatrix} \in \mathcal{F}(I,\mathbb{F}^n)\]</span></p><p>  易证此加法与数乘满足通常的运算法则，则集合V是线性空间，说明函数空间可以作为线性空间。</p>]]></content>
      
      
      <categories>
          
          <category> 矩阵分析 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>矩阵分析-0.引言</title>
      <link href="/2023/07/16/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-0.%E5%BC%95%E8%A8%80/"/>
      <url>/2023/07/16/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-0.%E5%BC%95%E8%A8%80/</url>
      
        <content type="html"><![CDATA[<h1 id="引言">引言</h1><p>  矩阵分析是笔者开启的第一个博客系列，之所以想把矩阵理论作为起点，是因为在统计与数据科学领域，矩阵是最基本也是最重要的数学工具，当我们在讨论高维空间、随机向量、多元正态分布这些统计领域最基本的概念时，我们都离不开矩阵这个数学工具。掌握矩阵理论，能够帮助我们更好地分析、处理高维空间中数学问题。</p><h2 id="矩阵分析的研究内容">矩阵分析的研究内容</h2><p>  与线性代数偏向于计算不同，矩阵分析的研究内容更加偏向于分析。矩阵分析的研究内容是非常广泛的，很难做到面面俱到，本博客的文章主要是参考一些主流的矩阵理论著作以及学习资料。以下是本系列打算讨论的一些重点内容：<br>1.<strong>线性空间</strong>：在研究问题时，我们通常把对象限定在某一个空间中，而线性空间便是代数中最基本的空间。从线性空间出发，我们会认识向量组、基、子空间等概念。<br>2.<strong>线性映射</strong>：当我们需要将一个线性空间中的对象映射到另一个线性空间时，我们便需要用到线性映射这个方法。基于线性映射，我们会讨论几何中的旋转变换、镜面反射等操作的矩阵表示。<br>3.<strong>矩阵等价与相似</strong>：矩阵的等价与相似是我们在线性代数中非常熟悉的概念，在矩阵分析中，我们将借助线性映射的概念进一步理解等价与相似的几何意义。另外，通过引入多项式矩阵以及Smith型、Jordan标准型等概念，我们将能够把较为复杂的矩阵相似问题转化为简单的矩阵等价问题。<br>4.<strong>内积</strong>：内积是解析几何中一个非常重要的概念，内积赋予空间向量以度量，使得我们可以定义范数、距离和角度等概念，从而建立内积空间的结构。内积空间是函数空间、向量空间和张量空间的基础。<br>5.<strong>矩阵微分</strong>：矩阵微积分是矩阵理论的重要组成部分。它对矩阵的导数、积分和微分方程进行了系统的研究，为解决矩阵和向量值函数的微分问题提供了理论基础。矩阵微分在机器学习、控制论等领域有广泛应用。<br>6.<strong>矩阵分解</strong>：矩阵分解是将一个矩阵拆解为多个简单矩阵或特殊形式矩阵的过程。矩阵分解在线性方程组求解、特征值计算、数据降维等方面有非常重要的作用。我们将会学习一些常见的矩阵分解技术，如LU分解、QR分解、奇异值分解等。</p><p>  以上是我打算在矩阵分析系列前期讨论的一些内容，当然，矩阵理论博大精深，远非一朝一夕能够掌握理解的，随着本人学习的深入，一些新的内容会陆续补充到这个系列中。</p><h2 id="矩阵分析的应用场景">矩阵分析的应用场景</h2><p>  矩阵理论在物理、控制、计算机等领域有着广泛的应用场景，由于本人是统计与数据科学领域的学生，所以我主要介绍一些矩阵理论在本领域的一些应用场景：</p><ul><li><strong>主成分分析（PCA）</strong>：PCA是一种常用的降维技术，它将高维数据转换为低维空间，同时最大程度保留原始数据的方差。PCA的核心是对数据协方差矩阵进行特征值分解，从而得到主成分。<br></li><li><strong>特征提取</strong>：在机器学习中，矩阵理论常用于特征提取。通过将数据矩阵进行降维、转换或者分解，可以得到更具有表征能力的特征，从而提高模型的性能。<br></li><li><strong>多元正态分布</strong>：在多元统计分析中，多元正态分布是一个重要的概率分布模型，用于描述多维随机变量的联合分布。矩阵理论提供了对多元正态分布的理论和应用支持，包括协方差矩阵、特征值分解、条件分布等。<br></li><li><strong>线性回归和广义线性模型</strong>：线性回归和广义线性模型是数据科学中常用的建模方法。它们使用矩阵来描述变量之间的线性关系，并通过矩阵求解技术来拟合模型和估计参数。<br></li><li><strong>时间序列分析</strong>：时间序列分析中，矩阵理论被用于处理多维时间序列数据，如协方差矩阵估计、谱分析等。<br></li><li><strong>神经网络</strong>：深度学习中的神经网络可以用矩阵表示网络的权重和输入输出。矩阵运算在神经网络的前向传播和反向传播过程中发挥着关键作用，实现模型的训练和优化。</li></ul><h2 id="愿景">愿景</h2><p>  矩阵分析是我开始写的第一个博客系列，我希望这个博客作为我学习笔记的同时，也能为读者解决遇到的问题。在我的想法中，我希望这个博客能一直处于更新状态，每当自己在矩阵理论方面有新的收获，便能把它记录在这里，愿自己能够一直坚持下来！</p><h2 id="相关学习资料">相关学习资料</h2><p>  矩阵理论有非常多的著作与学习资料，本博客主要参考的资料有：</p><ul><li><a href="https://www.bilibili.com/video/BV1Em4y1r7ss/?spm_id_from=333.337.search-card.all.click&amp;vd_source=234cf2ac075a1558881a6956450ddf89">Video1：《矩阵论》-严质彬 哈工大</a><br></li><li><a href="https://www.bilibili.com/video/BV1b4411j7V3/?spm_id_from=333.337.search-card.all.click&amp;vd_source=234cf2ac075a1558881a6956450ddf89">Video2：《数据分析、信号处理和机器学习中的矩阵方法》-Gilbert Strang MIT</a><br></li><li><a href="https://github.com/SXUSongJH/Book">Book1：《矩阵分析与应用》(第二版)-张贤达</a><br></li><li><a href="https://github.com/SXUSongJH/Book">Book2：《MatrixAnalysis》(Sencond Edition) - Roger A.Horn</a></li></ul><p>  在本系列的前期，将主要参考严质彬老师的课程。对于课程中没有涉及的内容，后期将借助其它几个资料进行补充。</p>]]></content>
      
      
      <categories>
          
          <category> 矩阵分析 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>这才是猛男该弹的曲子！！！</title>
      <link href="/2023/07/15/%E8%BF%99%E6%89%8D%E6%98%AF%E7%8C%9B%E7%94%B7%E8%AF%A5%E5%90%AC%E7%9A%84%E6%9B%B2%E5%AD%90/"/>
      <url>/2023/07/15/%E8%BF%99%E6%89%8D%E6%98%AF%E7%8C%9B%E7%94%B7%E8%AF%A5%E5%90%AC%E7%9A%84%E6%9B%B2%E5%AD%90/</url>
      
        <content type="html"><![CDATA[<p>  无语，最近对于睫毛弯弯这个曲子特着迷，吉他老师在教我光辉岁月时，我满脑子都想的是睫毛弯弯<span class="github-emoji"><span>😂</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f602.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>小时候咋没发现这个旋律这么好听。不行，我早晚得出一期这首歌的弹唱<span class="github-emoji"><span>🎶</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f3b6.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></p><center><iframe src="//player.bilibili.com/player.html?aid=384390831&amp;bvid=BV1fZ4y1t7db&amp;cid=731251859&amp;page=1" width="300" height="200" title="【吉他独奏】睫毛弯弯(王心凌)" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"></iframe></center>]]></content>
      
      
      <categories>
          
          <category> Daily </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
</search>
