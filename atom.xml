<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>喵老师&#39;s Blog</title>
  
  <subtitle>喵老师很忙~</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2024-06-25T08:48:55.973Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>喵老师</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>深度学习-7-生成模型5-扩散模型损失函数的三种等价形式</title>
    <link href="http://example.com/2024/06/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-7-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B5-%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E4%B8%89%E7%A7%8D%E7%AD%89%E4%BB%B7%E5%BD%A2%E5%BC%8F/"/>
    <id>http://example.com/2024/06/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-7-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B5-%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E4%B8%89%E7%A7%8D%E7%AD%89%E4%BB%B7%E5%BD%A2%E5%BC%8F/</id>
    <published>2024-06-03T08:01:44.000Z</published>
    <updated>2024-06-25T08:48:55.973Z</updated>
    
    <content type="html"><![CDATA[<h1 id="扩散模型损失函数的三种等价形式">扩散模型损失函数的三种等价形式</h1><p>  在前一个章节，我们推导了扩散模型 ELBO的理论形式，将其作为模型训练的损失函数，然后分析了损失函数中各项的具体计算方法，推导出了<strong>预测原始数据</strong>的损失函数形式。这一节我们将会介绍扩散模型另外两种损失函数的形式，即<strong>预测噪声</strong> 与<strong>分数匹配</strong>。扩散模型的原始论文 DDPM [2]，便是使用的<strong>预测噪声</strong>的损失函数形式，而之后的宋飏等的基于分数的生成模型[3] 则是使用了<strong>分数匹配</strong>的损失函数形式。在这一节，我们将会证明这三种损失函数是等价的。</p><h2 id="预测原始数据的损失函数形式">预测原始数据的损失函数形式</h2><p>  首先，我们还是来回顾一下上一节的结论。在上一篇博客文章中，我们将损失函数分解为了<strong>重构似然损失 <span class="math inline">\(L_{0}\)</span>、去噪匹配损失 <span class="math inline">\(L_{t-1}\)</span>、先验匹配损失 <span class="math inline">\(L_{T}\)</span></strong>。在这三项中，去噪匹配损失<span class="math inline">\(L_{t-1}\)</span>是损失函数中的主要部分，我们通过推导得到了其具体的计算公式：</p><p><span class="math display">\[\begin{align}    D_{KL}(q(x_{t-1}|x_{t},x_{0}) || p_{\theta}(x_{t-1}|x_{t})) =\frac{1}{2\sigma_{q}^{2}(t)}\frac{\bar{\alpha}_{t-1}(1-\alpha_{t})^{2}}{(1-\bar{\alpha}_{t})^{2}}\left[ ||\hat{x}_{\theta}(x_{t-1},t) - x_{0}||_{2}^{2} \right] \tag{1}\end{align}\]</span></p><p>  从(1)式中我们可以看出，优化去噪匹配损失实际上是让模型在每一步尽可能地预测原始数据<span class="math inline">\(x_{0}\)</span>。通过多步的迭代，可以使得模型的输出值<span class="math inline">\(\hat{x}_{\theta}(x_{t-1},t)\)</span>与原始数据 <span class="math inline">\(x_{0}\)</span> 更加相似。</p><h2 id="预测噪声的损失函数形式">预测噪声的损失函数形式</h2><p>  通过上一篇博客的推导(15)，我们可以 <span class="math inline">\(x_{0}\)</span> 与 <span class="math inline">\(x_{t}\)</span> 之间所满足的等式：</p><p><span class="math display">\[\begin{align}    x_{t} = \sqrt{\bar{\alpha}_{t}}x_{0} +\sqrt{1-\bar{\alpha}_{t}}\epsilon_{0} \tag{2}\end{align}\]</span></p><p>  基于 (2) 式，我们可以将 <span class="math inline">\(x_{0}\)</span>用 <span class="math inline">\(x_{t}\)</span> 与 <span class="math inline">\(\epsilon_{0}\)</span> 来表示：</p><p><span class="math display">\[\begin{align}    x_{0} = \frac{x_{t} -\sqrt{1-\bar{\alpha}_{t}}\epsilon_{0}}{\sqrt{\bar{\alpha}_{t}}} \tag{3}\end{align}\]</span></p><p>  通过 (3) 式，我们可以将编码器 <span class="math inline">\(q(x_{t-1}|x_{t},x_{0})\)</span>所满足的高斯分布的均值 <span class="math inline">\(\mu_{q}(x_{t},x_{0})\)</span> 转化为关于原始数据<span class="math inline">\(x_{0}\)</span> 与原始噪声 <span class="math inline">\(\epsilon_{0}\)</span> 的函数：</p><p><span class="math display">\[\begin{align}    \mu_{q}(x_{t},x_{0}) = \frac{1}{\sqrt{\alpha_{t}}}x_{t} -\frac{1-\alpha_{t}}{\sqrt{\alpha_{t}(1-\bar{\alpha}_{t})}}\epsilon_{0}\tag{4} \\\end{align}\]</span></p><p><strong><span class="math inline">\(Proof\)</span></strong></p><p><span class="math display">\[\begin{align}    \mu_{q}(x_{t},x_{0}) &amp;=\frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})x_{t} +\sqrt{\bar{\alpha}_{t-1}}(1-\alpha_{t})x_{0}}{1-\bar{\alpha}_{t}} \notag\\    &amp;= \frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})x_{t} +\sqrt{\bar{\alpha}_{t-1}}(1-\alpha_{t})\frac{x_{t} -\sqrt{1-\bar{\alpha}_{t}}\epsilon_{0}}{\sqrt{\bar{\alpha}_{t}}}}{1-\bar{\alpha}_{t}}\notag \\    &amp;= \frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})x_{t} +(1-\alpha_{t})\frac{x_{t} -\sqrt{1-\bar{\alpha}_{t}}\epsilon_{0}}{\sqrt{\alpha}_{t}}}{1-\bar{\alpha}_{t}}\notag \\    &amp;= \left(\frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_{t}} +\frac{1-\alpha_{t}}{(1-\bar{\alpha}_{t})\sqrt{\alpha_{t}}} \right)x_{t}-\frac{(1-\alpha_{t})\sqrt{1-\bar{\alpha}_{t}}}{(1-\bar{\alpha}_{t})\sqrt{\alpha_{t}}}\epsilon_{0}\notag \\    &amp;= \frac{1}{\sqrt{\alpha_{t}}}x_{t} -\frac{1-\alpha_{t}}{\sqrt{\alpha_{t}(1-\bar{\alpha}_{t})}}\epsilon_{0}\notag \\\end{align}\]</span></p><p>  在前一节中，我们将解码器 <span class="math inline">\(p_{\theta}(x_{t-1}|x_{t})\)</span>同样设置为高斯分布 <span class="math inline">\(N(x_{t-1};\mu_{\theta}(x_{t},t),\Sigma_{q}(t))\)</span>，且高斯分布的均值 <span class="math inline">\(\mu_{\theta}(x_{t},t)\)</span> 具有与编码器的均值<span class="math inline">\(\mu_{q}(x_{t},x_{0})\)</span>相同的形式，故解码器 <span class="math inline">\(p_{\theta}(x_{t-1}|x_{t})\)</span> 同样可以用 (4)式的形式表示，只是在解码过程中，我们没有 <span class="math inline">\(\epsilon_{0}\)</span> 的信息，故神经网络需要根据<span class="math inline">\(x_{t},t\)</span> 的信息预测噪声 <span class="math inline">\(\epsilon_{0}\)</span>。综上所述，我们可以解码器的均值重写为：</p><p><span class="math display">\[\begin{align}    \mu_{\theta}(x_{t},t) = \frac{1}{\sqrt{\alpha_{t}}}x_{t} -\frac{1-\alpha_{t}}{\sqrt{\alpha_{t}(1-\bar{\alpha}_{t})}}\hat{\epsilon}_{\theta}(x_{t},t)\tag{5} \\\end{align}\]</span></p><p>  这样，我们可以将编码器 <span class="math inline">\(q(x_{t-1}|x_{t},x_{0})\)</span> 与解码器 <span class="math inline">\(p_{\theta}(x_{t-1}|x_{t})\)</span> 之间的 KLDivergence (1) 式用噪声的预测误差重新表示：</p><p><span class="math display">\[\begin{align}    D_{KL}(q(x_{t-1}|x_{t},x_{0}) || p_{\theta}(x_{t-1}|x_{t})) =\frac{1}{2\sigma_{q}^{2}(t)}\frac{(1-\alpha_{t})^{2}}{(1-\bar{\alpha}_{t})\alpha_{t}}\left[ ||\hat{\epsilon}_{\theta}(x_{t},t) - \epsilon_{0} ||_{2}^{2} \right]\tag{6} \\\end{align}\]</span></p><p><strong><span class="math inline">\(Proof\)</span></strong></p><p><span class="math display">\[\begin{align}    &amp; D_{KL}(q(x_{t-1}|x_{t},x_{0}) || p_{\theta}(x_{t-1}|x_{t}))\notag \\    &amp;= \frac{1}{2\sigma_{q}^{2}(t)} \left[ ||\mu_{q} - \mu_{\theta}||_{2}^{2} \right] \notag \\    &amp;= \frac{1}{2\sigma_{q}^{2}(t)} \left[||\frac{1}{\sqrt{\alpha_{t}}}x_{t} -\frac{1-\alpha_{t}}{\sqrt{\alpha_{t}(1-\bar{\alpha}_{t})}}\epsilon_{0} -\frac{1}{\sqrt{\alpha_{t}}}x_{t} +\frac{1-\alpha_{t}}{\sqrt{\alpha_{t}(1-\bar{\alpha}_{t})}}\hat{\epsilon}_{\theta}(x_{t},t)||_{2}^{2} \right] \notag \\    &amp;= \frac{1}{2\sigma_{q}^{2}(t)} \left[ ||\frac{1-\alpha_{t}}{\sqrt{\alpha_{t}(1-\bar{\alpha}_{t})}}\hat{\epsilon}_{\theta}(x_{t},t)- \frac{1-\alpha_{t}}{\sqrt{\alpha_{t}(1-\bar{\alpha}_{t})}}\epsilon_{0}||_{2}^{2} \right] \notag \\    &amp;= \frac{1}{2\sigma_{q}^{2}(t)}\frac{(1-\alpha_{t})^{2}}{(1-\bar{\alpha}_{t})\alpha_{t}}\left[ ||\hat{\epsilon}_{\theta}(x_{t},t) - \epsilon_{0} ||_{2}^{2} \right]\notag \\\end{align}\]</span></p><p>  (6)式表明在训练过程中，我们的解码器每一步都需要尽可能地去预测原始噪声 <span class="math inline">\(\epsilon_{0}\)</span>。在前向加噪过程中，我们是将原始噪声<span class="math inline">\(\epsilon_{0}\)</span> 不断地加到原始数据<span class="math inline">\(x_{0}\)</span>中(2)，直至原始数据变为近似高斯噪声，故如果编码器能够根据 <span class="math inline">\(x_{t},t\)</span> 很好地预测噪声 <span class="math inline">\(\epsilon_{0}\)</span>，则在解码过程中，我们可以从高斯噪声开始，每一步逐步减去编码器所预测的原始噪声<span class="math inline">\(\hat{\epsilon}_{\theta}(x_{t},t)\)</span>，从而将高斯噪声还原回原始数据<span class="math inline">\(x_{0}\)</span>。以上过程可以用下图1表示。</p><center><img src="https://s2.loli.net/2024/06/06/li8W17MkhEJOeBp.png" width="80%" height="80%"><div data-align="center">Image1: 预测噪声训练过程</div></center><center><img src="https://s2.loli.net/2024/06/06/zvlHM8wZNYAcueU.png" width="80%" height="80%"><div data-align="center">Image2: DDPM原始论文的训练与采样算法</div></center><p>  在图2右侧的采样过程中，每一步解码，即减去预测的噪声后还加了一个噪声项<span class="math inline">\(\sigma_{t}z\)</span>，这一处理是模仿了前向扩散过程中的随机性，确保每一步生成的样本不是确定的，而是带有一定的随机性，从而可以生成多样化的样本。这是关键的，因为如果每一步都只是简单的去噪而不引入新的随机性，生成的样本将会缺乏多样性。</p><h2 id="分数匹配的损失函数形式">分数匹配的损失函数形式</h2><p>  在 DDPM 之后，Yang Song 等 [3] 在 2021 年提出了基于 VDM 的Score-Based Generative Model (SGM)。在这篇论文中，作者使用 SDEs建立起了扩散模型前向加噪与逆向去噪的一般框架，并利用得分函数作为损失函数来优化模型。关于SGM我们将会在下一篇博客中详细讨论，接下来我们主要来介绍一下基于分数的损失函数。<br>  为了得分函数函数，我们首先来介绍一下概率统计中的 Tweedie 公式。<br>  Tweedie公式表明，给定从指数族分布中抽取的样本，其真实均值可以通过样本的最大似然估计（也称为经验均值）加上一些涉及估计得分的修正项来估计。在只有一个观测样本的情况下，经验均值就是该样本本身。Tweedie公式通常用于减轻样本偏差；如果观测到的样本全部位于真实分布的一端，那么负得分会变大，并将样本的最大似然估计值校正到真实均值。<br>  具体来讲，假设我们从一个指数分布中抽取一些样本，但这些样本偏向分布的一端。单纯地使用这些样本的均值(最大似然估计) 会偏离真实的分布均值。Tweedie公式会通过添加一个修正项来纠正这种偏差。这个修正项会通过样本分布和得分函数来计算。得分函数是统计学中的一个概念，它是指对数似然函数关于参数的导数。如果样本都位于分布的一端，得分函数会变大，修正项会变大，从而将估计的均值向真实均值方向校正。  给定一个高斯随机变量 <span class="math inline">\(z \simN(z;\mu_{z},\Sigma_{z})\)</span>，由Tweedie 可以得到：</p><p><span class="math display">\[\begin{align}    \mathbb{E}[\mu_{z}|z] = z + \Sigma_{z}\nabla_{z}\log{p(z)} \tag{7}\end{align}\]</span></p><p>  在 VDM 中，通过高斯假设，我们推导了前向加噪过程中 <span class="math inline">\(x_{t}\)</span> 所满足的高斯分布：</p><p><span class="math display">\[\begin{align}    q(x_{t}|x_{0}) = N(x_{t}; \sqrt{\bar{\alpha}_{t}}x_{0}, (1 -\bar{\alpha}_{t})\boldsymbol{I}) \tag{8}\end{align}\]</span></p><p>  利用 Tweedie 公式，我们可以给出 <span class="math inline">\(x_{t}\)</span>在给定样本情况下的后验均值的修正估计：</p><p><span class="math display">\[\begin{align}    \mathbb{E}[\mu_{x_{t}}|x_{t}] = x_{t} + (1 -\bar{\alpha}_{t})\nabla_{x_{t}}\log{p(x_{t})} \tag{9}\end{align}\]</span></p><p>  由于在逆向去噪过程中，我们是不知道原始数据 <span class="math inline">\(x_{0}\)</span> 的，我们需要对 <span class="math inline">\(x_{0}\)</span>进行估计，前文中的预测原始数据的损失函数便是在做这件事。这里，通过结合<span class="math inline">\(x_{t}\)</span> 的真实均值 <span class="math inline">\(\sqrt{\bar{\alpha}_{t}}x_{0}\)</span>，我们可以给出后向去噪过程中<span class="math inline">\(x_{0}\)</span> 的估计值：</p><p><span class="math display">\[\begin{align}    x_{0} = \frac{x_{t} + (1 -\bar{\alpha}_{t})\nabla\log{p(x_{t})}}{\sqrt{\bar{\alpha}_{t}}} \tag{10}\\\end{align}\]</span></p><p>  其中，<span class="math inline">\(\nabla\log{p(x_{t})}\)</span> 为<span class="math inline">\(\nabla_{x_{t}}\log{p(x_{t})}\)</span>的简写形式。需要指出的是，在后向去噪过程中，<span class="math inline">\(x_{t}\)</span>所满足的高斯分布的均值是未知的，故解码过程中，(10)式中的得分函数 <span class="math inline">\(\nabla\log{p(x_{t})}\)</span>是未知的。而在前向加噪过程中，基于高斯假设，我们已经得知 <span class="math inline">\(x_{t}\)</span>的所满足的高斯分布(8)，故得分函数是可以计算的。<br>  现在我们需要将前向加噪过程中的 <span class="math inline">\(x_{t-1}\)</span>的均值改写成含有得分函数的形式，将(10)带入 <span class="math inline">\(\mu_{q}(x_{t},x_{0})\)</span>的原始表达式，通过推导可以得到以下等式成立：</p><p><span class="math display">\[\begin{align}    \mu_{q}(x_{t},x_{0}) = \frac{1}{\sqrt{\alpha_{t}}}x_{t} +\frac{1-\alpha_{t}}{\sqrt{\alpha_{t}}}\nabla\log{p(x_{t})} \tag{11} \\\end{align}\]</span></p><p><strong><span class="math inline">\(Proof\)</span></strong></p><p><span class="math display">\[\begin{align}    \mu_{q}(x_{t},x_{0}) &amp;=\frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})x_{t} +\sqrt{\bar{\alpha}_{t-1}}(1-\alpha_{t})x_{0}}{1-\bar{\alpha}_{t}} \notag\\    &amp;= \frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})x_{t} +\sqrt{\bar{\alpha}_{t-1}}(1-\alpha_{t})\frac{x_{t} + (1 -\bar{\alpha}_{t})\nabla\log{p(x_{t})}}{\sqrt{\bar{\alpha}_{t}}}}{1-\bar{\alpha}_{t}}\notag \\    &amp;= \frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})x_{t} +(1-\alpha_{t})\frac{x_{t} + (1 -\bar{\alpha}_{t})\nabla\log{p(x_{t})}}{\sqrt{\alpha}_{t}}}{1-\bar{\alpha}_{t}}\notag \\    &amp;= \left(\frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_{t}} +\frac{1-\alpha_{t}}{(1-\bar{\alpha}_{t})\sqrt{\alpha_{t}}} \right)x_{t}- \frac{1-\alpha_{t}}{\sqrt{\alpha_{t}}}\nabla\log{p(x_{t})} \notag \\    &amp;= \frac{1}{\sqrt{\alpha_{t}}}x_{t} +\frac{1-\alpha_{t}}{\sqrt{\alpha_{t}}}\nabla\log{p(x_{t})} \notag \\\end{align}\]</span></p><p>  因此，我们可以将逆向去噪过程中 <span class="math inline">\(x_{t-1}\)</span>的均值设置成与前向过程相同的形式，只是得分函数需要由神经网络进行估计：</p><p><span class="math display">\[\begin{align}    \mu_{\theta}(x_{t},t) = \frac{1}{\sqrt{\alpha_{t}}}x_{t} +\frac{1-\alpha_{t}}{\sqrt{\alpha_{t}}}s_{\theta}(x_{t},t) \tag{12} \\\end{align}\]</span></p><p>  结合 (11)、(12)式，我们可以得出基于得分函数的 <span class="math inline">\(q(x_{t-1} | x_{t},x_{0})\)</span> 与 <span class="math inline">\(p_{\theta}(x_{t-1}|x_{t})\)</span> 之间的KLDivergence 的表达式：</p><p><span class="math display">\[\begin{align}    D_{KL}(q(x_{t-1}|x_{t},x_{0}) || p_{\theta}(x_{t-1}|x_{t})) =\frac{1}{2\sigma_{q}^{2}(t)} \frac{(1-\alpha_{t})^{2}}{\alpha_{t}}\left[|| s_{\theta}(x_{t},t) - \nabla\log{p(x_{t})} ||_{2}^{2} \right]\tag{13}\end{align}\]</span></p><p><strong><span class="math inline">\(Proof\)</span></strong></p><p><span class="math display">\[\begin{align}    &amp; D_{KL}(q(x_{t-1}|x_{t},x_{0}) || p_{\theta}(x_{t-1}|x_{t}))\notag \\    &amp;= \frac{1}{2\sigma_{q}^{2}(t)} \left[ ||\mu_{q} - \mu_{\theta}||_{2}^{2} \right] \notag \\    &amp;= \frac{1}{2\sigma_{q}^{2}(t)} \left[||\frac{1}{\sqrt{\alpha_{t}}}x_{t} +\frac{1-\alpha_{t}}{\sqrt{\alpha_{t}}}s_{\theta}(x_{t},t) -\frac{1}{\sqrt{\alpha_{t}}}x_{t} -\frac{1-\alpha_{t}}{\sqrt{\alpha_{t}}}\nabla\log{p(x_{t})} ||_{2}^{2}\right] \notag \\    &amp;= \frac{1}{2\sigma_{q}^{2}(t)} \left[ ||\frac{1-\alpha_{t}}{\sqrt{\alpha_{t}}}s_{\theta}(x_{t},t) -\frac{1-\alpha_{t}}{\sqrt{\alpha_{t}}}\nabla\log{p(x_{t})} \right]\notag \\    &amp;= \frac{1}{2\sigma_{q}^{2}(t)}\frac{(1-\alpha_{t})^{2}}{\alpha_{t}}\left[ || s_{\theta}(x_{t},t) -\nabla\log{p(x_{t})} ||_{2}^{2} \right] \notag \\\end{align}\]</span></p><p>  这样我们就可以得到基于得分函数的损失函数，在解码过程中，神经网络需要通过给定的<span class="math inline">\(x_{t}, t\)</span>去预测真实的得分函数。得分函数给出了似然函数的梯度，即使得似然函数最大的方向，去噪过程中数据由高斯噪声，沿着神经网络所预测出的这个方向移动，从而到达最大的重构似然。<br>  实际上，数据在去噪过程移动的方向应该是噪声的反向，即“去噪”，这是符合我们的直觉的。事实也的确如此，联立(3) 式与 (10) 式，我们可以得到得分函数与原始噪声之间的联系：</p><p><span class="math display">\[\begin{align}    \nabla\log{p(x_{t})} =-\frac{1}{\sqrt{1-\bar{\alpha}_{t}}}\epsilon_{0} \tag{14}\end{align}\]</span></p><p>  得分函数衡量了如何在数据空间移动以使得对数似然最大化，由于在前向过程中，我们是将噪声不断地加入到图片中，因此在逆向过程中，我们很自然地应该向反方向移动，即逐渐去噪，以得到更高的对数似然，即与原始图片更加相似。<br>  以上我们推导了扩散模型损失函数的三种等价形式，包括<strong>预测原始数据、预测噪声、得分匹配</strong>。关于得分匹配损失函数，在下一节关于Score-Based Generative Model 中将会有更加详细的解释。</p><h2 id="reference">Reference</h2><p><strong>[1] Paper: Luo C. Understanding diffusion models: A unifiedperspective[J]. arXiv preprint arXiv:2208.11970, 2022.</strong><br><strong>[2] Paper: Ho J, Jain A, Abbeel P. Denoising diffusionprobabilistic models[J]. Advances in neural information processingsystems, 2020, 33: 6840-6851.</strong><br><strong>[3] Paper: Yang Song, Jascha Sohl-Dickstein, et al, "Score-BasedGenerative Modeling through Stochastic Differential Equations," inInternational Conference on Learning Representations,2021.</strong><br><strong>[4] Video: 想不出来昵称又想改, 扩散模型-DiffusionModel【李宏毅2023】, Blibili</strong><br><strong>[5] Blog: 苏剑林, 生成扩散模型漫谈(1-3), 科学空间</strong></p>]]></content>
    
    
    <summary type="html">本节主要介绍扩散模型损失函数的三种等价形式，即预测原始数据、预测噪声、分数匹配。另外还会介绍扩散系数该如何设置或者学习。</summary>
    
    
    
    <category term="深度学习" scheme="http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>深度学习-6-生成模型4-变分扩散模型</title>
    <link href="http://example.com/2024/05/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-6-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B4-%E5%8F%98%E5%88%86%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/"/>
    <id>http://example.com/2024/05/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-6-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B4-%E5%8F%98%E5%88%86%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/</id>
    <published>2024-05-23T03:36:01.000Z</published>
    <updated>2024-06-03T07:54:04.589Z</updated>
    
    <content type="html"><![CDATA[<h1 id="变分扩散模型vdm">变分扩散模型(VDM)</h1><p>  在上一节关于变分自编码的介绍中，我们已经讨论到了具有多层隐变量以及马尔可夫性质的变分自编码模型(MHVAE)，其基本形式与我们今天要介绍的变分扩散模型(VariationalDiffusion Models)已经非常相似，在 MHVAE 的基础上，VDM的主要改进有三个方面：</p><ul><li><strong>隐变量 <span class="math inline">\(z\)</span>的维度:</strong> VDM将隐变量 <span class="math inline">\(z\)</span>的维度设置成与数据 <span class="math inline">\(x\)</span> 一致。<br></li><li><strong>编码器 <span class="math inline">\(q(z|x)\)</span>的分布:</strong> 每个时刻 <span class="math inline">\(t\)</span>的编码器 <span class="math inline">\(q(z_{t}|z_{t-1})\)</span>不再是一个需要学习的分布，而是由前一时刻所输入 <span class="math inline">\(z_{t-1}\)</span> 为中心的高斯分布。<br></li><li><strong>隐变量高斯分布的参数:</strong> 隐变量高斯分布的参数随时间<span class="math inline">\(t\)</span> 改变，经过 <span class="math inline">\(T\)</span> 步后最终变为标准高斯分布。</li></ul><p>  我们来尝试理解一下这些改进的 motivations。在 VAE 中，我们首先将数据<span class="math inline">\(x\)</span> 由数据空间通过编码器 <span class="math inline">\(q_{\phi}(z|x)\)</span> 映射到隐空间中，隐变量<span class="math inline">\(z\)</span> 的维度比 <span class="math inline">\(x\)</span>要小，这一步的目的是希望隐变量能够抽象出数据 <span class="math inline">\(x\)</span>的一般分布特征，而忽略掉特殊细节；隐变量 <span class="math inline">\(z\)</span> 的分布为高斯分布 <span class="math inline">\(N(z;\boldsymbol{\mu}_{\phi}(x),\sigma_{\phi}^{2}(x)\boldsymbol{I})\)</span>，参数由解码器计算出。同时我们希望该高斯分布与标准高斯分布的KL Divergence尽可能小，即尽可能相似，这一方面是因为我们希望提升模型的泛化性能，避免模型学习到的模式过于单一，另一方面是因为在采样时我们需要从标准高斯分布采样出隐变量<span class="math inline">\(z\)</span>，再由解码器生成新的样本 <span class="math inline">\(x'\)</span>，在训练时要求隐变量 <span class="math inline">\(z\)</span>的分布与标准高斯分布尽可能相似也是希望能够与采样过程匹配。但事与愿违，由于训练目标的对抗性，隐变量的分布无法与高斯分布非常相似，另外单个隐变量对于分布特征的抽象能力也十分有限，这造成原始VAE 所生成的图片大多非常模糊，效果不佳。<br>  MHVAE采用了多层次隐变量的架构，通过叠加多个隐变量，使得编码器一步一步地将原始数据的分布特征抽象出来，再通过解码器一步一步对隐变量进行解码，生成新样本。从一步到多步，虽然计算过程变得更复杂，但模型的学习数据分布的能力会变得更强，经过多次编码，隐变量的分布特征变得越发不明显(抽象)，其与标准高斯分布的相似程度也会越高，这样就能更加匹配采样过程。但MHVAE也具有缺陷，它虽然改善了分布不匹配问题，但由于训练目标的对抗性，仍无法彻底解决这个问题。同时，由于存在多个参数化的编码器与解码器，模型参数量较多，模型的训练需要很长时间。  现在我们来讨论 VDM 的想法，既然采样过程是先从标准高斯分布采样出 <span class="math inline">\(z\)</span> ，再经过解码器生成新样本 <span class="math inline">\(x'\)</span>，VAE 与 MHVAE 均是希望应该将数据<span class="math inline">\(x\)</span>编码到与标准高斯分布相似的隐变量分布，以匹配采样过程，主要的困难在于很难学习出能够实现这一过程的编码器<span class="math inline">\(q_{\phi}(z|x)\)</span>。VDM的想法便是，既然编码器很难学，那干脆不学了，人为设定编码器，通过更长的步骤，将数据<span class="math inline">\(x\)</span>逐步编码到近似标准高斯分布(随机噪声)。既然要将数据 <span class="math inline">\(x\)</span>逐步编码为近似噪声，那干脆采用逐步加噪的方法，这种想法最为简洁，即：</p><p><span class="math display">\[x_{t} = \sqrt{\alpha_{t}} x_{t-1} +\sqrt{1-\alpha_{t}}\epsilon,\quad \epsilon \sim N(\epsilon;\boldsymbol{0,I})\]</span></p><p>  其中，<span class="math inline">\(x_{0}\)</span> 表示初始的数据<span class="math inline">\(x\)</span>，<span class="math inline">\(x_{1:T}\)</span>表示编码后的隐变量，对应于MHVAE中的 <span class="math inline">\(z_{1:T}\)</span>，基于这种形式，我们自然需要假设隐变量的维度与数据一致。同时，由于马尔可夫性质，<span class="math inline">\(x_{t}\)</span> 的分布自然是以 <span class="math inline">\(x_{t-1}\)</span>为中心的高斯分布(不考虑系数)。<br>  这样设置的好处是显而易见的，在这种条件下，编码器没有参数要学习，是一个线性过程，速度较快，则可以用更长的加噪步骤使得最终得到的隐变量分布<span class="math inline">\(q(x_{T}|x_{T-1})\)</span>与噪声更加接近。同时，加噪的过程也可以视为将数据 <span class="math inline">\(x\)</span>的分布特征进行抽象，例如一张清晰的猫的图片 <span class="math inline">\(x_{0}\)</span>在经过多次加噪后，只能看见模糊的猫的轮廓了，这也是对猫的图片的分布特征的一种压缩与抽象。解码器则是从随机噪声生成新样本，与编码过程互逆。在正向加噪过程中已经产生了每个步骤加噪前与加噪后的图片对，如果解码器能够训练成编码器的逆过程，即利用正向过程得到的图片对，基于加噪后的图片预测噪声，从而得到加噪前的图片，则可以完成逐步去噪的过程，生成与原始图片相似的新样本。以猫的图片的例子类比，这个过程是从猫的一般特征(模糊)，去生成细节更加丰富的猫的图片(清晰)。这个过程大大改善了以往VAE 所存在的分布不匹配问题。这便是我理解的 VDM 在 MHVAE 基础上的motivations，接下来我们具体来介绍 VDM 的细节。</p><h2 id="概率模型">概率模型</h2><p>  VDM 的概率图与 MHVAE 基本相同，其概率图如下图1所示：</p><center><img src="https://s2.loli.net/2024/05/27/F7RPJpD53LSaEVq.png" width="80%" height="80%"><div data-align="center">Image1: VDM 概率图</div></center><p>  其中 <span class="math inline">\(x_0\)</span> 表示原始数据，<span class="math inline">\(x_{1:T}\)</span> 表示隐变量。由前文的讨论可知，VDM的前向编码过程是一个不需要学习的逐步加噪过程：</p><p><span class="math display">\[\begin{align}    x_{t} = \sqrt{\alpha_{t}} x_{t-1} +\sqrt{1-\alpha_{t}}\epsilon,\quad \epsilon \sim N(\epsilon;\boldsymbol{0,I}) \tag{1}\end{align}\]</span></p><p>  其中 <span class="math inline">\(\alpha_{t}\)</span> 是随层次 <span class="math inline">\(t\)</span> 变化的常数(潜在可学习)。这样第 <span class="math inline">\(t\)</span> 层的编码器便是以 <span class="math inline">\(\sqrt{\alpha_{t}} x_{t-1}\)</span>为均值的高斯分布。同时，与 MHVAE 一样，VDM各层的转移概率分布也满足马尔可夫性质，故有：</p><p><span class="math display">\[\begin{align}    q(x_{1:T}|x_{0}) &amp;= \prod_{t=1}^{T}q(x_{t} | x_{t-1}) \tag{2} \\    q(x_{t} | x_{t-1}) &amp;= N(x_{t}; \sqrt{\alpha_{t}} x_{t-1},(1-\alpha_{t})\boldsymbol{I}) \tag{3}\end{align}\]</span></p><p>  从前文的第三个假设中，我们可以得知，最终层次的隐变量 <span class="math inline">\(x_{T}\)</span> 的先验分布为标准高斯分布。VDM的逆向去噪过程需要通过参数化的解码器 <span class="math inline">\(p_{\theta}(x_{t-1}|x_{t})\)</span>，逐步将图片由高斯噪声<span class="math inline">\(x_{T}\)</span>，还原回原始数据 <span class="math inline">\(x_{0}\)</span>。通过马尔可夫性质，我们可以写出 VDM的联合分布：</p><p><span class="math display">\[\begin{align}    p(x_{0:T}) &amp;= p(x_{T})\prod_{t=1}^{T}p_{\theta}(x_{t-1}|x_{t})\tag{4} \\    p(x_{T}) &amp;= N(x_{T};\boldsymbol{0,I}) \tag{5} \\\end{align}\]</span></p><p>  与 MHVAE 不同的是，在 VDM 中，我们只需要学习解码器的参数 <span class="math inline">\(\boldsymbol{\theta}\)</span>。当训练完成后，采样过程便是先从标准高斯分布<span class="math inline">\(p(x_{T})\)</span> 中采样出高斯噪声 <span class="math inline">\(x_{T}'\)</span>，再通过学习到的各层解码器<span class="math inline">\(p_{\theta}(x_{t-1}|x_{t})\)</span> 经过<span class="math inline">\(T\)</span> 步解码后，生成新的数据 <span class="math inline">\(x_{0}'\)</span>。</p><h2 id="变分下界elbo">变分下界(ELBO)</h2><p>  与 MHVAE 一样，VDM 同样是对似然函数的变分下界进行优化。</p><p><strong>VDM's ELBO</strong></p><p><span class="math display">\[\begin{align}    \log{p(x)} \ge \mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{p(x_{0:T})}{q(x_{1:T}|x_{0})}} \right] \tag{6}\end{align}\]</span></p><p><strong><span class="math inline">\(Proof\)</span></strong></p><p><span class="math display">\[\begin{align}    \log{p(x)} &amp;= \log{p(x)} \int q(x_{1:T}|x_{0})dx_{1:T} \notag \\    &amp;= \int \log{p(x)} q(x_{1:T}|x_{0})dx_{1:T} \notag \\    &amp;= \mathbb{E}_{q(x_{1:T}|x_{0})} \left[ \log{p(x)} \right]\notag \\    &amp;= \mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{p(x_{0:T})q(x_{1:T}|x_{0})}{p(x_{1:T}|x_{0})q(x_{1:T}|x_{0})}}\right] \notag \\    &amp;= \mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{p(x_{0:T})}{q(x_{1:T}|x_{0})}} \right] +\mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{q(x_{1:T}|x_{0})}{p(x_{1:T}|x_{0})}} \right] \notag \\    &amp;= \mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{p(x_{0:T})}{q(x_{1:T}|x_{0})}} \right] +D_{KL}(q(x_{1:T}|x_{0}) || p(x_{1:T}|x_{0})) \tag{7} \\    &amp;\ge \mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{p(x_{0:T})}{q(x_{1:T}|x_{0})}} \right] \notag\end{align}\]</span></p><p>  由以上证明的(7)式我们可以得知，似然函数与ELBO之间的差为 <span class="math inline">\(q(x_{1:T}|x_{0})\)</span> 与 <span class="math inline">\(p(x_{1:T}|x_{0})\)</span> 之间的 KLDivergence，其表示给定原始数据 <span class="math inline">\(x_{0}\)</span>后，编码过程的联合分布与解码过程的联合分布之间的KL距离。最大化 ELBO等价于最小化这个 KLDivergence。这个距离越小，则说明正向加噪与逆向去噪越匹配，模型的生成效果越好。进一步地，利用马尔可夫性质，我们可以将这个KL Divergence 分解成三项：</p><p><span class="math display">\[\begin{align}    &amp; D_{KL}(q(x_{1:T}|x_{0}) || p(x_{1:T}|x_{0})) \tag{8}\\    =&amp;\sum_{t=1}^{T-1}\mathbb{E}_{q(x_{t-1},x_{t+1} | x_{0})} \left[D_{KL}(q(x_{t}|x_{t-1}) || p_{\theta}(x_{t}|x_{t+1})) \right]\tag{consistency term}\\    &amp;+ \mathbb{E}_{q(x_{T-1}|x_{0})}\left[ D_{KL}(q(x_{T}|x_{T-1})|| p(x_{T})) \right] \tag{prior matching term}\\    &amp;- \mathbb{E}_{q(x_{1}|x_{0})}\left[\log{\frac{p_{\theta}(x_{0}|x_{1})}{p(x_{0})}} \right]\tag{reconstruction term}\end{align}\]</span></p><p><strong><span class="math inline">\(Proof\)</span></strong></p><p><span class="math display">\[\begin{align}    &amp; D_{KL}(q(x_{1:T}|x_{0}) || p(x_{1:T}|x_{0})) \notag \\    &amp; = \mathbb{E}_{q(x_{1:T}|x_{0})}\left[\log{\frac{q(x_{1:T}|x_{0})}{p(x_{1:T}|x_{0})}} \right] \notag\\    &amp; = \mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{p(x_{0})\prod_{t=1}^{T}q(x_{t}|x_{t-1})}{p(x_{T})\prod_{t=1}^{T}p_{\theta}(x_{t-1}|x_{t})}}\right] \notag \\    &amp; = \mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{p(x_{0})q(x_{T}|x_{T-1})\prod_{t=1}^{T-1}q(x_{t}|x_{t-1})}{p(x_{T})p_{\theta}(x_{0}|x_{1})\prod_{t=1}^{T-1}p_{\theta}(x_{t}|x_{t+1})}}\right] \notag \\    &amp; =\sum_{t=1}^{T-1}\mathbb{E}_{q(x_{t-1},x_{t},x_{t+1}|x_{0})}\left[\log{\frac{q(x_{t}|x_{t-1})}{p_{\theta}(x_{t}|x_{t+1})}} \right] +\mathbb{E}_{q(x_{T-1},x_{T}|x_{0})}\left[\log{\frac{q(x_{T}|x_{T-1})}{p(x_{T})}} \right] -\mathbb{E}_{q(x_{1}|x_{0})}\left[\log{\frac{p_{\theta}(x_{0}|x_{1})}{p(x_{0})}} \right] \notag \\    &amp; =\sum_{t=1}^{T-1}\underbrace{\mathbb{E}_{q(x_{t-1},x_{t+1}|x_{0})}\left[D_{KL}(q(x_{t}|x_{t-1}) || p_{\theta}(x_{t}|x_{t+1}))\right]}_{consistency \ term} +\underbrace{\mathbb{E}_{q(x_{T-1}|x_{0})}\left[ D_{KL}( q(x_{T}|x_{T-1})|| p(x_{T})) \right]}_{prior \ matching \ term}  -\underbrace{\mathbb{E}_{q(x_{1}|x_{0})}\left[\log{\frac{p_{\theta}(x_{0}|x_{1})}{p(x_{0})}} \right]}_{reconstruction\ term} \notag \\\end{align}\]</span></p><p>  我们对 KL Divergence 进行了分解，得到了 consistency term, priormatching term, reconstruction term 三项。 最小化 KL Divergence等价于最小化这三项，即使得 consistency term, prior matching term尽可能小，reconstruction term尽可能大。我们先不详细解释这三项的含义，接下来我们同样对 VDM 的 ELBO(6)进行分解，我们会发现 ELBO 分解后的结果与 KL Divergence几乎一致，只是符号相反：</p><p><span class="math display">\[\begin{align}   &amp; \mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{p(x_{0:T})}{q(x_{1:T}|x_{0})}} \right] \tag{9} \\   =&amp; \mathbb{E}_{q(x_{1}|x_{0})}[\log{p_{\theta}(x_{0}|x_{1})}]\tag{reconstruction term} \\   -&amp; \mathbb{E}_{q(x_{T-1}|x_{0})}\left[ D_{KL}( q(x_{T}|x_{T-1})|| p(x_{T})) \right] \tag{prior matching term}\\   -&amp; \sum_{t=1}^{T-1}\mathbb{E}_{q(x_{t-1},x_{t+1}|x_{0})}\left[D_{KL}(q(x_{t}|x_{t-1}) || p_{\theta}(x_{t}|x_{t+1})) \right]\tag{consistency term} \\\end{align}\]</span></p><p><strong><span class="math inline">\(Proof\)</span></strong></p><p><span class="math display">\[\begin{align}    &amp; \mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{p(x_{0:T})}{q(x_{1:T}|x_{0})}} \right] \notag \\    &amp; = \mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{p(x_{T})\prod_{t=1}^{T}p_{\theta}(x_{t-1}|x_{t})}{\prod_{t=1}^{T}q(x_{t}|x_{t-1})}}\right] \notag \\    &amp;= \mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{p(x_{T})p_{\theta}(x_{0}|x_{1})\prod_{t=2}^{T}p_{\theta}(x_{t-1}|x_{t})}{q(x_{T}|x_{T-1})\prod_{t=1}^{T-1}q(x_{t}|x_{t-1})}}\right] \notag \\    &amp;= \mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{p(x_{T})p_{\theta}(x_{0}|x_{1})\prod_{t=1}^{T-1}p_{\theta}(x_{t}|x_{t+1})}{q(x_{T}|x_{T-1})\prod_{t=1}^{T-1}q(x_{t}|x_{t-1})}}\right] \notag \\    &amp;= \mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{p(x_{T})p_{\theta}(x_{0}|x_{1})}{q(x_{T}|x_{T-1})}} \right] +\mathbb{E}_{q(x_{1:T}|x_{0})}\left[ \log{\prod_{i=1}^{T-1}}\frac{p_{\theta}(x_{t}|x_{t+1})}{q(x_{t}|x_{t-1})} \right] \notag \\      &amp;= \mathbb{E}_{q(x_{1}|x_{0})}[\log{p_{\theta}(x_{0}|x_{1})}] +\mathbb{E}_{q(x_{T-1},x_{T}|x_{0})} \left[\log{\frac{p(x_{T})}{q(x_{T}|x_{T-1})}} \right] +\sum_{i=1}^{T-1}\mathbb{E}_{q(x_{1:T}|x_{0})}\left[ \log{\frac{p_{\theta}(x_{t}|x_{t+1})}{q(x_{t}|x_{t-1})}} \right] \notag \\    &amp;=\underbrace{\mathbb{E}_{q(x_{1}|x_{0})}[\log{p_{\theta}(x_{0}|x_{1})}]}_{reconstruction\ term} -\underbrace{\mathbb{E}_{q(x_{T-1}|x_{0})}[D_{KL}(q(x_{T}|x_{T-1}) ||p(x_{T}))]}_{prior \ matching \ term} -\sum_{i=1}^{T-1}\underbrace{\mathbb{E}_{q(x_{t-1},x_{t+1}|x_{0})}\left[D_{KL}(q(x_{t}|x_{t-1}) || p_{\theta}(x_{t} || x_{t+1}))\right]}_{consistency \ term} \notag \\\end{align}\]</span></p><p>  由以上的推导我们将 ELBO 分解为与 KL Divergence相似的三项，只是符号相反，这也从另一个方面说明了最大化 ELBO实际上等价于最小化 KL Divergence。最大化 ELBO 的过程是使得reconstruction term 尽可能大，prior matching term, consistency term尽可能小。接下来以 ELBO的分解结果为例，我们尝试理解一下这三项的含义。</p><ul><li><strong>reconstruction term:</strong>重构项。这一项的含义是将原始数据 <span class="math inline">\(x_{0}\)</span> 编码一次后得到隐变量 <span class="math inline">\(x_{1}\)</span> 后再通过解码器还原回 <span class="math inline">\(x_{0}\)</span>，所得到的对数似然。这一项在 VAE中也存在，这一项的值越大，表明数据在编码与解码后与原始数据更相似，即生成的效果更好。<br></li><li><strong>prior matching term:</strong>先验匹配项。这一项的含义是最终隐变量 <span class="math inline">\(x_{T}\)</span> 的后验分布 <span class="math inline">\(q(x_{T}|x_{T-1})\)</span> 与其先验分布 <span class="math inline">\(p(x_{T})\)</span> 之间的 KL Divergence的期望。这一项越小，则先验与后验越匹配，说明编码过程得到的最终隐变量的分布与标准高斯分布越接近，更能匹配采样过程。<br></li><li><strong>consistency term:</strong>一致性项。这一项的含义是从前向和后向两个过程努力使 <span class="math inline">\(x_{t}\)</span>处的分布保持一致。如图2所示，对于每一个中间时间步 <span class="math inline">\(t\)</span>，从噪声图像中得到的去噪图片的分布 <span class="math inline">\(p_{\theta}(x_{t}|x_{t+1})\)</span>应该与从干净图像中得到的相应加噪步骤得到的图片的分布 <span class="math inline">\(q(x_{t}|x_{t-1})\)</span> 相匹配，这在数学上通过KLDivergence得到了体现。这一项越小，说明解码器 <span class="math inline">\(p_{\theta}(x_{t}|x_{t-})\)</span>被训练的越好。</li></ul><center><img src="https://s2.loli.net/2024/05/29/Aq8Eb5kvrWURtNX.png" width="80%" height="80%"><div data-align="center">Image2: 一致性</div></center><p>  通过以上的推导，VDM 的 ELBO均分解成了期望的形式，我们可以使用蒙特卡洛方法来对这些项进行近似，然而，实际上使用我们刚才推导出的(9)式来优化ELBO可能是次优的；因为一致性项在每个时间步<span class="math inline">\(t\)</span> 上都被计算为两个随机变量 <span class="math inline">\(x_{t-1},x_{t+1}\)</span>的期望值，因此其蒙特卡洛估计的方差有可能高于每个时间步仅使用一个随机变量估计的项。由于它是通过对<span class="math inline">\(T-1\)</span>个时间步求和来计算的，因此对于大的T值，ELBO的最终估计值可能具有很高的方差。<br>  为了改善这个问题，我们尝试对 (9)式做一些变形。基于马尔可夫性与贝叶斯公式，我们可以得到以下等式：</p><p><span class="math display">\[\begin{align}    q(x_{t}|x_{t-1}) = q(x_{t}|x_{t-1},x_{0}) =\frac{q(x_{t-1})q(x_{t}|x_{0})}{q(x_{t-1}|x_{0})} \tag{10}\end{align}\]</span></p><p>  通过(10)式，我们可以将 VDM 的 ELBO 分解为如下形式：</p><p><span class="math display">\[\begin{align}   &amp; \mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{p(x_{0:T})}{q(x_{1:T}|x_{0})}} \right] \tag{11} \\   =&amp; \mathbb{E}_{q(x_{1}|x_{0})}[\log{p_{\theta}(x_{0}|x_{1})}]\tag{reconstruction term} \\   -&amp; D_{KL}( q(x_{T}|x_{0}) || p(x_{T}))  \tag{prior matchingterm}\\   -&amp; \sum_{t=2}^{T}\mathbb{E}_{q(x_{t}|x_{0})}\left[D_{KL}(q(x_{t-1}|x_{t},x_{0}) || p_{\theta}(x_{t-1}|x_{t})) \right]\tag{denoising matching term} \\\end{align}\]</span></p><p><strong><span class="math inline">\(Proof\)</span></strong></p><p><span class="math display">\[\begin{align}    &amp; \mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{p(x_{0:T})}{q(x_{1:T}|x_{0})}} \right] \notag \\    &amp;= \mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{p(x_{T})\prod_{t=1}^{T}p_{\theta}(x_{t-1}|x_{t})}{\prod_{t=1}^{T}q(x_{t}|x_{t-1})}}\right] \notag \\    &amp;= \mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{p(x_{T})p_{\theta}(x_{0}|x_{1})\prod_{t=2}^{T}p_{\theta}(x_{t-1}|x_{t})}{q(x_{1}|x_{0})\prod_{t=2}^{T}q(x_{t}|x_{t-1})}}\right] \notag \\    &amp;= \mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{p(x_{T})p_{\theta}(x_{0}|x_{1})}{q(x_{1}|x_{0})}} +\log{\prod_{t=2}^{T}\frac{p_{\theta}(x_{t-1}|x_{t})}{q(x_{t}|x_{t-1},x_{0})}}\right] \notag \\    &amp;= \mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{p(x_{T})p_{\theta}(x_{0}|x_{1})}{q(x_{1}|x_{0})}} +\log{\prod_{t=2}^{T}\frac{p_{\theta}(x_{t-1}|x_{t})}{\frac{q(x_{t-1}|x_{t},x_{0})\cancel{q(x_{t}|x_{0})}}{\cancel{q(x_{t-1}|x_{0})}}}}\right] \notag \\    &amp;= \mathbb{E}_{q(x_{1:T}|x_{0})} \left[\log{\frac{p(x_{T})p_{\theta}(x_{0}|x_{1})}{\cancel{q(x_{1}|x_{0})}}}+\log{\frac{\cancel{q(x_{1}|x_{0})}}{q(x_{T}|x_{0})}} +\log{\prod_{t=2}^{T}\frac{p_{\theta}(x_{t-1}|x_{t})}{q(x_{t-1}|x_{t},x_{0})}}\right] \notag \\    &amp;= \mathbb{E}_{q(x_{1}|x_{0})}[\log{p_{\theta}(x_{0}|x_{1})}] -\mathbb{E}_{q(x_{T}|x_{0})}\left[ \log{\frac{q(x_{T}|x_{0})}{p(x_{T})}}\right] -\sum_{t=2}^{T} \mathbb{E}_{q(x_{t},x_{t-1}|x_{0})}\left[\log{\frac{q(x_{t-1}|x_{t},x_{0})}{p_{\theta}(x_{t-1}|x_{t})}} \right]\notag \\    &amp;=\underbrace{\mathbb{E}_{q(x_{1}|x_{0})}[\log{p_{\theta}(x_{0}|x_{1})}]}_{reconstruction\ term} - \underbrace{D_{KL}(q(x_{T}|x_{0}) || p(x_{T}))}_{prior \matching \ term} -\sum_{t=2}^{T}\underbrace{\mathbb{E}_{q(x_{t}|x_{0})}[D_{KL}(q(x_{t-1}|x_{t},x_{0})|| p_{\theta}(x_{t-1}|x_{t}))]}_{denoising \ matching \ term} \notag \\\end{align}\]</span></p><p>  reconstruction term 与 prior matching term的含义与(9)基本一致。差别较大的是(9)式中的 consistency term 与(11)式中的 denoising matching term。与 consistency term 相比，denoisingmatching term 中的每一时间步 <span class="math inline">\(t\)</span>，只需要计算一个随机变量 <span class="math inline">\(x_{t}\)</span>的期望，显著改善了蒙特卡洛估计的方差较大的问题。同时，最小化 denoisingmatching term 意味着在每一个时间步 <span class="math inline">\(t\)</span>，通过解码器去噪后的数据的分布 <span class="math inline">\(p_{\theta}(x_{t-1}|x_{t})\)</span>与真实加噪过程中加入噪声前的图片的分布 <span class="math inline">\(q(x_{t-1}|x_{t},x_{0})\)</span> 相匹配，即 KLDivergence尽可能小。这一项越小，说明解码器每一步预测噪声的能力越强，即从一般抽象特征去生成更加细节的特征的能力越强，生成图片与原始图片就会越相似，这一过程如图3所示。</p><center><img src="https://s2.loli.net/2024/05/29/FAxezibV6GnNpv3.png" width="80%" height="80%"><div data-align="center">Image3: 去噪匹配</div></center><h2 id="损失函数">损失函数</h2><p>  在前文中，我们推导了 VDM's ELBO 的理论形式(11)，通过最大化 ELBO来近似最大化对数似然，得到待估参数 <span class="math inline">\(\theta\)</span>。现在我们要利用 VDM的假设条件，根据 ELBO的理论形式，来得到具体用于模型训练的损失函数。<br>  通过前文的分析，我们可以将VDM 的损失函数写作如下的三部分:</p><p><span class="math display">\[\begin{align}    \boldsymbol{L}(\theta) = -ELBO = \mathbb{E}_{q}\left[\underbrace{D_{KL}(q(x_{T}|x_{0}) || p(x_{T}))}_{L_{T}} +\sum_{t=2}^{T}\underbrace{D_{KL}(q(x_{t-1}|x_{t},x_{0}) ||p_{\theta}(x_{t-1}|x_{t}))}_{L_{t-1}} -\underbrace{\log{p_{\theta}(x_{0}|x_{1})}}_{L_{0}} \right] \tag{12}\end{align}\]</span></p><p>  接下来我们来逐个讨论这三项的具体形式。</p><h3 id="先验匹配损失-l_t">先验匹配损失 <span class="math inline">\(L_{T}\)</span></h3><p>  这一项是衡量最终隐变量 <span class="math inline">\(X_{T}\)</span>的先验分布与后验分布的相似程度，其中 <span class="math inline">\(p(x_{T})\)</span> 是标准高斯分布，当给定数据 <span class="math inline">\(x_{0}\)</span> 后， <span class="math inline">\(x_{T}\)</span> 的后验分布 <span class="math inline">\(q(x_{T}|x_{0})\)</span> 可以不依赖于参数 <span class="math inline">\(\theta\)</span> 计算出，故 <span class="math inline">\(L_{T}\)</span>损失函数中相当于常数，可以不考虑。</p><h3 id="去噪匹配损失-l_t-1">去噪匹配损失 <span class="math inline">\(L_{t-1}\)</span></h3><p>  去噪匹配损失 <span class="math inline">\(L_{t-1}\)</span>在损失函数中占主导地位，其衡量了每个时间步 <span class="math inline">\(t\)</span>，编码器的去噪后得到的图片与加噪过程中该时刻的真实图片的相似程度。在<span class="math inline">\(L_{t-1}\)</span> 中我们最主要是需要计算<span class="math inline">\(D_{KL}(q(x_{t-1}|x_{t},x_{0}) ||p_{\theta}(x_{t-1}|x_{t}))\)</span>，对于其中的编码器 <span class="math inline">\(q(x_{t-1}|x_{t},x_{0})\)</span>，由贝叶斯公式以及马尔可夫性质可以得到：</p><p><span class="math display">\[\begin{align}    &amp; q(x_{t-1}|x_{t},x_{0}) =\frac{q(x_{t}|x_{t-1},x_{0})q(x_{t-1}|x_{0})}{q(x_{t}|x_{0})} =\frac{q(x_{t}|x_{t-1})q(x_{t-1}|x_{0})}{q(x_{t}|x_{0})} \tag{13} \\    &amp; q(x_{t}|x_{t-1}) = N(x_{t}; \sqrt{\alpha_{t}}x_{t-1},(1-\alpha_{t})\boldsymbol{I}) \tag{14}\end{align}\]</span></p><p>  在前文中，我们已经得知了正向加噪过程满足递推公式：</p><p><span class="math display">\[x_{t} = \sqrt{\alpha_{t}} x_{t-1} +\sqrt{1-\alpha_{t}}\epsilon,\quad \epsilon \sim N(\epsilon;\boldsymbol{0,I})\]</span></p><p>  利用递推公式，我们可以计算出 <span class="math inline">\(q(x_{t}|x_{0})\)</span> 所满足的高斯分布：</p><p><span class="math display">\[\begin{align}    q(x_{t}|x_{0}) = N(x_{t}; \sqrt{\bar{\alpha}_{t}}x_{0}, (1 -\bar{\alpha}_{t})\boldsymbol{I}),\quad \bar{\alpha}_{t} =\prod_{i=1}^{t}\alpha_{i} \tag{15} \\\end{align}\]</span></p><p><strong><span class="math inline">\(Proof\)</span></strong></p><p><span class="math display">\[\begin{align}    x_{t} &amp;= \sqrt{\alpha_{t}} x_{t-1} +\sqrt{1-\alpha_{t}}\epsilon_{t-1}^{*} \notag \\    &amp;= \sqrt{\alpha_{t}} \left( \sqrt{\alpha_{t-1}} x_{t-2} +\sqrt{1-\alpha_{t-1}}\epsilon_{t-2}^{*} \right) +\sqrt{1-\alpha_{t}}\epsilon_{t-1}^{*} \notag \\    &amp;= \sqrt{\alpha_{t}\alpha_{t-1}} x_{t-2} +\sqrt{\alpha_{t}-\alpha_{t}\alpha_{t-1}}\epsilon_{t-2}^{*} +\sqrt{1-\alpha_{t}}\epsilon_{t-1}^{*} \notag \\    &amp;= \sqrt{\alpha_{t}\alpha_{t-1}} x_{t-2} +\sqrt{\sqrt{\alpha_{t}-\alpha_{t}\alpha_{t-1}}^{2} +\sqrt{1-\alpha_{t}}^{2}}\epsilon_{t-2} \notag \\    &amp;= \sqrt{\alpha_{t}\alpha_{t-1}} x_{t-2} + \sqrt{1 -\alpha_{t}\alpha_{t-1}}\epsilon_{t-2} \notag \\    &amp;= \dotsb \notag \\    &amp;= \sqrt{\prod_{i=1}^{t}\alpha_{i}}x_{0} +\sqrt{1-\prod_{i=1}^{t}\alpha_{i}} \epsilon_{0} \notag \\    &amp;= \sqrt{\bar{\alpha}_{t}}x_{0} +\sqrt{1-\bar{\alpha}_{t}}\epsilon_{0} \sim N(x_{t};\sqrt{\bar{\alpha}_{t}}x_{0}, (1 - \bar{\alpha}_{t})\boldsymbol{I})\notag \\\end{align}\]</span></p><p>  利用 (15) 式，我们可以得到 <span class="math inline">\(x_{t-1}\)</span> 的分布：</p><p><span class="math display">\[\begin{align}    q(x_{t-1}|x_{0}) = N(x_{t-1}; \sqrt{\bar{\alpha}_{t-1}}x_{0}, (1 -\bar{\alpha}_{t-1})\boldsymbol{I}) \tag{16} \\\end{align}\]</span></p><p>  联立(14)、(15)、(16)式，我们可以发现(13)式的分子分母均为高斯分布，由高斯分布的联合分布与边际分布均为高斯分布可知，<span class="math inline">\(q(x_{t-1}|x_{t},x_{0})\)</span>，同样满足高斯分布，现在我们需要利用(13)式来计算其均值与方差，通过计算我们可以得到如下结论：</p><p><span class="math display">\[\begin{align}    q(x_{t-1}|x_{t},x_{0}) = N(x_{t-1}; \mu_{q}(x_{t},x_{0}),\Sigma_{q}(t)) \tag{17} \\\end{align}\]</span></p><p><span class="math display">\[\begin{align}    \mu_{q}(x_{t},x_{0}) &amp;=\frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})x_{t} +\sqrt{\bar{\alpha}_{t-1}}(1-\alpha_{t})x_{0}}{1-\bar{\alpha}_{t}},\quad\Sigma_{q}(t) = \sigma_{q}^{2}(t)\boldsymbol{I} =\frac{1-\alpha_{t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_{t}}\boldsymbol{I}\tag{18} \\\end{align}\]</span></p><p><strong><span class="math inline">\(Proof\)</span></strong></p><p><span class="math display">\[\begin{align}    q(x_{t-1}|x_{t},x_{0}) &amp;=\frac{q(x_{t}|x_{t-1})q(x_{t-1}|x_{0})}{q(x_{t}|x_{0})} \notag \\    &amp;= \frac{N(x_{t}; \sqrt{\alpha_{t}}x_{t-1},(1-\alpha_{t})\boldsymbol{I})N(x_{t-1}; \sqrt{\bar{\alpha}_{t-1}}x_{0},(1 - \bar{\alpha}_{t-1})\boldsymbol{I})}{N(x_{t};\sqrt{\bar{\alpha}_{t}}x_{0}, (1 -\bar{\alpha}_{t})\boldsymbol{I}),\quad \bar{\alpha}_{t}} \notag \\    &amp; \propto \exp \left(-\frac{1}{2}\left[\frac{(x_{t}-\sqrt{\alpha_{t}}x_{t-1})^2}{1-\alpha_{t}} + \frac{(x_{t-1}- \sqrt{\bar{\alpha}_{t-1}}x_{0})^2}{1 - \bar{\alpha}_{t-1}} -\frac{(x_{t} - \sqrt{\bar{\alpha}_{t}}x_{0})^2}{1 - \bar{\alpha}_{t}}\right] \right)  \notag \\    &amp;= \exp\left( -\frac{1}{2}\left[\frac{(-2\sqrt{\alpha_{t}}x_{t}x_{t-1}+\alpha_{t}x_{t-1}^{2})}{1-\alpha_{t}}+ \frac{(x_{t-1}^{2}-2\sqrt{\bar{\alpha}_{t-1}}x_{t-1}x_{0})}{1 -\bar{\alpha}_{t-1}} + C(x_{t},x_{0}) \right] \right)  \notag \\    &amp; \propto \exp\left( -\frac{1}{2}\left[\frac{1-\bar{\alpha}_{t}}{(1-\alpha_{t})(1-\bar{\alpha}_{t-1})}x_{t-1}^{2}- 2\left( \frac{\sqrt{\alpha_{t}}x_{t}}{1-\alpha_{t}} +\frac{\sqrt{\bar{\alpha}_{t-1}}x_{0}}{1-\bar{\alpha}_{t-1}}\right)x_{t-1} \right] \right)   \notag \\    &amp;= \exp\left( -\frac{1}{2}\left(\frac{1-\bar{\alpha}_{t}}{(1-\alpha_{t})(1-\bar{\alpha}_{t-1})}\right)\left[ x_{t-1}^{2}-2\frac{\left(\frac{\sqrt{\alpha_{t}}x_{t}}{1-\alpha_{t}} +\frac{\sqrt{\bar{\alpha}_{t-1}}x_{0}}{1-\bar{\alpha}_{t-1}}\right)(1-\alpha_{t})(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_{t}}x_{t-1}\right] \right)  \notag \\    &amp;= \exp\left( -\frac{1}{2}\left(\frac{1}{\frac{(1-\alpha_{t})(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_{t}}}\right)\left[x_{t-1}^{2}-2\frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})x_{t} +\sqrt{\bar{\alpha}_{t-1}}(1-\alpha_{t})x_{0}}{1-\bar{\alpha}_{t}}x_{t-1}\right] \right)  \notag \\    &amp; \propto N(x_{t+1};\underbrace{\frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})x_{t} +\sqrt{\bar{\alpha}_{t-1}}(1-\alpha_{t})x_{0}}{1-\bar{\alpha}_{t}}}_{\mu_{q}(x_{t},x_{0})},\underbrace{\frac{(1-\alpha_{t})(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_{t}}\boldsymbol{I}}_{\Sigma_{q}(t)=\sigma_{q}^{2}(t)\boldsymbol{I}})\notag \\\end{align}\]</span></p><p>  通过以上的推导，我们得到了加噪过程中<span class="math inline">\(x_{t-1}\)</span>所满足的高斯分布，要计算 <span class="math inline">\(L_{t-1}\)</span> 式中的 KLDivergence，我们还需要去噪过程中 <span class="math inline">\(x_{t-1}\)</span> 的分布。<br>  为了使得去噪过程与加噪过程尽可能匹配，我们同样将去噪过程建模为高斯过程，即<span class="math inline">\(p_{\theta}(x_{t-1}|x_{t})\)</span>满足高斯分布。去噪过程的高斯分布的方差与对应的加噪过程的方差一致，而均值是由参数化的神经网络计算得出。对于均值的计算，VDM将<span class="math inline">\(p_{\theta}(x_{t-1}|x_{t})\)</span>所满足的高斯分布的均值设定为与 <span class="math inline">\(q(x_{t-1}|x_{t},x_{0})\)</span> 具有相同的形式，即<span class="math inline">\(\mu_{q}(x_{t},x_{0})\)</span>，但在去噪过程中是没有给定<span class="math inline">\(x_{0}\)</span>的，故神经网络在均值计算中的实际作用是输出 <span class="math inline">\(x_{0}\)</span> 的预测值 <span class="math inline">\(\hat{x}_{\theta}(x_{t-1},t)\)</span> ，从而得到<span class="math inline">\(p_{\theta}(x_{t-1}|x_{t})\)</span>所满足的高斯分布的均值。以上建模过程总结的数学表达式如下：</p><p><span class="math display">\[\begin{align}    p_{\theta}(x_{t-1}|x_{t}) = N(x_{t-1}; \mu_{\theta}(x_{t},t),\Sigma_{p}(t))  \tag{19}\end{align}\]</span></p><p><span class="math display">\[\begin{align}    \mu_{\theta}(x_{t},t) &amp;=\frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})x_{t} +\sqrt{\bar{\alpha}_{t-1}}(1-\alpha_{t})\hat{x}_{\theta}(x_{t-1},t)}{1-\bar{\alpha}_{t}},\quad\Sigma_{p}(t) = \Sigma_{q}(t) =\frac{1-\alpha_{t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_{t}}\boldsymbol{I}\tag{20} \\\end{align}\]</span></p><p>  通过以上的建模，去噪匹配损失 <span class="math inline">\(L_{t-1}\)</span> 中的 KL Divergence实际上是计算两个方差相同的高斯分布的 KLDivergence，这使得问题变得非常简单，因为高斯分布的 KL Divergence是有显式表达式的，其表达式如下所示：</p><p><span class="math display">\[\begin{align}    D_{KL}(N(x;\mu_{x},\Sigma_{x}) || N(y;\mu_{y},\Sigma_{y})) =\frac{1}{2}\left[ \log{\frac{|\Sigma_{y}|}{|\Sigma_{x}|}}-d +tr(\Sigma_{y}^{-1}\Sigma_{x}) + (\mu_{y} -\mu_{x})^{T}\Sigma_{y}^{-1}(\mu_{y}-\mu_{x}) \right] \tag{21}\end{align}\]</span></p><p>  其中，<span class="math inline">\(d\)</span>是高斯分布的维度。结合(17)、(18)、(19)、(20)、(21)式，我们现在可以来计算<span class="math inline">\(L_{t-1}\)</span> 中的 KL Divergence的具体表达式了，其结果如下：</p><p><span class="math display">\[\begin{align}    D_{KL}(q(x_{t-1}|x_{t},x_{0}) || p_{\theta}(x_{t-1}|x_{t})) =\frac{1}{2\sigma_{q}^{2}(t)}\frac{\bar{\alpha}_{t-1}(1-\alpha_{t})^{2}}{(1-\bar{\alpha}_{t})^{2}}\left[ ||\hat{x}_{\theta}(x_{t-1},t) - x_{0}||_{2}^{2} \right] \tag{22}\end{align}\]</span></p><p><strong><span class="math inline">\(Proof\)</span></strong></p><p><span class="math display">\[\begin{align}    &amp; D_{KL}(q(x_{t-1}|x_{t},x_{0}) || p_{\theta}(x_{t-1}|x_{t}))\notag \\    &amp;= D_{KL}(N(x_{t-1}; \mu_{q}, \Sigma_{q}(t)) || N(x_{t-1};\mu_{\theta}, \Sigma_{p}(t)))  \notag \\    &amp;= \frac{1}{2}\left[\log{\frac{|\Sigma_{q}(t)|}{|\Sigma_{q}(t)|}}-d +tr(\Sigma_{q}(t)^{-1}\Sigma_{q}(t)) + (\mu_{q} -\mu_{\theta})^{T}\Sigma_{q}(t)^{-1}(\mu_{q}-\mu_{\theta}) \right] \notag\\    &amp;= \frac{1}{2}\left[ \log{1}-d + d + (\mu_{q} -\mu_{\theta})^{T}\Sigma_{q}(t)^{-1}(\mu_{q}-\mu_{\theta}) \right] \notag\\    &amp;= \frac{1}{2}\left[(\mu_{q} -\mu_{\theta})^{T}(\sigma_{q}^{2}(t)\boldsymbol{I})^{-1}(\mu_{q}-\mu_{\theta})\right] \notag \\    &amp;= \frac{1}{2\sigma_{q}^{2}(t)} \left[ ||\mu_{q} - \mu_{\theta}||_{2}^{2} \right] \notag \\    &amp;=  \frac{1}{2\sigma_{q}^{2}(t)} \left[||\frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})x_{t} +\sqrt{\bar{\alpha}_{t-1}}(1-\alpha_{t})x_{0}}{1-\bar{\alpha}_{t}}-\frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})x_{t} +\sqrt{\bar{\alpha}_{t-1}}(1-\alpha_{t})\hat{x}_{\theta}(x_{t-1},t)}{1-\bar{\alpha}_{t}}||_{2}^{2}\right]\notag\\    &amp;= \frac{1}{2\sigma_{q}^{2}(t)} \left[||\frac{\sqrt{\bar{\alpha}_{t-1}}(1-\alpha_{t})}{1-\bar{\alpha}_{t}}(\hat{x}_{\theta}(x_{t-1},t) - x_{0}) ||_{2}^{2}\right]  \notag \\    &amp;=\frac{1}{2\sigma_{q}^{2}(t)}\frac{\bar{\alpha}_{t-1}(1-\alpha_{t})^{2}}{(1-\bar{\alpha}_{t})^{2}}\left[ ||\hat{x}_{\theta}(x_{t-1},t) - x_{0}||_{2}^{2} \right] \notag \\\end{align}\]</span></p><p>  对于(12)式中的期望，可以在每个训练批次中使用 Monte Carlo estimate方法来估计。通过 (22) 式 我们可以得知，VDM在逆向去噪过程中，每一步的优化目标都是在给定噪声图片 <span class="math inline">\(x_{t-1}\)</span> 的情况下，预测出原始图片 <span class="math inline">\(x_{0}\)</span>。这种损失函数的设定方式可以总结为<strong>预测原始数据</strong>，在之后的章节我们会讨论扩散模型损失函数的另外两种等价形式，分别为<strong>预测噪声</strong>以及<strong>分数匹配</strong>。</p><h3 id="重构似然损失-l_0">重构似然损失 <span class="math inline">\(L_{0}\)</span></h3><p>  对于损失函数中的 <span class="math inline">\(L_{0}\)</span> 项，在DDPM 的原始论文[2]中，采用了一个独立的离散编码器。这是因为在之前的加噪去噪步骤中，我们都将图片数据的取值由<span class="math inline">\(\{ 0,1,\dotsb,255 \}\)</span>的离散数值映射到 <span class="math inline">\([-1,1]\)</span>。这一方面是使数据标准化，便于神经网络处理；另一方面是因为采样过程是从标准高斯分布中进行采样，再由解码器逐步去噪，故需要在训练时的加噪去噪过程对离散数据进行映射后来匹配采样时的数值范围。<br>  具体来讲，由之前步骤训练得到的参数化的神经网络，以及去噪得到的数据<span class="math inline">\(x_{1}\)</span>，我们可以同样可以得到 <span class="math inline">\(p_{\theta}(x_{0}|x_{1})\)</span> 所满足的高斯分布<span class="math inline">\(N(x_{0};\mu_{\theta}(x_{1},1),\sigma_{1}^{2})\)</span>，但在这里我们不能像去噪过程一样，直接使用该高斯分布来计算<span class="math inline">\(x_{0}\)</span> 的对数似然，这是因为原始数据<span class="math inline">\(x_{0}\)</span>是离散数据，而高斯分布本身是连续的，直接使用高斯分布来计算离散对数本事是不够准确的。在DDPM 的原始论文[2]中，作者采用了一个离散边界函数计算高斯分布在离散区间上的积分，从而精确计算离散数据的对数似然。其计算公式如下：</p><p><span class="math display">\[\begin{align}    p_{\theta}(x_{0}|x_{1}) = \prod_{i=1}^{d}\int_{\delta_{-}(x_{0}^{i})}^{\delta_{+}(x_{0}^{i})} N(x_{0};\mu_{\theta}(x_{1},1),\sigma_{1}^{2}) dx \tag{22} \\\end{align}\]</span></p><p><span class="math display">\[\begin{align}    \delta_{+}(x_{0}^{i}) = \left \{\begin{array}{l}\infty &amp; if \ x = 1 \\x + \frac{1}{255} &amp; if \ x &lt; 1 \\\end{array} \right. \quad \delta_{-}(x_{0}^{i}) = \left \{\begin{array}{l}-\infty &amp; if \ x = -1 \\x - \frac{1}{255} &amp; if \ x &gt; -1 \\\end{array} \right.\tag{23}\end{align}\]</span></p><p>  其中，<span class="math inline">\(i\)</span> 表示数据 <span class="math inline">\(x_{0}\)</span> 的每一个维度，<span class="math inline">\(\delta_{+}(x_{0}^{i}),\delta_{-}(x_{0}^{i})\)</span>表示每个数据维度的离散值的上下边界。由于在做数据映射时，是使用 <span class="math inline">\(x' = (2x - 255) / 255\)</span>将数据取值范围由 <span class="math inline">\(\{ 0,1,\dotsb,255\}\)</span> 映射到 <span class="math inline">\([0,1]\)</span>的，故<span class="math inline">\(x' \pm \frac{1}{255} = [2(x \pm 0.5)-255]/255\)</span>，故(23)式设置的离散边界实际上是等价于在离散值上下分别加上0.5后再计算高斯分布的积分。使用这种方法可以较为精确的计算离散数据的对数似然。</p><h2 id="总结">总结</h2><p>  在这篇博客中，我们首先讨论了 VDM 相较于之前的 MHVAE又做了哪些假设，以及做出这些假设的 motivations。之后与 VAE类似，我们讨论了 VDM 的 ELBO的理论表达式，其可以分解为三项，在此基础上，我们计算了 ELBO三个分解项在损失函数中的具体表达式。<br>  但实际上，DDPM在训练中并不是使用<strong>预测原始数据</strong>，即(22)式作为损失函数，而是使用<strong>预测噪声</strong>作训损失函数，在之后的一些研究中，例如基于分数的生成模型，则是使用<strong>分数匹配</strong>作为损失函数，这三种扩散模型的损失函数实际上是等价的，这一点由于篇幅的限制我们不再做过多的讨论。在下一节，我们将重点讨论三种损失函数的等价关系，如何做训练以及采样，以及扩散模型系数该如何设置或学习。</p><h2 id="reference">Reference</h2><p><strong>[1] Paper: Luo C. Understanding diffusion models: A unifiedperspective[J]. arXiv preprint arXiv:2208.11970, 2022.</strong><br><strong>[2] Paper: Ho J, Jain A, Abbeel P. Denoising diffusionprobabilistic models[J]. Advances in neural information processingsystems, 2020, 33: 6840-6851.</strong><br><strong>[3] Video: 想不出来昵称又想改, 扩散模型-DiffusionModel【李宏毅2023】, Blibili</strong><br><strong>[4] Blog: 苏剑林, 生成扩散模型漫谈(1-3), 科学空间</strong></p>]]></content>
    
    
    <summary type="html">本节主要介绍生成模型中的变分扩散模型(VDM)，会论述其概率模型、变分下界ELBO、模型训练、损失函数的三种等价关系、采样过程等内容。</summary>
    
    
    
    <category term="深度学习" scheme="http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>深度学习-5-生成模型3-变分自编码</title>
    <link href="http://example.com/2024/05/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-5-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B3-%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81/"/>
    <id>http://example.com/2024/05/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-5-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B3-%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81/</id>
    <published>2024-05-22T09:06:14.000Z</published>
    <updated>2024-05-23T08:02:18.456Z</updated>
    
    <content type="html"><![CDATA[<h1 id="变分自编码vae">变分自编码(VAE)</h1><p>  GMM诞生于上世纪，其本身受制于当时的计算机性能以及数据量的大小，伴随着计算机技术的发展，在2010年之后，深度学习领域崛起，生成模型的研究也迎来了大的变革。很多学者研究了如何将利用神经网络模型拟合非线性的强大性能来改善生成模型的效果。2014年，Kingma.D.P在ICLR上发表了著名的论文《Auto-encoding variationalbayes》[2]，提出了变分自编码模型(VariationalAuto-encoding)，其采用基于神经网络的编码器-解码器架构，利用编码器对数据的隐变量进行建模，再通过一个解码器将隐变量映射到数据空间来生成数据。同时VAE也提出了变分推断的思想，即对似然函数的变分下界来进行优化。<br>  VAE的出现对现代生成模型具有重大影响，其建立的基本框架，直到今天仍然被Diffusion等模型采用，就在今年5月份，ICLR颁发了首届时间检验奖，用于表彰具有长期影响力的论文，VAE的原始论文[3]获得了该奖项，由此便可说明其在生成模型领域的巨大影响。</p><h2 id="概率模型">概率模型</h2><p>  与GMM相同，VAE同样采用了隐变量假设，即认为观测数据 <span class="math inline">\(x\)</span> 受到隐变量 <span class="math inline">\(z\)</span>的影响，但与GMM不同的是，其采用了标准高斯分布作为隐变量的先验分布，同时引入了贝叶斯思想，即后验分布的概念，其概率图如下图3所示：</p><center><img src="https://s2.loli.net/2024/05/20/f79nweONT41RS6D.png" width="80%" height="80%"><div data-align="center">Image3: VAE 概率图</div></center><p>  对于图中的未知的条件概率 <span class="math inline">\(p(x|z),q(z|x)\)</span>，VAE采用了参数化的神经网络模型对其进行逼近，其分别被称为解码器<span class="math inline">\(p_{\theta}(x|z)\)</span>，表示将隐变量解码到数据空间中，即生成新样本；编码器<span class="math inline">\(q_{\phi}(z|x)\)</span>，表示将观测数据编码到隐空间中，编码器设置为高斯分布，其参数由神经网络模型给出。VAE的概率模型可以总结为：</p><p><span class="math display">\[\begin{align}    &amp; Latend \ prior: p(z) = N(z; \boldsymbol{0, I}) \tag{17} \\    &amp; Encoder: q_{\phi}(z|x) = N(z; \boldsymbol{\mu}_{\phi}(x),\sigma_{\phi}^{2}(x)\boldsymbol{I}) \rightarrow q(z|x)  \tag{18}  \\    &amp; Decoder: p_{\theta}(x|z) \rightarrow p(x|z)  \tag{19} \\\end{align}\]</span></p><h2 id="变分下界elbo">变分下界(ELBO)</h2><p>  在估计概率模型的参数时，我们通常会使用“最大似然法”，然而尽管有EM算法等迭代算法，但在面对复杂的概率模型时，求解似然参数仍然是一件困难且耗时的工作，对此VAE提出变分下界的概念，也叫做证据下界(EvidenceLower Bound)，通过最大化变分下界来代替直接对于似然函数的优化。<br>  <strong>(ELBO)</strong> 对于存在隐变量 <span class="math inline">\(z\)</span>的模型，其似然函数具有以下不等式成立：</p><p><span class="math display">\[\begin{align}    \log{p(x)} \ge \mathbb{E}_{q_{\phi}(z|x)} \left[\log\frac{p(x,z)}{q_{\phi}(z|x)} \right] \tag{20}\end{align}\]</span></p><p><strong><span class="math inline">\(proof\)</span></strong></p><p><span class="math display">\[\begin{align}    \log{p(x)} &amp;= \log{p(x)} \int_{z} q_{\phi}(z|x) dz = \int_{z}q_{\phi}(z|x)\log{p(x)} dz \notag \\    &amp;= \mathbb{E}_{q_{\phi}(z|x)}[\log{p(x)}] =\mathbb{E}_{q_{\phi}(z|x)}\left[\log{\frac{p(x,z)q_{\phi}(z|x)}{q(z|x)q_{\phi}(z|x)}} \right] \notag \\    &amp;= \mathbb{E}_{q_{\phi}(z|x)} \left[\log{\frac{p(x,z)}{q_{\phi}(z|x)}} \right] + \mathbb{E}_{q_{\phi}(z|x)}\left[ \log{\frac{q_{\phi}(z|x)}{q(z|x)}} \right] \notag \\    &amp;= \mathbb{E}_{q_{\phi}(z|x)} \left[\log{\frac{p(x,z)}{q_{\phi}(z|x)}} \right] + D_{KL}(q_{\phi}(z|x) ||q(z|x)) \tag{21} \\    &amp; \ge \mathbb{E}_{q_{\phi}(z|x)} \left[\log{\frac{p(x,z)}{q_{\phi}(z|x)}} \right] \tag{22}\end{align}\]</span></p><p>  从式(21)可以看出，<strong>似然函数 <span class="math inline">\(\log{p(x)}\)</span> 实际上等于ELBO加上真实后验<span class="math inline">\(q(z|x)\)</span> 与近似后验 <span class="math inline">\(q_{\phi}(z|x)\)</span> 之间的 KLDivergence，由于似然函数 <span class="math inline">\(\log{p(x)}\)</span>并不依赖于参数 <span class="math inline">\(\phi\)</span>，其在优化过程中可看作一个固定的常数，因此使用ELBO代替似然函数作为最大化的目标函数是有意义的，因为其实际上等价于最小化真实后验<span class="math inline">\(q(z|x)\)</span> 与近似后验 <span class="math inline">\(q_{\phi}(z|x)\)</span> 之间的 KLDivergence.</strong>   通过(17)式与(19)式，我们可以对 ELBO进一步推导：</p><p><span class="math display">\[\begin{align}    \mathbb{E}_{q_{\phi}(z|x)} \left[ \log{\frac{p(x,z)}{q_{\phi}(z|x)}}\right] &amp;= \mathbb{E}_{q_{\phi}(z|x)} \left[\log{\frac{p(z)p_{\theta}(x|z)}{q_{\phi}(z|x)}} \right]  \notag \\    &amp;= \mathbb{E}_{q_{\phi}(z|x)} \left[ \log{p_{\theta}(x|z)}\right] - \mathbb{E}_{q_{\phi}(z|x)} \left[\log{\frac{q_{\phi}(z|x)}{p(z)}} \right]  \notag \\    &amp;= \underbrace{\mathbb{E}_{q_{\phi}(z|x)} \left[\log{p_{\theta}(x|z)} \right]}_{reconstruction \ term} -\underbrace{D_{KL}(q_{\phi}(z|x) || p(z))}_{prior \ matching \ term}\tag{23} \\\end{align}\]</span></p><p>  通过推导，我们发现 VAE 的 ELBO 包括两项，其意义如下：</p><ul><li><strong>reconstruction term:</strong> 该项代表了解码器基于隐变量<span class="math inline">\(z\)</span> 重构输入数据 <span class="math inline">\(x\)</span>的分布的对数似然。通过最大化这个对数似然，模型被训练以更好地从潜在变量重构输入数据，确保潜在变量<span class="math inline">\(z\)</span>能够有效地捕捉到输入数据的关键特征。<br></li><li><strong>prior matching term:</strong>该项了学习到的变分分布与对隐变量的先验分布的相似程度。最小化这个项鼓励编码器学习一个实际的分布，而不是崩溃成狄拉克delta函数。</li></ul><p>  要最大化 ELBO就是要使第一项尽可能大，同时第二项尽可能小，但这两个目标是想抗衡的，若第二项很小，则说明<span class="math inline">\(q_{\phi}(z|x)\)</span>近似于标准高斯分布，则意味着 <span class="math inline">\(z\)</span>没有任何辨识度，并没有学习到 <span class="math inline">\(x\)</span>的分布特征，此时第一项就不可能非常小；反过来，如果第一项较大，即说明<span class="math inline">\(z\)</span> 具有很高的辨识度，<span class="math inline">\(q_{\phi}(z|x)\)</span>会大幅度偏离标准高斯分布，则第二项不会很小。因此类似于 GAN，这两项的优化实际上是一个对抗过程。在变分自编码器中，ELBO是优化目标，它同时包括了对数据重构的质量（第一项）和对模型复杂性的控制（第二项）。最大化ELBO意味着我们尝试达到这两个目标的最佳平衡：即最大化数据的重构质量，同时保持潜在表示的多样性和广泛性。</p><h2 id="模型训练">模型训练</h2><p>  综上所述，VAE的优化函数可以写成：</p><p><span class="math display">\[\begin{align}    (\hat{\phi},\hat{\theta}) = \argmax_{\phi,\theta}\mathbb{E}_{q_{\phi}(z|x)} \left[ \log{p_{\theta}(x|z)} \right] -D_{KL}(q_{\phi}(z|x) || p(z)) \tag{24}\end{align}\]</span></p><p>  我们来将目标函数进行细化，对于第一项，我们可以使用Monte Carloestimate 来进行估计，即：</p><p><span class="math display">\[\begin{align}    \mathbb{E}_{q_{\phi}(z|x)} \left[ \log{p_{\theta}(x|z)} \right]\approx \frac{1}{N}\sum_{i=1}^{N} \log{p_{\theta}(x|z_i)} \tag{25}\end{align}\]</span></p><p>  其中，<span class="math inline">\(N\)</span>为训练集的样本容量，<span class="math inline">\(z_i\)</span>为从隐变量的后验分布 <span class="math inline">\(q_{\phi}(z|x)\)</span>采样出的样本。同时，由于 <span class="math inline">\(q_{\phi}(z|x)\)</span> 与 <span class="math inline">\(p(z)\)</span>均为高斯分布，故第二项很容易就能得到：</p><p><span class="math display">\[\begin{align}    D_{KL}(q_{\phi}(z|x) || p(z)) = \frac{1}{2}\sum_{k=1}^{d}\left(\mu_{\phi,k}^{2}(x) + \sigma_{\phi,k}^{2}(x) -\log{\sigma_{\phi,k}^{2}(x)}-1 \right) \tag{26}\end{align}\]</span></p><p>  因此(24)式可以写为：</p><p><span class="math display">\[\begin{align}     \argmax_{\phi,\theta} \frac{1}{N}\sum_{i=1}^{N}\log{p_{\theta}(x|z_i)} - \frac{1}{2}\sum_{k=1}^{d}\left(\mu_{\phi,k}^{2}(x) + \sigma_{\phi,k}^{2}(x) -\log{\sigma_{\phi,k}^{2}(x)}-1 \right) \tag{27}\end{align}\]</span></p><p>  然而，这里有一个问题：随机采样过程的不可导性。在第一项中，我们需要从<span class="math inline">\(q_{\phi}(z|x)\)</span> 中采样出 <span class="math inline">\(z\)</span>用于计算损失，但在神经网络中，我们需要计算损失函数关于网络参数的梯度以进行反向传播和参数更新。然而，由于采样步骤是一个随机过程，并且具有“跳跃”的性质(从概率分布中直接抽取样本)，它本身不是一个可导的操作，我们无法对<span class="math inline">\(\phi\)</span> 进行更新。<br>  为了解决这个问题，VAE提出了一种重要技术：重参数化技巧(reparameterizationtrick)。重新参数化技巧将随机变量改写为噪声变量的确定性函数；这允许通过梯度下降对非随机项进行优化。具体而言，对于从<span class="math inline">\(N(z; \boldsymbol{\mu}_{\phi}(x),\sigma_{\phi}^{2}(x)\boldsymbol{I})\)</span> 采样出的样本 <span class="math inline">\(z\)</span>，我们可以将其重写为：</p><p><span class="math display">\[\begin{align}    z = \boldsymbol{\mu}_{\phi}(x) + \sigma_{\phi}^{2}(x) \odot\epsilon,\quad \epsilon \sim N(\epsilon; \boldsymbol{0,I}) \tag{28}\end{align}\]</span></p><p>  其中 <span class="math inline">\(\boldsymbol{\mu}_{\phi}(x)\)</span> 与 $_{}^{2}(x)$ 均由神经网络模型计算，通以上处理，我们便可以对参数 <span class="math inline">\(\phi,\theta\)</span>进行求导，进而利用SGD等算法进行参数更新。<br>  对于解码器 <span class="math inline">\(p_{\theta}(x|z)\)</span>的分布，在VAE的原始论文给出了两种候选方案：伯努利分布或高斯分布。综上所述，VAE的训练过程可以用下图4表示：</p><center><img src="https://s2.loli.net/2024/05/20/j5x4IMcqKQFk6ul.png" width="80%" height="80%"><div data-align="center">Image4: VAE 训练示意图</div></center><h2 id="采样">采样</h2><p>  在完成对VAE的训练后，我们便可以使用VAE来进行采样，即生成新的样本<span class="math inline">\(x'\)</span>。首先重隐变量的先验分布<span class="math inline">\(N(z; \boldsymbol{0,I})\)</span>中采样出隐变量 <span class="math inline">\(z\)</span>，再利用解码器<span class="math inline">\(p_{\theta}(x|z)\)</span>将隐变量映射到数据空间得到新样本 <span class="math inline">\(x'\)</span>。</p><h2 id="分层-vae">分层 VAE</h2><p>  在对VAE的研究中，一些学者提出了其更具有拓展性的框架，即分层变分自变量(HierarchicalVariational Auto-encoders, HVAE)，将 VAE由单个隐变量拓展到多个隐变量的层次，下图5即为 HVAE 的概率图：</p><center><img src="https://s2.loli.net/2024/05/20/3AXuqS9jR1lNdPw.png" width="80%" height="80%"><div data-align="center">Image5: HVAE 概率图</div></center><p>  通过设置多个隐变量，模型可以逐步学习到更高层次、更抽象的特征。在具有<span class="math inline">\(T\)</span> 个隐变量的HVAE中，我们来考虑一种特殊的情况，即具有马尔可夫性质的 HVAE(MHVAE)，此时，某个隐变量的分布仅取决于与之相邻的隐变量。这样，我们可以对重写概率模型：</p><p><span class="math display">\[\begin{align}    &amp; p(x, z_{1:T}) =p(z_{T})p_{\theta}(x|z_1)\prod_{t=2}^{T}p_{\theta}(z_{t-1}|z_{t})  \tag{29}\\    &amp; q_{\phi}(z_{1:T}|x) = q_{\phi}(z_{1}|x)\prod_{t=2}^{T}q_{\phi}(z_{t}|z_{t-1}) \tag{30} \\\end{align}\]</span></p><p>  同时我们可以将 MHVAE 的 ELBO 写为：</p><p><span class="math display">\[\begin{align}    \log{p(x)} &amp;= \log{\int_{z_{1:T}} p(x, z_{1:T})} dz_{1:T} \notag\\    &amp;= \log{\int_{z_{1:T}} \frac{p(x,z_{1:T})q_{\phi}(z_{1:T}|x)}{q_{\phi}(z_{1:T}|x)}dz_{1:T}} \notag \\    &amp;= \log{\mathbb{E_{q_{\phi}(z_{1:T}|x)}} \left[ \frac{p(x,z_{1:T})}{q_{\phi}(z_{1:T}|x)} \right]} \notag \\    &amp;\ge \mathbb{E_{q_{\phi}(z_{1:T}|x)}} \left[ \log\frac{p(x,z_{1:T})}{q_{\phi}(z_{1:T}|x)} \right] \tag{31}\end{align}\]</span></p><p>  利用(29)式与(30)式，ELBO可以进一步写为：</p><p><span class="math display">\[\begin{align}    \mathbb{E_{q_{\phi}(z_{1:T}|x)}} \left[ \log\frac{p(x,z_{1:T})}{q_{\phi}(z_{1:T}|x)} \right] =\mathbb{E_{q_{\phi}(z_{1:T}|x)}} \left[\log\frac{p(z_{T})p_{\theta}(x|z_1)\prod_{t=2}^{T}p_{\theta}(z_{t-1}|z_{t})}{q_{\phi}(z_{1}|x)\prod_{t=2}^{T}q_{\phi}(z_{t}|z_{t-1})} \right] \tag{32}\end{align}\]</span></p><p>  想必读者们已经注意到，MHVAE的基本架构和 Diffusion Model已经非常相似，接下来我们将介绍 Diffusion Model 在 MHVAE上又做了哪些工作。</p><h2 id="references">References</h2><p><strong>[1] Paper: Luo C. Understanding diffusion models: A unifiedperspective[J]. arXiv preprint arXiv:2208.11970, 2022.</strong><br><strong>[2] Paper: Kingma, D. P. and Welling, M. (2014). Auto-encodingvariational bayes. In Proceedings of the International Conference onLearning Representations (ICLR).</strong><br><strong>[4] Blog: 苏剑林 变分自编码(1-3), 科学空间</strong></p>]]></content>
    
    
    <summary type="html">本节主要介绍生成模型中的变分自编码(VAE)，会论述概率模型、变分下界ELBO、模型训练、采样等内容。</summary>
    
    
    
    <category term="深度学习" scheme="http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>深度学习-4-生成模型2-高斯混合模型</title>
    <link href="http://example.com/2024/05/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-4-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B2-%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B/"/>
    <id>http://example.com/2024/05/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-4-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B2-%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B/</id>
    <published>2024-05-22T08:59:46.000Z</published>
    <updated>2024-05-23T08:02:40.346Z</updated>
    
    <content type="html"><![CDATA[<h1 id="高斯混合模型gmm">高斯混合模型(GMM)</h1><p>  高斯混合模型(Gaussian Mixture Model,GMM)是一种常用的概率生成模型，<strong>其主要思想是通过多个高斯分布的加权组合来表示更为复杂的数据分布</strong>。早在19世纪末期，KariPearson便提出了用高斯分布的混合来处理生物统计方面的问题，之后随着数理统计学科的发展，高斯混合模型的理论得到了很大的完善，但如何高效地进行参数估计一直是其面临的主要问题。直到1977年，Dempster,Laird和Rubin提出了EM算法，用于在含有隐变量的概率模型中进行参数估计。EM算法非常适合于GMM参数的估计问题，EM算法的提出大大推动了GMM的发展。</p><h2 id="概率模型">概率模型</h2><p>  我们有一批训练数据：<span class="math inline">\(T_{train} = \{x_1,x_2,\dotsb,x_{N} \}\)</span>，我们希望去估计数据的分布 <span class="math inline">\(p(x)\)</span>，现在我们对数据的分布做如下假设：</p><ul><li><strong>混合高斯:</strong> 数据分布 <span class="math inline">\(p(x)\)</span>可以表示为多个高斯分布的混合(加权平均)。</li><li><strong>隐变量:</strong> 观测数据 <span class="math inline">\(x\)</span> 的生成受隐变量 <span class="math inline">\(z\)</span> 的影响，隐变量 <span class="math inline">\(z\)</span>决定了多个高斯分布是如何进行混合的(加权系数)。</li></ul><center><img src="https://s2.loli.net/2024/05/19/sdJbKu5qxm8YvBz.jpg" width="80%" height="80%"><div data-align="center">Image1: 混合高斯假设</div></center><p>  图1展示了混合高斯模型的几何理解，黑色的点表示训练数据，左图是一维分布的情况，可以发现训练数据存在两个集中区域，我们便用两个高斯分布(绿色曲线)去表示这个两个区域的数据分布，而真实的数据分布(橙色曲线)可以用这两个高斯分布的加权平均表示。右图是二维分布的情况。<br>  我们假设训练数据 <span class="math inline">\(T_{train}\)</span> 的分布<span class="math inline">\(p(x)\)</span> 是由 <span class="math inline">\(M\)</span> 个高斯分布混合而成，则对于隐变量 <span class="math inline">\(z\)</span>，我们假设其先验分布为一个 <span class="math inline">\(M\)</span> 维的离散分布(2)：</p><p><span class="math display">\[\begin{equation}    p(z = c_{k}) = p_{k}, \quad k = 1,\dotsb, M\end{equation}\]</span></p><p>  给定隐变量 <span class="math inline">\(z\)</span>, 观测变量 <span class="math inline">\(x\)</span> 的条件分布为一个高斯分布(3)：</p><p><span class="math display">\[\begin{equation}    p(x | z) = N(x; \mu_{z}, \Sigma_{z})\end{equation}\]</span></p><p>  通过(2)式与(3)式，训练数据 <span class="math inline">\(T_{train}\)</span> 的分布 <span class="math inline">\(p(x)\)</span>可以表示为多个高斯分布的加权平均(4)：</p><p><span class="math display">\[\begin{align}p(x) &amp;= \int_{z} p(x, z)dz = \sum_{z} p(x, z) \tag{4}\\&amp;=\sum_{k=1}^{M} p(x, z = c_{k}) = \sum_{k=1}^{M} p(z = c_{k})p(x |z = c_{k}) \notag\\&amp;=\sum_{k=1}^{M} p_{k} N(x; \mu_{k}, \Sigma_{k}) \notag\\\end{align}\]</span></p><p>  其中，隐变量的分布的概率值 <span class="math inline">\(p_{k}\)</span>即为高斯分布的加权系数。高斯混合模型的概率图可以表示为图2：</p><center><img src="https://s2.loli.net/2024/05/20/rBiV9cLqkzF1DOs.png" width="60%" height="60%"><div data-align="center">Image2: GMM 概率图</div></center><h2 id="参数估计">参数估计</h2><p>  GMM的参数估计使用了EM算法，EM算法的迭代公式如下(5):</p><p><span class="math display">\[\begin{align}    \theta^{(t+1)} = \argmax_{\theta}{E_{p(z|x, \theta^{(t)})} \left[\log{p(x, z|\theta)} \right]} \tag{5}\end{align}\]</span></p><p>  在GMM中，待估参数 <span class="math inline">\(\theta\)</span>可以为：</p><p><span class="math display">\[\theta = \{p_1,p_2,\dotsb,p_{M};(\mu_1,\Sigma_1),\dotsb,(\mu_{M},\Sigma_{M})\}\]</span></p><p>  接下来我们使用EM算法来求解GMM的参数。<br>  <strong>E-step</strong></p><p><span class="math display">\[\begin{align}    Q(\theta, \theta^{(t)}) &amp;= E_{p(z|x, \theta^{(t)})} \left[\log{p(x, z|\theta)} \right] \tag{6} \\    &amp; = \int_{z} p(z|x, \theta^{(t)})\log{p(x,z|\theta)}dz  \notag\\    &amp;= \sum_{z} \left[ \prod_{i=1}^{N}p(z_{i} | x_{i}, \theta^{(t)})\log{\left( \prod_{i=1}^{N}p(x_i, z_i | \theta) \right)} \right]  \notag\\    &amp;= \sum_{z} \left[\sum_{i=1}^{N}\log{p(x_i,z_i|\theta)}\prod_{i=1}^{N}p(z_{i} | x_{i},\theta^{(t)}) \right]  \tag{7} \\\end{align}\]</span></p><p>  这里，我们需要对(7)式做一些简化，我们来看第1项如何处理：</p><p><span class="math display">\[\begin{align}    &amp; \quad \sum_{z} \left[ \log{p(x_1, z_1 | \theta)}\prod_{i=1}^{N}p(z_i | x_{i}, \theta^{(t)}) \right]  \notag \\    &amp;= \sum_{z_1} \left[ \log{p(x_1, z_1 | \theta)}p(z_{1} | x_1,\theta^{(t)} ) \right] \sum_{z_2, \dotsb, z_{N}} \left[\prod_{i=2}^{N}p(z_i | x_{i}, \theta^{(t)}) \right ] \notag \\    &amp;= \sum_{z_1} \left[ \log{p(x_1, z_1 | \theta)}p(z_{1} | x_1,\theta^{(t)} ) \right] \tag{8} \\\end{align}\]</span></p><p>  将(7)式中的每一项用(8)式的形式进行替代，可以得到 <span class="math inline">\(Q(\theta, \theta^{(t)})\)</span>的化简形式(9)：</p><p><span class="math display">\[\begin{align}    Q(\theta, \theta^{(t)}) = \sum_{i=1}^{N}\sum_{z_i} p(z_i | x_i,\theta^{(t)}) \log{p(x_i, z_i | \theta)} \tag{9}\end{align}\]</span></p><p>  由前文(2)式，(3)式可知：</p><p><span class="math display">\[\begin{align}    p(x_i, z_i |\theta) &amp;= p(z_i | \theta)p(x_i | z_i, \theta)\notag \\    &amp;= \boldsymbol{p}_{z} N(x_i; \mu_{z}, \Sigma_{z}) \tag{10} \\\end{align}\]</span></p><p>  将(10)式带入(9)：</p><p><span class="math display">\[\begin{align}    Q(\theta, \theta^{(t)}) &amp;= \sum_{i=1}^{N}\sum_{z_i} p(z_i | x_i,\theta^{(t)}) \log{\boldsymbol{p}_{z} N(x_i; \mu_{z}, \Sigma_{z})}\notag \\    &amp;= \sum_{k=1}^{M}\sum_{i=1}^{N} \left[ \log{p_{k} + \log{N(x_i;\mu_{k}, \Sigma_{k})}} \right] p(z_i = c_k | x_i,\theta^{(t)})  \tag{11} \\\end{align}\]</span></p><p>  在完成了期望的化简后，接下来对期望进行最大化以求解参数。</p><p><strong>M-step</strong><br>  首先对 隐变量 <span class="math inline">\(z\)</span> 的分布的未知参数<span class="math inline">\(p_k\)</span> 进行求解：</p><p><span class="math display">\[\begin{align}    p_{k} =&amp; \argmax_{p_{k}} \sum_{k=1}^{M}\sum_{i=1}^{N}\log{p_{k}} \cdot p(z_i = c_k | x_i, \theta^{(t)}) \tag{12} \\    &amp; s.t. \sum_{k=1}^{M}p_{k} = 1 \notag\end{align}\]</span></p><p>  优化问题(12)的拉格朗日函数：</p><p><span class="math display">\[\begin{align}    \mathcal{L}(\boldsymbol{p}, \lambda) = \sum_{k=1}^{M}\sum_{i=1}^{N}\log{p_{k}} \cdot p(z_i = c_k | x_i, \theta^{(t)}) +\lambda(\sum_{k=1}^{M}p_{k} - 1) \tag{13}\end{align}\]</span></p><p><span class="math display">\[\begin{align}    \frac{\partial{\mathcal{L}(\boldsymbol{p},\lambda)}}{\partial{p_{k}}} = \sum_{i=1}^{N} \frac{p(z_i = c_k | x_i,\theta^{(t)})}{p_{k}} + \lambda := 0 \tag{14}\end{align}\]</span></p><p><span class="math display">\[\begin{split}    &amp; \Rightarrow \sum_{i=1}^{N} p(z_i = c_k | x_i, \theta^{(t)}) +\lambda p_{k} = 0 \\    &amp; \Rightarrow \sum_{i=1}^{N}\sum_{k=1}^{M} p(z_i = c_k | x_i,\theta^{(t)}) + \lambda\sum_{k=1}^{M}p_{k} =0 \\    &amp; \Rightarrow N+\lambda=0,\quad \lambda= -N\end{split}\]</span></p><p>  将 <span class="math inline">\(\lambda = -N\)</span>带入(14)式可以求得 <span class="math inline">\(p_{k}\)</span>的MLE，即更新后的值为：</p><p><span class="math display">\[\begin{align}    p_{k}^{(t+1)} = \frac{1}{N}\sum_{i=1}^{N}p(z_i = c_k | x_i,\theta^{(t)})  \tag{15}\end{align}\]</span></p><p>  由(4)式与(10)式可得：</p><p><span class="math display">\[\begin{align}    p(z_i = c_{k} | x_i, \theta^{(t)}) &amp;= \frac{p(x_i, z_i=c_{k} |\theta^{(t)})}{p(x_i | \theta^{(t)})}  \notag \\    &amp;= \frac{p_{k}^{(t)}N(x_i;\mu_{k}^{(t)},\Sigma_{k}^{(t)})}{\sum_{k=1}^{M}p_{k}^{(t)}N(x_i;\mu_{k}^{(t)},\Sigma_{k}^{(t)})}  \tag{16}\end{align}\]</span></p><p>  将(16)式带入(15)式得：</p><p><span class="math display">\[p_{k}^{(t+1)} =\frac{1}{N}\sum_{i=1}^{N} \frac{p_{k}^{(t)}N(x_i;\mu_{k}^{(t)},\Sigma_{k}^{(t)})}{\sum_{k=1}^{M}p_{k}^{(t)}N(x_i;\mu_{k}^{(t)},\Sigma_{k}^{(t)})},\quad \boldsymbol{p}^{(t+1)} =\begin{bmatrix}    p_{1}^{(t+1)} \\    p_{2}^{(t+1)} \\    \vdots \\    p_{M}^{(t+1)} \\\end{bmatrix}\]</span></p><p>  同理，可以求得 <span class="math inline">\(\boldsymbol{\mu}^{(t+1)},\boldsymbol{\Sigma}^{(t+1)}\)</span>。</p><h2 id="采样">采样</h2><p>  在对GMM的参数进行估计后，我们便可以使用GMM来进行采样，即生成新的样本<span class="math inline">\(x'\)</span>。GMM的生成过程非常简单，首先根据隐变量<span class="math inline">\(z\)</span>的分布，确定进行采样的高斯分布，然后再从相应的高斯分布采样得到 <span class="math inline">\(x'\)</span>。</p><h2 id="gmm的总结与启示">GMM的总结与启示</h2><p>  从今天的角度来看，GMM是一个比较简单的生成模型，其生成的样本的效果也不尽人意，但我依然想首先介绍GMM是因为其对于生成模型的发展起到了不可忽视的启发作用，具体有如下几点：</p><ul><li><strong>概率分布的混合思想:</strong>GMM的基本思想是通过多个高斯分布的加权组合来表示复杂的数据分布。这一思想为现代生成模型提供了起始，即通过组合简单的概率分布来表示复杂的概率分布。<br></li><li><strong>参数估计:</strong>GMM通过EM算法迭代优化似然函数，这一思想也被现代生成模型所采纳。在VAE、DiffusionModel中，参数的求解仍然是使用SGD等迭代算法对似然函数的下界(ELBO)进行优化。<br></li><li><strong>隐变量假设:</strong>GMM的一个重要特点是引入隐变量来解释观测数据，这为后来的生成模型提供了一个重要框架，现代生成模型几乎都采用了隐变量假设。</li></ul><p>  当然，GMM本身也存在很多缺陷，主要有以下几个方面：</p><ul><li><strong>固定的分布假设:</strong> GMM假设隐变量 <span class="math inline">\(z\)</span>的分布是一个简单的离散分布，这种假设不适应复杂的真实数据。<br></li><li><strong>局部最优解:</strong>GMM的参数估计通常使用EM算法，该算法容易收敛到局部最优解，尤其是在数据分布复杂或高维的情况下。<br></li><li><strong>计算性能:</strong>GMM在处理高维数据时，计算成本和参数估计的复杂性显著增加，难以适应大规模数据集。</li></ul><p>  在之后，我们会介绍变分自编码模型(VAE)，其继承了GMM的主要思想，同时一定程度上改善了GMM的部分缺陷。</p><h2 id="references">References</h2><p><strong>[1] Book: 李航,统计学习方法(第2版)</strong><br><strong>[2] Video: bilibili,shuhuai008,高斯混合模型系列.</strong></p>]]></content>
    
    
    <summary type="html">本节主要介绍生成模型的中的高斯混合模型，会阐述其概率模型、参数估计、采样等部分。</summary>
    
    
    
    <category term="深度学习" scheme="http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>深度学习-3.生成模型1-引言</title>
    <link href="http://example.com/2024/05/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-3-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B1-%E5%BC%95%E8%A8%80/"/>
    <id>http://example.com/2024/05/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-3-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B1-%E5%BC%95%E8%A8%80/</id>
    <published>2024-05-18T07:31:38.000Z</published>
    <updated>2024-05-22T09:18:01.752Z</updated>
    
    <content type="html"><![CDATA[<h1 id="生成模型">生成模型</h1><p>  在过去三年里，人工智能领域发生了翻天覆地的变化，革命性的产品层出不穷，语言生成模型ChatGPT、Llama、Claude;图片生成模型Midjourney、stable diffusion、DALL;视频生成模型Sora。这些人工智能产品带给了世界前所未有的变化，深深影响了人们的生活，英伟达CEO黄仁勋在2023年的GTC大会上直言：“现在是AI的iphone时刻”。在这一轮人工智能狂潮中，以Diffusion Model 为代表的生成模型扮演了至关重要的角色，stablediffusion、DALL、Sora等产品的底层模型便是 Diffusion Model，其对当今 AIGC的发展起到了不可忽视的推动作用。<br>  早在今年的2月份，OpenAI发布Sora的演示视频时，笔者便想创作博客来介绍其底层的Diffusion Model，但思来想去，还是认为自己的理解过于浅显，对于很多地方的motivation并没有比较清晰的认识，生成模型是笔者的研究方向之一，因此我并不想囫囵吞枣，浮于表面，遂埋头继续钻研。直到上个月，我阅读了Google Brain 的 Calvin Luo 创作的介绍 Diffusion Model原理的文章[1]，这篇文章从一个统一的视角介绍了VAE、DiffusionModel、Score-BasedModel，解答了笔者心中的很多疑问，并将我过去半年在生成模型上的认识很好地串联了起来。在完成了对于所学知识的整合后，笔者终于有信心来开始生成模型这个系列。</p><h2 id="数据分布假设">数据分布假设</h2><p>  在生成模型中，我们通常假设数据点 <span class="math inline">\(\boldsymbol{x}\)</span> 来自于一个真实的分布 <span class="math inline">\(p_{true}(x)\)</span>，生成模型的目的就是通过训练数据集<span class="math inline">\(T_{train}\)</span> 去学习 <span class="math inline">\(p_{true}(x)\)</span> 的近似分布 <span class="math inline">\(p_{g}(x)\)</span>，有了这个近似分布后，我们便可以利用这个分布进行采样，从而生成与真实数据点<span class="math inline">\(\boldsymbol{x}\)</span>高度相似的样本。例如，现在我们有一批<span class="math inline">\(256 \times 256\)</span>的猫的图片，我们想要生成一些类似的猫的图片。已有的猫的图片便是我们的训练数据集<span class="math inline">\(T_{cat}\)</span>，每一张图片便是一个数据点 <span class="math inline">\(\boldsymbol{x} \in \mathbb{R}^{256 \times 256\times 3}\)</span>。我们设 "猫的图片(<span class="math inline">\(256\times 256\)</span>)" 是一个统计学上的总体<span class="math inline">\(X_{cat}\)</span>，其分布为 <span class="math inline">\(p_{cat}(x)\)</span>，则总体中的每一个样本，即每一张<span class="math inline">\(256 \times 256\)</span> 的猫的图片便是服从<span class="math inline">\(p_{cat}(x)\)</span> 的一个高维随机向量 <span class="math inline">\(\boldsymbol{x} \in \mathbb{R}^{256 \times 256\times 3}\)</span>。此时，我们的训练数据集 <span class="math inline">\(T_{cat}\)</span> 便可看作是从总体 <span class="math inline">\(X_{cat}\)</span>中抽取的一个样本，其样本分布为<span class="math inline">\(p_{data}(x)\)</span>，由大数定律可知，在大样本条件下，样本分布会趋向于总体分布，故我们可以认为<span class="math inline">\(p_{data} \rightarrowp_{cat}\)</span>，因此如果我们能够学习到 <span class="math inline">\(p_{data}\)</span> 的一个近似分布 <span class="math inline">\(p_{g}\)</span>，则 <span class="math inline">\(p_{g}\)</span> 也是总体 <span class="math inline">\(p_{cat}\)</span> 的一个近似分布，我们便可以利用<span class="math inline">\(p_{g}\)</span>来进行采样，即生成相似的猫的图片。<br>  虽然我们希望能够直接学习数据的分布 <span class="math inline">\(p_{data}\)</span>，但这在实际中是很难实现的，主要有以下两点原因：</p><ul><li><strong>数据分布的复杂性:</strong>真实世界的数据通常具有复杂的高维分布，例如前文提到的猫的图片是便来自于<span class="math inline">\(256 \times 256 \times 3\)</span>维的分布，直接建模这种分布需要非常复杂的函数。<br></li><li><strong>计算复杂度:</strong>直接对高维数据进行概率密度估计需要大量的数据和计算资源，这往往是难以承受的。</li></ul><p>  因此，在实际中，我们通常会引入隐变量假设。<strong>隐变量假设认为存在一些无法直接观测的隐变量<span class="math inline">\(\boldsymbol{z}\)</span>，这些变量可以解释观测数据的统计特性和依赖关系。隐变量与观测变量之间的依赖关系通常通过特定的概率分布来描述，通过引入隐变量，可以将观测数据的复杂分布特征简化为更为简单的分布特征。</strong>在生成模型中，我们所假设的隐变量通常比观测数据的维度要低，通过对隐变量进行建模，我们实际上是进行了数据压缩，对原始的特征空间进行降维，提取出数据的潜在特征，这些特征往往具有较好的可解释性。例如，在VAE中，隐变量可以表示数据的某些关键特征，如图像的形状、颜色等。<br>  还是以猫的图片为例，在观测数据的分布 <span class="math inline">\(p_{data}\)</span>是一个高维分布，我们可以清晰地观察数据 <span class="math inline">\(\boldsymbol{x}\)</span>(猫)的各种细微特征，例如毛发、瞳孔的颜色、牙齿的形状等。而观测数据 <span class="math inline">\(\boldsymbol{x}\)</span> 背后的隐变量 <span class="math inline">\(\boldsymbol{z}\)</span>是对原始数据特征的一种压缩，它的维度比原始数据更低，如果同样将 <span class="math inline">\(\boldsymbol{z}\)</span>看作图片，那么这在这张图片上可能会有猫的大致轮廓、眼睛、耳朵的轮廓、模糊的毛发颜色等，虽然这张图片的细节特征并没有<span class="math inline">\(\boldsymbol{x}\)</span>丰富，但我们仍然可以看出这是一张猫的图片。这张图片 <span class="math inline">\(\boldsymbol{z}\)</span> 便是对观测数据 <span class="math inline">\(\boldsymbol{x}\)</span>的分布特征一种抽象与压缩，它从现实世界中千奇百怪的猫的图片中抽象出猫的一般特征，如果我们的生成模型能够学习到猫的这种一般特征，便能从这种一般特征的基础上去生成各种猫的图片。</p><h2 id="两种学习方式">两种学习方式</h2><p>  在生成模型，对于一些未知的分布，我们通常会采用参数化的概率模型或者神经网络模型是逼近，而对于参数的学习，目前两大主流的方法为：<strong>最大似然法</strong>以及<strong>对抗训练</strong>。最大似然法是统计推断中最经典的参数估计方法，其通过最大化数据分布的似然函数来确定分布中的未知参数，即：</p><p><span class="math display">\[\begin{equation}    \hat{\theta} = \argmax_{\theta} \log{p_{\theta}(x)}\end{equation}\]</span></p><p>  现如今很多生成模型的学习策略便采用了最大似然法，例如高斯混合模型(GMM)、变分自编码(VAE)、扩散模型(DiffusionModel)。<br>   (1) 式的优化并不是一项简单的工作，在 GMM中，由于模型较为简单，我们可以采用 EM 算法进行求解，而在 VAE、DiffusionModel 中，我们则是对似然函数 <span class="math inline">\(\log{p(x)}\)</span> 的证据下界(Evidence LowerBound) 进行优化。然而在 2014 年，lan.Goodfellow提出了一种完全不同的学习策略，可以绕过求解最大似然这个棘手的问题，这便是基于对抗思想的生成对抗网络(GAN)，对于GAN 的主要思想，可以阅读笔者之前的一篇博客文章<a href="https://sxusongjh.github.io/2023/11/06/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-2-Paper-%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9CGANs%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8D%81%E5%B9%B4%E9%97%B4%E6%9C%80%E9%85%B7%E7%9A%84idea/">《生成对抗网络GANs——深度学习二十年间最酷的idea!》</a>，在这篇博客中我介绍了GAN 的原始论文，感兴趣的读者可以自行阅读。<br>  由于本系列博客的初衷是介绍扩散模型，为了视角的统一，笔者会主要介绍几类基于最大似然的生成模型，包括GMM、VAE、DiffusionModel、Score-based Model。</p><h2 id="conference">Conference</h2><p><strong>[1] Paper: Luo C. Understanding diffusion models: A unifiedperspective[J]. arXiv preprint arXiv:2208.11970, 2022.</strong></p>]]></content>
    
    
    <summary type="html">本节主要介绍生成模型的基本概念。</summary>
    
    
    
    <category term="深度学习" scheme="http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>Optimal Transport-6.Sinkhorn Algorithm</title>
    <link href="http://example.com/2024/05/06/Optimal%20Transport-6.Sinkhorn%20Algorithm/"/>
    <id>http://example.com/2024/05/06/Optimal%20Transport-6.Sinkhorn%20Algorithm/</id>
    <published>2024-05-06T08:11:33.000Z</published>
    <updated>2024-05-07T01:43:32.179Z</updated>
    
    <content type="html"><![CDATA[<h1 id="sinkhorn-algorithm">Sinkhorn Algorithm</h1><p>  在上一节中我们介绍了带熵正则项的最优传输问题，通过在原始的最优传输问题中引入熵正则项，可以原问题转化为严格的凸问题，从而可以使用更高效的算法来更快地求解最优传输问题，计算效率的提高在最优传输的应用领域至关重要。本节我们将会介绍求解带熵正则项的最优传输问题的经典算法——<strong>Sinkhorn算法</strong>。我们将会从算法推导、收敛性证明、代码实现几个方面来讨论。<br>  Sinkhorn算法是一种用于解决凸优化问题的迭代算法，主要用于解决带有非负矩阵约束的最优传输问题。其主要思想是通过迭代地调整两个非负矩阵的行和列，使它们近似满足指定的边际约束条件。<br>  Sinkhorn算法最早由RichardSinkhorn在1964年提出，用于解决数学物理中的问题。后来，该算法被应用于最优传输问题，并在机器学习、图像处理等领域得到了广泛应用。随着计算机算力的提高和研究的深入，Sinkhorn算法及其变体在处理大规模数据和优化问题上发挥了重要作用。</p><h2 id="algorithm-iteration">Algorithm iteration</h2><p>  首先，我们来回顾一下上一节熵正则的内容。在引入熵正则项后，原始的最优传输问题变成了如下(1)形式：</p><p><span class="math display">\[\begin{equation}    L_{\boldsymbol{C}}^{\varepsilon}(\boldsymbol{a},\boldsymbol{b}) :=\min_{\boldsymbol{P} \in \boldsymbol{U}(\boldsymbol{a},\boldsymbol{b})}\left&lt; \boldsymbol{P},\boldsymbol{C} \right&gt; - \varepsilonH(\boldsymbol{P})\end{equation}\]</span></p><p><span class="math display">\[U(\boldsymbol{\alpha, \beta}):=\{\boldsymbol{P}: \boldsymbol{P1_{m}}=\boldsymbol{a},\boldsymbol{P^{T}1_{n}}=\boldsymbol{b} \}, \quadH(\boldsymbol{P})=-\sum_{i,j}p_{ij}\log{p_{ij}}\]</span></p><p>  为了求导的便利性，我们将熵的表达式做一些小的修改：</p><p><span class="math display">\[H(\boldsymbol{P})=-\sum_{i,j}p_{ij}\log{p_{ij}}+1=-\sum_{i,j}p_{ij}(\log{p_{ij}}-1)\]</span></p><p>  由于只是在原始的优化问题的目标函数上加了一个常数，故此修改并不会对问题(1)的最优解。为了推导Sinkhorn算法的迭代公式，我们首先来写出优化问题(1)的拉格朗日函数：</p><p><span class="math display">\[\mathcal{L}(\boldsymbol{P,f,g})=\left&lt;\boldsymbol{P,C} \right&gt;-\varepsilon H(\boldsymbol{P})-\left&lt;\boldsymbol{f,P1_{m}-a} \right&gt;-\left&lt; \boldsymbol{g,P^{T}1_{n}-b}\right&gt;\]</span></p><p>  由费马定理可知，对于优化问题(1)的最优解，有下式成立：</p><p><span class="math display">\[\frac{\partial\mathcal{L}(\boldsymbol{P,f,g})}{\partialp_{ij}}=c_{ij}+\varepsilon\log{p_{ij}}-f_{i}-g_{j}=0\]</span></p><p><span class="math display">\[\Rightarrowp_{ij}=e^{\frac{f_i+g_j-c_{ij}}{\varepsilon}}=e^{\frac{f_i}{\varepsilon}}e^{\frac{-c_{ij}}{\varepsilon}}e^{\frac{g_j}{\varepsilon}}\]</span></p><p>  由以上的结果可知，优化问题(1)的最优解的元素可以分解成三部分之积，进一步，我们可以将以上结果写成矩阵形式，设：</p><p><span class="math display">\[\boldsymbol{u}=\begin{bmatrix}    e^{\frac{f_1}{\varepsilon}} \\    e^{\frac{f_2}{\varepsilon}} \\    \vdots \\    e^{\frac{f_n}{\varepsilon}} \\\end{bmatrix},\quad \boldsymbol{v}=\begin{bmatrix}    e^{\frac{g_1}{\varepsilon}} \\    e^{\frac{g_2}{\varepsilon}} \\    \vdots \\    e^{\frac{g_m}{\varepsilon}} \\\end{bmatrix},\quad \boldsymbol{K}=\begin{bmatrix}    e^{\frac{-c_{11}}{\varepsilon}} &amp;e^{\frac{-c_{12}}{\varepsilon}} &amp; \dotsb &amp;e^{\frac{-c_{1m}}{\varepsilon}} \\    e^{\frac{-c_{21}}{\varepsilon}} &amp;e^{\frac{-c_{22}}{\varepsilon}} &amp; \dotsb &amp;e^{\frac{-c_{2m}}{\varepsilon}} \\      \vdots &amp; \vdots &amp;  &amp; \vdots \\    e^{\frac{-c_{n1}}{\varepsilon}} &amp;e^{\frac{-c_{n2}}{\varepsilon}} &amp; \dotsb &amp;e^{\frac{-c_{nm}}{\varepsilon}} \\\end{bmatrix}\]</span></p><p>  则优化问题(1)的最优解可以写成：</p><p><span class="math display">\[\boldsymbol{P}=diag(\boldsymbol{u})\boldsymbol{K}diag(\boldsymbol{v})\]</span></p><p>  考虑边际约束条件：</p><p><span class="math display">\[\left \{\begin{array}{l}\boldsymbol{P1_{m}}=diag(\boldsymbol{u})\boldsymbol{K}diag(\boldsymbol{v})\boldsymbol{1_{m}}=\boldsymbol{a}\\\boldsymbol{P^{T}1_{n}}=  diag(\boldsymbol{v})\boldsymbol{K^{T}}diag(\boldsymbol{u})\boldsymbol{1_{n}}=\boldsymbol{b}\end{array} \right.\]</span></p><p><span class="math display">\[\becausediag(\boldsymbol{u})\boldsymbol{1_{n}}=\boldsymbol{u}, \quaddiag(\boldsymbol{v})\boldsymbol{1_{m}}=\boldsymbol{v}\]</span></p><p><span class="math display">\[\Rightarrow \left \{\begin{array}{l}\boldsymbol{P1_{m}}=diag(\boldsymbol{u})(\boldsymbol{K}\boldsymbol{v})=\boldsymbol{u} \odot(\boldsymbol{Kv})=\boldsymbol{a} \\\boldsymbol{P^{T}1_{n}} =diag(\boldsymbol{v})(\boldsymbol{K^{T}}\boldsymbol{u})=\boldsymbol{v}\odot (\boldsymbol{K^{T}u})=\boldsymbol{b} \\\end{array} \right.\]</span></p><p>  在实际求解优化问题(1)时，由于成本矩阵 <span class="math inline">\(\boldsymbol{C}\)</span> 与惩罚系数 <span class="math inline">\(\varepsilon\)</span> 是已知的，故矩阵 <span class="math inline">\(\boldsymbol{K}\)</span>是已知的，因此我们需要求解向量 <span class="math inline">\((\boldsymbol{u,v})\)</span>使得其满足边际条件，则问题(1)的最优解即为：<span class="math inline">\(\boldsymbol{P}=diag(\boldsymbol{u})\boldsymbol{K}diag(\boldsymbol{v})\)</span>.该问题在数值分析领域被称为矩阵缩放问题，一个直观的解决方法是通过迭代求解。<strong>首先固定住<span class="math inline">\(\boldsymbol{v}\)</span>，更新<span class="math inline">\(\boldsymbol{u}\)</span>，使其满足边界条件；然后固定更新后的<span class="math inline">\(\boldsymbol{u}\)</span>，更新<span class="math inline">\(\boldsymbol{v}\)</span>，使其同样满足边界条件。</strong>这便是Sinkhorn算法的基本思想，写成迭代公式如下：</p><p><span class="math display">\[\Rightarrow \left \{\begin{array}{l}\boldsymbol{u}^{(l+1)} =\frac{\boldsymbol{a}}{\boldsymbol{K}\boldsymbol{v}^{(l)}} \\\\\boldsymbol{v}^{(l+1)} =\frac{\boldsymbol{b}}{\boldsymbol{K}\boldsymbol{u}^{(l+1)}} \\\end{array} \right. \quad\boldsymbol{v}^{(0)}=\boldsymbol{1_{m}}\]</span></p><p>  其中，除法运算表示向量之间各个元素对应相除。<span class="math inline">\(\boldsymbol{v}^{(0)}\)</span>可以初始化为任意的正值向量。通过不断的迭代，我们可以得到近似解<span class="math inline">\((\boldsymbol{u^{*},v^{*}})\)</span>，此时，最优运输矩阵及SinkhornDistance则为：</p><p><span class="math display">\[\boldsymbol{P^{*}} =diag(\boldsymbol{u^{*}})\boldsymbol{K}diag(\boldsymbol{v^{*}}), \quadSinkhorn \ Distance = \left&lt; \boldsymbol{P^{*}, C}\right&gt;\]</span></p><p>  矩阵<span class="math inline">\(\boldsymbol{K}\)</span>被称为 Gibbsdistributions，实际上，对于优化问题(1)的最优解<span class="math inline">\(\boldsymbol{P^{*}}\)</span>，其实际上是矩阵<span class="math inline">\(\boldsymbol{K}\)</span>在可行解集<span class="math inline">\(U(\boldsymbol{a,b})\)</span>上的投影，即(2)：</p><p><span class="math display">\[\begin{equation}    \boldsymbol{P^{*}}=Proj_{U(\boldsymbol{a,b})}^{KL}(\boldsymbol{K}):= \argmin_{\boldsymbol{P \in U(\boldsymbol{a,b})}} KL(\boldsymbol{P |K})\end{equation}\]</span></p><p>  问题(2)被称为<strong>静态薛定谔问题</strong>。进一步地，Sinkhorn算法的迭代过程实际上也是一个迭代投影过程，设：</p><p><span class="math display">\[\mathcal{C}_{\boldsymbol{a}}^{1}:=\{\boldsymbol{P}: \boldsymbol{P1_{m}=a} \} \quad and \quad\mathcal{C}_{\boldsymbol{b}}^{2} := \{ \boldsymbol{P}:\boldsymbol{P^{T}1_{n}=b} \}\]</span></p><p>  则有 <span class="math inline">\(U(\boldsymbol{a,b})=\mathcal{C}_{\boldsymbol{a}}^{1}\cap\mathcal{C}_{\boldsymbol{b}}^{2}\)</span>，<strong>Bregman迭代投影</strong>的过程如下：</p><p><span class="math display">\[\boldsymbol{P}^{(l+1)} :=Proj_{\mathcal{C}_{\boldsymbol{a}}^{1}}^{KL}(\boldsymbol{P}^{(l)}) \quadand \quad \boldsymbol{P}^{(l+2)} :=Proj_{\mathcal{C}_{\boldsymbol{b}}^{2}}^{KL}(\boldsymbol{P}^{(l+1)})\]</span></p><p>  其与Sinkhorn迭代过程的对应关系为：</p><p><span class="math display">\[\boldsymbol{P}^{(2l)} :=diag(\boldsymbol{u}^{(l)})\boldsymbol{K}diag(\boldsymbol{v}^{(l)})\]</span></p><p><span class="math display">\[\boldsymbol{P}^{(2l+1)} :=diag(\boldsymbol{u}^{(l+1)})\boldsymbol{K}diag(\boldsymbol{v}^{(l)})\]</span></p><p><span class="math display">\[\boldsymbol{P}^{(2l+2)} :=diag(\boldsymbol{u}^{(l+1)})\boldsymbol{K}diag(\boldsymbol{v}^{(l+1)})\]</span></p><h2 id="algorithm-covergence">Algorithm Covergence</h2><h2 id="algorithm-implementation">Algorithm Implementation</h2><p>  接下来，我们来通过代码实现一下Sinkhorn算法，首先来给定原始分布、目标分布、成本矩阵以及惩罚系数：</p><p><span class="math display">\[\boldsymbol{a} = \begin{bmatrix}    0.2 \\    0.5 \\    0.3 \\\end{bmatrix}, \boldsymbol{b}=\begin{bmatrix}    0.3 \\    0.4 \\    0.3 \\\end{bmatrix}, \quad \boldsymbol{C}=\begin{bmatrix}    0.1 &amp; 0.2 &amp; 0.3 \\    0.4 &amp; 0.5 &amp; 0.6 \\    0.7 &amp; 0.8 &amp; 0.9 \\\end{bmatrix},\quad \varepsilon=0.01\]</span></p><p>  相应的Python代码为：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">a = np.array([<span class="hljs-number">0.2</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.3</span>])<br>b = np.array([<span class="hljs-number">0.3</span>, <span class="hljs-number">0.4</span>, <span class="hljs-number">0.3</span>])<br>C = np.array([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>],<br>              [<span class="hljs-number">0.4</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.6</span>],<br>              [<span class="hljs-number">0.7</span>, <span class="hljs-number">0.8</span>, <span class="hljs-number">0.9</span>]])<br>reg_param = <span class="hljs-number">0.01</span><br></code></pre></td></tr></tbody></table></figure><p>  按照前文推导出的Sinkhorn算法的迭代公式，我们可以写出其实现代码：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sinkhorn</span>(<span class="hljs-params">a, b, C, reg_param, max_iters=<span class="hljs-number">100</span></span>):<br><br>    n = <span class="hljs-built_in">len</span>(a)<br>    m = <span class="hljs-built_in">len</span>(b)<br>    u = np.ones(n)<br>    v = np.ones(m)<br>    K = np.exp(-C / reg_param)<br><br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_iters):<br>        u = a / (K.dot(v))<br>        v = b / (K.T.dot(u))<br><br>    transport_matrix = np.diag(u).dot(K).dot(np.diag(v))<br>    sinkhorn_distance = np.<span class="hljs-built_in">sum</span>(transport_matrix * C)<br><br>    <span class="hljs-keyword">return</span> transport_matrix, sinkhorn_distance<br><br>transport_matrix, sinkhorn_distance = sinkhorn(a, b, C, reg_param)<br>```  <br>&amp;emsp;&amp;emsp;我们同样可以使用Python中专门用于解决最优传输领域问题的库POT来进行求解，其代码如下：  <br><br>```python<br><span class="hljs-keyword">import</span> ot<br><br>transport_matrix = ot.sinkhorn(a, b, C, reg_param)<br>sinkhorn_distance = ot.sinkhorn2(a, b, C, reg_param)<br></code></pre></td></tr></tbody></table></figure><p>  这两种求解方法得到最优运输矩阵及Sinkhorn距离相同，均为：</p><p><span class="math display">\[\boldsymbol{P^{*}}=\begin{bmatrix}    0.06 &amp; 0.08 &amp; 0.06 \\    0.15 &amp; 0.2 &amp; 0.15 \\    0.09 &amp; 0.12 &amp; 0.09 \\\end{bmatrix}, \quad Sinkhorn \ Distance = 0.53\]</span></p><h2 id="reference">Reference</h2><ul><li><strong>[1] Book: Peyré G, Cuturi M. Computational optimaltransport[J]. Center for Research in Economics and Statistics WorkingPapers, 2017 (2017-86).</strong></li></ul>]]></content>
    
    
    <summary type="html">本节主要介绍用于求解带熵正则项的最优传输问题的Sinkhorn算法。</summary>
    
    
    
    <category term="最优传输理论" scheme="http://example.com/categories/%E6%9C%80%E4%BC%98%E4%BC%A0%E8%BE%93%E7%90%86%E8%AE%BA/"/>
    
    
  </entry>
  
  <entry>
    <title>Optimal Transport-5.Entropy Regularization</title>
    <link href="http://example.com/2024/04/20/Optimal%20Transport-5.Entropy%20Regulazation/"/>
    <id>http://example.com/2024/04/20/Optimal%20Transport-5.Entropy%20Regulazation/</id>
    <published>2024-04-20T04:44:17.000Z</published>
    <updated>2024-04-22T14:23:30.688Z</updated>
    
    <content type="html"><![CDATA[<h1 id="entropy-regularization">Entropy Regularization</h1><p>  在之前的文章中，我们从概率视角描述了最优传输问题。设 <span class="math inline">\(X,Y\)</span> 是服从分布 <span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta}\)</span>的两个随机变量，运输矩阵为 <span class="math inline">\(\boldsymbol{P}\)</span>，成本矩阵为 <span class="math inline">\(\boldsymbol{C}\)</span>，则分布 <span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta}\)</span>之间的最优传输问题可以被定义为(1)式：</p><p><span class="math display">\[\begin{equation}    L_{\boldsymbol{C}}(\boldsymbol{\alpha},\boldsymbol{\beta}) :=\min_{\boldsymbol{P} \in\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta})} \left&lt;\boldsymbol{P},\boldsymbol{C} \right&gt; = \min_{(X,Y)} \{\mathbb{E}_{(X,Y)}(c(X,Y)): X \sim \boldsymbol{\alpha}, Y \sim\boldsymbol{\beta} \}\end{equation}\]</span></p><p>  我们在之前讨论过，运输矩阵 <span class="math inline">\(\boldsymbol{P}\)</span> 可以看作随机变量 <span class="math inline">\(X,Y\)</span>的联合分布。从Monge的前向运输法到Kantorovich的局部前向运输，最优传输问题的解通常是满足边界条件下，"耦合度"最低的运输方式，即最优运输矩阵<span class="math inline">\(\boldsymbol{P}\)</span>是一个很稀疏的矩阵，其非零元素主要集中在对角线附近。如果我们用概率论与信息论的视角来分析，这意味原始最优传输问题的解是<strong>满足边际分布条件下，熵最小的联合分布。</strong><br>  熵正则的主要思想对联合分布<span class="math inline">\(\boldsymbol{P}\)</span>的信息熵进行惩罚，以期望求得的联合分布<span class="math inline">\(\boldsymbol{P^{*}}\)</span>的熵更大。熵正则的数学表达式如下(2)式：</p><p><span class="math display">\[\begin{equation}    L_{\boldsymbol{C}}^{\varepsilon}(\boldsymbol{\alpha},\boldsymbol{\beta}):= \min_{\boldsymbol{P} \in\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta})} \left&lt;\boldsymbol{P},\boldsymbol{C} \right&gt; - \varepsilon H(\boldsymbol{P})\end{equation}\]</span></p><p>  其中，常数 <span class="math inline">\(\varepsilon\)</span>为惩罚系数，用于控制对联合分布的熵的惩罚力度，<span class="math inline">\(\varepsilon\)</span> 越大，则我们解得的联合分布<span class="math inline">\(\boldsymbol{P^{*}}\)</span>的熵越大。除了这种形式，有一些文献也将熵正则的定义为如下形式(3)式：</p><p><span class="math display">\[\begin{equation}    L_{\boldsymbol{C}}^{\gamma}(\boldsymbol{\alpha},\boldsymbol{\beta}):= \min_{\boldsymbol{P} \in\boldsymbol{U_{\gamma}}(\boldsymbol{\alpha},\boldsymbol{\beta})}\left&lt; \boldsymbol{P},\boldsymbol{C} \right&gt;\end{equation}\]</span></p><p><span class="math display">\[\boldsymbol{U_{\gamma}}(\boldsymbol{\alpha},\boldsymbol{\beta})= \{ \boldsymbol{P} \in\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta}) |KL(\boldsymbol{P} || \boldsymbol{\alpha \beta^{T}}) \leq \gamma \} = \{\boldsymbol{P} \in\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta}) |h(\boldsymbol{P}) \ge h(\boldsymbol{\alpha}) + h(\boldsymbol{\beta}) -\gamma  \} \subset\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta})\]</span></p><p>  (2)和(3)两种熵正则的形式实际上是等价的，熵正则在最优传输理论中起着十分重要的作用，<strong>在本节我们将会讨论熵正则的作用；信息论以及几何视角下的熵正则内涵，以及证明其求解结果能够作为分布之间距离的度量。</strong></p><h2 id="why-entropy-regularzation">Why Entropy Regularzation ?</h2><p>  对于熵正则的作用，我主要会从两个方面来讨论，<strong>数值计算以及最大熵思想。</strong></p><h3 id="lightspeed-computation-of-optimal-transport">LightspeedComputation of Optimal Transport</h3><p>  在很多对于最优传输问题中熵正则项的作用，大部分文章都会提到引入正则项能够提高最优传输问题的计算效率，这当然是其最突出的优点。<br>  原始的最优传输问题是一个线性规划问题，对于线性规划问题，通常的求解方法为单纯形法以及内点法，这些算法的计算复杂度为<span class="math inline">\(O(d^{3}\log(d))\)</span>，<span class="math inline">\(d\)</span>为随机变量的维度。这些算法能够处理一些简单的最优传输问题，但随着维度的增加，巨大的计算成本使得最优传输很难应用到高维数据中。而在机器学习领域中，很多数据都是高维数据，例如长宽均为256的一张图片，其一个数据样本便是<span class="math inline">\(\mathbb{R}^{256 \times 256 \times3}\)</span>的空间中的一个向量。如果想在这种数据集上应用最优传输理论来做一些工作，例如图像生成、图像匹配、风格变化等，庞大的计算成本便是一个很难解决的问题。<br>  <strong>通过在原始的最优传输问题中引入一个熵正则项，可以使得该优化问题变成一个严格的凸问题。</strong>最优传输问题本身在没有正则化时可能不是严格凸的，特别是在离散设置中，可能存在多个最优解，这使得我们求解的计算成本大大增加，例如在单纯形算法中，如果我们需要找到所有的最优解，我们可能需要遍历多面体所有极点，这在高维问题中意味着庞大的计算开销，因为此时多面体可能存在很多极点。而严格的凸性意味着该优化问题只存在唯一的最优解，此时我们只要找到一个最优解，则这个最优解一定是唯一的，算法便可以终止了。对于严格的凸问题我们已经有很多高效的数值求解方法。<br>  <strong>引入熵正则项后的最优传输问题可以通过特定的算法，如Sinkhorn迭代算法来高效求解。</strong>这些算法通常基于矩阵缩放技术，即通过迭代调整矩阵的行和列以使得传输矩阵逼近目标边缘分布。这种方法简单、高效，并且易于实现。Franklin和Lorenz(1989) 以及 Knight (2008)的研究表明，Sinkhorn算法具有线性收敛性。这意味着每次迭代后，误差以固定比率减少，确保了算法在实际应用中的快速收敛。对于Sinkhorn算法，我们将在下一篇博客中详细讨论，这里不作过多的展开。<br>  <strong>Sinkhorn算法能够依靠矩阵乘法的运算来进行迭代，这使得我们能够使用GPU强大的并行计算能力来快速求解高维空间中的最优传输问题。</strong>近二十年来，计算机硬件厂商对GPU不断地进行升级迭代，使得电脑GPU的性能不断提升，而GPU的多核结构十分适合用于矩阵运算，计算性能的提示是得以往一些难以取得进展的领域得到了突破，深度学习便是一个很好的例子，而深度学习的兴起又反过来促使硬件厂商不断提升GPU性能，GPU的算力得到了高速发展。Sinkhorn算法可以使用矩阵乘法进行迭代运算，这使得最优传输问题的求解能够享受到GPU性能不断提升的红利，这大大促进了最优传输理论在机器学习领域的应用。</p><h3 id="maximum-entropy-principle">Maximum Entropy Principle</h3><p>  除开耳熟能详的计算优势外，熵正则所蕴含的最大熵原理的思想是另外一个我想重点讨论的点。熵这个概念最早可以追溯到19世纪R.J.E.Clausius对于热力学的研究，他引入熵这个概念来描述系统的无序程度或者信息缺失的状态。热力学第二定律指出，<strong>一个孤立的系统的总熵不会自发减少，即熵总是趋于增加的。</strong>最大熵思想最初由物理学家 J.W.Gibbs在《物理学原理》这本著作中提出。Gibbs讨论了如何从统计力学的角度来理解熵，以及如何通过概率分布来计算系统的熵，特别是在平衡状态下。Gibbs的工作强调了<strong>在已知部分信息的情况下，选择熵最大的分布可以带来最大的“不确定性”或“混乱度”，这种分布认为每一种可能状态出现的机会均等，直到有额外信息来提供更多的偏好。</strong><br>  我们用一个例子来说明一下系统的这种熵增现象。Albert Einstein在1905年发表了一篇划时代的论文《关于运动着的水分子所引起的、需用显微镜观察的悬浮粒子的运动》，在这篇论文中，他发展了数学模型来描述微观粒子在流体中的随机运动，这种运动后来被称为布朗运动，也称随机扩散过程。我们便用粒子的随机扩散过程体会熵增的过程。<br>  假设现在有一条“又长又细的水管”，我们可以将这个水管看作实数轴，我们在这个实数轴的原点<span class="math inline">\(O\)</span>处滴入一滴粒子数量为<span class="math inline">\(C\)</span>的墨水，设 <span class="math inline">\(\tau\)</span> 代表扩散过程的时间间隔，<span class="math inline">\(\rho(y)\)</span> 表示在时间<span class="math inline">\(\tau\)</span>内粒子移动距离为<span class="math inline">\(y\)</span>的概率分布，其是一个概率密度函数。<span class="math inline">\(f(x,t)\)</span> 表示<span class="math inline">\(t\)</span>时刻在实数轴<span class="math inline">\(x\)</span>处的粒子的数量，其也是一个概率密度函数，同时是关于时间<span class="math inline">\(t\)</span>的一个随机过程。<br>  假设扩散过程是对称的，即<span class="math inline">\(\rho(y)\)</span>和<span class="math inline">\(f(x,t)\)</span>均为关于原点<span class="math inline">\(O\)</span>的对称分布，此时有:</p><p><span class="math display">\[\rho(y) = \rho(-y) \Leftrightarrow\int_{-\infty}^{+\infty}y\rho(y)=0\]</span></p><p>  由假设我们可以得知这个系统的初始状态 <span class="math inline">\(f(0,0) =C\)</span>，即墨水集中在原点处，扩散过程还没有发生，<strong>此时我们可以将这种初始状态看作一个狄拉克<span class="math inline">\(\delta\)</span>分布，这种分布是熵最小的分布，其熵实际上为零，系统不具有随机性。</strong>我们要来推导<span class="math inline">\(f(x,t)\)</span>的表达式，由粒子质量守恒定律可以得到方程：</p><p><span class="math display">\[f(x,t+\tau) =\int_{-\infty}^{+\infty}f(x-y,t)\rho(y)dy\]</span></p><p>  另外，如果我们把<span class="math inline">\(f(x,t)\)</span>看作关于<span class="math inline">\(t\)</span>的一元函数，我们可以将<span class="math inline">\(f(x,t+\tau)\)</span>在<span class="math inline">\(t\)</span>处进行泰勒展开：</p><p><span class="math display">\[f(x,t+\tau) = f(x,t)+\frac{\partialf}{\partial t}\tau+O(\tau)\]</span></p><p>  同理，我们可以把<span class="math inline">\(f(x-y,t)\)</span>在<span class="math inline">\(x\)</span>处展开：</p><p><span class="math display">\[f(x-y,t)=f(x,t) - \frac{\partialf}{\partial x}y + \frac{1}{2} \frac{\partial^{2}f}{\partialx^{2}}y^{2}+O(y^{2})\]</span></p><p>  联立以上三个等式得到扩散过程的SDE：</p><p><span class="math display">\[f(x,t)+\frac{\partial f}{\partial t}\tau= \int_{-\infty}^{+\infty} \left[ f(x,t) - \frac{\partial f}{\partialx}y + \frac{1}{2} \frac{\partial^{2}f}{\partial x^{2}}y^{2} \right]\rho(y)dy\]</span></p><p>  通过化简，我们得到了最终的<strong>扩散方程(4):</strong></p><p><span class="math display">\[\begin{equation}    \frac{\partial f}{\partial t} = \frac{D}{2\tau}\frac{\partial^{2}f}{\partial x^{2}}\end{equation}\]</span></p><p>  求解这个偏微分方程，我们发现<strong>随机过程<span class="math inline">\(f(x,t)\)</span>是一个高斯过程(5):</strong></p><p><span class="math display">\[\begin{equation}    f(x,t) = \frac{1}{\sqrt{2\pi Ct}}\exp\left( -\frac{x^{2}}{2Ct}\right)\end{equation}\]</span></p><p>  在之前机器学习系列的《交叉熵与KL散度》这篇博客文章中，我们证明了在给定均值与方差的条件下，高斯分布具有最大的熵，具体来说，<strong>若有定义在整个实数轴上的随机变量<span class="math inline">\(X\)</span>，给定其均值为<span class="math inline">\(\mu\)</span>，方差为<span class="math inline">\(\sigma^{2}\)</span>，则当<span class="math inline">\(X \sim N(\mu,\sigma^{2})\)</span>时，随机变量<span class="math inline">\(X\)</span>的熵<span class="math inline">\(H(X)\)</span>最大，其熵为<span class="math inline">\(H(X)=\frac{1}{2}\log(2\pi e\sigma^{2})\)</span>。而粒子的扩散过程是一个高斯过程，其在每个时间点<span class="math inline">\(t\)</span>是都是一个高斯分布，具有最大的信息熵，同时随着时间<span class="math inline">\(t\)</span>的增大，高斯分布的方差逐渐增大，其熵会进一步增大，当时间<span class="math inline">\(t\)</span>趋于无穷时，高斯分布的熵也趋于无穷。整个粒子的扩散过程从开始熵为零的单点分布，到熵为无穷的高斯分布，系统的扩散总是趋向于熵最大的方式。</strong></p><center><img src="https://s2.loli.net/2024/04/22/k1cszEIoYygA9MU.jpg" width="60%" height="60%"><div data-align="center">Image1: 扩散过程</div></center><p><br></p><p>  举上面这个例子，我是想让读者对熵增的过程有一个直观的感受，最大熵原理便是基于熵增定律，选择熵最大的分布是一个理智的选择，因为这符合自然界的规律。言归正传，最大熵原理和最优传输的熵正则理论有什么联系了？实际上我们只要对(2)式做一个小的等价变形，就可以发现其中的联系了，(2)式实际上也等价于(6)：</p><p><span class="math display">\[\begin{equation}    L_{\boldsymbol{C}}^{\eta}(\boldsymbol{\alpha},\boldsymbol{\beta}) :=\max_{\boldsymbol{P} \in\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta})}  H(\boldsymbol{P})- \eta \left&lt; \boldsymbol{P},\boldsymbol{C} \right&gt;\end{equation}\]</span></p><p>  <strong>我们原先对于熵正则的理解是在考虑熵的约束的条件下使得运输成本最小化，(6)式告诉我们，从最大熵原理的角度看，熵正则也可以理解成在考虑运输成本的情况下，使得联合分布<span class="math inline">\(P(X,Y)\)</span>的熵最大。</strong><br>  实际上，早在1969年，Alan Wilson在将最优传输理论应用到交通运输领域时，便考虑到了最大熵原理。交通系统往往具有复杂性于动态性，其包含了多种变量的随机性。例如，交通流量会受到天气、时间、事故等因素的影响。最优运输问题的传统解决方案往往倾向于找出成本最低的几条路线，这在理论上是有效的，但这些路线可能无法准确地反映这些动态变化。Wilson在最优传输理论中引入熵正则项，这种正则化旨在使运输方案更加现实，通过平滑运输路线的分布，从而更好地模仿现实世界网络中观察到的实际交通流。</p><h2 id="from-the-perspective-of-information-theory-and-geometry">Fromthe Perspective of Information Theory and Geometry</h2><h3 id="entropic-constraints-on-joint-probabilities">EntropicConstraints on Joint Probabilities</h3><p>  在(1)式中，运输矩阵<span class="math inline">\(\boldsymbol{P}\)</span>的可行解集<span class="math inline">\(U(\boldsymbol{\alpha},\boldsymbol{\beta})\)</span>可以看作随机变量<span class="math inline">\(X,Y\)</span>的联合分布。对于联合分布<span class="math inline">\(P(X,Y)\)</span>，有：</p><p><span class="math display">\[\boldsymbol{\hat{P}} =\argmax_{\boldsymbol{P} \in U(\boldsymbol{\alpha,\beta})}H(\boldsymbol{P}) = \boldsymbol{\alpha \beta^{T}}\]</span></p><p>  即当<span class="math inline">\(P(X,Y)=\boldsymbol{\alpha\beta^{T}}\)</span>时，联合分布的熵最大。如果我们想要最优传输问题的解具有较大的熵，则我们可以将最优传输问题的可行解集约束到<span class="math inline">\(\boldsymbol{\hat{P}}\)</span>附近，这样可行使得求解出的联合分布便可以具有具有较大的信息熵。要实现这种约束，可以对解集<span class="math inline">\(U(\boldsymbol{\alpha,\beta})\)</span>中的<span class="math inline">\(\boldsymbol{P}\)</span>与<span class="math inline">\(\boldsymbol{\hat{P}}\)</span>的KL散度进行约束，这样可以使得解集中的<span class="math inline">\(\boldsymbol{P}\)</span>与<span class="math inline">\(\boldsymbol{\hat{P}}\)</span>的分布相似，即约束在<span class="math inline">\(\boldsymbol{\hat{P}}\)</span>附近，这样我们得到的新的可行解集为：</p><p><span class="math display">\[\boldsymbol{U_{\gamma}}(\boldsymbol{\alpha},\boldsymbol{\beta})= \{ \boldsymbol{P} \in\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta}) |KL(\boldsymbol{P} || \boldsymbol{\alpha \beta^{T}}) \leq \gamma\}\]</span></p><p>  其中，<span class="math inline">\(\gamma\)</span>是一个常数，用来控制约束程度。由信息论的知识我们可以有以下结论：</p><p><span class="math display">\[\begin{split}        &amp; \frac{1}{2} \left( H(\boldsymbol{\alpha}) +H(\boldsymbol{\beta}) \right) \leq H(\boldsymbol{P}) \leqH(\boldsymbol{\alpha}) + H(\boldsymbol{\beta}) = H(\boldsymbol{\alpha\beta^{T}}) \\          &amp; KL(\boldsymbol{P} || \boldsymbol{\alpha \beta^{T}}) =H(\boldsymbol{\alpha}) + H(\boldsymbol{\beta}) - H(\boldsymbol{P}) = I(X|| Y)\end{split}\]</span></p><p>  第一个式子确定了联合分布<span class="math inline">\(P(X,Y)\)</span>的上下界，第二个式子说明<span class="math inline">\(\boldsymbol{P}\)</span> 与 <span class="math inline">\(\boldsymbol{\alpha \beta^{T}}\)</span>的KL散度实际上是随机变量<span class="math inline">\(X,Y\)</span>的<strong>互信息量</strong>。互信息量是一种衡量两个随机变量之间相互依赖程度的度量。给定两个随机变量X和Y，它们的互信息量<span class="math inline">\(I(X \parallel Y)\)</span>可以用联合概率分布来计算：</p><p><span class="math display">\[I(X \parallel Y) =\sum_{x}\sum_{y}P(x,y)\log{\frac{P(x,y)}{P(x)P(y)}} = H(X)-H(X |Y)\]</span></p><p>  互信息量的含义为给定<span class="math inline">\(Y\)</span>后，随机变量<span class="math inline">\(X\)</span>所包含的信息的减少量。减少量越大，说明<span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span>的依赖程度越大。也可以理解为联合概率分布<span class="math inline">\(P(X,Y)\)</span>所提供的信息与独立分布<span class="math inline">\(P(X),P(Y)\)</span>所提供的信息之间的信息差，信息差越大，说明<span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span>的依赖关系越强。<br>  因此对<span class="math inline">\(\boldsymbol{P}\)</span>与<span class="math inline">\(\boldsymbol{\alpha \beta^{T}}\)</span>之间的KL散度的约束，也可以理解成对随机变量<span class="math inline">\(X,Y\)</span>之间的互信息量的约束，即依赖程度的约束。我们希望找到随机变量<span class="math inline">\(X,Y\)</span>之间依赖程度足够小的联合分布:</p><p><span class="math display">\[\boldsymbol{U_{\gamma}}(\boldsymbol{\alpha},\boldsymbol{\beta})= \{ \boldsymbol{P} \in\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta}) |KL(\boldsymbol{P} || \boldsymbol{\alpha \beta^{T}}) \leq \gamma \} = \{\boldsymbol{P} \in\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta}) |h(\boldsymbol{P}) \ge h(\boldsymbol{\alpha}) + h(\boldsymbol{\beta}) -\gamma  \} \subset\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta})\]</span></p><p>  此时引入熵正则的最优传输问题便转化为(3)式的形式，如果我们去写(3)式的拉格朗日函数，可以很容易将优化问题(3)转化为优化问题(2)，这里不再说明。<br>  我们来讨论一下极限情况，设<span class="math inline">\(\boldsymbol{P^{*}}\)</span>是原始最优传输问题(1)的最优解，<span class="math inline">\(\boldsymbol{P^{\gamma}}\)</span>是引入熵正则约束的最优传输问题(3)的最优解，很容易可以得到以下结论：</p><ul><li>当 <span class="math inline">\(\gamma \rightarrow \infty\)</span>时，<span class="math inline">\(\boldsymbol{U_{\gamma}}(\boldsymbol{\alpha},\boldsymbol{\beta})\rightarrow\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta})\)</span>，此时<span class="math inline">\(\boldsymbol{P^{\gamma}} \rightarrow\boldsymbol{P^{*}},L_{\boldsymbol{C}}^{\gamma}(\boldsymbol{\alpha},\boldsymbol{\beta})\rightarrowL_{\boldsymbol{C}}(\boldsymbol{\alpha},\boldsymbol{\beta}).\)</span><br></li><li>当 <span class="math inline">\(\gamma \rightarrow 0\)</span>时，<span class="math inline">\(KL(\boldsymbol{P} || \boldsymbol{\alpha\beta^{T}}) \rightarrow 0\)</span>，此时 <span class="math inline">\(\boldsymbol{P^{\gamma}} \rightarrow\boldsymbol{\alpha \beta^{T}},L_{\boldsymbol{C}}^{\gamma}(\boldsymbol{\alpha},\boldsymbol{\beta})\rightarrow \boldsymbol{\alpha^{T}C\beta}\)</span></li></ul><h3 id="geometric-intuition">Geometric Intuition</h3><p>  我们可以从几何的角度来理解熵正则，这里我使用 Marco Cuturi2013年发表在Nips上的论文《Sinkhorn Distances: Lightspeed Computation ofOptimal Transport》中的配图来说明：</p><center><img src="https://s2.loli.net/2024/04/22/fvY9PXokwb4ZHxR.png" width="60%" height="60%"><div data-align="center">Image2: 熵正则的几何示例</div></center><p><br></p><p>  论文中的部分符号与本博客不完全一致，读者需要自行对照理解，这里的论述以配图中的符号为标准。在之前关于最优传输的博客中，我们说明了原始的最优传输问题(1)实际上是一个线性规划问题，其解<span class="math inline">\(\boldsymbol{P}\)</span>被约束在一个<span class="math inline">\(\mathbb{R^{d \timesd}}\)</span>空间中的一个多面体<span class="math inline">\(U(\boldsymbol{r,c})\)</span>中，如图2中的红色多面体所示。在关于线性规划单纯形的讨论中，我们证明了线性规划问题的最优解一定在多面体的极点上，设原始最优传输问题(1)的最优解为<span class="math inline">\(\boldsymbol{P^{*}}\)</span>，即图2中多面体上绿色的极点。多面体的内部存在着某一点<span class="math inline">\(\boldsymbol{rc^{T}}\)</span>，其表示熵最大的联合分布<span class="math inline">\(\boldsymbol{P}\)</span>。在前文的讨论中，我们说明了引入熵正则项的最优传输问题(3)实际上是将可行解集从原来的多面体，约束到熵最大的解<span class="math inline">\(\boldsymbol{rc^{T}}\)</span>的附近，并将问题变成了一个严格的凸问题，这个新的严格凸的可行解集即为图中的蓝色区域。设问题(3)的最优解为<span class="math inline">\(\boldsymbol{P_{\alpha}}\)</span>，即图2中凸区域上的蓝色点。<span class="math inline">\(\boldsymbol{P_{\alpha}}\)</span>关于凸区域的切线是垂直于梯度方向<span class="math inline">\(\boldsymbol{M}\)</span>。当 <span class="math inline">\(\alpha \rightarrow 0\)</span> 时，<span class="math inline">\(\boldsymbol{P_{\alpha}} \rightarrow\boldsymbol{rc^{T}}\)</span>；当 <span class="math inline">\(\alpha\rightarrow \infty\)</span> 时，<span class="math inline">\(\boldsymbol{P_{\alpha}} \rightarrow\boldsymbol{P^{*}}\)</span>，因此随着 <span class="math inline">\(\alpha\)</span> 的增大，问题(3)的最优解<span class="math inline">\(\boldsymbol{P_{\alpha}}\)</span>会在多面体的内部形成一条路径，这条路径从起点<span class="math inline">\(\boldsymbol{rc^{T}}\)</span>逐渐逼近到<span class="math inline">\(\boldsymbol{P^{*}}\)</span>，如图2中红色的曲线所示。由于问题(3)实际上是与问题(2)等价的，给定一个<span class="math inline">\(\alpha\)</span>，就会有一个对应的<span class="math inline">\(\lambda\)</span>，当 <span class="math inline">\(\lambda \rightarrow 0\)</span> 时，<span class="math inline">\(\boldsymbol{P^{\lambda}} \rightarrow\boldsymbol{rc^{T}}\)</span>；当 <span class="math inline">\(\lambda\rightarrow \infty\)</span> 时，<span class="math inline">\(\boldsymbol{P^{\lambda}} \rightarrow\boldsymbol{P^{*}}\)</span>。因此这条路径也可看作是问题(2)的最优解<span class="math inline">\(\boldsymbol{P^{\lambda}}\)</span>形成的。图2中右端放大的图示的含义是，虽然理论上当<span class="math inline">\(\lambda \rightarrow \infty\)</span>时，<span class="math inline">\(\boldsymbol{P^{\lambda}} \rightarrow\boldsymbol{P^{*}}\)</span>，但计算机的数值精度是有限的，不可能真正现实趋于无穷大，因此在实际计算中，硬件所能表示的最大的<span class="math inline">\(\lambda_{max}\)</span>所对应的解<span class="math inline">\(\boldsymbol{P^{\lambda_{max}}}\)</span>只是<span class="math inline">\(\boldsymbol{P^{*}}\)</span>的一个近似值。</p><h2 id="sinkhorn-distance">Sinkhorn Distance</h2><p>  在引入熵正则项后，我们可以定义一个新的距离：</p><p><span class="math display">\[d_{\boldsymbol{C,\gamma}}(\boldsymbol{\alpha,\beta}):=  \min_{\boldsymbol{P} \in\boldsymbol{U_{\gamma}}(\boldsymbol{\alpha},\boldsymbol{\beta})}\left&lt; \boldsymbol{P},\boldsymbol{C} \right&gt;\]</span></p><p>若成本矩阵<span class="math inline">\(\boldsymbol{C}\)</span>为度量矩阵，即：</p><p><span class="math display">\[\boldsymbol{C} \in \{ \boldsymbol{C} \in\mathbb{R_{+}^{d \times d}}: \boldsymbol{C^{T}}=\boldsymbol{C};\foralli,j \leq d, c_{ij}=0 \Leftrightarrow i=j; \forall i,j,k \leq d, c_{ij}\leq c_{ik}+c_{kj} \}\]</span></p><p>则 <span class="math inline">\(d_{\boldsymbol{C,\gamma}}(\boldsymbol{\alpha,\beta})\)</span>被定义为分布 <span class="math inline">\(\boldsymbol{\alpha,\beta}\)</span>之间的<strong>Sinkhorn Distance</strong>。<br>  证明 Sinkhorn Distance 能够作为分布之间的距离类似于之前博客中Wasserstein Distance的证明过程，这里由于篇幅原因不再多加赘述。基于熵正则的最优传输问题所导出的Sinkhorn Distance 具有广泛的应用，以下是几个典型领域：</p><ul><li><strong>图像处理与计算机视觉</strong>：在图像生成、图像配准和图像分类等任务中，可以使用Sinkhorn距离来度量两个图像之间的相似性，尤其是在非刚性配准和变形问题中。<br></li><li><strong>自然语言处理</strong>：在文本生成、文本分类和词嵌入等任务中，Sinkhorn距离可以用于比较两个文本之间的相似性，帮助解决文本对齐和翻译等问题。<br></li><li><strong>机器学习与模式识别</strong>：在模式匹配、聚类和异常检测等任务中，Sinkhorn距离可以用作特征之间相似性的度量，从而提高模型的性能和鲁棒性。<br></li><li><strong>经济学与运筹学</strong>：在经济学中，Sinkhorn距离可以用于衡量两个市场之间的相似性，从而帮助分析市场结构和预测市场变化。在运筹学中，它可以用于解决运输和分配等问题。</li></ul><h2 id="references">References</h2><ul><li><strong>[1] Book: Peyré G, Cuturi M. Computational optimaltransport[J]. Center for Research in Economics and Statistics WorkingPapers, 2017 (2017-86).</strong><br></li><li><strong>[2] Paper: Cuturi M. Sinkhorn distances: Lightspeedcomputation of optimal transport[J]. Advances in neural informationprocessing systems, 2013, 26.</strong><br></li><li><strong>[3] Video: B站《随机过程》张灏UP-我在人间凑数的这几年.</strong></li></ul>]]></content>
    
    
    <summary type="html">本节主要介绍最优传输中的熵正则理论。我们将会讨论熵正则的作用，信息论以及几何视角下的熵正则内涵，以及由此定义的分布之间新的距离度量——Sinkhorn Distance。</summary>
    
    
    
    <category term="最优传输理论" scheme="http://example.com/categories/%E6%9C%80%E4%BC%98%E4%BC%A0%E8%BE%93%E7%90%86%E8%AE%BA/"/>
    
    
  </entry>
  
  <entry>
    <title>Optimal Transport-4.Simplex Method</title>
    <link href="http://example.com/2024/04/08/Optimal%20Transport-4.Simplex%20Method/"/>
    <id>http://example.com/2024/04/08/Optimal%20Transport-4.Simplex%20Method/</id>
    <published>2024-04-08T04:11:10.000Z</published>
    <updated>2024-04-08T13:55:20.937Z</updated>
    
    <content type="html"><![CDATA[<h1 id="simplex-method">Simplex Method</h1><p>  单纯形方法(Simplex Method)是解决线性规划问题的一种重要算法。它由George Dantzig在1947年发明。线性规划问题涉及到在一系列线性等式或不等式约束条件下，找到一个线性函数的最大值或最小值。<strong>单纯形方法的基本思想是将问题的解空间视为一个几何形状（通常是多维的），这个几何形状的顶点代表可能的解。算法从这个形状的一个顶点开始，沿着边缘移动到相邻的顶点，以此方式逐步改进解，直到找到最优解为止。每一步都保证目标函数的值不会变差。</strong><br>  单纯形方法在实际应用中非常广泛，包括运筹学、工程优化、金融和经济学等领域。尽管存在某些局限性，它仍然是解决线性规划问题的一个强大工具。</p><h2 id="linear-programming-form-of-ot-problem">Linear Programming Formof OT Problem</h2><p>  在之前的章节中，我们已经介绍了 Kantorovich OT Problem的经济含义与数学形式，我们也说明了其能够转化为线性规划的标准形式，这里我们再回顾一下之前的内容。<br><strong>数学形式</strong><br>  Kantorovich OT Problem 的数学形式如下(1)：</p><p><span class="math display">\[\begin{equation}    L_{\boldsymbol{C}}(\boldsymbol{a}, \boldsymbol{b}) \quad\overset{\text{def.}}{=} \quad \min_{\boldsymbol{P} \in\boldsymbol{U}(\boldsymbol{a},\boldsymbol{b})} \left&lt;\boldsymbol{C},\boldsymbol{P} \right&gt;_{F}=\sum_{i,j}p_{ij}c_{ij}\end{equation}\]</span></p><p>其中 <span class="math inline">\(\boldsymbol{a} \in\mathbb{R}^{n},\boldsymbol{b} \in \mathbb{R}^{m},\boldsymbol{P},\boldsymbol{C} \in \mathbb{R}^{n \timesm}_{+}\)</span>，约束条件 <span class="math inline">\(\boldsymbol{U}(\boldsymbol{a},\boldsymbol{b})\)</span>为：</p><p><span class="math display">\[\boldsymbol{U}(\boldsymbol{a},\boldsymbol{b}) =\{ \boldsymbol{P} \in \mathbb{R}^{n \times m}_{+}: \boldsymbol{P}\mathbf{1}_{m}=\boldsymbol{a} \quad and \quad \boldsymbol{P^{T}}\mathbf{1}_{n}=\boldsymbol{b} \}\]</span></p><p><strong>经济含义</strong><br>  考虑一个实际问题，有 n 个生产木材的工厂，生产的木材需要供应给 m个城市，每个工厂每个月的产能是固定的，记为 <span class="math inline">\(\boldsymbol{a}, \boldsymbol{a} \in\mathbb{R}^{n}\)</span>；每个城市每个月木材的需求量也是固定的，记为<span class="math inline">\(\boldsymbol{b}, \boldsymbol{b} \in\mathbb{R}^{m}\)</span>。记 <span class="math inline">\(\boldsymbol{P} =[p_{ij}]_{n \times m} \in \mathbb{R}^{n \times m}_{+}\)</span>表示运输方案，其中 <span class="math inline">\(p_{ij}\)</span> 表示第 i个工厂运输到第 j 个城市的木材量。记 <span class="math inline">\(\boldsymbol{C} = [c_{ij}]_{n \times m} \in\mathbb{R}^{n \times m}_{+}\)</span> 表示运输成本，其中 <span class="math inline">\(c_{ij}\)</span>表示将单位木材从第i个工厂运输到第j个城市的成本。<br>  则 Kantorovich OT Problem实际上是在寻找满足供应与需求条件下，使得总成本最低的运输方案。</p><p><strong>线性规划形式</strong></p><p>  Kantorovich OT Problem实际上是一个线性规划问题，通过一定的变换，我们能够将原始的 KantorovichOT Problem 转化为线性规划形式。<br>  将矩阵 <span class="math inline">\(\boldsymbol{P},\boldsymbol{C}\)</span>写成列向量形式：</p><p><span class="math display">\[\boldsymbol{P}=\begin{bmatrix}    \boldsymbol{p_1}, \boldsymbol{p_2}, \cdots,\boldsymbol{p_m}\end{bmatrix},\quad \boldsymbol{C} = \begin{bmatrix}    \boldsymbol{c_1},\boldsymbol{c_2}, \cdots,\boldsymbol{c_m}\end{bmatrix}\]</span></p><p>  构造向量 <span class="math inline">\(\boldsymbol{p},\boldsymbol{c}\in \mathbb{R}^{nm}, \boldsymbol{d} \in \mathbb{R}^{n+m}\)</span>，矩阵<span class="math inline">\(A \in \mathbb{R}^{(n+m) \timesnm}\)</span>:</p><p><span class="math display">\[\boldsymbol{p} = \begin{bmatrix}    \boldsymbol{p_{1}^{T}}, \boldsymbol{p_{2}^{T}}, \cdots,\boldsymbol{p_{m}^{T}}\end{bmatrix}^{T}, \quad \boldsymbol{c} = \begin{bmatrix}    \boldsymbol{c_{1}^{T}}, \boldsymbol{c_{2}^{T}}, \cdots,\boldsymbol{c_{m}^{T}}\end{bmatrix}^{T}\]</span></p><p><span class="math display">\[\boldsymbol{A} = \begin{bmatrix}    \boldsymbol{1_{n}^{T}} \otimes \boldsymbol{I_{m}} \\    \boldsymbol{I_{n}} \otimes \boldsymbol{1_{m}^{T}} \\\end{bmatrix}, \quad \boldsymbol{d}=\begin{bmatrix}    \boldsymbol{a} \\    \boldsymbol{b} \\\end{bmatrix}\]</span></p><p>其中，符号"<span class="math inline">\(\otimes\)</span>"表示矩阵的kronecker's product。则(1)式可以被转化为：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{\boldsymbol{p} \in \mathbb{R^{nm}}} \quad&amp;\boldsymbol{c^{T}p} \\        s.t. \quad &amp; \boldsymbol{Ap} = \boldsymbol{d} \\        &amp; \boldsymbol{p} \ge \boldsymbol{0} \\    \end{split}\end{equation}\]</span></p><p>  优化问题(2)即为 Kantorovich OT Problem 的线性规划形式。</p><h2 id="theoretical-foundations-of-simplex-method">TheoreticalFoundations of Simplex Method</h2><p>  解决线性规划的单纯形算法的理论基础是<strong>凸多面体分解定理</strong>，在正式介绍单纯形算法之前，我们首先来介绍凸多面体分解定理。要想理解凸多面体分解定理，我们需要从凸多面体、极点、极方向等开始讲起。</p><h3 id="polyhedron">Polyhedron</h3><p><strong>凸包(Convex Hall)</strong><br>  凸包是点集中元素的凸组合所构成的集合。数学形似为：</p><p><span class="math display">\[C \subseteq R^{n},Conv C= \{\sum_{i=1}^{n}\theta_{i}x_{i} | \forall x_i \in C, \forall \theta_{i}\in [0,1], \sum_{i=1}^{n}\theta_i = 1, i = 1,...,n \}\]</span></p><p>  凸包是欧式空间中一组点集的最小凸集。</p><center><img src="https://s2.loli.net/2024/04/08/DfV5GLTQ8ncANRi.jpg" width="60%" height="60%"><div data-align="center">Image1: 凸包</div></center><p><br></p><p><strong>凸多面体(Convex Polyhedron)</strong><br>  凸多面体是有限个线性等式和不等式的解集。其数学形式为：</p><p><span class="math display">\[P = \{ x | a_{i}^{T}x \leq b_i,i=1,...,m; c_{j}^{T}x = d_{j}, j=1,...,p \}\]</span></p><p>  从凸多面体的数学形式上可以多面体是有限个半空间和超平面的交集，因此凸多面体也是凸集。</p><center><img src="https://s2.loli.net/2024/04/08/5yoEIzh3YAZRdX1.jpg" width="60%" height="60%"><div data-align="center">Image2: 凸多面体</div></center><p><br></p><p>  设线性规划问题(2)的可行解集为 <span class="math inline">\(S = \{\boldsymbol{x}| \boldsymbol{x} \in\mathbb{R}^{nm},\boldsymbol{Ax}=\boldsymbol{d},\boldsymbol{x} \ge 0\}\)</span>。由凸多面体的定义可知<strong>可行解集 <span class="math inline">\(S\)</span> 为凸多面体</strong>。</p><h2 id="properties-of-convex-polyhedron">Properties of ConvexPolyhedron</h2><p><strong>极点(Extreme Point)</strong><br>  对于凸集<span class="math inline">\(C\)</span>，若 <span class="math inline">\(x \in C\)</span> 不能表示成 <span class="math inline">\(C\)</span> 中另外两点的凸组合，则称 <span class="math inline">\(x\)</span> 为凸集 <span class="math inline">\(C\)</span> 的极点。极点的数学形式为：</p><p><span class="math display">\[Ep(C) = \{x | x \in C, \forall x_1,x_2\in C, \forall \theta \in [0,1], x \ne \theta x_1 + (1-\theta)x_2\}\]</span></p><p>  通过极点的定义可以得知：<strong>极点不能落在凸集中另外两点组成的线段上。</strong></p><p><strong>方向(Recession Direction)</strong><br>  对于凸集<span class="math inline">\(C\)</span>，若存在非零向量 <span class="math inline">\(d\)</span>，满足：对于任意 <span class="math inline">\(x \in C\)</span>，均有 <span class="math inline">\(x + \lambda d \in C, \forall \lambda &gt;0\)</span>，则称 <span class="math inline">\(d\)</span> 为凸集 <span class="math inline">\(C\)</span> 的一个方向。方向的数学形式为：</p><p><span class="math display">\[d: d \ne 0, \forall x \in C, \forall\lambda &gt;0, x + \lambda d \in C\]</span></p><p>  凸集<span class="math inline">\(C\)</span>中所有方向构成了一个方向锥：</p><p><span class="math display">\[RDcone(C) = \{d | d \ne 0, \forall x \inC, \forall \lambda &gt;0, x + \lambda d \in C \}\]</span></p><p><strong>极方向(Extreme Direction)</strong><br>  对于凸集<span class="math inline">\(C\)</span>，若方向 <span class="math inline">\(d\)</span>不能表示成另外两个方向的正线性组合，则称 <span class="math inline">\(d\)</span> 为凸集 <span class="math inline">\(C\)</span> 的极方向。极方向的数学形式为：</p><p><span class="math display">\[Ed(C) = \{ d | d \in RDcone(C),\foralld_1,d_2 \in RDcone(C), \forall \lambda_1,\lambda_2 &gt; 0,d \ne\lambda_1 d_1 + \lambda_2 d_2 \}\]</span></p><p>  <strong>凸集<span class="math inline">\(C\)</span>中任意一个方向<span class="math inline">\(d\)</span>可以表示成其极方向的线性组合。</strong></p><center><img src="https://s2.loli.net/2024/04/08/aTVmyZB4udYf7gA.jpg" width="60%" height="60%"><div data-align="center">Image3: 极点、方向、极方向</div></center><p><br></p><p><strong>极点与极方向的数学表示</strong><br>  设 <span class="math inline">\(\boldsymbol{x} \in \mathbb{R}^{n},\boldsymbol{A} \in \mathbb{R}^{m \times n}, \boldsymbol{b} \in\mathbb{R}^{m}, r(\boldsymbol{A})=m\)</span>，则有凸多面体：</p><p><span class="math display">\[S = \{\boldsymbol{x} |\boldsymbol{Ax}=\boldsymbol{b},\boldsymbol{x} \ge0 \}\]</span></p><p><strong>极点定理:</strong> <span class="math inline">\(\boldsymbol{x}\in S\)</span> 是凸多面体 <span class="math inline">\(S\)</span>的极点当且仅当存在 <span class="math inline">\(\boldsymbol{A}\)</span>的分块: <span class="math inline">\(\boldsymbol{A}=\begin{bmatrix}  \boldsymbol{B},\boldsymbol{N}\end{bmatrix}\)</span>，使得 <span class="math inline">\(\boldsymbol{x}\)</span> 可以表示为：</p><p><span class="math display">\[\boldsymbol{x} = \begin{bmatrix}    \boldsymbol{B^{-1}b} \\    \boldsymbol{0} \\\end{bmatrix}, \quad \boldsymbol{A}=\begin{bmatrix}    \boldsymbol{B},\boldsymbol{N}\end{bmatrix},\boldsymbol{B} \in \mathbb{R}^{m \times m}\]</span></p><p>其中，<span class="math inline">\(\boldsymbol{B}\)</span> 可逆，且<span class="math inline">\(\boldsymbol{B^{-1}b} \ge\boldsymbol{0}\)</span>。<br>  由极点定理可知，若 <span class="math inline">\(\boldsymbol{x}\)</span>为凸多面体 <span class="math inline">\(S\)</span> 的一个极点，则 <span class="math inline">\(\boldsymbol{x}\)</span> 正分量对应的矩阵 <span class="math inline">\(\boldsymbol{A}\)</span>的列必然线性无关。同时凸多面体的极点必然是有限个的。</p><p><strong>极方向定理:</strong> <span class="math inline">\(\boldsymbol{d} \in \mathbb{R}^{n}\)</span>是凸多面体 <span class="math inline">\(S\)</span> 的极方向当且仅当存在<span class="math inline">\(\boldsymbol{A}\)</span> 的分块: <span class="math inline">\(\boldsymbol{A}=\begin{bmatrix}  \boldsymbol{B},\boldsymbol{N}\end{bmatrix}\)</span>， 使得 <span class="math inline">\(\boldsymbol{d}\)</span> 可以表示为:</p><p><span class="math display">\[\boldsymbol{d} = t\begin{bmatrix}    -\boldsymbol{B^{-1}n_{j}} \\    \boldsymbol{e_{j}}\end{bmatrix},\quad \boldsymbol{A} = \begin{bmatrix}    \boldsymbol{B},\boldsymbol{N}\end{bmatrix},\boldsymbol{B} \in \mathbb{R}^{m \times m}\]</span></p><p>其中，<span class="math inline">\(\boldsymbol{B}\)</span> 可逆，<span class="math inline">\(t &gt; 0, \boldsymbol{B^{-1}n_{j}} \leq0\)</span>，<span class="math inline">\(\boldsymbol{n_{j}}\)</span>为矩阵 <span class="math inline">\(\boldsymbol{N}\)</span>的第j列，<span class="math inline">\(\boldsymbol{e_{j}} \in\mathbb{R}^{n-m}\)</span> 的第j个分量为1，其余为0.</p><h2 id="decomposition-of-convex-polyhedron">Decomposition of ConvexPolyhedron</h2><p>  设凸多面体 <span class="math inline">\(S\)</span> 的极点为 <span class="math inline">\(\boldsymbol{x_1,x_2,\dots,x_{k}}\)</span>，极方向为<span class="math inline">\(\boldsymbol{d_1,d_2,\dots,d_{I}}\)</span>，则<span class="math inline">\(\boldsymbol{x} \in S\)</span> 当且仅当 <span class="math inline">\(\boldsymbol{x}\)</span> 具有如下形式：</p><p><span class="math display">\[\boldsymbol{x} =\sum_{i=1}^{k}\lambda_{i}\boldsymbol{x_{i}} +\sum_{j=1}^{I}\mu_{j}\boldsymbol{d_{j}}\]</span></p><p>其中，<span class="math inline">\(\sum_{i=1}^{k}\lambda_{i}=1,\lambda_{i} \ge 0,i=1,\dots,k; \mu_{j} \ge 0, j = 1,\dots,I\)</span><br>  通过几何分析我们可以，该定理表示的含义为，<strong>以凸多面体的顶点构成的线段上的点为起点，以凸多面体的方向为方向所形成的所有射线的集合即为凸多么体中的点集。</strong></p><center><img src="https://s2.loli.net/2024/04/08/vYnHhyJeDd2PcTq.jpg" width="60%" height="60%"><div data-align="center">Image4: 多面体的分解</div></center><p><br></p><h2 id="simplex-method-of-linear-programming">Simplex Method of LinearProgramming</h2><p>  在介绍了单纯形方法的理论——凸多面体分解定理后，我们现在可以推导出单纯形算法，我们首先基于分解定理，对原始的线性规划问题做一些分析。</p><h3 id="analysis-for-lp-based-on-decomposition-theroem">Analysis for LPBased on Decomposition Theroem</h3><p>  线性规划问题的形式为：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{\boldsymbol{c} \in \mathbb{R^{n}}} \quad&amp;\boldsymbol{c^{T}x} \\        s.t. \quad &amp; \boldsymbol{Ax} = \boldsymbol{b} \\        &amp; \boldsymbol{x} \ge \boldsymbol{0} \\    \end{split}\end{equation}\]</span></p><p>其中，<span class="math inline">\(\boldsymbol{x} \in \mathbb{R}^{n},\boldsymbol{A} \in \mathbb{R}^{m \times n}, \boldsymbol{b} \in\mathbb{R}^{m}, r(\boldsymbol{A})=m\)</span>.<br>  定义可行解集 <span class="math inline">\(S = \{ \boldsymbol{x} |\boldsymbol{Ax}=\boldsymbol{b}\}\)</span>，则线性规划问题(3)也可以写成如下形式：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{\boldsymbol{c} \in \mathbb{R^{n}}} \quad&amp;\boldsymbol{c^{T}x} \\        s.t. \quad &amp; \boldsymbol{x} \in S \\    \end{split}\end{equation}\]</span></p><p>  由于可行解集 <span class="math inline">\(S\)</span>为一个凸多面体，设凸多面体 <span class="math inline">\(S\)</span>的极点为 <span class="math inline">\(\boldsymbol{x_1,x_2,\dots,x_{k}}\)</span>，极方向为<span class="math inline">\(\boldsymbol{d_1,d_2,\dots,d_{I}}\)</span>，则对<span class="math inline">\(\forall x \in S, \exists\lambda_{i},\mu_{j}\)</span>，使得:</p><p><span class="math display">\[\boldsymbol{x} =\sum_{i=1}^{k}\lambda_{i}\boldsymbol{x_{i}} +\sum_{j=1}^{I}\mu_{j}\boldsymbol{d_{j}}\]</span></p><p>其中，<span class="math inline">\(\sum_{i=1}^{k}\lambda_{i}=1,\lambda_{i} &gt; 0,i=1,\dots,k; \mu_{j} \ge 0, j = 1,\dots,I\)</span>.则优化问题(4)可以转化为：</p><p><span class="math display">\[\begin{split}    \min_{\boldsymbol{c} \in \mathbb{R^{n}}} \quad&amp;\boldsymbol{c^{T}} \left(\sum_{i=1}^{k}\lambda_{i}\boldsymbol{x_{i}} +\sum_{j=1}^{I}\mu_{j}\boldsymbol{d_{j}} \right) \\    s.t. \quad &amp; \sum_{i=1}^{k}\lambda_{i}=1,\lambda_{i} \ge 0 \\    \quad &amp; \mu_{j} \ge 0\end{split}\]</span></p><p>即：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{\boldsymbol{c} \in \mathbb{R^{n}}} \quad&amp;\sum_{i=1}^{k}\lambda_{i}\boldsymbol{c^{T}x_{i}} +\sum_{j=1}^{I}\mu_{j}\boldsymbol{c^{T}d_{j}} \\        s.t. \quad &amp; \sum_{i=1}^{k}\lambda_{i}=1,\lambda_{i} \ge 0\\        \quad &amp; \mu_{j} \ge 0    \end{split}\end{equation}\]</span></p><p>  对于优化问题(5)：</p><ul><li>若 <span class="math inline">\(\exists j, \boldsymbol{c^{T}d_{j}}&lt; 0\)</span>，令 <span class="math inline">\(\mu_{j} \rightarrow+\infty\)</span>，则该问题的最小值为负无穷，无意义。<br></li><li>若 <span class="math inline">\(\forall j, \boldsymbol{c^{T}d_{j}}\ge 0\)</span>，则有 <span class="math inline">\(\mu_{j} = 0, \forallj\)</span>，该问题存在最优解。</li></ul><p>  通过以上的分析，优化问题(5)可被化为：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{\boldsymbol{c} \in \mathbb{R^{n}}} \quad&amp;\sum_{i=1}^{k}\lambda_{i}\boldsymbol{c^{T}x_{i}} \\        s.t. \quad &amp; \sum_{i=1}^{k}\lambda_{i}=1,\lambda_{i} \ge 0\\    \end{split}\end{equation}\]</span></p><p>  对于优化问题(6)，我们实际上只需要找到 i 使得 <span class="math inline">\(\boldsymbol{c^{T}x_{i}}\)</span>最小，此时令对应的 <span class="math inline">\(\lambda_{i}=1\)</span>，目标函数即为最小值。故优化问题(6)又可被转化为：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{i} \quad \boldsymbol{c^{T}x_{i}} \\    \end{split}\end{equation}\]</span></p><p>  故实际上，<strong>线性规划问题的最优值一定在其可行解集的某个极点上取到，其最优解也是某一个极点。</strong></p><p><strong>结论</strong><br>  通过以上的分析，我们来总结一下得到的结论。考虑如下线性规划的形式：</p><p><span class="math display">\[\begin{split}    \min_{\boldsymbol{c} \in \mathbb{R^{n}}} \quad&amp;\boldsymbol{c^{T}x} \\    s.t. \quad &amp; \boldsymbol{Ax} = \boldsymbol{b} \\    &amp; \boldsymbol{x} \ge \boldsymbol{0} \\\end{split}\]</span></p><p>  其中，<span class="math inline">\(\boldsymbol{A} \in \mathbb{R}^{m\times n},r(\boldsymbol{A})=m\)</span>，记其可行解集为 <span class="math inline">\(S = \{ \boldsymbol{x} |\boldsymbol{Ax}=\boldsymbol{b},\boldsymbol{x} \ge \boldsymbol{0}\}\)</span>，设 <span class="math inline">\(S\)</span> 的极点为 <span class="math inline">\(\boldsymbol{x_1,x_2,\dots,x_{k}}\)</span>，极方向为<span class="math inline">\(\boldsymbol{d_1,d_2,\dots,d_{I}}\)</span>，则有：</p><ul><li><strong>线性规划有最优解当且仅当 <span class="math inline">\(\forallj, \boldsymbol{c^{T}d_{j}} \ge 0\)</span>.</strong></li><li><strong>若线性规划有最优值，则必然可在某个极点上达到。</strong></li></ul><h2 id="mathematical-analysis-of-the-simplex-method">MathematicalAnalysis of the Simplex Method</h2><p>  有了前文中关于线性规划问题解的结论后，我们便能够比较容易地理解单纯形法的基本思想了。单纯形法的基本思想是<strong>首先找到可行解集的某一个极点，判断其是否为最优解，若是则算法终止，否则寻找一个更有的极点。</strong><br>  这里有两个核心问题，<strong>如何判断某一个极点是否是最优解</strong>？以及<strong>如何找到一个更优的极点</strong>?如果这两个问题解决了，我们便能够依据单纯形的基本思想完成整个算法的设计。接下来，我们便来分析这两个问题。<br><strong>Analysis</strong><br>  设矩阵 <span class="math inline">\(\boldsymbol{A}\)</span>的某种分块为 <span class="math inline">\(\boldsymbol{A} =\begin{bmatrix}  \boldsymbol{B},\boldsymbol{N}\end{bmatrix}\)</span>，<span class="math inline">\(\boldsymbol{B} \in\mathbb{R}^{m}\)</span>，使得 <span class="math inline">\(\boldsymbol{B}\)</span> 可逆，且 <span class="math inline">\(\boldsymbol{B^{-1}b} \ge\boldsymbol{0}\)</span>，令：</p><p><span class="math display">\[\boldsymbol{\bar{x}} = \begin{bmatrix}    \boldsymbol{B^{-1}b} \\    \boldsymbol{0} \\\end{bmatrix}\]</span></p><p>由极点定理可知 <span class="math inline">\(\boldsymbol{\bar{x}}\)</span> 是可行解集 <span class="math inline">\(S\)</span> 的某一个极点。<strong>接下来我们要判断<span class="math inline">\(\boldsymbol{\bar{x}}\)</span>是否是最优解。</strong><br>  取 <span class="math inline">\(\boldsymbol{x} \in S\)</span>，根据矩阵<span class="math inline">\(\boldsymbol{A}\)</span>的分块结果，相应地将向量 <span class="math inline">\(\boldsymbol{x}\)</span> 与 <span class="math inline">\(\boldsymbol{c}\)</span> 进行分块：</p><p><span class="math display">\[\boldsymbol{x} = \begin{bmatrix}    \boldsymbol{x_{B}} \\    \boldsymbol{x_{N}} \\\end{bmatrix}, \quad \boldsymbol{c} = \begin{bmatrix}    \boldsymbol{c_{B}} \\    \boldsymbol{c_{N}} \\\end{bmatrix}\]</span></p><p><span class="math display">\[\boldsymbol{Ax}=\boldsymbol{b} \quad\Rightarrow \quad \begin{bmatrix}    \boldsymbol{B,N}\end{bmatrix}\begin{bmatrix}    \boldsymbol{x_{B}} \\    \boldsymbol{x_{N}} \\\end{bmatrix}=\boldsymbol{Bx_{B}}+\boldsymbol{Nx_{N}}=\boldsymbol{b}\quad \Rightarrow \quad \boldsymbol{x_{B}} =\boldsymbol{B^{-1}b}-\boldsymbol{B^{-1}Nx_{N}}\]</span></p><p>  若对 <span class="math inline">\(\forall x \in S\)</span>，均有<span class="math inline">\(\boldsymbol{c^{T}x}-\boldsymbol{c^{T}\bar{x}} \ge0\)</span>，则 <span class="math inline">\(\boldsymbol{\bar{x}}\)</span>为最优解。对判断条件 <span class="math inline">\(\boldsymbol{c^{T}x}-\boldsymbol{c^{T}\bar{x}} \ge0\)</span> 做一些化简：</p><p><span class="math display">\[\begin{split}    \boldsymbol{c^{T}x}-\boldsymbol{c^{T}\bar{x}} =&amp; \begin{bmatrix}        \boldsymbol{c_{B}^{T},c_{N}^{T}} \\    \end{bmatrix}\begin{bmatrix}        \boldsymbol{x_{B}} \\        \boldsymbol{x_{N}} \\    \end{bmatrix} - \begin{bmatrix}        \boldsymbol{c_{B}^{T},c_{N}^{T}} \\    \end{bmatrix}\begin{bmatrix}        \boldsymbol{B^{-1}b} \\        \boldsymbol{0} \\    \end{bmatrix} =\boldsymbol{c_{B}^{T}x_{B}}+\boldsymbol{c_{N}^{T}x_{N}} -\boldsymbol{c_{B}^{T}B^{-1}b}  \\    &amp;= \boldsymbol{c_{B}^{T}B^{-1}b} -\boldsymbol{c_{B}^{T}B^{-1}Nx_{N}}+\boldsymbol{c_{N}^{T}x_{N}} -\boldsymbol{c_{B}^{T}B^{-1}b} = \boldsymbol{c_{N}^{T}x_{N}} -\boldsymbol{c_{B}^{T}B^{-1}Nx_{N}} \\    &amp;= \left( \boldsymbol{c_{N}^{T}} - \boldsymbol{c_{B}^{T}B^{-1}N}\right)\boldsymbol{x_{N}}\end{split}\]</span></p><p>  <strong>(1)</strong> 若 <span class="math inline">\(\boldsymbol{c_{N}^{T}} -\boldsymbol{c_{B}^{T}B^{-1}N} \ge \boldsymbol{0}\)</span>，则有 <span class="math inline">\(\boldsymbol{c^{T}x}-\boldsymbol{c^{T}\bar{x}} \ge0\)</span>，<span class="math inline">\(\boldsymbol{\bar{x}}\)</span>即为最优解，此时我们已经找到了最优解，算法可以终止了。<br>  <strong>(2)</strong> 若 <span class="math inline">\(\boldsymbol{c_{N}^{T}} -\boldsymbol{c_{B}^{T}B^{-1}N} \ngeq \boldsymbol{0}\)</span>，此时 <span class="math inline">\(\boldsymbol{\bar{x}}\)</span>不是最优解，我们需要<strong>寻找一个更优的极点。</strong><br>  不妨设 <span class="math inline">\(\boldsymbol{c_{N}^{T}} -\boldsymbol{c_{B}^{T}B^{-1}N}\)</span> 中的第j个分量小于零，即 <span class="math inline">\(\boldsymbol{c_{Nj}^{T}} -\boldsymbol{c_{B}^{T}B^{-1}n_{j}} &lt; 0\)</span>，记：</p><p><span class="math display">\[\boldsymbol{d_{j}} = \begin{bmatrix}    -\boldsymbol{B^{-1}n_{j}} \\    \boldsymbol{e_{j}}\end{bmatrix}\]</span></p><p>则：</p><p><span class="math display">\[\boldsymbol{c^{T}d_{j}} =\begin{bmatrix}        \boldsymbol{c_{B}^{T},c_{N}^{T}} \\    \end{bmatrix}\begin{bmatrix}    -\boldsymbol{B^{-1}n_{j}} \\    \boldsymbol{e_{j}}\end{bmatrix} = \boldsymbol{c_{Nj}^{T}} -\boldsymbol{c_{B}^{T}B^{-1}n_{j}} &lt; 0\]</span></p><p>记 <span class="math inline">\(\boldsymbol{r_{j}} =\boldsymbol{B^{-1}n_{j}}\)</span><br>  <strong>1.</strong> 若 <span class="math inline">\(\boldsymbol{r_{j}}\leq \boldsymbol{0}\)</span>，则 <span class="math inline">\(\boldsymbol{d_{j}} \ge0\)</span>，由极方向定理可知: <span class="math inline">\(\boldsymbol{d_{j}}\)</span> 为 <span class="math inline">\(S\)</span> 的一个极方向，则对 <span class="math inline">\(\forall \lambda \ge 0,\boldsymbol{\bar{x}}+\lambda \boldsymbol{d_{j}} \inS\)</span>，故有：</p><p><span class="math display">\[\boldsymbol{c^{T}}\left(\boldsymbol{\bar{x}} + \lambda \boldsymbol{d_{j}} \right) =\boldsymbol{c^{T}\bar{x}}+\lambda \boldsymbol{c^{T}d_{j}} \rightarrow-\infty, \quad \lambda \rightarrow +\infty\]</span></p><p>  <strong>2.</strong> 若 <span class="math inline">\(\boldsymbol{r_{j}} \nleq\boldsymbol{0}\)</span>，则 <span class="math inline">\(\boldsymbol{d_{j}} \ngeq\boldsymbol{0}\)</span>，故 <span class="math inline">\(\boldsymbol{d_{j}}\)</span> 不是极方向，此时：</p><p><span class="math display">\[\boldsymbol{c^{T}}\left(\boldsymbol{\bar{x}} + \lambda \boldsymbol{d_{j}} \right) =\boldsymbol{c^{T}\bar{x}}+\lambda \boldsymbol{c^{T}d_{j}} &lt;\boldsymbol{c^{T}\bar{x}}\]</span></p><p>  由上式可知沿着 <span class="math inline">\(\boldsymbol{d_{j}}\)</span>方向，能够使得目标函数下降，此时我们需要<strong>选取一个合适的步长 <span class="math inline">\(\lambda\)</span>，</strong>使得下降速度尽可能大，但又不会使得新得到的解 <span class="math inline">\(\boldsymbol{\bar{x}} + \lambda\boldsymbol{d_{j}}\)</span> 违反约束条件。<br>  首先来检验等式约束，对于 <span class="math inline">\(\forall \lambda\ge 0\)</span>：</p><p><span class="math display">\[\begin{split}    \boldsymbol{A}\left( \boldsymbol{\bar{x}} + \lambda\boldsymbol{d_{j}} \right) =&amp; \boldsymbol{A\bar{x}} + \lambda\boldsymbol{Ad_{j}} \\    =&amp; \boldsymbol{b} + \lambda \begin{bmatrix}        \boldsymbol{B,N} \\    \end{bmatrix} \begin{bmatrix}        -\boldsymbol{B^{-1}n_{j}} \\        \boldsymbol{e_{j}} \\    \end{bmatrix} \\    =&amp; \boldsymbol{b} + \lambda \left( -\boldsymbol{n_{j}} +\boldsymbol{n_{j}} \right) \\    =&amp; \boldsymbol{b}  \end{split}\]</span></p><p>  再来检验不等式约束：</p><p><span class="math display">\[\boldsymbol{\bar{x}} + \lambda\boldsymbol{d_{j}} = \begin{bmatrix}    \boldsymbol{B^{-1}b} \\    \boldsymbol{0} \\\end{bmatrix} + \lambda \begin{bmatrix}    -\boldsymbol{B^{-1}n_{j}} \\    \boldsymbol{e_{j}}\end{bmatrix} \ge \boldsymbol{0}\]</span></p><p><span class="math display">\[\Rightarrow \lambda \boldsymbol{e_{j}}\ge 0,\quad \boldsymbol{B^{-1}b}-\lambda\boldsymbol{B^{-1}n_{j}}=\boldsymbol{B^{-1}b} - \lambda\boldsymbol{r_{j}} \ge \boldsymbol{0}\]</span></p><p>记 <span class="math inline">\(\boldsymbol{h} = \boldsymbol{B^{-1}b}\ge \boldsymbol{0}\)</span>，则：</p><p><span class="math display">\[\boldsymbol{h}-\lambda\boldsymbol{r_{j}} \ge \boldsymbol{0} \quad \Leftrightarrow \quad\begin{bmatrix}    \boldsymbol{h_{1}} - \lambda \boldsymbol{r_{j1}} \\    \vdots \\    \boldsymbol{h_{m}} - \lambda \boldsymbol{r_{jm}} \\\end{bmatrix} \ge \boldsymbol{0} \quad \Leftrightarrow \quad \forall i,\boldsymbol{h_{i}} - \lambda \boldsymbol{r_{ji}} \ge 0\]</span></p><p><span class="math display">\[\Rightarrow \quad \lambda \leq\frac{\boldsymbol{h_{i}}}{\boldsymbol{r_{ji}}}, \forall i,且\boldsymbol{r_{ji}} &gt; 0 \quad \Rightarrow \quad \lambda^{*} =\min_{i} \{ \frac{\boldsymbol{h_{i}}}{\boldsymbol{r_{ji}}} |\boldsymbol{r_{ji}} &gt; 0 \}\]</span></p><p>  通过以上分析，我们找到了一个最佳的步长 <span class="math inline">\(\lambda^{*}\)</span>，有一个非常重要的结论：<strong>利用<span class="math inline">\(\lambda^{*}\)</span> 构造的新解 <span class="math inline">\(\boldsymbol{\hat{x}}=\boldsymbol{\bar{x}}+\lambda\boldsymbol{d_{j}}\)</span> 为可行解集 <span class="math inline">\(S\)</span> 的一个新的极点。</strong><br>  此时我们只需要重复以上步骤判断 <span class="math inline">\(\boldsymbol{\hat{x}}\)</span>是否是最优解，若是则算法终止，否则再构造下一个极点。</p><h2 id="algorithm-steps-for-simplex-method">Algorithm Steps for SimplexMethod</h2><p>  输入: <span class="math inline">\(\boldsymbol{c} \in\mathbb{R}^{n}, \boldsymbol{A} \in \mathbb{R}^{m \times n},\boldsymbol{b} \in \mathbb{R}^{m}\)</span><br>  输出: <span class="math inline">\(\boldsymbol{x} \in\mathbb{R}^{n}\)</span><br>  算法步骤：<br>  (1) 确定某种 <span class="math inline">\(\boldsymbol{A}\)</span>的分块: <span class="math inline">\(\boldsymbol{A} =\begin{bmatrix}  \boldsymbol{B,N}\end{bmatrix}\)</span>，以及对应的初始极点：</p><p><span class="math display">\[\boldsymbol{x} = \begin{bmatrix}    \boldsymbol{B^{-1}b} \\    \boldsymbol{0} \\\end{bmatrix}\]</span></p><p>  (2) 判断 <span class="math inline">\(\boldsymbol{c_{N}^{T}} -\boldsymbol{c_{B}^{T}B^{-1}N} \ge \boldsymbol{0}?\)</span>若是，则算法终止，输出 <span class="math inline">\(\boldsymbol{x}\)</span>，否则转入下一步(3);<br>  (3) 找到 <span class="math inline">\(\boldsymbol{c_{N}^{T}} -\boldsymbol{c_{B}^{T}B^{-1}N}\)</span> 的负值分量<span class="math inline">\(j\)</span>，检验是否 <span class="math inline">\(\exists j\)</span>，使得 <span class="math inline">\(\boldsymbol{r_{j}}=\boldsymbol{Bn_{j}} \leq\boldsymbol{0}\)</span>，若存在，则该问题是无界的，无最小值，算法终止，否则转入(4);<br>  (4) 挑选一个<span class="math inline">\(j\)</span>，构造一个新的极点<span class="math inline">\(\boldsymbol{\bar{x}}\)</span>:</p><p><span class="math display">\[\boldsymbol{\bar{x}} = \boldsymbol{x} +\lambda^{*} \boldsymbol{d_{j}},\quad \boldsymbol{d_{j}} =\begin{bmatrix}    -\boldsymbol{B^{-1}n_{j}} \\    \boldsymbol{e_{j}}\end{bmatrix},\quad \lambda^{*} = \min_{i} \{\frac{\boldsymbol{h_{i}}}{\boldsymbol{r_{ji}}} | \boldsymbol{r_{ji}}&gt; 0 \}\]</span></p><p>  (5) 根据 <span class="math inline">\(\boldsymbol{\bar{x}}\)</span>，对矩阵<span class="math inline">\(\boldsymbol{A}\)</span>进行重新分块 <span class="math inline">\(\boldsymbol{A} =\begin{bmatrix}  \boldsymbol{\bar{B},\bar{N}}\end{bmatrix}\)</span>，重复(2)~(5)步。</p><h2 id="python-impletation-of-simplex-method">Python Impletation ofSimplex Method</h2><p>  给定算法的输入：</p><p><span class="math display">\[\boldsymbol{c}=\begin{bmatrix}    1 \\    2 \\    3 \\    4 \\    5 \\\end{bmatrix},\quad \boldsymbol{A}= \begin{bmatrix}    1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\    2 &amp; 2 &amp; 3 &amp; 4 &amp; 5 \\\end{bmatrix},\quad \boldsymbol{b}=\begin{bmatrix}    10 \\    20 \\\end{bmatrix}\]</span></p><p>  我们可以调用 Python 的 <code>scipy.optimize</code> 类中的<code>linprog</code>方法来执行单纯形算法，代码如下：</p><div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.optimize <span class="im">import</span> linprog</span><span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span><span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>]</span><span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span><span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>A_eq <span class="op">=</span> [</span><span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>],</span><span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>] </span><span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>]</span><span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>b_eq <span class="op">=</span> [</span><span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="dv">10</span>, </span><span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="dv">20</span>  </span><span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>]</span><span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span><span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> linprog(c, A_eq<span class="op">=</span>A_eq, b_eq<span class="op">=</span>b_eq, method<span class="op">=</span><span class="st">'simplex'</span>)</span><span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span><span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(result[<span class="st">'x'</span>])</span></code></pre></div><p>  使用单纯形算法解得最优解为：</p><p><span class="math display">\[\boldsymbol{x^{*}} = \begin{bmatrix}    10 &amp; 0 &amp; 0 &amp; 0 &amp; 0\end{bmatrix}^{T}\]</span></p><h2 id="reference">Reference</h2><p><strong>[1] Video: 最优化理论与方法-第十一讲-线性规划, superfatseven,B站.</strong><br><strong>[2] Book: Boyd S P, Vandenberghe L. Convex optimization[M].Cambridge university press, 2004.</strong></p>]]></content>
    
    
    <summary type="html">本节主要介绍解决线性规划问题的单纯形方法。</summary>
    
    
    
    <category term="最优传输理论" scheme="http://example.com/categories/%E6%9C%80%E4%BC%98%E4%BC%A0%E8%BE%93%E7%90%86%E8%AE%BA/"/>
    
    
  </entry>
  
  <entry>
    <title>Optimal Transport-3.Kantorovich-OT Dual Problem</title>
    <link href="http://example.com/2024/03/16/Optimal%20Transport-3.Dual%20Problem/"/>
    <id>http://example.com/2024/03/16/Optimal%20Transport-3.Dual%20Problem/</id>
    <published>2024-03-16T03:04:14.000Z</published>
    <updated>2024-04-08T04:12:22.286Z</updated>
    
    <content type="html"><![CDATA[<h1 id="kantorovich-ot-dual-problem">Kantorovich-OT Dual Problem</h1><p>  在前面的小节，我们介绍了 KantorovichOT问题的基本形式，其基本形式如下：</p><p><span class="math display">\[L_{\boldsymbol{C}}(\boldsymbol{a},\boldsymbol{b}) \quad \overset{\text{def.}}{=} \quad\min_{\boldsymbol{P} \in \boldsymbol{U}(\boldsymbol{a},\boldsymbol{b})}\left&lt; \boldsymbol{C},\boldsymbol{P}\right&gt;_{F}=\sum_{i,j}p_{ij}c_{ij}\]</span></p><p>其中，<span class="math inline">\(\boldsymbol{P} \in\mathbb{R}_{+}^{n \times m}\)</span> 为运输矩阵，<span class="math inline">\(\boldsymbol{C} \in \mathbb{R}_{+}^{n \timesm}\)</span> 为成本矩阵，且约束条件为：</p><p><span class="math display">\[\boldsymbol{U}(\boldsymbol{a},\boldsymbol{b}) =\{ \boldsymbol{P} \in \mathbb{R}^{n \times m}_{+}: \boldsymbol{P}\mathbf{1}_{m}=\boldsymbol{a} \quad and \quad \boldsymbol{P^{T}}\mathbf{1}_{n}=\boldsymbol{b} \}\]</span></p><p>  从今天的视角来看，KantorovichOT问题本质上是一种线性规划问题，然而在 Kantorovich 给出 OT问题的上述形式时，严格的线性规划理论还没有被建立。作为线性规划理论的主要建立人，Kantorovich对于 OT问题的研究，在其建立线性规划理论上起到了重要的推动作用，线性规划中很多与经济资源分配的实际案例，便来源于Kantorovich 对于 OT 问题的研究。Kantorovich在线性规划理论中，提出了著名的<strong>对偶关系</strong>，这成为了最优化学科的核心理论之一。<br>  在本节，我们将首先介绍一些凸优化与对偶关系的基础知识，并给出Kantorovich OT问题的标准线性规划形式，最后我们将讨论其对偶问题的经济内涵。</p><h2 id="convex-optimization-foundations">Convex OptimizationFoundations</h2><h3 id="basic-concept">Basic Concept</h3><p><strong>仿射集</strong><br>  设<span class="math inline">\(C\)</span>为某一集合，若 <span class="math inline">\(\forall x_1,x_2 \in C, \theta \in \mathbb{R},\theta x_1+(1-\theta)x_2 \in C\)</span>，则称集合<span class="math inline">\(C\)</span>为仿射集。</p><p><strong>仿射函数</strong><br>  设有映射 <span class="math inline">\(f: \mathbb{R}^{n} \rightarrow\mathbb{R}^{m}\)</span>，若 <span class="math inline">\(f(x)=Ax+b, A \in\mathbb{R}^{m \times n}, b \in \mathbb{R}^{m}\)</span>，则称映射<span class="math inline">\(f\)</span>为仿射函数。</p><p><strong>凸集</strong><br>  设<span class="math inline">\(C\)</span>为某一集合，若 <span class="math inline">\(\forall x_1,x_2 \in C, \theta \in [0,1], \thetax_1+(1-\theta)x_2 \in C\)</span>，则称集合<span class="math inline">\(C\)</span>为凸集。</p><p><strong>凸函数</strong><br>  一个函数<span class="math inline">\(f(x)\)</span>被称为凸函数，如果它的定义域 <span class="math inline">\(dom f\)</span> 为凸集，并且对 <span class="math inline">\(\forall x_1,x_2 \in dom f, \alpha \in[0,1]\)</span>，有以下不等式成立：</p><p><span class="math display">\[f[\alpha x_1+(1-\alpha) x_2] \leq \alphaf(x_1)+(1-\alpha)f(x_2)\]</span></p><p>  <strong>凸函数的一阶条件</strong><br>  设函数<span class="math inline">\(f(x)\)</span><strong>一阶可微</strong>，<span class="math inline">\(x \in \mathbb{R}^{n}\)</span>，则<span class="math inline">\(f(x)\)</span>为凸函数的<strong>充要条件</strong>为：<span class="math inline">\(dom \space f\)</span>为凸集，且对<span class="math inline">\(\forall x,y \in dom \spacef\)</span>，有以下不等式成立：</p><p><span class="math display">\[f(y) \ge f(x)+\nablaf^{T}(x)(y-x)\]</span></p><p>  <strong>凸函数的二阶条件</strong><br>  设函数<span class="math inline">\(f(x)\)</span><strong>二阶可微</strong>，<span class="math inline">\(x \in \mathbb{R}^{n}\)</span>，则<span class="math inline">\(f(x)\)</span>为凸函数的<strong>充要条件</strong>为：<span class="math inline">\(dom \space f\)</span>为凸集，且对<span class="math inline">\(\forall x \in dom \space f\)</span>，有<span class="math inline">\(\nabla^{2}f(x) \succeq 0\)</span>，即<span class="math inline">\(Hessain\)</span>矩阵半正定。</p><p><strong>最优化问题</strong><br>  最优化问题的基本形式(1)：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{x} \quad &amp; f(x) \\        s.t. \quad &amp; m_{i}(x) \leq 0,\quad i=1,2,\dots,M \\        &amp; n_{j}(x) = 0,\quad j=1,2,\dots,N    \end{split}\end{equation}\]</span></p><ul><li><span class="math inline">\(x=\begin{bmatrix}  x_1,x_2,\dots,x_n\end{bmatrix}^{T},x \in \mathbb{R}^{n}\)</span>称为<strong>优化变量(Optimization Variable)</strong>.<br></li><li><span class="math inline">\(f: \mathbb{R}^{n} \rightarrow\mathbb{R}\)</span>，称为<strong>目标函数(ObjectiveFunction)</strong>.<br></li><li><span class="math inline">\(m_i: \mathbb{R}^{n} \rightarrow\mathbb{R}\)</span>，称为<strong>不等式约束(InequalityConstraint)</strong>.<br></li><li><span class="math inline">\(n_j: \mathbb{R}^{n} \rightarrow\mathbb{R}\)</span>，称为<strong>等式约束(EqualityConstraint)</strong>.<br></li><li><span class="math inline">\(D = \left( dom \space f \right) \bigcap\{x \vert m_{i}(x) \leq 0,i=1,2,\dots,M\} \bigcap \{x \vert h_{j}(x) =0,j=1,2,\dots,N\}\)</span>，称为最优化问题的<strong>域(Domain)</strong>，<span class="math inline">\(x \in D\)</span> 称为<strong>可行解(FeasibleSolution)</strong>.</li><li><span class="math inline">\(p^{*} = \inf \{ f(x) \vert x \in D\}\)</span>，称为最优化问题的<strong>最优值(Optimum)</strong>.<br></li><li><span class="math inline">\(f(x^{*})=p^{*}\)</span>，称<span class="math inline">\(x^{*}\)</span>为最优化问题的<strong>最优解(OptimalSolution)</strong>.<br></li><li><span class="math inline">\(X_{opt} = \{ x \vert x \in D,f(x)=p^{*}\}\)</span>，称为最优化问题的<strong>最优解集(OptimalSet)</strong>.</li></ul><p><strong>凸优化问题</strong><br>  若在优化问题(7)中，目标函数<span class="math inline">\(f(x)\)</span>为凸函数，不等式约束<span class="math inline">\(m_i(x)\)</span>为凸函数，等式约束<span class="math inline">\(n_j(x)\)</span>为仿射函数，则称该优化问题为凸优化问题。</p><h3 id="dual-problem">Dual Problem</h3><p><strong>拉格朗日函数</strong><br>  原问题(1)的拉格朗日函数为：</p><p><span class="math display">\[L(x,\lambda,\eta)=f(x)+\sum_{i=1}^{M}\lambda_im_i(x)+\sum_{j=1}^{N}\eta_j n_j(x)\]</span></p><p><span class="math display">\[\lambda_i \ge 0, \quadi=1,2,\dots,M\]</span></p><p><strong>原问题的无约束形式</strong><br>  原问题(1)的无约束形式(2)为：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{x} \max_{\lambda,\eta} \quad &amp; L(x,\lambda,\eta)  \\        s.t. \quad &amp; \lambda_i \ge 0, \quad i=1,2,\dots,M \\    \end{split}\end{equation}\]</span></p><p>  原问题(1)与其无约束形式(2)是等价的。</p><p><strong>对偶问题</strong><br>  原问题(1)的拉格朗日对偶问题(3)为：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \max_{\lambda,\eta} \min_{x} \quad &amp; L(x,\lambda,\eta) \\        s.t. \quad &amp; \lambda_i \ge 0, \quad i = 1,2,\dots,M    \end{split}\end{equation}\]</span></p><p><strong>弱对偶关系</strong><br>  原问题(1)与其对偶问题(3)满足弱对偶关系：</p><p><span class="math display">\[\max_{\lambda,\eta} \min_{x}L(x,\lambda,\eta) \leq \min_{x} \max_{\lambda,\eta}L(x,\lambda,\eta)\]</span></p><p><strong>强对偶关系</strong><br>  若原问题(1)与其对偶问题(3)满足：</p><p><span class="math display">\[\max_{\lambda,\eta} \min_{x}L(x,\lambda,\eta) = \min_{x} \max_{\lambda,\eta}L(x,\lambda,\eta)\]</span></p><p>  则称原问题(1)与其对偶问题(3)满足强对偶关系。</p><p><strong><span class="math inline">\(Slater\)</span>条件</strong><br>  若原问题(1)是凸问题，同时 <span class="math inline">\(\exists x \inrelint(D)\)</span>，使得约束满足：</p><p><span class="math display">\[\begin{split}    &amp; m_{i}(x) &lt; 0, \quad i=1,2,\dots,M \\    &amp; n_{j}(x) = 0, \quad j=1,2,\dots,N \\\end{split}\]</span></p><p>  则原问题与对偶问题满足强对偶关系。</p><ul><li>注：<span class="math inline">\(relint(D)\)</span>表示原始凸问题的域的相对内部，即域内除了边界点以外的所有点。</li></ul><p>  <span class="math inline">\(Slater\)</span>条件是一个<strong>充分不必要条件</strong>，若满足<span class="math inline">\(Slater\)</span>条件，则强对偶一定成立，不满足<span class="math inline">\(Slater\)</span>条件，强对偶也可能成立。大多数凸优化问题均满足<span class="math inline">\(Slater\)</span>条件，即有强对偶性。</p><h3 id="linear-programming">Linear Programming</h3><p>  线性规划问题的一般形式为(4)：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{x} \quad &amp; c^{T}x + d \\        s.t. \quad &amp;  Gx \leq h \\        &amp; Ax=b  \\    \end{split}\end{equation}\]</span></p><p>其中，<span class="math inline">\(x,c,d \in\mathbb{R}^{n}\)</span>；<span class="math inline">\(G \in \mathbb{R}^{M\times n}, h \in \mathbb{R}^{M}; A \in \mathbb{R}^{N \times n}, b \in\mathbb{R}^{N}\)</span>.<br>  一般的线性规划问题都可以通过变形，转化成线性规划的标准形式(5)：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{x} \quad &amp; c^{T}x  \\        s.t. \quad &amp;  Ax \leq b \\        &amp; x \leq 0    \end{split}\end{equation}\]</span></p><p>  有一个重要的结论是：<strong>线性规划问题是凸问题且满足强对偶关系。</strong></p><h2 id="linear-programming-form-of-kantorovich-ot-problem">LinearProgramming Form of Kantorovich-OT Problem</h2><p>  Kantorovich-OT Problem本质上是一个线性规划问题，我们可以通过变形，将其转化为线性规划形式，下面我们尝试将Kantorovich-OTProblem 转化为与其等价的线性规划问题。</p><h3 id="primal-linear-programming">Primal Linear Programming</h3><p>  原始的Kantorovich-OT Problem的形式如下：</p><p><span class="math display">\[L_{\boldsymbol{C}}(\boldsymbol{a},\boldsymbol{b}) \quad \overset{\text{def.}}{=} \quad\min_{\boldsymbol{P} \in \boldsymbol{U}(\boldsymbol{a},\boldsymbol{b})}\left&lt; \boldsymbol{C},\boldsymbol{P}\right&gt;_{F}=\sum_{i,j}p_{ij}c_{ij}\]</span></p><p>  运输矩阵 <span class="math inline">\(\boldsymbol{P}\)</span> 与成本矩阵 <span class="math inline">\(\boldsymbol{C}\)</span>可以写成列向量形式：</p><p><span class="math display">\[\boldsymbol{P}=\begin{bmatrix}    \boldsymbol{p_1}, \boldsymbol{p_2}, \cdots,\boldsymbol{p_m}\end{bmatrix},\quad \boldsymbol{C} = \begin{bmatrix}    \boldsymbol{c_1},\boldsymbol{c_2}, \cdots,\boldsymbol{c_m}\end{bmatrix}\]</span></p><p>  构造向量 <span class="math inline">\(\boldsymbol{p},\boldsymbol{c}\in \mathbb{R}^{nm}\)</span>，矩阵 <span class="math inline">\(A \in\mathbb{R}^{(n+m) \times nm}\)</span>:</p><p><span class="math display">\[\boldsymbol{p} = \begin{bmatrix}    \boldsymbol{p_{1}^{T}}, \boldsymbol{p_{2}^{T}}, \cdots,\boldsymbol{p_{m}^{T}}\end{bmatrix}^{T}, \quad \boldsymbol{c} = \begin{bmatrix}    \boldsymbol{c_{1}^{T}}, \boldsymbol{c_{2}^{T}}, \cdots,\boldsymbol{c_{m}^{T}}\end{bmatrix}^{T}\]</span></p><p><span class="math display">\[\boldsymbol{A} = \begin{bmatrix}    \boldsymbol{1_{n}^{T}} \otimes \boldsymbol{I_{m}} \\    \boldsymbol{I_{n}} \otimes \boldsymbol{1_{m}^{T}} \\\end{bmatrix}\]</span></p><p>其中，符号"<span class="math inline">\(\otimes\)</span>"表示矩阵的kronecker's product。</p><p><span class="math display">\[\boldsymbol{c^{T}}\boldsymbol{p}=\sum_{i=1}^{m}\boldsymbol{c_{i}^{T}}\boldsymbol{p_{i}}=\sum_{i,j}p_{ij}c_{ij}\]</span></p><p><span class="math display">\[\boldsymbol{A}\boldsymbol{p} =\begin{bmatrix}    \boldsymbol{1_{n}^{T}} \otimes \boldsymbol{I_{m}} \\    \boldsymbol{I_{n}} \otimes \boldsymbol{1_{m}^{T}} \\\end{bmatrix}\boldsymbol{p}=\begin{bmatrix}    (\boldsymbol{1_{n}^{T}} \otimes \boldsymbol{I_{m}})\boldsymbol{p} \\    (\boldsymbol{I_{n}} \otimes \boldsymbol{1_{m}^{T}})\boldsymbol{p} \\\end{bmatrix} \in \mathbb{R}^{n+m}\]</span></p><p>其中，</p><p><span class="math display">\[\begin{split}    (\boldsymbol{1_{n}^{T}} \otimes \boldsymbol{I_{m}})\boldsymbol{p}&amp;= \begin{bmatrix}        \boldsymbol{I_{m}}, \boldsymbol{I_{m}},\cdots,\boldsymbol{I_{m}}    \end{bmatrix}_{m \times nm} \cdot \boldsymbol{p} \\    &amp;= \begin{bmatrix}        \sum_{j}p_{1 \cdot j} \\        \sum_{j}p_{2 \cdot j} \\        \vdots \\        \sum_{j}p_{n \cdot j}    \end{bmatrix}_{m \times 1} = \boldsymbol{P1_{m}} = \boldsymbol{a}\end{split}\]</span></p><p><span class="math display">\[\begin{split}    (\boldsymbol{I_{n}} \otimes \boldsymbol{1_{m}^{T}})\boldsymbol{p}&amp;= \begin{bmatrix}        \boldsymbol{1_{m}^{T}} &amp; &amp; &amp; \\        &amp; \boldsymbol{1_{m}^{T}} &amp; &amp; \\        &amp; &amp; \ddots &amp; \\        &amp; &amp; &amp; \boldsymbol{1_{m}^{T}}    \end{bmatrix}_{n \times nm} \cdot \boldsymbol{p} \\    &amp;= \begin{bmatrix}        \sum_{j}p_{j \cdot 1} \\        \sum_{j}p_{j \cdot 2} \\        \vdots \\        \sum_{j}p_{j \cdot m} \\    \end{bmatrix}_{n \times 1} = \boldsymbol{P^{T}1_{n}}=\boldsymbol{b}\end{split}\]</span></p><p>故有：</p><p><span class="math display">\[\boldsymbol{Ap}=\begin{bmatrix}    (\boldsymbol{1_{n}^{T}} \otimes \boldsymbol{I_{m}})\boldsymbol{p} \\    (\boldsymbol{I_{n}} \otimes \boldsymbol{1_{m}^{T}})\boldsymbol{p} \\\end{bmatrix}=\begin{bmatrix}    \boldsymbol{a} \\    \boldsymbol{b} \\\end{bmatrix} \overset{\text{def.}}{=} \boldsymbol{d} \in\mathbb{R^{n+m}}\]</span></p><p>故 Kantorovich-OT Problem 可以转化为等价形式(6):</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{\boldsymbol{p} \in \mathbb{R^{nm}}} \quad&amp;\boldsymbol{c^{T}p} \\        s.t. \quad &amp; \boldsymbol{Ap} = \boldsymbol{d} \\        &amp; \boldsymbol{p} \ge \boldsymbol{0} \\    \end{split}\end{equation}\]</span></p><p>以上问题(6)即为 Kantorovich-OT Problem的一般线性规划形式，其中等式约束也可以改为不等式约束。</p><h3 id="dual-linear-programming">Dual Linear Programming</h3><p>  接下来我们来讨论一下问题(6)的对偶问题，首先写出问题(6)的拉格朗日函数：</p><p><span class="math display">\[\mathbf{L}(\boldsymbol{p},\boldsymbol{h},\boldsymbol{u})= \boldsymbol{c^{T}p} + \boldsymbol{h^{T}} \left[ \boldsymbol{Ap} -\boldsymbol{d} \right] - \boldsymbol{u^{T}p}\]</span></p><p>原问题的无约束形式为：</p><p><span class="math display">\[\begin{split}    \min_{\boldsymbol{p}}\max_{\boldsymbol{h,u}} \quad &amp;\mathbf{L}(\boldsymbol{p},\boldsymbol{h},\boldsymbol{u}) \\    s.t. \quad &amp; \boldsymbol{u} \ge \boldsymbol{0}\end{split}\]</span></p><p>则原问题(6)的对偶问题(7)为：</p><p><span class="math display">\[\begin{equation}    \max_{\boldsymbol{h,u}}\min_{\boldsymbol{p}} \quad\mathbf{L}(\boldsymbol{p},\boldsymbol{h},\boldsymbol{u})\end{equation}\]</span></p><p>我们首先来考察一下内部极小化问题：</p><p><span class="math display">\[\begin{split}    \min_{p} \mathbf{L}(\boldsymbol{p},\boldsymbol{h},\boldsymbol{u})&amp;= \boldsymbol{c^{T}p} + \boldsymbol{h^{T}}\left( \boldsymbol{Ap} -\boldsymbol{d} \right) - \boldsymbol{u^{T}p} \\    &amp;= \left(  \boldsymbol{c^{T}} + \boldsymbol{h^{T}A} -\boldsymbol{u^{T}} \right)\boldsymbol{p} - \boldsymbol{h^{T}d} \\    &amp;= \left( \boldsymbol{c} + \boldsymbol{A^{T}h - \boldsymbol{u}}\right)^{T}\boldsymbol{p} - \boldsymbol{h^{T}d}\end{split}\]</span></p><p>令 <span class="math inline">\(\boldsymbol{h'}=-\boldsymbol{h}\)</span>，则有：</p><p><span class="math display">\[\min_{p}\mathbf{L}(\boldsymbol{p},\boldsymbol{h},\boldsymbol{u}) = \left(\boldsymbol{c} - \boldsymbol{u} - \boldsymbol{A^{T}h'}\right)^{T}\boldsymbol{p} + \boldsymbol{(h')^{T}d}\]</span></p><p><span class="math display">\[\min_{p}\mathbf{L}(\boldsymbol{p},\boldsymbol{h'},\boldsymbol{u}) = \left \{\begin{array}{rcl}\boldsymbol{(h')^{T}d}, &amp; {\boldsymbol{A^{T}h'} \leq\boldsymbol{c} - \boldsymbol{u}}\\-\infty,&amp; {\boldsymbol{A^{T}h'} \nleq \boldsymbol{c} -\boldsymbol{u}}\\\end{array} \right.\]</span></p><p>故问题(7)等价于:</p><p><span class="math display">\[\begin{split}    \max_{\boldsymbol{h',u}} \quad &amp; \boldsymbol{(h')^{T}d}\\    s.t. \quad &amp; \boldsymbol{A^{T}h'} \leq \boldsymbol{c} -\boldsymbol{u} \\    &amp; \boldsymbol{u} \ge \boldsymbol{0}\end{split}\]</span></p><p>合并两个等式约束得到原问题(6)的对偶问题(8):</p><p><span class="math display">\[\begin{equation}    \begin{split}    \max_{\boldsymbol{h'} \in \mathbb{R}^{n+m}} \quad &amp;\boldsymbol{d^{T}h'} \\    s.t. \quad &amp; \boldsymbol{A^{T}h'} \leq \boldsymbol{c} \\    &amp; \boldsymbol{h'} \ge \boldsymbol{0}\end{split}\end{equation}\]</span></p><h2 id="economic-interpretation-of-kantorovich-ot-dual-problem">Economicinterpretation of Kantorovich-OT Dual Problem</h2><p>  Kantorovich-OT Problem的对偶问题具有现实的经济解释，下面我们首先导出其对偶问题的形式。其原问题为：</p><p><span class="math display">\[\min_{\boldsymbol{P} \in\boldsymbol{U}(\boldsymbol{a},\boldsymbol{b})} \left&lt;\boldsymbol{C},\boldsymbol{P}\right&gt;_{F}=\sum_{i,j}p_{ij}c_{ij}\]</span></p><p><span class="math display">\[\boldsymbol{U}(\boldsymbol{a},\boldsymbol{b}) =\{ \boldsymbol{P} \in \mathbb{R}^{n \times m}_{+}: \boldsymbol{P}\mathbf{1}_{m}=\boldsymbol{a} \quad and \quad \boldsymbol{P^{T}}\mathbf{1}_{n}=\boldsymbol{b} \}\]</span></p><p>原问题的拉格朗日函数：</p><p><span class="math display">\[\mathbf{L}(\boldsymbol{P},\boldsymbol{f},\boldsymbol{g})= \left&lt; \boldsymbol{C},\boldsymbol{P} \right&gt; + \left&lt;\boldsymbol{a} - \boldsymbol{P1_{m}},\boldsymbol{f} \right&gt;+\left&lt;\boldsymbol{b} - \boldsymbol{P^{T}1_{n}},\boldsymbol{g}\right&gt;\]</span></p><p>其中，<span class="math inline">\(\boldsymbol{f} \in \mathbb{R}^{n},\boldsymbol{g} \in \mathbb{R}^{m}\)</span>。则原问题的无约束形式为：</p><p><span class="math display">\[\min_{\boldsymbol{P} \ge0}\max_{\boldsymbol{f},\boldsymbol{g}}\quad \left&lt;\boldsymbol{C},\boldsymbol{P} \right&gt; + \left&lt; \boldsymbol{a} -\boldsymbol{P1_{m}},\boldsymbol{f} \right&gt;+\left&lt; \boldsymbol{b} -\boldsymbol{P^{T}1_{n}},\boldsymbol{g} \right&gt;\]</span></p><p>上式等价于</p><p><span class="math display">\[\max_{\boldsymbol{f},\boldsymbol{g}}\left&lt; \boldsymbol{a},\boldsymbol{f} \right&gt; + \left&lt;\boldsymbol{b},\boldsymbol{g} \right&gt; + \min_{\boldsymbol{P} \ge\boldsymbol{0}} \left&lt; \boldsymbol{C -\boldsymbol{f1_{m}^{T}}-\boldsymbol{1_{n}g^{T}}}, \boldsymbol{P}\right&gt;\]</span></p><p>令 <span class="math inline">\(\boldsymbol{Q}=\boldsymbol{C -\boldsymbol{f1_{m}^{T}}-\boldsymbol{1_{n}g^{T}}} = \boldsymbol{C} -\boldsymbol{f}\oplus\boldsymbol{g}\)</span>，有：</p><p><span class="math display">\[\min_{\boldsymbol{P} \ge \boldsymbol{0}}\left&lt; \boldsymbol{Q},\boldsymbol{P} \right&gt; = \left \{\begin{array}{rcl}0, &amp; {\boldsymbol{Q} \ge \boldsymbol{0}}\\-\infty,&amp; {otherwise}\\\end{array} \right.\]</span></p><p>故原问题的对偶问题可以写成(9):</p><p><span class="math display">\[\begin{equation}    L_{\boldsymbol{C}}(\boldsymbol{a},\boldsymbol{b}) =\max_{(\boldsymbol{f},\boldsymbol{g}) \in\boldsymbol{R}(\boldsymbol{C})} \left&lt; \boldsymbol{f},\boldsymbol{a}\right&gt; + \left&lt; \boldsymbol{g,\boldsymbol{b}} \right&gt;\end{equation}\]</span></p><p><span class="math display">\[\boldsymbol{R}(\boldsymbol{C})\overset{def.}{=} \{ (\boldsymbol{f},\boldsymbol{g}) \in \mathbb{R}^{n}\times \mathbb{R}^{m}: \boldsymbol{f}\oplus\boldsymbol{g} \leq\boldsymbol{C} \}\]</span></p><p>  借助原问题的经济含义，我们来思考一下对偶问题的经济解释。在原问题中，我们需要将n 个木材工厂生产的木材运输到 m 个城市，各个木材工厂的产量为 <span class="math inline">\(\boldsymbol{a} \in\mathbb{R}_{+}^{n}\)</span>，各个城市的需求量为 <span class="math inline">\(\boldsymbol{b} \in\mathbb{R}_{+}^{m}\)</span>。在原问题中，我们所需要求解的运输矩阵 <span class="math inline">\(\boldsymbol{P}\)</span> 包含了从工厂 i 到城市 j的具体运输计划，即需要将 工厂 i 生产多少木材运输到城市 j。成本矩阵 <span class="math inline">\(\boldsymbol{C}\)</span> 则表示从工厂 i 到城市 j每单位木材运输所消耗的成本。约束条件 <span class="math inline">\(\boldsymbol{U}(\boldsymbol{a},\boldsymbol{b})\)</span>表示运输计划需要符合各个工厂的产量以及各个城市的需求量。<br>  <strong>中间商运输</strong><br>  考虑目标函数。木材供应商面对复杂的运输问题，他选择将运输的工作外包给某个中间商。该中间商会首先从各个木材工厂统一将木材进行回收，然后再分发给不同的城市。假设木材的回收价格为<span class="math inline">\(\boldsymbol{f} \in\mathbb{R}^{n}\)</span>，<span class="math inline">\(f_{i}\)</span>表示从木材工厂 i 回收单位木材的价格；木材的分发价格为 <span class="math inline">\(\boldsymbol{g} \in \mathbb{R}^{m}\)</span>，<span class="math inline">\(g_{i}\)</span> 表示将单位木材分发给城市 i的价格。中间商的目的自然是最大化它的利润，故对于中间商来说，这个运输的优化问题为：</p><p><span class="math display">\[\max_{\boldsymbol{f},\boldsymbol{g}}\quad \left&lt; \boldsymbol{f},\boldsymbol{a} \right&gt; + \left&lt;\boldsymbol{g},\boldsymbol{b} \right&gt;\]</span></p><p>  考虑约束条件。对于木材供应商来说，他认为如果中间商的定价满足 <span class="math inline">\(\boldsymbol{f} \oplus \boldsymbol{g} \leq\boldsymbol{C}\)</span>，则意味着自己一定不会吃亏，因为这意味着 <span class="math inline">\(\forall i,j, f_i+g_j \leq\boldsymbol{C}_{ij}\)</span>，即对于将单位木材从工厂 i 运输到城市j，中间商收取的费用一定是小于等于自己运输的成本。因此供应商将这条定价约束写进了招商合同。这样对于中间商来说，它所面对的最优化问题即为：</p><p><span class="math display">\[\begin{equation}    L_{\boldsymbol{C}}(\boldsymbol{a},\boldsymbol{b}) =\max_{(\boldsymbol{f},\boldsymbol{g}) \in\boldsymbol{R}(\boldsymbol{C})} \left&lt; \boldsymbol{f},\boldsymbol{a}\right&gt; + \left&lt; \boldsymbol{g,\boldsymbol{b}} \right&gt;\end{equation}\]</span></p><p><span class="math display">\[\boldsymbol{R}(\boldsymbol{C})\overset{def.}{=} \{ (\boldsymbol{f},\boldsymbol{g}) \in \mathbb{R}^{n}\times \mathbb{R}^{m}: \boldsymbol{f}\oplus\boldsymbol{g} \leq\boldsymbol{C} \}\]</span></p><p>  考虑经济收益。由于供应商面对的原问题本质上是一个线性规划问题，故其满足强对偶关系，这意味着中间商可以通过合理地设置收费价格<span class="math inline">\(\boldsymbol{f},\boldsymbol{g}\)</span>使得：</p><p><span class="math display">\[\min_{\boldsymbol{P} \in\boldsymbol{U}(\boldsymbol{a},\boldsymbol{b})} \left&lt;\boldsymbol{P},\boldsymbol{C} \right&gt; =\max_{(\boldsymbol{f},\boldsymbol{g}) \in\boldsymbol{R}(\boldsymbol{C})} \left&lt; \boldsymbol{f},\boldsymbol{a}\right&gt; + \left&lt; \boldsymbol{g},\boldsymbol{b}\right&gt;\]</span></p><p>  值得注意的是，中间商所设置的价格 <span class="math inline">\(\boldsymbol{f},\boldsymbol{g}\)</span>可以为负数，这也很好理解，中间商可以补贴的形式，从供应商那里收取木材，即将<span class="math inline">\(\boldsymbol{f}\)</span>的某些值设置为负数；而对于需要木材的城市，收取高额的分发费用，即增加<span class="math inline">\(\boldsymbol{g}\)</span>的某些分量的值，这样对于中间商来说，它仍然可以保证利润不变，但却给供应商营造了一种获利的错觉。</p><h2 id="reference">Reference</h2><ul><li><strong>[1] Book: Peyré G, Cuturi M. Computational optimaltransport[J]. Center for Research in Economics and Statistics WorkingPapers, 2017 (2017-86).</strong><br></li><li><strong>[2] Lecture: 李向东. 最优传输理论及其应用.BIMSA</strong></li></ul>]]></content>
    
    
    <summary type="html">本节主要介绍线性规划与对偶关系的基础知识，以及Kantorovich OT问题的对偶问题的经济含义。</summary>
    
    
    
    <category term="最优传输理论" scheme="http://example.com/categories/%E6%9C%80%E4%BC%98%E4%BC%A0%E8%BE%93%E7%90%86%E8%AE%BA/"/>
    
    
  </entry>
  
  <entry>
    <title>Optimal Transport-2.Wasserstein Distance</title>
    <link href="http://example.com/2024/02/15/Optimal%20Transport-2.Wassertein%20Distance/"/>
    <id>http://example.com/2024/02/15/Optimal%20Transport-2.Wassertein%20Distance/</id>
    <published>2024-02-15T12:38:15.000Z</published>
    <updated>2024-04-08T04:12:36.338Z</updated>
    
    <content type="html"><![CDATA[<h1 id="wasserstein-distance">Wasserstein Distance</h1><p>  在上一节 Monge-Kantorovich Problem中，我们介绍了最优传输问题。最优传输一个重要的应用是它可以用来衡量分布之间的距离，从而将距离的概念由点与点之间拓展到分布与分布之间。本节我们将介绍分布之间的距离定义，即Wasserstein Distance，以及为什么其能够用于表示分布之间的距离。</p><h2 id="metric-properties-on-probility-space">Metric Properties onProbility Space</h2><p>  在上一节中，我们从概率视角描述了最优传输问题。设 <span class="math inline">\(X,Y\)</span> 是服从分布 <span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta}\)</span>的两个随机变量，运输矩阵为 <span class="math inline">\(\boldsymbol{P}\)</span>，成本矩阵为 <span class="math inline">\(\boldsymbol{C}\)</span>，则分布 <span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta}\)</span>之间的最优传输问题可以被定义为：</p><p><span class="math display">\[L_{\boldsymbol{C}}(\boldsymbol{\alpha},\boldsymbol{\beta}):= \min_{\boldsymbol{P} \in\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta})} \left&lt;\boldsymbol{P},\boldsymbol{C} \right&gt; = \min_{(X,Y)} \{\mathbb{E}_{(X,Y)}(c(X,Y)): X \sim \boldsymbol{\alpha}, Y \sim\boldsymbol{\beta} \}\]</span></p><p>  <span class="math inline">\(L_{\boldsymbol{C}}(\boldsymbol{\alpha},\boldsymbol{\beta})\)</span>的含义是将分布 <span class="math inline">\(\boldsymbol{\alpha}\)</span>传输到分布 <span class="math inline">\(\boldsymbol{\beta}\)</span>所花费的最小成本，我们很自然地就会想到 <span class="math inline">\(L_{\boldsymbol{C}}(\boldsymbol{\alpha},\boldsymbol{\beta})\)</span>也许能够表示分布 <span class="math inline">\(\boldsymbol{\alpha}\)</span> 和 <span class="math inline">\(\boldsymbol{\beta}\)</span>之间的距离或相似度。当然，要说明这个问题，我们需要证明函数 <span class="math inline">\(L_{\boldsymbol{C}}\)</span>满足概率空间中距离函数的性质。<br>  设分布 $, $ 取自概率空间 <span class="math inline">\(\mathcal{X}\)</span>，<span class="math inline">\(W(\boldsymbol{\alpha},\boldsymbol{\beta})\)</span>是分布 <span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta}\)</span>之间的距离函数，如果函数 <span class="math inline">\(W\)</span>满足:</p><ul><li><strong>非负性(Non-negativity):</strong> 对 <span class="math inline">\(\forall \boldsymbol{\alpha},\boldsymbol{\beta} \in\mathcal{X}, W(\boldsymbol{\alpha},\boldsymbol{\beta}) \ge0.\)</span><br></li><li><strong>同一性(Identity of Indiscernibles):</strong> <span class="math inline">\(W(\boldsymbol{\alpha},\boldsymbol{\beta})=0\)</span>当且仅当 <span class="math inline">\(\boldsymbol{\alpha} =\boldsymbol{\beta}.\)</span><br></li><li><strong>对称性(Symmetry):</strong> <span class="math inline">\(\forall \boldsymbol{\alpha},\boldsymbol{\beta} \in\mathcal{X}, W(\boldsymbol{\alpha},\boldsymbol{\beta}) =W(\boldsymbol{\beta},\boldsymbol{\alpha}).\)</span><br></li><li><strong>三角不等式(Triangle Inequality):</strong> <span class="math inline">\(\forall\boldsymbol{\alpha},\boldsymbol{\beta},\boldsymbol{\gamma} \in\mathcal{X}, W(\boldsymbol{\alpha},\boldsymbol{\gamma}) \leqW(\boldsymbol{\alpha},\boldsymbol{\beta})+W(\boldsymbol{\beta},\boldsymbol{\gamma}).\)</span></li></ul><p>  学者们通过研究发现，当对成本矩阵 <span class="math inline">\(\boldsymbol{C}\)</span>设置一些条件后，可以使得概率空间中最优传输问题的解 <span class="math inline">\(L_{\boldsymbol{C}}(\boldsymbol{\alpha},\boldsymbol{\beta})\)</span>满足距离函数的性质，从而使得其可以用于衡量分布之间的距离。</p><h2 id="wasserstein-distance-1">Wasserstein Distance</h2><h3 id="definition">Definition</h3><p>  我们首先来定义离散分布下的 Wasserstein Distance。设 <span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta} \in\sum_{n}:=\{ \boldsymbol{x} \in \mathbb{R}^{n}_{+}:\boldsymbol{x^{T}}\mathbf{1}_{n}=1 \}\)</span>，设矩阵 <span class="math inline">\(\boldsymbol{D} \in \mathbb{R}^{n \timesn}\)</span> 是一个度量矩阵，即矩阵 <span class="math inline">\(\boldsymbol{D}\)</span> 满足:<br><strong>(1)</strong> <span class="math inline">\(\boldsymbol{D} \in\mathbb{R}^{n \times n}_{+};\)</span><br><strong>(2)</strong> <span class="math inline">\(\boldsymbol{D}_{i,j}=0\)</span>，当且仅当 <span class="math inline">\(i=j\)</span>;<br><strong>(3)</strong> <span class="math inline">\(\boldsymbol{D}\)</span>是对称矩阵;<br><strong>(4)</strong> <span class="math inline">\(\forall i,j,k \in \{1,\dotsb,n\}, \boldsymbol{D}_{i,k} \leq\boldsymbol{D}_{i,j}+\boldsymbol{D}_{j,k}\)</span>.<br>令成本矩阵 <span class="math inline">\(\boldsymbol{C} =\boldsymbol{D}^{p}= \left[ \boldsymbol{D}_{i,j}^{p} \right]_{n \times n}\in \mathbb{R}^{n \times n}_{+}(p \ge 1)\)</span>，定义：</p><p><span class="math display">\[W_{p}(\boldsymbol{\alpha},\boldsymbol{\beta}) :=L_{\boldsymbol{D}^{p}}(\boldsymbol{\alpha,\boldsymbol{\beta}})^{1/p}\]</span></p><p>则称 <span class="math inline">\(W_{p}(\boldsymbol{\alpha},\boldsymbol{\beta})\)</span>为概率分布 <span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta}\)</span>之间的 <strong>p-Wasserstein 距离</strong>。<br>  现在来证明<span class="math inline">\(W_{p}\)</span>可以作为概率空间<span class="math inline">\(\sum_{n}\)</span>上的距离函数。</p><h3 id="proof">Proof</h3><p><span class="math display">\[W_{p}(\boldsymbol{\alpha},\boldsymbol{\beta}) =L_{\boldsymbol{D}^{p}}(\boldsymbol{\alpha,\boldsymbol{\beta}})^{1/p} =\left( \min_{\boldsymbol{P} \in\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta})} \left&lt;\boldsymbol{P},\boldsymbol{D}^{p} \right&gt;\right)^{\frac{1}{p}}\]</span></p><p>其中 <span class="math inline">\(\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta})= \{ \boldsymbol{P} \in \mathbb{R}^{n \times n}_{+} :\boldsymbol{P}\mathbf{1}_{n}=\boldsymbol{\alpha} \quad and \quad\boldsymbol{P^{T}}\mathbf{1}_n=\boldsymbol{\beta} \}\)</span>.<br>  要证明 <span class="math inline">\(W_{p}\)</span>可以作为概率空间<span class="math inline">\(\sum_{n}\)</span>上的距离函数，则需要证明 <span class="math inline">\(W_{p}\)</span>满足概率空间中距离函数的性质，即非负性、同一性、对称性、三角不等式。<br>  <strong>(1) 非负性证明</strong><br>   <span class="math inline">\(\boldsymbol{P},\boldsymbol{D}^{p} \in\mathbb{R}^{n \times n}_{+} \Rightarrow \left&lt;\boldsymbol{P},\boldsymbol{D}^{p}\right&gt;=\sum_{ij}\boldsymbol{P}_{ij}\boldsymbol{D}^{p}_{ij} \ge 0\Rightarrow W_{p}(\boldsymbol{\alpha}, \boldsymbol{\beta}) \ge0.\)</span><br>  <strong>(2) 同一性证明</strong><br>  由度量矩阵的性质可知: <span class="math inline">\(\boldsymbol{D}_{i,i}=0, \forall i \in \{1,\dotsb,n \}\)</span>，则有 <span class="math inline">\(\boldsymbol{D}_{i,i}^{p}=0\)</span>，即成本矩阵<span class="math inline">\(\boldsymbol{D}^{p}\)</span>的对角线元素均为零。<br>  当 <span class="math inline">\(\boldsymbol{\alpha}=\boldsymbol{\beta}\)</span>时，可行域 <span class="math inline">\(\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\alpha})= \{ \boldsymbol{P} \in \mathbb{R}^{n \times n}_{+} :\boldsymbol{P}\mathbf{1}_{n}=\boldsymbol{P^{T}}\mathbf{1}_n=\boldsymbol{\alpha}\}\)</span>，则 <span class="math inline">\(\boldsymbol{P}^{*}=diag(\boldsymbol{\alpha}) \in\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\alpha})\)</span>，此时：</p><p><span class="math display">\[\left&lt;  \boldsymbol{P}^{*},\boldsymbol{D}^{p}\right&gt;=\sum_{i}\boldsymbol{\alpha}_{i}\boldsymbol{D}_{i,i}^{p}=0\RightarrowW_{p}(\boldsymbol{\alpha},\boldsymbol{\alpha})=0\]</span></p><p>  当 <span class="math inline">\(W_{p}(\boldsymbol{\alpha},\boldsymbol{\beta})=0\)</span>时，由于成本矩阵 <span class="math inline">\(\boldsymbol{D}^{p}\)</span>的非对角线元素均大于零，故运输矩阵 <span class="math inline">\(\boldsymbol{P}\)</span>的非对角线元素均为零，即运输矩阵 <span class="math inline">\(\boldsymbol{P}\)</span> 为对角矩阵，<span class="math inline">\(\boldsymbol{P}=\boldsymbol{P}^{T}\)</span>. 此时有<span class="math inline">\(\boldsymbol{P}\mathbf{1}_{n}=\boldsymbol{P^{T}}\mathbf{1}_n\)</span>，即<span class="math inline">\(\boldsymbol{\alpha}=\boldsymbol{\beta}\)</span>.<br>  <strong>(3) 对称性证明</strong><br>  设 <span class="math inline">\(\boldsymbol{P}^{*}\)</span> 为<span class="math inline">\(W_{p}(\boldsymbol{\alpha},\boldsymbol{\beta})\)</span>所对应的最优运输矩阵，则有:</p><p><span class="math display">\[W_{p}(\boldsymbol{\alpha},\boldsymbol{\beta})=\left&lt;  \boldsymbol{P}^{*},\boldsymbol{D}^{p}\right&gt;^{\frac{1}{p}}\]</span></p><p>  由于成本矩阵 <span class="math inline">\(\boldsymbol{D}^{p}\)</span> 是对称矩阵，故有：</p><p><span class="math display">\[W_{p}(\boldsymbol{\alpha},\boldsymbol{\beta})=\left&lt;  \boldsymbol{P}^{*},\boldsymbol{D}^{p}\right&gt;^{\frac{1}{p}}=\left&lt;  \boldsymbol{(P^{*})^{T}},\boldsymbol{D}^{p}\right&gt;^{\frac{1}{p}}\]</span></p><p>  <span class="math inline">\(\boldsymbol{(P^{*})^{T}}\mathbf{1}_{n}=\boldsymbol{\beta},\boldsymbol{P}^{*}\mathbf{1}_{n}=\boldsymbol{\alpha} \Rightarrow\boldsymbol{(P^{*})^{T}} \in\boldsymbol{U}(\boldsymbol{\beta},\boldsymbol{\alpha})\)</span>. 由于<span class="math inline">\(\boldsymbol{U}(\boldsymbol{\beta},\boldsymbol{\alpha})\)</span>与 <span class="math inline">\(\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta})\)</span>中的运输矩阵是对应转置的关系，故有：</p><p><span class="math display">\[W_{p}(\boldsymbol{\beta},\boldsymbol{\alpha})=\left(\min_{\boldsymbol{P} \in\boldsymbol{U}(\boldsymbol{\beta},\boldsymbol{\alpha})} \left&lt;\boldsymbol{P},\boldsymbol{D}^{p} \right&gt;\right)^{\frac{1}{p}}=\left&lt;  \boldsymbol{(P^{*})^{T}},\boldsymbol{D}^{p}\right&gt;^{\frac{1}{p}}\]</span></p><p><span class="math display">\[\RightarrowW_{p}(\boldsymbol{\alpha},\boldsymbol{\beta}) =W_{p}(\boldsymbol{\beta},\boldsymbol{\alpha})\]</span><br>  <strong>(4) 三角不等式性质证明</strong><br>  设 <span class="math inline">\(\boldsymbol{\gamma} \in\sum_{n}\)</span>, 现证明：<span class="math inline">\(W_{p}(\boldsymbol{\alpha},\boldsymbol{\gamma})\leqW_{p}(\boldsymbol{\alpha},\boldsymbol{\beta})+W_{p}(\boldsymbol{\beta},\boldsymbol{\gamma})\)</span>.<br>  设 <span class="math inline">\(\boldsymbol{P}\)</span> 是 <span class="math inline">\(W_{p}(\boldsymbol{\alpha},\boldsymbol{\beta})\)</span>所对应的最优运输矩阵，<span class="math inline">\(\boldsymbol{Q}\)</span> 是 <span class="math inline">\(W_{p}(\boldsymbol{\beta},\boldsymbol{\gamma})\)</span>所对应的最优运输矩阵，则有</p><p><span class="math display">\[\begin{split}    W_{p}(\boldsymbol{\alpha},\boldsymbol{\beta}) &amp;=\left&lt;  \boldsymbol{P},\boldsymbol{D}^{p} \right&gt;^{\frac{1}{p}} =\left(\sum_{ij}\boldsymbol{P}_{ij}\boldsymbol{D}^{p}_{ij}\right)^{\frac{1}{p}}  \\    W_{p}(\boldsymbol{\beta},\boldsymbol{\gamma}) &amp;=\left&lt;  \boldsymbol{Q},\boldsymbol{D}^{p} \right&gt;^{\frac{1}{p}} =\left(\sum_{ij}\boldsymbol{Q}_{ij}\boldsymbol{D}^{p}_{ij}\right)^{\frac{1}{p}}  \\\end{split}\]</span></p><p>  定义：</p><p><span class="math display">\[\tilde{\boldsymbol{\beta}} =[\tilde{\boldsymbol{\beta}}_{j}],\quad \tilde{\boldsymbol{\beta}}_{j} =\left \{ \begin{array}{lr}    \boldsymbol{\beta}_{j}, \quad\boldsymbol{\beta}_{j} &gt; 0 \\    1, \quad\boldsymbol{\beta}_{j} = 0\end{array} \right.\]</span></p><p><span class="math display">\[\boldsymbol{S} :=\boldsymbol{P}diag(1/\tilde{\boldsymbol{\beta}})\boldsymbol{Q} \in\mathbb{R}^{n \times n}_{+}\]</span></p><p>  则有：</p><p><span class="math display">\[\begin{split}    \boldsymbol{S}\mathbf{1}_{n} &amp;=\boldsymbol{P}diag(1/\tilde{\boldsymbol{\beta}})\boldsymbol{Q}\mathbf{1}_{n}=\boldsymbol{P}diag(1/\tilde{\boldsymbol{\beta}})\boldsymbol{\beta}  \\    &amp;=\boldsymbol{P}\boldsymbol{[\boldsymbol{\beta}_{j}/\tilde{\boldsymbol{\beta}}_{j}]_{n}}= \boldsymbol{P}\mathbf{1}_{Supp(\boldsymbol{\beta})} =\boldsymbol{P}\mathbf{1}_{n} \\    &amp;= \boldsymbol{\alpha}\end{split}\]</span></p><p>  同理可得：<span class="math inline">\(\boldsymbol{S}^{T}\mathbf{1}_{n}=\boldsymbol{\gamma}\)</span>，则可以得到:<span class="math inline">\(\boldsymbol{S} \in\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\gamma})\)</span>.</p><p><span class="math display">\[\begin{split}    W_{p}(\boldsymbol{\alpha}, \boldsymbol{\gamma}) &amp;= \left(\min_{\boldsymbol{P} \in\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\gamma})} \left&lt;\boldsymbol{P},\boldsymbol{D}^{p} \right&gt; \right)^{\frac{1}{p}} \leq\left&lt; \boldsymbol{S},\boldsymbol{D}^{p} \right&gt;^{\frac{1}{p}}  \\    &amp;= \left(  \sum_{ik}\boldsymbol{D}_{ik}^{p}\boldsymbol{S}_{ik}\right)^{\frac{1}{p}} =\left(  \sum_{ik}\boldsymbol{D}_{ik}^{p}\sum_{j}\frac{\boldsymbol{P}_{ij}\boldsymbol{Q}_{jk}}{\tilde{\boldsymbol{\beta}}_{j}}\right)^{\frac{1}{p}} =\left(  \sum_{ijk}\boldsymbol{D}_{ik}^{p}\frac{\boldsymbol{P}_{ij}\boldsymbol{Q}_{jk}}{\tilde{\boldsymbol{\beta}}_{j}}\right)^{\frac{1}{p}}  \\    &amp; \leq\left(  \sum_{ijk}(\boldsymbol{D}_{ij}+\boldsymbol{D}_{jk})^{p}\frac{\boldsymbol{P}_{ij}\boldsymbol{Q}_{jk}}{\tilde{\boldsymbol{\beta}}_{j}}\right)^{\frac{1}{p}} \leq\left(  \sum_{ijk}\boldsymbol{D}_{ij}^{p}\frac{\boldsymbol{P}_{ij}\boldsymbol{Q}_{jk}}{\tilde{\boldsymbol{\beta}}_{j}}\right)^{\frac{1}{p}} +\left(  \sum_{ijk}\boldsymbol{D}_{jk}^{p}\frac{\boldsymbol{P}_{ij}\boldsymbol{Q}_{jk}}{\tilde{\boldsymbol{\beta}}_{j}}\right)^{\frac{1}{p}}  \\    &amp;=\left(  \sum_{ij}\boldsymbol{D}_{ij}^{p}\boldsymbol{P}_{ij}\sum_{k}\frac{\boldsymbol{Q}_{jk}}{\tilde{\boldsymbol{\beta}}_{j}}\right)^{\frac{1}{p}} +\left(  \sum_{jk}\boldsymbol{D}_{jk}^{p}\boldsymbol{Q}_{jk}\sum_{i}\frac{\boldsymbol{P}_{ij}}{\tilde{\boldsymbol{\beta}}_{j}}\right)^{\frac{1}{p}}  \\    &amp;= \left(  \sum_{ij}\boldsymbol{D}_{ij}^{p}\boldsymbol{P}_{ij}\right)^{\frac{1}{p}} +\left(  \sum_{jk}\boldsymbol{D}_{jk}^{p}\boldsymbol{Q}_{jk}\right)^{\frac{1}{p}}  \\    &amp;= W_{p}(\boldsymbol{\alpha},\boldsymbol{\beta}) +W_{p}(\boldsymbol{\beta},\boldsymbol{\gamma})  \\\end{split}\]</span></p><p>  故有：</p><p><span class="math display">\[W_{p}(\boldsymbol{\alpha},\boldsymbol{\gamma})\leqW_{p}(\boldsymbol{\alpha},\boldsymbol{\beta})+W_{p}(\boldsymbol{\beta},\boldsymbol{\gamma})\]</span></p><p>  综上所述，<span class="math inline">\(W_{p}\)</span>可以作为概率空间<span class="math inline">\(\sum_{n}\)</span>上的距离函数。</p><h2 id="ground-cost">Ground Cost</h2><p>  证明了<span class="math inline">\(W_{p}\)</span>可以作为概率空间<span class="math inline">\(\sum_{n}\)</span>上的距离函数。接下来我们就可以考虑如何定义度量矩阵<span class="math inline">\(\boldsymbol{D}\)</span>，从而生成成本矩阵<span class="math inline">\(\boldsymbol{C}\)</span>, 得到成本矩阵<span class="math inline">\(\boldsymbol{C}\)</span>后，我们便可以来计算分布<span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta}\)</span>之间的 Wasserstein距离。当我们在欧式空间中考虑最优传输问题时，一种常用的生成成本矩阵<span class="math inline">\(\boldsymbol{C}\)</span>的方法是 <strong>GroundCost</strong>。<br>  Ground Cost 使用原始分布与目标分布的取值之差的 <span class="math inline">\(L_2\)</span> 范数来定义度量矩阵 <span class="math inline">\(\boldsymbol{D}\)</span>，容易验证矩阵 <span class="math inline">\(\boldsymbol{D}\)</span>满足度量矩阵的性质，然后使用度量矩阵的平方生成成本矩阵 <span class="math inline">\(\boldsymbol{C}\)</span>，即 <span class="math inline">\(\boldsymbol{C}=\boldsymbol{D}^2\)</span>，故Ground Cost 是欧式空间中的一种 2-Wasserstein 距离。<br>  仍然考虑离散分布 <span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta}\)</span>，设:</p><p><span class="math display">\[\boldsymbol{\alpha} = \begin{bmatrix}    \alpha_1 \\    \alpha_2 \\    \vdots \\    \alpha_n \\\end{bmatrix}, \quad \boldsymbol{\beta} = \begin{bmatrix}    \beta_1 \\    \beta_2 \\    \vdots \\    \beta_n \\\end{bmatrix}\]</span></p><p>则分布 <span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta}\)</span>的分布列可以写成：</p><style>.center {  width: auto;  display: table;  margin-left: auto;  margin-right: auto;}</style><p align="center"><font face="黑体" size="2."></font></p><div class="center"><table><thead><tr class="header"><th style="text-align: center;"></th><th style="text-align: center;">1</th><th style="text-align: center;">2</th><th style="text-align: center;"><span class="math inline">\(\cdots\)</span></th><th style="text-align: center;">n</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">p</td><td style="text-align: center;"><span class="math inline">\(\alpha_1\)</span></td><td style="text-align: center;"><span class="math inline">\(\alpha_2\)</span></td><td style="text-align: center;"><span class="math inline">\(\cdots\)</span></td><td style="text-align: center;"><span class="math inline">\(\alpha_n\)</span></td></tr></tbody></table><table><thead><tr class="header"><th style="text-align: center;"></th><th style="text-align: center;">1</th><th style="text-align: center;">2</th><th style="text-align: center;"><span class="math inline">\(\cdots\)</span></th><th style="text-align: center;">n</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">p</td><td style="text-align: center;"><span class="math inline">\(\beta_1\)</span></td><td style="text-align: center;"><span class="math inline">\(\beta_2\)</span></td><td style="text-align: center;"><span class="math inline">\(\cdots\)</span></td><td style="text-align: center;"><span class="math inline">\(\beta_n\)</span></td></tr></tbody></table></div><p>定义度量矩阵<span class="math inline">\(\boldsymbol{D}\)</span>为离散分布<span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta}\)</span>取值之差的<span class="math inline">\(L_2\)</span>范数：</p><p><span class="math display">\[\boldsymbol{D} =[\boldsymbol{D}_{ij}]_{n \times n}=[ ||i-j||_{2} ]_{n \times n} =\begin{bmatrix}    0 &amp; \cdots &amp; ||1-n||_{2}  \\    \vdots &amp; &amp; \vdots \\    ||n-1||_{2} &amp; \cdots &amp; 0 \\\end{bmatrix}\]</span></p><p>定义成本矩阵<span class="math inline">\(\boldsymbol{C}\)</span>为度量矩阵<span class="math inline">\(\boldsymbol{D}\)</span>的平方：</p><p><span class="math display">\[ \boldsymbol{C} = \boldsymbol{D}^{2} =[\boldsymbol{D}_{ij}^{2}]_{n \times n} = \begin{bmatrix}    0 &amp; \cdots &amp; ||1-n||_{2}^{2}  \\    \vdots &amp; &amp; \vdots \\    ||n-1||_{2}^{2} &amp; \cdots &amp; 0 \\\end{bmatrix}\]</span></p><p>概率分布 <span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta}\)</span> 之间的 Wasserstein 距离可以被定义为：</p><p><span class="math display">\[W_{2}(\boldsymbol{\alpha},\boldsymbol{\beta}) =L_{\boldsymbol{C}}(\boldsymbol{\alpha,\boldsymbol{\beta}})^{1/2} =\left( \min_{\boldsymbol{P} \in\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta})} \left&lt;\boldsymbol{P},\boldsymbol{C} \right&gt;\right)^{\frac{1}{2}}\]</span></p><h2 id="example">Example</h2><p>  我们用一个实际的例子来展示如何基于 Ground Cost 来计算离散分布之间的Wasserstein 距离。我们将使用Python中专门用于OT问题的库<strong>POT</strong> 来完成这个实例的计算。首先导入所需要的包：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> ot<br></code></pre></td></tr></tbody></table></figure><p>  假设离散分布 <span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta} \in \sum_{5}:=\{ \boldsymbol{x} \in\mathbb{R}^{5}_{+}: \boldsymbol{x^{T}}\mathbf{1}_{5}=1 \}\)</span>：</p><p><span class="math display">\[\boldsymbol{\alpha} = \begin{bmatrix}    0.1 \\    0.3 \\    0.2 \\      0.1 \\    0.3 \\\end{bmatrix}, \quad \boldsymbol{\beta} = \begin{bmatrix}    0.1 \\    0.3 \\    0.2 \\    0.3 \\    0.1 \\\end{bmatrix}\]</span></p><p>  画出离散分布 <span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta}\)</span> 的概率分布直方图：</p><div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> pbar(x,y,color,title):</span><span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    plt.bar(x,y, width<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span>color,alpha<span class="op">=</span><span class="fl">0.7</span>)</span><span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    plt.title(title)</span><span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'Value'</span>)</span><span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'Probability'</span>)</span><span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 1*2 plot</span></span><span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> multiplot(x,y_1,y_2):</span><span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">4</span>))</span><span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">1</span>)</span><span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    pbar(x,y_1, color<span class="op">=</span><span class="st">"blue"</span>, title<span class="op">=</span><span class="st">'alpha distribution'</span>)</span><span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>)</span><span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    pbar(x,y_2, color<span class="op">=</span><span class="st">"green"</span>, title<span class="op">=</span><span class="st">'beta distribution'</span>)</span><span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    plt.show()</span><span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span><span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># values of probalility distribution</span></span><span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.array([<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>])</span><span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># probability vector</span></span><span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> np.array([<span class="fl">0.1</span>,<span class="fl">0.3</span>,<span class="fl">0.2</span>,<span class="fl">0.1</span>,<span class="fl">0.3</span>])</span><span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> np.array([<span class="fl">0.1</span>,<span class="fl">0.3</span>,<span class="fl">0.2</span>,<span class="fl">0.3</span>,<span class="fl">0.1</span>])</span><span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># draw distribution barplot</span></span><span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>multiplot(x,a,b)</span></code></pre></div><p>得到的图像为：</p><center><img src="https://s2.loli.net/2024/03/04/amv6pIF52GMkEds.png" width="60%" height="60%"><div data-align="center">Image1: 原始分布与目标分布的直方图</div></center><p><br></p><p>  基于 Ground Cost 我们可以定义成本矩阵 <span class="math inline">\(\boldsymbol{C}\)</span>，相应的代码为：</p><div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ground cost</span></span><span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ground_cost(n):</span><span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    C <span class="op">=</span> np.zeros((n, n))</span><span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span><span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(n):</span><span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> i<span class="op">+</span><span class="dv">1</span></span><span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> j<span class="op">+</span><span class="dv">1</span></span><span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>            C[i][j] <span class="op">=</span> (x<span class="op">-</span>y)<span class="op">**</span><span class="dv">2</span></span><span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> C</span><span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span><span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> ground_cost(<span class="dv">5</span>)</span></code></pre></div><p>  得到的成本矩阵<span class="math inline">\(\boldsymbol{C}\)</span>为：</p><p><span class="math display">\[\boldsymbol{C} = \begin{bmatrix}    0 &amp; 1 &amp; 4 &amp; 9 &amp; 16 \\    1 &amp; 0 &amp; 1 &amp; 4 &amp; 9 \\    4 &amp; 1 &amp; 0 &amp; 1 &amp; 4 \\    9 &amp; 4 &amp; 1 &amp; 0 &amp; 1 \\    16 &amp; 9 &amp; 4 &amp; 1 &amp; 0 \\\end{bmatrix}\]</span></p><p>则概率分布 <span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta}\)</span> 之间的 Wasserstein 距离可以被定义为：</p><p><span class="math display">\[W_{2}(\boldsymbol{\alpha},\boldsymbol{\beta}) =L_{\boldsymbol{C}}(\boldsymbol{\alpha,\boldsymbol{\beta}})^{1/2} =\left( \min_{\boldsymbol{P} \in\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta})} \left&lt;\boldsymbol{P},\boldsymbol{C} \right&gt;\right)^{\frac{1}{2}}\]</span></p><p>  我们可以使用POT库的API来求解离散分布 <span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta}\)</span>之间的最优传输矩阵 <span class="math inline">\(P^{*}\)</span> 以及Wasserstein 距离 <span class="math inline">\(W_{2}(\boldsymbol{\alpha},\boldsymbol{\beta})\)</span>，其代码如下：</p><div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># optimal transport matrix</span></span><span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> ot.emd(a, b, C)</span><span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># wasserstein distence</span></span><span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>wasserstein_distence <span class="op">=</span> ot.emd2(a, b, C)</span><span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span><span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(P.<span class="bu">round</span>(<span class="dv">4</span>))</span><span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">round</span>(np.sqrt(wasserstein_distence),<span class="dv">4</span>))</span></code></pre></div><p>求解结果如下：</p><p><span class="math display">\[\boldsymbol{P}^{*} = \begin{bmatrix}    0.1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\    0 &amp; 0.3 &amp; 0 &amp; 0 &amp; 0 \\    0 &amp; 0 &amp; 0.2 &amp; 0 &amp; 0 \\    0 &amp; 0 &amp; 0 &amp; 0.1 &amp; 0 \\    0 &amp; 0 &amp; 0 &amp; 0.2 &amp; 0.1 \\\end{bmatrix},\quadW_{2}(\boldsymbol{\alpha},\boldsymbol{\beta})=0.4472\]</span></p><h2 id="reference">Reference</h2><ul><li><strong>[1] Book: Peyré G, Cuturi M. Computational optimaltransport[J]. Center for Research in Economics and Statistics WorkingPapers, 2017 (2017-86).</strong></li></ul>]]></content>
    
    
    <summary type="html">本节主要介绍Wasserstein Distance的定义以及证明其能够作为概率空间中距离的度量函数。</summary>
    
    
    
    <category term="最优传输理论" scheme="http://example.com/categories/%E6%9C%80%E4%BC%98%E4%BC%A0%E8%BE%93%E7%90%86%E8%AE%BA/"/>
    
    
  </entry>
  
  <entry>
    <title>Optimal Transport-1.Monge-Kantorovich Problem</title>
    <link href="http://example.com/2024/02/07/Optimal%20Transport-1-Monge-Kantorovich%20Problem/"/>
    <id>http://example.com/2024/02/07/Optimal%20Transport-1-Monge-Kantorovich%20Problem/</id>
    <published>2024-02-07T06:32:06.000Z</published>
    <updated>2024-03-04T15:29:50.657Z</updated>
    
    <content type="html"><![CDATA[<h1 id="monge-kantorovich-problem">Monge-Kantorovich Problem</h1><p>  最优传输理论(Optimal TransportTheroy)是应用数学的一个分支，主要研究的是概率分布之间的最优转移方式以及相关的距离度量。它的核心思想是通过最小化两个概率分布之间的转移成本，来定义这两个分布之间的距离。<br>  最优传输问题的历史可以追溯到18世纪。著名法国数学家 Monge在1781年提出了最优传输问题的早期形式，即Monge-Problem。他关注的是如何以最小的成本将一个土堆移动到另一个位置，这被看作是最优传输理论的起源。时至今日，很多介绍最优传输理论的科普文章依然喜欢使用“土堆移动、推土机”等例子来论述最优传输理论。Monge的工作奠定了这一理论的基础。另外一个对最优传输理论的发展具有关键作用的是前苏联数学家Kantorovich。Kantorovich在20世纪40年代从实际经济资源分配问题中重新导出了最优传输问题，他基于测度论对最优传输问题进行了严格的定义，并推导出了一系列重要的理论成果。Kantorovich的工作对最优传输理论的数学形式和理论基础的建立起到了关键作用。由于在线性规划和资源分配方面的重要贡献，Kantorovich于1975年获得诺贝尔经济学奖。此外还有多位数学家对最优传输理论的发展做出了重要贡献，Wassertein、Villani、Berg等数学家从理论、应用、数值计算等方面丰富了最优传输理论。<br>  最优传输理论的应用涵盖多个领域：</p><ul><li><strong>1.图像处理和计算机视觉:</strong>在图像处理中，最优传输理论被用来衡量图像之间的相似性，从而进行图像匹配、图像检索等任务。在计算机视觉领域，它被用于解决图像生成和变换的问题。<br></li><li><strong>2.机器学习:</strong>在机器学习中，最优传输理论被应用于生成模型、领域自适应等问题，以提高模型的泛化性能。<br></li><li><strong>3.经济学和金融学:</strong>在经济学中，最优传输理论被用于研究资源的分配和经济结构的演变。在金融学中，它可以用来量化不同资产之间的差异和联系。<br></li><li><strong>4.统计学和信息论:</strong>最优传输理论在统计学和信息论中也有广泛的应用，尤其是在测度空间中定义概率分布之间的距离。</li></ul><p>  总的来说，最优传输理论在多个领域都展现了广泛的应用，其在处理概率分布之间的关系和相似性的能力为研究者提供了有力的工具。</p><h2 id="monge-problem">Monge Problem</h2><p>  GaspardMonge(1746-1818)是法国著名的数学家和物理学家，他在数学、物理学和工程学领域做出了突出的贡献，被认为是现代微分几何的奠基人之一，对现代科学和数学的发展产生了深远的影响。<br>  在18世纪，第一次工业革命正在欧洲如火如荼地进行着，经济与军事活动催生了大量的数学、物理理论的诞生。最优传输理论的萌芽便来源于军事活动中所遇到的实际问题。彼时，英国与法国正在争夺欧洲霸权，为此进行了一系列的战争。在过去的战争中，防御工事的修建至关重要，修建防御工事需要将大量的土堆从某地转移到阵地，并堆砌成确定的形状。修建工事需要消耗大量的人力物力，这便催生了一个实际问题，<strong>如何将某一形状的土堆移动到另一个地方并堆成另一种形状，使得这整个过程中所消耗的资源最少。</strong><br>  Monge此时正在法国军队中担任工程研究人员，负责为军队提供有关土木工程和防御工事的重要建议。Monge敏锐地注意到了这个问题，并对其展开了研究。1781年，Monge发表了著作——<strong>Mémoiresur la théorie de déblais et deremblais(关于挖掘和填充的备忘录).</strong>Monge在著作中对该问题进行了论述，并提出了一种解决方案。<br>  Monge首先对所要研究的问题做出了一系列假设：</p><ul><li><strong>所要运输的土堆都是由质量相等的不可再分的分子所构成的，且土堆中分子的分布是均匀的。</strong></li><li><strong>每个分子的运输价格都是相等的，与该分子的重量和它所被传输的距离成正比。</strong></li><li><strong>总的运输价格是所运输的每个分子的质量乘以分子所传输的距离的总和。</strong></li></ul><p>基于以上的假设，Monge提出了"前向运输法"，并给出了从一维到三维的实例。</p><h3 id="点到点的最优传输">点到点的最优传输</h3><p>  我们首先来考虑最简单的两点运输情形，如下图1所示：</p><center><img src="https://s2.loli.net/2024/02/09/HvhSCsApP1a5Q38.jpg" width="40%" height="60%"><div data-align="center">Image1: 两点运输</div></center><p>A区域中有两个分子需要运输到B区域的指定位置，总共有两种运输方案，图1中分别用绿色与橙色的箭头表示。由三角形两边之和大于第三边可知，橙色方案的运输距离要大于绿色方案，又因为分子的质量是相同的，所有我们可以很容易得到，绿色方案的运输效率要高于橙色方案。<br>  再来考虑多点的情形，如下图2所示</p><center><img src="https://s2.loli.net/2024/02/09/vkGiCydSJEOT7BF.jpg" width="60%" height="60%"><div data-align="center">Image2: 多点运输</div></center><p>我们需要将点1、2、3运输到点4、5、6处，图2中给出了A、B、C三种运输方案。首先来比较方案A、B，方案A，B的不同在于点1、3到点5、6的运输路线，基于上文两点运输的思考，我们可以得知B方案的运输效率要高于方案A，同理，C方案的运输效率要高于B方案。<br>  基于以上的思考，Monge认为点到点的运输要遵循"前向法"，即运输路线不存在交叉。Monge同时将这种思想拓展到更高维的情形。</p><h3 id="平面到平面的最优运输">平面到平面的最优运输</h3><p>  现在考虑将某一平面区域运输到另一个平面区域，基于“前向法”，Monge给出了如下图3所示的运输方案：</p><center><img src="https://s2.loli.net/2024/02/09/2JhmZywrsRd9zpF.jpg" width="60%" height="60%"><div data-align="center">Image3: 平面运输</div></center><p>现在需要将平面区域Ⅰ中的分子运输到平面区域Ⅱ，<strong>Monge认为区域Ⅰ中的A、C两点要运输到区域Ⅱ的B、D两点，直线AB、CD相较于O点，由O点出发可以确定两条边界射线<span class="math inline">\(l_3,l_4\)</span>。在锥型区域中，可以用很多条射线将运输平面进行分割。例如夹角"非常小"的射线<span class="math inline">\(l_1,l_2\)</span>与区域Ⅰ相交于<span class="math inline">\(M_1、M_3、H_1、H_3\)</span>，与区域Ⅱ相交于<span class="math inline">\(M_4、M_6、H_4、H_6\)</span>，分割出区域<span class="math inline">\(M_1M_3H_1H_3\)</span>和<span class="math inline">\(M_4M_6H_4H_6\)</span>，根据"前向法"，区域Ⅰ中<span class="math inline">\(M_1M_3H_1H_3\)</span>的分子要运输到区域Ⅱ中<span class="math inline">\(M_4M_6H_4H_6\)</span>中，同时运输时也要遵循“前向法”，即橙色区域要运输到橙色区域，绿色区域要运送到绿色区域。由无数个分割出的小区域依据“前向法”进行运输，这种运输方式的所消耗的资源最少。</strong><br>  Monge借助微分几何的知识证明了这些从O点出发的运输射线都垂直于某一曲线R，并通过解析几何的知识求出了曲线R的解析式，以及最优运输的映射方程<span class="math inline">\(y =T(x)\)</span>。基于“前向法”，Monge同时也对三维运输的映射方程进行了求解，具体的求解过程这里不作详细的介绍，有兴趣的读者可自行查阅相关资料。</p><center><img src="https://s2.loli.net/2024/02/11/FG4H59woPsLAJMy.png" width="60%" height="60%"><div data-align="center">Image4: Monge著作中的作图</div></center><h3 id="待解决的问题">待解决的问题</h3><p>  虽然Monge借助“前向法”给出了最优传输问题的一种解决方法，但他同时也在著作中承认，实际的运输问题是非常复杂的，他的解法只是一种非常理想化的方法，并且这种方法也存在着缺陷，主要有以下几个方面:</p><ul><li><strong>1.被运输的每个分子的质量可能是不同的。</strong></li><li><strong>2.每个区域的密度有可能不同，即面积相同的区域所含有的分子数量可能不相等。</strong></li><li><strong>3.当目标区域是非凸区域时，“前向法”在中间区域的映射方程是无解的。</strong></li></ul><p>  在Monge-Problem被提出后，后世的学者也不断地对这一问题进行研究，但并没有取得突破性的进展。</p><h2 id="kantorovich-relaxation">Kantorovich Relaxation</h2><p>  19世纪-20世纪初，集合论、测度论、概率论等数学分支得到了充分的发展壮大，Monge-Problem的解决也迎来了转机。前苏联数学家kantorovich在解决Monge-Problem中起到了不可忽视的作用。然而，有趣的是，Kantorovich最初在解决这个问题时，并没有意识到自己所面对的问题与Monge-Problem之间的关联，直到将近10年后，Kantorovich才在一篇新的论文中阐述了自己的方法可以解决Monge-Problem。<br>  1938年，圣彼得堡胶合板托拉斯的研究人员找到圣彼得堡大学数学系，想要圣彼得堡大学的数学家们帮助解决在生产中所遇到的一个实际问题。<strong>托拉斯有八台不同类型的机器，每台机器能够生产五种不同型号的胶合板。每种类型的机器生产这五种胶合板的效率不同，各种不同的胶合板的产量是由现有的需求量决定的。为了在最短的时间内生产这些胶合板，应该如何给每种类型的机器分配生产任务？</strong><br>  与高深的数学理论相比，这看起来是一个非常简单的数学问题，仅仅只需要求解方程组而已，然而要求出这个问题的最优解却并不是一个简单的工作，因为当时有关线性规划的理论还没有被提出。彼时，26岁的Kantorovich正在圣彼得堡大学担任教职，这个问题最终交由他来研究。1939年，Kantorovich发表了论文《数学方法中的问题理论》，以经济资源分配与运输问题为背景，正式提出了"最优传输问题"，这一工作被认为是线性规划理论的先驱性工作。1947年，kantorovich发表论文《关于数学规划问题的一般理论》，正式提出了线性规划理论及其一般解法。</p><h3 id="最优传输问题">最优传输问题</h3><p>  我们来考虑一个实际问题，有 n 个生产木材的工厂，生产的木材需要供应给m 个城市，每个工厂每个月的产能是固定的，记为 <span class="math inline">\(\boldsymbol{a}, \boldsymbol{a} \in\mathbb{R}^{n}\)</span>；每个城市每个月木材的需求量也是固定的，记为<span class="math inline">\(\boldsymbol{b}, \boldsymbol{b} \in\mathbb{R}^{m}\)</span>。记 <span class="math inline">\(\boldsymbol{P} =[p_{ij}]_{n \times m} \in \mathbb{R}^{n \times m}_{+}\)</span>表示运输方案，其中 <span class="math inline">\(p_{ij}\)</span> 表示第 i个工厂运输到第 j 个城市的木材量。记 <span class="math inline">\(\boldsymbol{C} = [c_{ij}]_{n \times m} \in\mathbb{R}^{n \times m}_{+}\)</span> 表示运输成本，其中 <span class="math inline">\(c_{ij}\)</span>表示将单位木材从第i个工厂运输到第j个城市的成本。记：</p><p><span class="math display">\[\boldsymbol{U}(\boldsymbol{a},\boldsymbol{b}) =\{ \boldsymbol{P} \in \mathbb{R}^{n \times m}_{+}: \boldsymbol{P}\mathbf{1}_{m}=\boldsymbol{a} \quad and \quad \boldsymbol{P^{T}}\mathbf{1}_{n}=\boldsymbol{b} \}\]</span></p><p>  则最优传输问题可以被定义为：</p><p><span class="math display">\[L_{\boldsymbol{C}}(\boldsymbol{a},\boldsymbol{b}) \quad \overset{\text{def.}}{=} \quad\min_{\boldsymbol{P} \in \boldsymbol{U}(\boldsymbol{a},\boldsymbol{b})}\left&lt; \boldsymbol{C},\boldsymbol{P}\right&gt;_{F}=\sum_{i,j}p_{ij}c_{ij}\]</span></p><p><span class="math inline">\(L_{\boldsymbol{C}}(\boldsymbol{a},\boldsymbol{b})\)</span>即为最优运输方案。由于目标函数是线性函数，且<span class="math inline">\(\boldsymbol{U}(\boldsymbol{a},\boldsymbol{b})\)</span>为 <span class="math inline">\(n+m\)</span>个等式定义的凸多胞体，故最优传输问题是一个典型的线性规划问题。</p><h3 id="概率视角">概率视角</h3><p>  Kantorovich使用测度论定义了被运输的对象。在Monge的理论中，运输是确定性的，即每个分子都不可再分，且会被整体被运输到另一个位置，<strong>Kantorovich放松了这种确定性条件，他认为每个分子是可以进行切分的，一个分子所包含的质量可以被运输到不同的位置，这种运输是具有概率性的</strong>。</p><h4 id="离散概率分布运输问题">离散概率分布运输问题</h4><p>  设 <span class="math inline">\(X, Y\)</span>是两个服从多项分布的随机变量，取值于 <span class="math inline">\(\{1,2,\dotsb, d \}\)</span>。<span class="math inline">\(X,Y\)</span>的概率分布 <span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta}\)</span>取自概率单纯形 <span class="math inline">\(\sum_{d}:=\{ x \in\mathbb{R}^{d}_{+}: \boldsymbol{x^{T}}\mathbf{1}_{d}=1\}\)</span>。运输矩阵 <span class="math inline">\(\boldsymbol{P} \in\mathbb{R}^{d \times d}_{+}\)</span>。记:</p><p><span class="math display">\[\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta}):= \{ \boldsymbol{P} \in \mathbb{R}^{d \times d}_{+} :\boldsymbol{P}\mathbf{1}_{d}=\boldsymbol{\alpha} \quad and \quad\boldsymbol{P^{T}}\mathbf{1}_d=\boldsymbol{\beta} \}\]</span></p><p>从概率视角看，集合 <span class="math inline">\(\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta})\)</span>包含了随机变量<span class="math inline">\(X,Y\)</span>所有可能的联合分布<span class="math inline">\(\pi(X,Y)\)</span>，即矩阵<span class="math inline">\(\boldsymbol{P}=[p_{ij}]_{d \times d} =[\pi(x=i,y=j)]\)</span>。设成本矩阵为<span class="math inline">\(\boldsymbol{C} \in \mathbb{R}^{d \timesd}_{+}\)</span>。则多项分布 <span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta}\)</span>之间的最优传输问题可以被定义为：</p><p><span class="math display">\[L_{\boldsymbol{C}}(\boldsymbol{\alpha},\boldsymbol{\beta}):= \min_{\boldsymbol{P} \in\boldsymbol{U}(\boldsymbol{\alpha},\boldsymbol{\beta})} \left&lt;\boldsymbol{P},\boldsymbol{C} \right&gt; = \min_{(X,Y)} \{\mathbb{E}_{(X,Y)}(c(X,Y)): X \sim \boldsymbol{\alpha}, Y \sim\boldsymbol{\beta} \}\]</span></p><p><span class="math inline">\((X,Y)\)</span>表示取值于<span class="math inline">\(\mathcal{X} \times\mathcal{Y}\)</span>的联合分布。</p><h4 id="连续概率分布运输问题">连续概率分布运输问题</h4><p>  连续分布的运输问题与离散问题相似，不同点在于随机变量 <span class="math inline">\(X,Y\)</span> 服从的是连续分布 <span class="math inline">\(\boldsymbol{\alpha},\boldsymbol{\beta}\)</span>，最优运输同样可以定义为：</p><p><span class="math display">\[L_{\boldsymbol{C}}(\boldsymbol{\alpha},\boldsymbol{\beta}):= \min_{(X,Y)} \{ \mathbb{E}_{(X,Y)}(c(X,Y)): X \sim\boldsymbol{\alpha}, Y \sim \boldsymbol{\beta} \}\]</span></p><h3 id="局部前向法">局部前向法</h3><p>  通过求解离散分布和连续分布的最优传输问题，我们可以发现最优传输方案与Monge的"前向法"存在联系，下图5是最优传输的结果实例：</p><center><img src="https://s2.loli.net/2024/02/12/BZ8uThM5CcbQzm4.png" width="60%" height="60%"><div data-align="center">Image5: 概率分布最优传输结果实例</div></center><p>从右图离散分布的最优运输结果来看，最优运输是满足“局部前向法"的，即在满足边界分布的条件下，遵循前向运输法则。</p><h2 id="references">References</h2><ul><li><strong>[1] Book: Peyré G, Cuturi M. Computational optimaltransport[J]. Center for Research in Economics and Statistics WorkingPapers, 2017 (2017-86).</strong><br></li><li><strong>[2] Lecture: 李向东. 最优传输理论及其应用.BIMSA</strong><br></li><li><strong>[3] Paper: Cuturi M. Sinkhorn distances: Lightspeedcomputation of optimal transport[J]. Advances in neural informationprocessing systems, 2013, 26.</strong></li></ul>]]></content>
    
    
    <summary type="html">本节主要介绍Monge与Kantorovich对最优传输问题的描述，以及最优传输问题作用在概率分布变换上的形式。</summary>
    
    
    
    <category term="最优传输理论" scheme="http://example.com/categories/%E6%9C%80%E4%BC%98%E4%BC%A0%E8%BE%93%E7%90%86%E8%AE%BA/"/>
    
    
  </entry>
  
  <entry>
    <title>机器学习-7-线性判别分析</title>
    <link href="http://example.com/2024/02/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-7-%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/"/>
    <id>http://example.com/2024/02/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-7-%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/</id>
    <published>2024-02-02T16:57:37.000Z</published>
    <updated>2024-02-02T17:48:50.467Z</updated>
    
    <content type="html"><![CDATA[<h1 id="线性判别分析">线性判别分析</h1><p>  线性判别分析(Linear DiscriminantAnalysis，简称LDA)是一种在机器学习和统计学中常用于分类和降维的方法。它的主要目标是在特征空间中找到一个合适的投影方向，将高维数据点投影到低维空间中，使得这些数据点易于分类。LDA在特征选择、降维和模式识别等领域都有广泛的应用。  线性判别分析最早由著名统计学家<span class="math inline">\(RA.Fisher\)</span>于1936年提出，他的工作被认为是<span class="math inline">\(LDA\)</span>的奠基。线性判别分析经过多个阶段的发展，从最初的二分类问题到多分类问题，以及对不同数据类型的适应，一直在模式识别与机器学习领域中发挥着重要作用。同时，他也启发了其他降维和分类方法的发展。</p><h2 id="基本思想">基本思想</h2><p>  线性判别分析的基本思想为：在<span class="math inline">\(n\)</span>维特征空间中，找到一个最佳的投影方向，使得在将训练集中的数据点投影到该方向上后，类别间的散度较大，类别内的散点较小，这样我们可以在该投影方向上找到一个分界点，能够对训练数据集完全正确分类。对于新的实例，将其投影到最佳投影方向上，利用分界点对其进行分类。为了寻找到最佳的投影方向，需要设置与类间散度和类内散度有关的损失函数，使得在最小化损失函数的过程中，类间散度增大而类内散度减小，这样最终得到的投影方向便是最佳投影方向。线性判别分析的基本思想可用下图1来描述：</p><center><img src="https://s2.loli.net/2023/10/09/g2xVFLyehT3D8NS.jpg" width="60%" height="60%"><div data-align="center">Image1: 线性判别分析的基本思想</div></center><h2 id="模型">模型</h2><p>  线性判别分析可以应用于二分类问题或者多分类问题，这里我们主要讨论呢二分类问题下的线性判别模型。</p><p><strong>输入</strong></p><ul><li><strong>输入空间:</strong> <span class="math inline">\(\mathcal{X} =\mathbb{R}^{n}\)</span><br></li><li><strong>输入实例:</strong> <span class="math inline">\(x =\begin{bmatrix}  x^{(1)},x^{(2)},\dots,x^{(n)} \\ \end{bmatrix} \in\mathcal{X}\)</span></li></ul><p><strong>输出</strong></p><ul><li><strong>输出空间:</strong> <span class="math inline">\(\mathcal{Y} =\{-1,+1\}\)</span><br></li><li><strong>输出实例:</strong> <span class="math inline">\(y \in\mathcal{Y}\)</span></li></ul><p>  其中输出空间<span class="math inline">\(\mathcal{Y}\)</span>只包含+1和-1的一个集合，+1与-1分别代表二分类问题中的正类<span class="math inline">\(C_1\)</span>与负类<span class="math inline">\(C_2\)</span>。输出实例<span class="math inline">\(y\)</span>代表输出实例<span class="math inline">\(x\)</span>的类别。</p><p><strong>数据集</strong><br>  感知机模型的训练数据集<span class="math inline">\(T_{train}\)</span>为：</p><p><span class="math display">\[T_{train}=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}\]</span></p><p>  其中，<span class="math inline">\(x_i \in\mathcal{X}=\mathbb{R}^{n},y_i \in\mathcal{Y}=\{+1,-1\},i=1,2,\dots,N\)</span>，<span class="math inline">\(N\)</span>表示训练数据的样本容量。</p><p><strong>模型</strong><br>  设最佳投影方向为<span class="math inline">\(\hat{w}\)</span>，且<span class="math inline">\(||\hat{w}||_{2}=1\)</span>，正类与负类的样本均值点分别为：</p><p><span class="math display">\[\bar{x}_{C_1}=\frac{1}{N_1}\sum_{x_i \inC_1}x_i,\space \bar{x}_{C_2}=\frac{1}{N_2}\sum_{x_i \inC_2}x_i\]</span></p><p>  其中，<span class="math inline">\(N_1\)</span>表示训练集中正类样本的样本容量，<span class="math inline">\(N_2\)</span>表示训练集中负类样本的样本容量。将<span class="math inline">\(\bar{x}_{C_1}\)</span>与<span class="math inline">\(\bar{x}_{C_2}\)</span>投影到最佳投影方向<span class="math inline">\(\hat{w}\)</span>后的投影距离分别为<span class="math inline">\(\hat{w}^{T}\bar{x}_{C_1}\)</span>和<span class="math inline">\(\hat{w}^{T}\bar{x}_{C_2}\)</span>，设置分界点<span class="math inline">\(threshold\)</span>为：</p><p><span class="math display">\[threshold =\frac{\hat{w}^{T}\bar{x}_{C_1}+\hat{w}^{T}\bar{x}_{C_2}}{2}\]</span></p><p>  对于新的实例点<span class="math inline">\(x\)</span>，同样将其投影到最佳投影方向<span class="math inline">\(\hat{w}\)</span>，则投影距离为<span class="math inline">\(\hat{w}^{T}x\)</span>，其类别的判断准则为：</p><p><span class="math display">\[\hat{y} = \left \{\begin{array}{rcl}+1, &amp; {\hat{w}^{T}x &gt; threshold}\\-1, &amp; {\hat{w}^{T}x \leq threshold}\\\end{array} \right.\]</span></p><p><strong>假设空间</strong><br>  模型的假设空间<span class="math inline">\(\mathcal{H}\)</span>实际上是特征空间中所有的投影方向：</p><p><span class="math display">\[\mathcal{H} = \{w \vert w \in\mathbb{R}^{n}\}\]</span></p><p>  此时模型的参数空间<span class="math inline">\(\Theta =\mathcal{H}\)</span>.</p><h2 id="策略">策略</h2><p>  前文提到，线性判别分析的最佳投影方向要满足使得训练集样本在投影后有类间散度大，而类内散度小的特点，我们可以据此来设定损失函数。<br>  设特征空间中任意投影方向为<span class="math inline">\(w\)</span>，且<span class="math inline">\(||w||_{2}=1\)</span>，则特征空间中的数据点<span class="math inline">\(x\)</span>在<span class="math inline">\(w\)</span>上的投影距离为<span class="math inline">\(w^{T}x\)</span>.将训练集中的数据点投影到该方向上，令<span class="math inline">\(z_i=w^{T}x_i\)</span>，则训练数据集中正类与负类在投影到<span class="math inline">\(w\)</span>后的平均投影距离分别为：</p><p><span class="math display">\[\bar{z}_1 =\frac{1}{N_1}\sum_{i=1}^{N_1}z_i=\frac{1}{N_1}\sum_{x_i \inC_1}w^{T}x_i\]</span></p><p><span class="math display">\[\bar{z}_2 =\frac{1}{N_2}\sum_{i=1}^{N_2}z_i=\frac{1}{N_2}\sum_{x_i \inC_2}w^{T}x_i\]</span></p><p>  <span class="math inline">\(\bar{z}_1\)</span>与<span class="math inline">\(\bar{z}_2\)</span>的差的绝对值表示投影后正类数据与负类数据的中心点之间的距离，我们可以据此来表示类间散度，这个距离越大，说明投影后两个数据整体分离地越远。为了求导的方便，我们用平方代替绝对值，这样我们可以定义类间散度：</p><p><span class="math display">\[S_{be}=(\bar{z}_1-\bar{z}_2)^2\]</span></p><p>  在考虑类间散度的同时，我们也希望投影后，同一个类别的数据尽量聚拢，即类内散度较小，我们可以用投影距离的组内方差来描述类内散度。投影后正类和负类数据点的投影距离的组内方差分别为：</p><p><span class="math display">\[S_1 =\frac{1}{N_1}\sum_{i=1}^{N_1}(z_i-\bar{z}_1)(z_i-\bar{z}_1)^T=\frac{1}{N_1}\sum_{x_i\in C_1}(w^T x_i-\frac{1}{N_1}\sum_{x_i \in C_1}w^{T}x_i)(w^Tx_i-\frac{1}{N_1}\sum_{x_i \in C_1}w^{T}x_i)^T\]</span></p><p><span class="math display">\[S_2 =\frac{1}{N_2}\sum_{i=1}^{N_2}(z_i-\bar{z}_2)(z_i-\bar{z}_2)^T=\frac{1}{N_2}\sum_{x_i\in C_2}(w^T x_i-\frac{1}{N_2}\sum_{x_i \in C_2}w^{T}x_i)(w^Tx_i-\frac{1}{N_2}\sum_{x_i \in C_2}w^{T}x_i)^T\]</span></p><p>  我们希望正类与负类样本在投影后的组内方差均较小，因此我们可以将类内散度定义为：</p><p><span class="math display">\[S_{in}=S_1+S_2\]</span></p><p>  根据判别分析的基本思想，我们希望投影后数据点有类间散度大，类内散度小的特点，因此我们可以将损失函数定义为：</p><p><span class="math display">\[L(w)=-\frac{(\bar{z}_1-\bar{z}_2)^2}{S_1+S_2}\]</span></p><p>  当我们最小化损失函数<span class="math inline">\(L(w)\)</span>时，可以在增大类间散度的同时，减小类内散度。损失函数的最小值点<span class="math inline">\(\hat{w}\)</span>便是我们要寻找的最佳投影方向。<br>  我们对类间散度与类内散度做一下化简，以简化损失函数，便于优化。</p><p><span class="math display">\[\begin{align*}    \bar{z}_1-\bar{z}_2 &amp;=  \frac{1}{N_1}\sum_{x_i \inC_1}w^{T}x_i-\frac{1}{N_2}\sum_{x_i \in C_2}w^{T}x_i  \\    &amp;= w^T \left(\frac{1}{N_1}\sum_{i=1}^{N_1}x_i-\frac{1}{N_2}\sum_{i=1}^{N_2}x_i\right) \\    &amp;= w^T(\bar{x}_{C_1}-\bar{x}_{C_2})\end{align*}\]</span></p><p><span class="math display">\[\begin{align*}    S_1 &amp;= \frac{1}{N_1}\sum_{x_i \in C_1}(w^Tx_i-\frac{1}{N_1}\sum_{x_i \in C_1}w^{T}x_i)(w^Tx_i-\frac{1}{N_1}\sum_{x_i \in C_1}w^{T}x_i)^T \\    &amp;=\frac{1}{N_1}\sum_{i=1}^{N_1}w^{T}(x_i-\bar{x}_{C_1})(x_i-\bar{x}_{C_1})^{T}w  \\    &amp;= w^{T} \left(\frac{1}{N_1}\sum_{i=1}^{N_1}(x_i-\bar{x}_{C_1})(x_i-\bar{x}_{C_1})^{T}\right)w\end{align*}\]</span></p><p>  记<span class="math inline">\(S_{C_1}=\frac{1}{N_1}\sum_{i=1}^{N_1}(x_i-\bar{x}_{C_1})(x_i-\bar{x}_{C_1})^{T}\)</span>，表示投影前正类的组内方差，则投影后正类的组内方差为：</p><p><span class="math display">\[S_1=w^{T}S_{C_1}w\]</span></p><p>  同理可得：</p><p><span class="math display">\[S_2=w^{T}S_{C_2}w\]</span></p><p><span class="math display">\[S_{C_2}=\frac{1}{N_1}\sum_{i=1}^{N_1}(x_i-\bar{x}_{C_1})(x_i-\bar{x}_{C_1})^{T}\]</span></p><p>  则类内散度可以化简为：</p><p><span class="math display">\[S_1+S_2=w^{T}S_{C_1}w+w^{T}S_{C_2}w=w^{T}(S_{C_1}+S_{C_2})w\]</span></p><p><strong>损失函数</strong>   化简后，最终得到的损失函数为：</p><p><span class="math display">\[\begin{align*}    L(w)&amp;=-\frac{w^{T}(\bar{x}_{C_1}-\bar{x}_{C_2})(\bar{x}_{C_1}-\bar{x}_{C_2})^{T}w}{w^{T}(S_{C_1}+S_{C_2})w}\\    &amp;= -\frac{w^{T}Aw}{w^{T}Bw}  \\\end{align*}\]</span></p><p>  其中，<span class="math inline">\(A=(\bar{x}_{C_1}-\bar{x}_{C_2})(\bar{x}_{C_1}-\bar{x}_{C_2})^{T},B=S_{C_1}+S_{C_2}\)</span>.</p><h2 id="算法">算法</h2><p>  我们需要解决的优化问题为：</p><p><span class="math display">\[\min_{w}L(w)=-\frac{w^{T}Aw}{w^{T}Bw}\]</span></p><p>  对<span class="math inline">\(w\)</span>求一阶偏导：并令其为零：</p><p><span class="math display">\[\begin{align*}    \frac{\partial L(w)}{\partial w} &amp;= -\frac{\partial(w^{T}Aw)(w^{T}Bw)^{-1}}{\partial w} \\    &amp;= (2Aw)(w^{T}Bw)^{-1}-(w^{T}Aw)(w^{T}Bw)^{-2}(2Bw)=0  \\\end{align*}\]</span></p><p><span class="math display">\[\RightarrowAw(w^{T}Bw)-(w^{T}Aw)(Bw)=0\]</span></p><p><span class="math display">\[\Rightarrow(w^{T}Aw)Bw=Aw(w^{T}Bw)\]</span></p><p><span class="math display">\[\Rightarrow w=\frac{w^{T}Bw}{w^{T}Aw}B^{-1}Aw\]</span></p><p>  由于我们只需要求得最佳投影方向，而不需要关系其长度，因此有：</p><p><span class="math display">\[w \varpropto B^{-1}Aw\]</span></p><p>  表示<span class="math inline">\(w\)</span>的方向与<span class="math inline">\(B^{-1}Aw\)</span>一致。由于<span class="math inline">\(Aw=(\bar{x}_{C_1}-\bar{x}_{C_2})(\bar{x}_{C_1}-\bar{x}_{C_2})^{T}w\)</span>，而<span class="math inline">\((\bar{x}_{C_1}-\bar{x}_{C_2})^{T}w \in\mathbb{R}\)</span>为标量，故有：</p><p><span class="math display">\[w \varproptoB^{-1}(\bar{x}_{C_1}-\bar{x}_{C_2})\]</span></p><p>  <span class="math inline">\(\becauseB=S_{C_1}+S_{C_2}\)</span>，因此我们求得的最佳投影方向为：</p><p><span class="math display">\[w \varpropto(S_{C_1}+S_{C_2})^{-1}(\bar{x}_{C_1}-\bar{x}_{C_2})\]</span></p><p><span class="math display">\[\hat{w} =\frac{(S_{C_1}+S_{C_2})^{-1}(\bar{x}_{C_1}-\bar{x}_{C_2})}{||(S_{C_1}+S_{C_2})^{-1}(\bar{x}_{C_1}-\bar{x}_{C_2})||_2}\]</span></p><h2 id="线性判别分析实例及python实现">线性判别分析实例及Python实现</h2><p>  首先生成训练数据。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Data Generation</span><br>np.random.seed(<span class="hljs-number">520</span>)<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">data_generation</span>(<span class="hljs-params">n,basepoint,ylabel</span>):<br>    x1 = np.random.random((n,))+np.array([basepoint[<span class="hljs-number">0</span>]]*n)<br>    x2 = np.random.random((n,))+np.array([basepoint[<span class="hljs-number">1</span>]]*n)<br>    y = np.array([ylabel]*n)<br>    data = pd.DataFrame({<span class="hljs-string">"x1"</span>:x1,<span class="hljs-string">"x2"</span>:x2,<span class="hljs-string">"y"</span>:y})<br>    <span class="hljs-keyword">return</span> data<br><br><span class="hljs-comment"># positive data</span><br>positivedata = data_generation(n=<span class="hljs-number">30</span>,basepoint=(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>),ylabel=<span class="hljs-number">1</span>)<br><span class="hljs-comment"># negative data</span><br>negativedata = data_generation(n=<span class="hljs-number">20</span>,basepoint=(<span class="hljs-number">2</span>,<span class="hljs-number">1</span>),ylabel=<span class="hljs-number">0</span>)<br><span class="hljs-comment"># train data</span><br>train_data = pd.concat([positivedata,negativedata])<br>train_data = shuffle(train_data)<br>train_data.index = <span class="hljs-built_in">range</span>(train_data.shape[<span class="hljs-number">0</span>])<br>train_data.head(<span class="hljs-number">5</span>)<br><br><span class="hljs-comment"># train data scatter plot</span><br>plt.scatter(x=positivedata[<span class="hljs-string">"x1"</span>],y=positivedata[<span class="hljs-string">"x2"</span>],marker=<span class="hljs-string">"o"</span>,c=<span class="hljs-string">"green"</span>,label=<span class="hljs-string">"positive data"</span>)<br>plt.scatter(x=negativedata[<span class="hljs-string">"x1"</span>],y=negativedata[<span class="hljs-string">"x2"</span>],marker=<span class="hljs-string">"x"</span>,c=<span class="hljs-string">"red"</span>,label=<span class="hljs-string">"negative data"</span>)<br>plt.xlabel(<span class="hljs-string">"x1"</span>)<br>plt.ylabel(<span class="hljs-string">"x2"</span>)<br>plt.xlim((<span class="hljs-number">0</span>,<span class="hljs-number">4</span>))<br>plt.ylim((<span class="hljs-number">0</span>,<span class="hljs-number">4</span>))<br>plt.xticks(np.arange(<span class="hljs-number">0</span>,<span class="hljs-number">4</span>,<span class="hljs-number">0.5</span>))<br>plt.yticks(np.arange(<span class="hljs-number">0</span>,<span class="hljs-number">4</span>,<span class="hljs-number">0.5</span>))<br>plt.legend()<br></code></pre></td></tr></tbody></table></figure><center><img src="https://s2.loli.net/2024/02/03/YtmOUQ8PNdGD1Mi.png" width="60%" height="60%"><div data-align="center">Image2: 训练数据</div></center><p>  利用前文所提出的算法，计算线性判别模型的参数 <span class="math inline">\(w\)</span> .</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">LDA_param_solving</span>(<span class="hljs-params">train_data</span>):<br>    positive_data = train_data[train_data[<span class="hljs-string">"y"</span>]==<span class="hljs-number">1</span>]<br>    negative_data = train_data[train_data[<span class="hljs-string">"y"</span>]==<span class="hljs-number">0</span>]<br><br>    pos_X = np.array(positive_data.iloc[:,:-<span class="hljs-number">1</span>])<br>    neg_X = np.array(negative_data.iloc[:,:-<span class="hljs-number">1</span>])<br><br>    pos_X_mean = np.mean(pos_X,axis=<span class="hljs-number">0</span>)<br>    pos_X_var = np.cov(pos_X,rowvar=<span class="hljs-literal">False</span>)<br>    neg_X_mean = np.mean(neg_X,axis=<span class="hljs-number">0</span>)<br>    neg_X_var = np.cov(neg_X,rowvar=<span class="hljs-literal">False</span>)<br><br>    w = np.dot(np.linalg.inv(pos_X_var+neg_X_var),pos_X_mean-neg_X_mean)<br>    w = w/np.linalg.norm(w)<br><br>    <span class="hljs-keyword">return</span> w.<span class="hljs-built_in">round</span>(<span class="hljs-number">8</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">LDA_threshold</span>(<span class="hljs-params">train_data</span>):<br>    w = LDA_param_solving(train_data)<br><br>    positive_data = train_data[train_data[<span class="hljs-string">"y"</span>]==<span class="hljs-number">1</span>]<br>    negative_data = train_data[train_data[<span class="hljs-string">"y"</span>]==<span class="hljs-number">0</span>]<br>    pos_X = np.array(positive_data.iloc[:,:-<span class="hljs-number">1</span>])<br>    neg_X = np.array(negative_data.iloc[:,:-<span class="hljs-number">1</span>])<br>    pos_X_mean = np.mean(pos_X,axis=<span class="hljs-number">0</span>)<br>    neg_X_mean = np.mean(neg_X,axis=<span class="hljs-number">0</span>)<br><br>    t = (np.dot(w,pos_X_mean)+np.dot(w,neg_X_mean))/<span class="hljs-number">2</span><br><br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">round</span>(t,<span class="hljs-number">8</span>)<br><br>w_hat = LDA_param_solving(train_data=train_data)<br>threshold = LDA_threshold(train_data)<br></code></pre></td></tr></tbody></table></figure><p>  得到的模型参数及threshold为:</p><p><span class="math display">\[w = \begin{bmatrix}    -0.8235 \\    0.5673 \\\end{bmatrix}, \quad threshlod=-0.4689\]</span></p><p>  画出模型的决策边界：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python">x1_line = np.linspace(-<span class="hljs-number">3</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1000</span>)<br>x2_line = (w_hat[<span class="hljs-number">1</span>]/w_hat[<span class="hljs-number">0</span>])*x1_line<br>theta = math.atan(w_hat[<span class="hljs-number">1</span>]/w_hat[<span class="hljs-number">0</span>])<br>threshold_x1 = -math.cos(-theta)*threshold<br>threshold_x2 = math.sin(-theta)*threshold<br>decision_boundary_x2 = (-w_hat[<span class="hljs-number">0</span>]/w_hat[<span class="hljs-number">1</span>])*x1_line+(threshold_x2+(w_hat[<span class="hljs-number">0</span>]/w_hat[<span class="hljs-number">1</span>])*threshold_x1)<br>plt.figure(figsize=(<span class="hljs-number">7</span>, <span class="hljs-number">7</span>))<br>plt.scatter(x=threshold_x1,y=threshold_x2,marker=<span class="hljs-string">"p"</span>,c=<span class="hljs-string">"black"</span>,label=<span class="hljs-string">"threshold"</span>,s=<span class="hljs-number">100</span>,alpha=<span class="hljs-number">1</span>)<br>plt.scatter(x=positivedata[<span class="hljs-string">"x1"</span>],y=positivedata[<span class="hljs-string">"x2"</span>],marker=<span class="hljs-string">"o"</span>,c=<span class="hljs-string">"green"</span>,label=<span class="hljs-string">"positive data"</span>)<br>plt.scatter(x=negativedata[<span class="hljs-string">"x1"</span>],y=negativedata[<span class="hljs-string">"x2"</span>],marker=<span class="hljs-string">"x"</span>,c=<span class="hljs-string">"red"</span>,label=<span class="hljs-string">"negative data"</span>)<br>plt.plot(x1_line,x2_line,c=<span class="hljs-string">"blue"</span>,label=<span class="hljs-string">"Best Project Direction"</span>)<br>plt.plot(x1_line,decision_boundary_x2,c=<span class="hljs-string">"purple"</span>,label=<span class="hljs-string">"decision boundary"</span>)<br>ax = plt.subplot()<br>ax.spines[<span class="hljs-string">'top'</span>].set_visible(<span class="hljs-literal">False</span>)<br>ax.spines[<span class="hljs-string">'right'</span>].set_visible(<span class="hljs-literal">False</span>)<br>ax.spines[<span class="hljs-string">'bottom'</span>].set_position((<span class="hljs-string">'data'</span>,<span class="hljs-number">0</span>))<br>ax.spines[<span class="hljs-string">'left'</span>].set_position((<span class="hljs-string">'data'</span>,<span class="hljs-number">0</span>))<br>plt.xlim((-<span class="hljs-number">3</span>,<span class="hljs-number">3</span>))<br>plt.ylim((-<span class="hljs-number">3</span>,<span class="hljs-number">3</span>))<br>plt.xticks(np.arange(-<span class="hljs-number">3</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>))<br>plt.yticks(np.arange(-<span class="hljs-number">3</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>))<br>plt.legend(loc=<span class="hljs-string">"lower left"</span>)<br></code></pre></td></tr></tbody></table></figure><center><img src="https://s2.loli.net/2024/02/03/v5zwfZsA6dYotO1.png" width="60%" height="60%"><div data-align="center">Image3: 决策边界</div></center><h2 id="参考">参考</h2><p><strong>[1] Video: bilibili,shuhuai008,线性判别分析</strong><br><strong>[2] Blog: CSDN,SongGu1996,线性判别分析(Linear DiscriminantAnalysis，LDA)</strong></p>]]></content>
    
    
    <summary type="html">本节主要介绍机器学习中的二维线性判别分析的理论推导及Python代码实现。</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>机器学习-6.隐马尔可夫模型</title>
    <link href="http://example.com/2023/12/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-6-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/"/>
    <id>http://example.com/2023/12/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-6-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/</id>
    <published>2023-12-26T04:06:49.000Z</published>
    <updated>2024-02-02T17:44:13.460Z</updated>
    
    <content type="html"><![CDATA[<h1 id="隐马尔可夫模型">隐马尔可夫模型</h1><p>  隐马尔可夫模型(hidden Markovmodel,HMM)是一种用于对时许数据建模的概率图模型。它主要应用于对观察序列的概率分布进行建模，这些观察序列背后存在一个不可见的状态序列。HMM的主要思想可以总结如下：</p><ul><li><strong>状态和观察:</strong>HMM包含两种类型的变量，即隐藏的状态序列和可见的观察序列。状态序列表示系统内部的状态，而观察序列是我们可以观察的外部现象。<br></li><li><strong>马尔可夫性质:</strong>HMM假设状态序列是一个马尔可夫链，即系统的未来状态只依赖于当前状态，而与过去的状态无关。这意味着在给定当前当前状态下，未来状态与过去状态的信息是独立的。</li><li><strong>状态转移概率:</strong>HMM用状态转移概率描述系统从一个状态转移到另一个状态的可能性。这些概率被组织成状态转移矩阵，矩阵的元素表示从一个状态转移到另一个状态的概率。</li><li><strong>观察概率:</strong>对于每个状态，HMM定义了生成每个观察值的概率分布。这些概率被组织成观察概率矩阵。</li><li><strong>初始概率:</strong>HMM还需要定义系统在初始时刻处于每个状态的概率，这些概率称为初始概率。</li><li><strong>前向算法和后向算法:</strong>HMM使用前向算法和后向算法来计算给定观测序列的概率。<br></li><li><strong>Baum-Welch算法:</strong>用于无监督学习的算法，通过观察序列来调整模型参数，使其更好地匹配观察数据。Baum-Welch算法本质上就是EM算法。</li></ul><p>  隐马尔可夫模型在各个领域都具有重要的应用，包括<strong>时序数据建模，语音识别，自然语言处理，生物信息学等</strong>。总体而言，HMM在多个领域中都发挥着关键的作用，为时序数据建模和分析提供了灵活而强大的工具。</p><h2 id="基本概念">基本概念</h2><h3 id="马尔可夫链mc">马尔可夫链(MC)</h3><p>  设有随机序列 <span class="math inline">\(S =\{S_1,S_2,\dots,S_t,S_{t+1},\dots \}\)</span>，若 <span class="math inline">\(S_{t+1}\)</span> 只依赖于前一时刻 <span class="math inline">\(S_t\)</span>，不依赖于 <span class="math inline">\(S_1,S_2,\dots,S_{t-1}\)</span>，即：</p><p><span class="math display">\[P(S_{t+1} | S_1,S_2,\dots,S_t)=P(S_{t+1}| S_t)\]</span></p><p>  则称随机序列<span class="math inline">\(S\)</span>为马尔可夫链(Markov Chain)。</p><h3 id="隐马尔可夫模型的定义">隐马尔可夫模型的定义</h3><p>  <strong>定义1(隐马尔可夫模型)</strong>隐马尔可夫模型是关于时序数据的概率模型，描述由一个隐藏的马尔可夫链随机生成不可观测的状态序列，再由各个状态生成一个观测从而产生观测序列的过程。隐藏的马尔可夫链随机生成的状态的序列，称为状态序列(statesequence)；每个状态生成一个观测，而由此产生的观测的随机序列，称为观测序列(observationsequence)。序列的每一个位置又可以看作是一个时刻。</p><p>  HMM模型可以用如下的概率图表示：</p><center><img src="https://s2.loli.net/2023/11/26/w4LvBDr9m7oXkf3.jpg" width="80%" height="60%"><div data-align="center">Image1: HMM模型的概率图</div></center><h3 id="模型参数">模型参数</h3><p>  隐马尔可夫模型由<strong>初始概率分布</strong>、<strong>状态转移概率分布</strong>、以及<strong>观测概率分布</strong>确定。下面我们来介绍这些模型参数的含义，在这之前先做一些符号定义。</p><p><strong>状态序列</strong><br>  设 <span class="math inline">\(I\)</span>为状态序列，<span class="math inline">\(Q\)</span> 是所有可能状态的集合，记为：</p><p><span class="math display">\[Q=\{q_1,q_2,\dots,q_N\},\quadI=\{i_1,i_2,\dots,i_{T}\}, \forall i \in Q\]</span></p><p>其中，<span class="math inline">\(N\)</span>是可能的状态数。</p><p><strong>观测序列</strong><br>  设 <span class="math inline">\(O\)</span> 是 状态序列 <span class="math inline">\(I\)</span> 所对应的观测序列，<span class="math inline">\(V\)</span> 是所有可能的观测的集合，记为：</p><p><span class="math display">\[V = \{v_1,v_2,\dots,v_{M}\},\quadO=\{o_1,o_2,\dots,o_{T}\},\forall o \in V\]</span></p><p><strong>状态转移概率矩阵</strong><br>  设 <span class="math inline">\(A\)</span> 为状态转移概率矩阵：</p><p><span class="math display">\[A = [a_{ij}]_{N \times N}\]</span></p><p>其中，</p><p><span class="math display">\[a_{ij} = P(i_{t+1}=q_{j} |i_{t}=q_{i}),\quad i,j=1,2,\dots,N\]</span></p><p>即系统在时刻 <span class="math inline">\(t\)</span> 处于状态 <span class="math inline">\(q_{i}\)</span> 的条件下在时刻 <span class="math inline">\(t+1\)</span> 转移到状态 <span class="math inline">\(q_{j}\)</span> 的概率。</p><p><strong>观测概率矩阵</strong><br>  设 <span class="math inline">\(B\)</span> 是观测概率矩阵：</p><p><span class="math display">\[B=[b_{j}(k)]_{N \times M}\]</span></p><p>其中，</p><p><span class="math display">\[b_{j}(k)=P(o_{t}=v_{k} |i_{t}=q_{j}),\quad k=1,2,\dots,M;\quad j=1,2,\dots,N\]</span></p><p>即系统在时刻 <span class="math inline">\(t\)</span> 处于状态 <span class="math inline">\(q_{j}\)</span> 的条件下生成观测 <span class="math inline">\(v_{k}\)</span> 的概率。</p><p><strong>初始状态概率向量</strong><br>  设 <span class="math inline">\(\pi\)</span> 是初始状态概率向量：</p><p><span class="math display">\[\pi = \begin{bmatrix}    \pi_1,\pi_2,\dots,\pi_N\end{bmatrix}^{T}\]</span></p><p>其中，</p><p><span class="math display">\[\pi_{i}=P(i_{1}=q_{i}),\quadi=1,2,\dots,N\]</span></p><p>  隐马尔可夫模型由<strong>初始状态概率向量<span class="math inline">\(\pi\)</span></strong>、<strong>状态转移概率矩阵<span class="math inline">\(A\)</span></strong>、<strong>观测概率矩阵<span class="math inline">\(B\)</span></strong>决定。因此，隐马尔可夫模型的参数<span class="math inline">\(\lambda\)</span>可用三元符号表示，即：</p><p><span class="math display">\[\lambda = (A,B,\pi)\]</span></p><p>  <span class="math inline">\(A,B,\pi\)</span>称为隐马尔可夫模型的三要素。</p><h2 id="模型假设">模型假设</h2><p>  隐马尔可夫模型有两个基本假设：<br>  (1) 齐次马尔可夫性假设，即假设隐藏的马尔可夫链在任意时刻 <span class="math inline">\(t\)</span>的状态只依赖于前一时刻的状态，与其他时刻的状态及观测无关，也与时刻 <span class="math inline">\(t\)</span> 无关：</p><p><span class="math display">\[P(i_{t} |i_{t-1},\dots,i_{1};o_{t-1},\dots,o_{1})=P(i_{t} | i_{t-1}),\quadt=1,2,\dots,T\]</span></p><p>  (2)观测独立性假设，即假设任意时刻的观测只依赖于该时刻的马尔可夫链的状态，与其他观测及状态无关：</p><p><span class="math display">\[P(o_{t} |i_{t},\dots,i_{1};o_{t-1},\dots,o_{1})=P(o_{t} | i_{t})\]</span></p><h2 id="基本问题">基本问题</h2><p>  隐马尔可夫模型有3个基本问题，包括概率计算问题、学习问题、解码问题。</p><h3 id="概率计算问题">(1) 概率计算问题</h3><p>  给定模型参数 <span class="math inline">\(\lambda=(A,B,\pi)\)</span>和观测序列 <span class="math inline">\(O=\{o_1,o_2,\dots,o_{T}\}\)</span>，计算在给定模型参数<span class="math inline">\(\lambda\)</span> 的条件下观测序列 <span class="math inline">\(O\)</span> 出现的概率 <span class="math inline">\(P(O | \lambda)\)</span>。</p><h3 id="学习问题">(2) 学习问题</h3><p>  已知观测序列 <span class="math inline">\(O=\{o_1,o_2,\dots,o_{T}\}\)</span>，估计模型参数<span class="math inline">\(\lambda=(A,B,\pi)\)</span>，使得在该模型下观测序列概率<span class="math inline">\(P(O | \lambda)\)</span> 最大，即：</p><p><span class="math display">\[\hat{\lambda}=\arg\max_{\lambda} P(O |\lambda)\]</span></p><p>即用极大似然估计的方法估计参数。</p><h3 id="解码问题">(3) 解码问题</h3><p>  已知模型参数 <span class="math inline">\(\lambda=(A,B,\pi)\)</span>和观测序列 <span class="math inline">\(O=\{o_1,o_2,\dots,o_{T}\}\)</span>，求给定观测序列条件下概率<span class="math inline">\(P(I | O)\)</span> 最大的状态序列 <span class="math inline">\(I=\{i_1,i_2,\dots,i_{T}\}\)</span>，即：</p><p><span class="math display">\[\hat{I} = \arg\max_{I} P(I |O)\]</span></p><p>根据所预测的<span class="math inline">\(I\)</span>的时刻不同，解码问题又可分为预测问题与滤波问题：</p><ul><li>预测问题：<span class="math inline">\(\hat{i}_{t+1} = \arg\maxP(i_{t+1} | o_1,\dots,o_{t})\)</span><br></li><li>滤波问题：<span class="math inline">\(\hat{i}_{t} = \arg\max P(i_{t}| o_1,\dots,o_{t})\)</span></li></ul><h2 id="概率计算问题-1">概率计算问题</h2><p>  已知模型参数 <span class="math inline">\(\lambda=(A,B,\pi)\)</span>和观测序列 <span class="math inline">\(O=\{o_1,o_2,\dots,o_{T}\}\)</span>，计算 <span class="math inline">\(P(O | \lambda)\)</span>。概率计算问题主要有<strong>直接计算法、前向计算法、后向计算法</strong>。</p><h3 id="直接计算法">直接计算法</h3><p>  直接计算法的思路是通过列举所有可能的长度为 <span class="math inline">\(T\)</span> 状态序列 <span class="math inline">\(I=\{i_1,i_2,\dots,i_{T}\}\)</span>，求各个状态序列<span class="math inline">\(I\)</span> 与观测序列 <span class="math inline">\(O=\{o_1,o_2,\dots,o_{T}\}\)</span> 的联合概率<span class="math inline">\(P(O,I |\lambda)\)</span>，然后对所有可能的状态序列求和，得到 <span class="math inline">\(P(O | \lambda)\)</span>。计算过程如下：</p><p><span class="math display">\[P(O | \lambda) = \sum_{I}P(I,O |\lambda)=\sum_{I}P(O | I,\lambda)P(I | \lambda)\]</span></p><p><span class="math display">\[\begin{split}    P(I | \lambda) &amp;= P(i_1,i_2,\dots,i_{T} | \lambda) \\    &amp;= P(i_{T} | i_1,\dots,i_{T-1};\lambda)P(i_1,\dots,i_{T-1} |\lambda) \\    &amp;= P(i_{T} | i_{T-1};\lambda)P(i_1,\dots,i_{T-1} | \lambda) \\    &amp;= \left( a_{i_{T-1}i_{T}} \right) \left(\pi_{i_1}a_{i_{1}i_{2}} \dotsb a_{i_{T-2}i_{T-1}} \right) \\    &amp;= \pi_{i_1}a_{i_{1}i_{2}}a_{i_{2}i_{3}} \dotsb a_{i_{T-1}i_{T}}\end{split}\]</span></p><p><span class="math display">\[\begin{split}    P(O | I,\lambda) &amp;= P(o_1,o_2,\dots,o_{T} |i_1,i_2,\dots,i_{T};\lambda) \\    &amp;= b_{i_1}(o_1)b_{i_2}(o_2) \dotsb b_{i_T}(o_T)\end{split}\]</span></p><p><span class="math display">\[\begin{split}    P(O | \lambda) &amp;= \sum_{I}P(O | I,\lambda)P(I | \lambda) \\    &amp;=\sum_{i_1,i_2,\dots,i_T}\pi_{i1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2)\dotsb a_{i_{T-1}i_{T}}b_{i_{T}}(o_{T})\end{split}\]</span></p><p>  直接计算法的思路非常直观，容易理解，但缺点是计算量很大，是 <span class="math inline">\(O(TN^{T})\)</span>阶的，随着时间的推移呈指数型增长，这种算法在实际中是不可取的。实际上，在概率计算问题中，我们更常用的是前向计算法和后向计算法。</p><h3 id="前向计算法">前向计算法</h3><p>  在导出前向算法之前，我们首先来定义<strong>前向概率:</strong></p><p><span class="math display">\[\alpha_{t}(i)=P(o_1,o_2,\dots,o_{t};i_{t}=q_{i} |\lambda)\]</span></p><p>前向概率表示在给定模型参数 <span class="math inline">\(\lambda\)</span> 的条件下，到时刻 <span class="math inline">\(t\)</span> 部分观测序列为 <span class="math inline">\(o_1,o_2,\dots,o_{t}\)</span> 且状态为 <span class="math inline">\(q_{i}\)</span>的概率。则观测序列概率可以表示为：</p><p><span class="math display">\[P(O |\lambda)=\sum_{i=1}^{N}P(o_1,o_2,\dots,o_{T};i_{T}=q_{k} |\lambda)=\sum_{i=1}^{N}\alpha_{T}(k)\]</span></p><p>  前向计算法的主要思想是递推地求得前向概率 <span class="math inline">\(\alpha_{t}(i)\)</span> 及观测序列概率 <span class="math inline">\(P(O |\lambda)\)</span>，前向计算法的过程可用下图理解：</p><center><img src="https://s2.loli.net/2023/11/27/ry1pAg8RZwhTGkV.jpg" width="60%" height="50%"><div data-align="center">Image2: 前向递推</div></center><p>  接下来我们需要找到 <span class="math inline">\(\alpha_{t}(i)\)</span> 和 <span class="math inline">\(\alpha_{t+1}(j)\)</span> 之间的递推关系式：</p><p><span class="math display">\[\begin{split}    \alpha_{t+1}(j) &amp;= P(o_1,\dots,o_{t},o_{t+1};i_{t+1}=q_{j} |\lambda) \\    &amp;=\sum_{i=1}^{N}P(o_1,\dots,o_{t},o_{t+1};i_{t}=q_{i},i_{t+1}=q_{j} |\lambda) \\    &amp;= \sum_{i=1}^{N} P(o_{t+1} |o_1,\dots,o_{t};i_{t}=q_{i},i_{t+1}=q_{j};\lambda)P(o_1,\dots,o_{t};i_{t}=q_{i},i_{t+1}=q_{j}| \lambda) \\    &amp;=\sum_{i=1}^{N}P(o_{t+1}|i_{t+1}=q_{j};\lambda)P(i_{t+1}=q_{j}|o_1,\dots,o_{t};i_{t}=q_{i};\lambda)P(o_1,\dots,o_{t};i_{t}=q_{i}| \lambda) \\    &amp;= \sum_{i=1}^{N}P(o_{t+1}|i_{t+1}=q_{j};\lambda)P(i_{t+1}=q_{j}| i_{t}=q_i;\lambda)P(o_1,\dots,o_{t};i_{t}=q_{i} | \lambda) \\    &amp;= \sum_{i=1}^{N}b_{j}(o_{t+1})a_{ij}\alpha_{t}(i)\end{split}\]</span></p><p>  当 <span class="math inline">\(t=1\)</span> 时，有：</p><p><span class="math display">\[\alpha_{1}(i)=\pi_{i}b_{i}(o_1)\]</span></p><p>  综上所述，前向计算法的递推关系式可以总计为：</p><p><span class="math display">\[\begin{split}    \alpha_{1}(i) &amp;= \pi_{i}b_{i}(o_1),\quad i=1,2,\dots,N \\    \alpha_{t+1}(j) &amp;=b_{j}(o_{t+1})\sum_{i=1}^{N}a_{ij}\alpha_{t}(i),\quad j=1,2,\dots,N\end{split}\]</span></p><h4 id="前向算法">前向算法</h4><p>  <strong>观测序列概率的前向算法</strong><br>  输入：隐马尔可夫模型的参数 <span class="math inline">\(\lambda=(A,B,\pi)\)</span>，观测序列 O；<br>  输出：观测序列概率 <span class="math inline">\(P(O |\lambda)\)</span>。<br>  (1) 初值</p><p><span class="math display">\[\alpha_{1}(i)=\pi_{i}b_{i}(o_1),\quadi=1,2,\dots,N\]</span></p><p>  (2) 递推 <span class="math inline">\(\quad\)</span> 对 <span class="math inline">\(t=1,2,\dots,T-1,\)</span></p><p><span class="math display">\[\alpha_{t+1}(j) =b_{j}(o_{t+1})\sum_{i=1}^{N}a_{ij}\alpha_{t}(i),\quadj=1,2,\dots,N\]</span></p><p>  (3) 终止</p><p><span class="math display">\[P(O |\lambda)=\sum_{i=1}^{N}\alpha_{T}(i)\]</span></p><p>  前向算法的计算复杂度为 <span class="math inline">\(O(TN^{2})\)</span>，优于直接计算法。</p><h3 id="后向计算法">后向计算法</h3><p>  后向计算法与前向计算法大致相同，不同点在于后向计算法是从后向前递推。我们首先来定义<strong>后向概率:</strong></p><p><span class="math display">\[\beta_{t}(i) =P(o_{t+1},o_{t+2},\dots,o_{T} | i_{t}=q_{i};\lambda)\]</span></p><p>后向概率表示在给定模型参数 <span class="math inline">\(\lambda\)</span>，系统到时刻 <span class="math inline">\(t\)</span> 状态为 <span class="math inline">\(q_{i}\)</span> 的条件下，从 <span class="math inline">\(t+1\)</span> 到 <span class="math inline">\(T\)</span> 的部分观测序列为 <span class="math inline">\(o_{t+1},o_{t+2},\dots,o_{T}\)</span>的概率。则观测序列概率可以表示为：</p><p><span class="math display">\[\begin{split}    P(O | \lambda) &amp;= P(o_1,o_2,\dots,o_{T} | \lambda) \\    &amp;= \sum_{i=1}^{N} P(o_1,o_2,\dots,o_{T};i_{1}=q_{i} | \lambda)\\    &amp;= \sum_{i=1}^{N} P(o_1,o_2,\dots,o_{T} |i_{1}=q_{i};\lambda)P(i_1=q_{i} | \lambda) \\    &amp;= \sum_{i=1}^{N} P(o_1 |o_2,\dots,o_{T};i_{1}=q_{i};\lambda)P(o_2,\dots,o_{T} |i_{1}=q_{i};\lambda)\pi_{i} \\      &amp;= \sum_{i=1}^{N}P(o_1 | i_{1}=q_{i};\lambda)\beta_{1}(i)\pi_{i}\\    &amp;= \sum_{i=1}^{N} \pi_{i} b_{i}(o_1) \beta_{1}(i)\end{split}\]</span></p><p>  后向计算法的主要思想是递推地求得后向概率 <span class="math inline">\(\beta_{t}(i)\)</span> 及观测序列概率 <span class="math inline">\(P(O |\lambda)\)</span>，后向计算法的过程可用下图理解：</p><center><img src="https://s2.loli.net/2023/11/27/9jd7FOnlkuo2rM4.jpg" width="60%" height="60%"><div data-align="center">Image3: 后向递推</div></center><p>  之后我们来导出 <span class="math inline">\(\beta_{t}(i)\)</span> 和<span class="math inline">\(\beta_{t+1}(j)\)</span>之间的递推关系式：</p><p><span class="math display">\[\begin{split}    \beta_{t}(i) &amp;= P(o_{t+1},\dots,o_{T} | i_{t}=q_{i};\lambda) \\    &amp;= \sum_{j=1}^{N}P(o_{t+1},\dots,o_{T};i_{t+1}=q_{j} |i_{t}=q_{i};\lambda) \\    &amp;= \sum_{j=1}^{N}P(o_{t+1},\dots,o_{T} |i_{t+1}=q_{j},i_{t}=q_{i};\lambda)P(i_{t+1}=q_{j} | i_{t}=q_{i};\lambda)\\    &amp;= \sum_{j=1}^{N}P(o_{t+1},\dots,o_{T} |i_{t+1}=q_{j};\lambda)a_{ij} \\    &amp;= \sum_{j=1}^{N}P(o_{t+1} |o_{t+2},\dots,o_{T};i_{t+1}=q_{j};\lambda)P(o_{t+2},\dots,o_{T} |i_{t+1}=q_{j};\lambda)a_{ij} \\    &amp;= \sum_{j=1}^{N}P(o_{t+1} | i_{t+1}=q_{j};\lambda)\beta_{t+1}(j) a_{ij}  \\    &amp;= \sum_{j=1}^{N} b_{j}(o_{t+1})a_{ij} \beta_{t+1}(j)\end{split}\]</span></p><p>  当 <span class="math inline">\(t=T\)</span>时，给定初始的后向概率：</p><p><span class="math display">\[\beta_{T}(i) = 1,\quadi=1,2,\dots,N\]</span></p><p>  综上所述，后向计算法的递推关系式可以总计为：</p><p><span class="math display">\[\begin{split}    \beta_{T}(i) &amp;= 1,\quad i=1,2,\dots,N \\    \beta_{t}(i) &amp;= \sum_{j=1}^{N} b_{j}(o_{t+1})a_{ij}\beta_{t+1}(j),\quad t=T-1,\dots,1;i=1,\dots,N\end{split}\]</span></p><h4 id="后向算法">后向算法</h4><p><strong>观测序列概率的后向算法</strong><br>  输入：隐马尔可夫模型参数 <span class="math inline">\(\lambda=(A,B,\pi)\)</span>，观测序列 <span class="math inline">\(O\)</span>;<br>  输出：观测序列概率 <span class="math inline">\(P(O |\lambda)\)</span>。<br>  (1) 初值</p><p><span class="math display">\[\beta_{T}(i) = 1,\quadi=1,2,\dots,N\]</span></p><p>  (2) 递推 <span class="math inline">\(\quad\)</span> 对 <span class="math inline">\(t=T-1,T-2,\dots,1\)</span></p><p><span class="math display">\[\beta_{t}(i) = \sum_{j=1}^{N}b_{j}(o_{t+1})a_{ij} \beta_{t+1}(j),\quad i=1,\dots,N\]</span></p><p>  (3) 终止</p><p><span class="math display">\[P(O | \lambda) = \sum_{i=1}^{N} \pi_{i}b_{i}(o_1) \beta_{1}(i)\]</span></p><p>  后向算法的计算复杂度为 <span class="math inline">\(O(TN^{2})\)</span>，与前向算法相同，优于直接计算法。</p><h2 id="学习问题-1">学习问题</h2><p>  隐马尔可夫模型的学习，根据训练数据是包括观测序列和对应的状态序列还是只有观测序列，可以分为监督学习与无监督学习。监督学习主要利用极大似然法来估计模型参数，无监督学习则是利用<span class="math inline">\(Baum-Welch\)</span> 算法，也就是 <span class="math inline">\(EM\)</span> 算法来估计参数。</p><h3 id="监督学习方法">监督学习方法</h3><p>  假设已给训练数据包含 <span class="math inline">\(S\)</span>个长度相同的观测序列和对应的状态序列 <span class="math inline">\(\{(O_1,I_1),(O_2,I_2),\dotsb,(O_S,I_S)\}\)</span>，可以利用极大似然估计法来估计隐马尔可夫模型的参数。</p><p>  <strong>1.转移概率 <span class="math inline">\(a_{ij}\)</span>的估计</strong><br>  设样本中时刻 <span class="math inline">\(t\)</span> 处于状态 <span class="math inline">\(q_i\)</span> 且时刻 <span class="math inline">\(t+1\)</span> 转移到状态 <span class="math inline">\(q_j\)</span> 的频数为 <span class="math inline">\(A_{ij}\)</span>，那么状态转移概率 <span class="math inline">\(a_{ij}\)</span> 的估计为：</p><p><span class="math display">\[\hat{a}_{ij}=\frac{A_{ij}}{\sum_{j=1}^{N}A_{ij}},\quadi=1,2,\dotsb,N;\quad j=1,2,\dotsb,N\]</span></p><p>  <strong>2.观测概率 <span class="math inline">\(b_{j}(k)\)</span>的估计</strong><br>  设样本中状态为 <span class="math inline">\(q_j\)</span> 并观测为 <span class="math inline">\(v_k\)</span> 的频数是 <span class="math inline">\(B_{jk}\)</span>，那么状态为 <span class="math inline">\(q_j\)</span> 观测为 <span class="math inline">\(v_k\)</span> 的概率 <span class="math inline">\(b_{j}(k)\)</span> 的估计为</p><p><span class="math display">\[\hat{b}_{j}(k)=\frac{B_{jk}}{\sum_{k=1}^{M}B_{jk}},\quadj=1,2,\quad,N;\quad k=1,2,\dotsb,M\]</span></p><p>  <strong>3.初始状态概率 <span class="math inline">\(\pi_{i}\)</span>的估计</strong><br>  设样本中初始状态为 <span class="math inline">\(q_i\)</span> 的频数为<span class="math inline">\(Q_i\)</span>，则初始状态概率 <span class="math inline">\(\pi_i\)</span> 的估计为</p><p><span class="math display">\[\hat{\pi}_{i}=\frac{Q_{i}}{S},\quadi=1,2,\dotsb,N\]</span></p><h3 id="无监督学习方法">无监督学习方法</h3><p>  虽然监督学习的方法操作十分简便，也非常容易理解，但监督学习需要对训练数据进行标注，而人工标注训练数据往往代价很高，因此，有时就会利用无监督学习的方法。无监督学习所使用的算法为<span class="math inline">\(Baum-Welch\)</span>，实际上为 <span class="math inline">\(EM\)</span> 算法。   假设给定训练数据只包含 <span class="math inline">\(S\)</span> 个长度为 <span class="math inline">\(T\)</span> 的观测序列 <span class="math inline">\(\{O_1,O_2,\dotsb,O_S\}\)</span>而没有对应的状态序列，目标是学习隐马尔可夫模型 <span class="math inline">\(\lambda=(A,B,\pi)\)</span>的参数。我们将观测序列数据看作观测数据 <span class="math inline">\(O\)</span>，状态序列数据看作不可观测的隐数据 <span class="math inline">\(I\)</span>，那么隐马尔可夫模型实际上是一个含有隐变量的概率模型</p><p><span class="math display">\[P(O | \lambda)=\sum_{I}P(O |I,\lambda)P(I | \lambda)\]</span></p><p>它的参数学习可以由 <span class="math inline">\(EM\)</span>算法实现。</p><p>  <strong>1.确定完全数据的对数似然函数</strong><br>  所有的观测数据写成 <span class="math inline">\(O=(o_1,o_2,\dotsb,o_{T})\)</span>，所有隐数据写成<span class="math inline">\(I=(i_1,i_2,\dotsb,i_{T})\)</span>，完全数据为<span class="math inline">\((O,I)=(o_1,o_2,\dotsb,o_{T};i_1,i_2,\dotsb,i_{T})\)</span>。完全数据的对数似然函数为：</p><p><span class="math display">\[L(\lambda) = \log{P(O,I |\lambda)}\]</span></p><p>  <strong>2.EM 算法的 E 步：求 <span class="math inline">\(Q\)</span>函数 <span class="math inline">\(Q(\lambda,\bar{\lambda})\)</span></strong><br>  由 <span class="math inline">\(Q\)</span> 函数的定义得</p><p><span class="math display">\[\begin{split}    Q(\lambda,\bar{\lambda}) &amp;= E_{I}[\log{P(O,I|\lambda)} |O,\bar{\lambda}]  \\    &amp;=\sum_{I}\frac{\log{P(O,I |\lambda)}P(O,I|\bar{\lambda})}{P(O|\bar{\lambda})}\end{split}\]</span></p><p>其中，<span class="math inline">\(\bar{\lambda}\)</span>是隐马尔可夫模型当前的估计值，<span class="math inline">\(\lambda\)</span>是下一步要极大化的隐马尔可夫模型参数。由于 <span class="math inline">\(P(O | \bar{\lambda})\)</span>为常数，对优化没有影响，可以舍去；同时由概率计算中的直接计算法可得：</p><p><span class="math display">\[P(O,I |\lambda)=\pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2) \dotsba_{i_{T-1}i_{T}}b_{i_{T}}(o_{T})\]</span></p><p>于是函数 <span class="math inline">\(Q(\lambda,\bar{\lambda})\)</span> 可以写成：</p><p><span class="math display">\[\begin{split}    Q(\lambda,\bar{\lambda}) &amp;=\sum_{I}\log{P(O,I|\lambda)P(O,I|\bar{\lambda})} \\    &amp;= \sum_{I} P(O,I | \bar{\lambda}) \log{[\pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2) \dotsba_{i_{T-1}i_{T}}b_{i_{T}}(o_{T}) ]}  \\    &amp;= \sum_{I} P(O,I | \bar{\lambda}) \log{\left[\pi_{i_1}\left(\prod_{t=1}^{T-1}a_{i_{t}i_{t+1}}\right)\left(\prod_{t=1}^{T}b_{i_t}(o_t) \right) \right]} \\    &amp;=\sum_{I}P(O,I | \bar{\lambda}) \left[\log(\pi_{i})+\sum_{t=1}^{T-1}\log(a_{i_{t}i_{t+1}})+\sum_{t=1}^{T}\log(b_{i_t}(o_t))\right] \\    &amp;=\sum_{I}\log(\pi_{i})P(O,I | \bar{\lambda})+\sum_{I}\left(\sum_{t=1}^{T-1}\log(a_{i_{t}i_{t+1}}) \right)P(O,I | \bar{\lambda}) \\    &amp;+ \sum_{I}\left( \sum_{t=1}^{T}\log(b_{i_t}(o_t)) \right)P(O,I| \bar{\lambda})\end{split}\]</span></p><p>式中求和都是对所有数据的序列总长度 <span class="math inline">\(T\)</span> 进行的。通过观察 <span class="math inline">\(Q(\lambda,\bar{\lambda})\)</span> 的计算式可以看出<span class="math inline">\(Q(\lambda,\bar{\lambda})\)</span> 的第一项<span class="math inline">\(\sum_{I}\log(\pi_{i})P(O,I |\bar{\lambda})\)</span> 只与参数 <span class="math inline">\(\lambda\)</span> 中的初始概率 <span class="math inline">\(\pi\)</span> 有关；第二项 <span class="math inline">\(\sum_{I}\left(\sum_{t=1}^{T-1}\log(a_{i_{t}i_{t+1}}) \right)P(O,I |\bar{\lambda})\)</span> 只与参数 <span class="math inline">\(\lambda\)</span> 中的状态转移概率矩阵 <span class="math inline">\(A\)</span> 有关；第三项 <span class="math inline">\(\sum_{I}\left( \sum_{t=1}^{T}\log(b_{i_t}(o_t))\right)P(O,I | \bar{\lambda})\)</span> 只与参数 <span class="math inline">\(\lambda\)</span> 中的观测概率矩阵 <span class="math inline">\(B\)</span> 有关。因此，可以令：</p><p><span class="math display">\[\begin{split}    Q_{1}(\pi,\bar{\lambda}) &amp;= \sum_{I}\log(\pi_{i})P(O,I |\bar{\lambda}) \\    Q_{2}(A,\bar{\lambda}) &amp;= \sum_{I}\left(\sum_{t=1}^{T-1}\log(a_{i_{t}i_{t+1}}) \right)P(O,I | \bar{\lambda}) \\    Q_{3}(B,\bar{\lambda}) &amp;= \sum_{I}\left(\sum_{t=1}^{T}\log(b_{i_t}(o_t)) \right)P(O,I | \bar{\lambda})\end{split}\]</span></p><p>则 <span class="math inline">\(Q(\lambda,\bar{\lambda})\)</span>可以写成：</p><p><span class="math display">\[Q(\lambda,\bar{\lambda})=Q_{1}(\pi,\bar{\lambda})+Q_{2}(A,\bar{\lambda})+Q_{3}(B,\bar{\lambda})\]</span></p><p>  <strong>3.EM 算法的 M 步：极大化 <span class="math inline">\(Q\)</span> 函数 <span class="math inline">\(Q(\lambda,\bar{\lambda})\)</span> 求模型参数 <span class="math inline">\(A,B,\pi\)</span></strong><br>  通过极大化 <span class="math inline">\(Q(\lambda,\bar{\lambda})\)</span> 可以得到模型参数<span class="math inline">\(A,B,\pi\)</span> 的估计值。<br>  <strong>(1) 估计初始状态概率 <span class="math inline">\(\pi\)</span></strong><br>  <span class="math inline">\(Q(\lambda,\bar{\lambda})\)</span> 中只有<span class="math inline">\(Q_{1}(\pi,\bar{\lambda})\)</span>与初始状态概率 <span class="math inline">\(\pi\)</span>有关，因此优化问题可以写成：</p><p><span class="math display">\[\begin{split}    &amp; \max_{\pi} \quad Q_{1}(\pi,\bar{\lambda}) \\    &amp; \space s.t. \quad \sum_{i=1}^{N} \pi_{i}=1 \\\end{split}\]</span></p><p>优化问题的拉格朗日函数：</p><p><span class="math display">\[L(\pi,\gamma)=\sum_{I}\log(\pi_{i})P(O,I| \bar{\lambda})+\gamma \left( \sum_{i=1}^{N}\pi_{i}-1\right)\]</span></p><p>由费马定理得：</p><p><span class="math display">\[\frac{\partial{L(\pi,\gamma)}}{\partial{\pi_{i}}}= \frac{P(O,i_1=i | \bar{\lambda})}{\pi_{i}}+\gamma=0,\quadi=1,2,\dotsb,N\]</span></p><p>得到：</p><p><span class="math display">\[\gamma\pi_{i}=-P(O,i_1=i|\bar{\lambda})\]</span></p><p>两边同时对 <span class="math inline">\(i\)</span> 求和得：</p><p><span class="math display">\[\begin{split}    &amp; \gamma\sum_{i=1}^{N}\pi_{i} =-\sum_{i=1}^{N}P(O,i_{1}=i|\bar{\lambda})=P(O | \bar{\lambda}) \\    &amp; \Rightarrow \gamma = P(O | \bar{\lambda}) \\\end{split}\]</span></p><p>从而得到 <span class="math inline">\(\pi_{i}\)</span>的估计值为：</p><p><span class="math display">\[\hat{\pi}_{i}=\frac{P(O,i_1=i|\bar{\lambda})}{P(O| \bar{\lambda})}\]</span></p><p>  <strong>(2) 估计状态转移矩阵 <span class="math inline">\(A\)</span></strong><br>  <span class="math inline">\(Q(\lambda,\bar{\lambda})\)</span> 中只有<span class="math inline">\(Q_{2}(A,\bar{\lambda})\)</span>与状态转移概率矩阵 <span class="math inline">\(A\)</span>有关，因此优化问题可以写成：</p><p><span class="math display">\[\begin{split}    &amp; \max_{A} \quad Q_{2}(A,\bar{\lambda}) \\    &amp; \space s.t. \quad \sum_{j=1}^{N}a_{ij}=1 \\\end{split}\]</span></p><p>优化问题的拉格朗日函数：</p><p><span class="math display">\[\begin{split}    L(A,\gamma) &amp;= \sum_{I}\left(\sum_{t=1}^{T-1}\log(a_{i_{t}i_{t+1}}) \right)P(O,I |\bar{\lambda})+\gamma\left( \sum_{j=1}^{N}a_{ij}-1 \right) \\    &amp;=\sum_{t=1}^{T-1}\sum_{i=1}^{N}\sum_{j=1}^{N}\left(P(O,i_{t}=i,i_{t+1}=j|\bar{\lambda})\log{a_{ij}} \right)+\gamma\left(\sum_{j=1}^{N}a_{ij}-1 \right)\end{split}\]</span></p><p>由费马定理得：</p><p><span class="math display">\[\frac{\partial{L(A,\gamma)}}{\partial{a_{ij}}}=\frac{\sum_{t=1}^{T-1}P(O,i_{t}=i,i_{t+1}=j|\bar{\lambda})}{a_{ij}}+\gamma=0,\quadi,j=1,\dotsb,N\]</span></p><p>得到：</p><p><span class="math display">\[\gammaa_{ij}=-\sum_{t=1}^{T-1}P(O,i_{t}=i,i_{t+1}=j|\bar{\lambda})\]</span></p><p>两边同时对 <span class="math inline">\(j\)</span> 求和：</p><p><span class="math display">\[\begin{split}    &amp;\gamma\sum_{j=1}^{N}a_{ij}=-\sum_{t=1}^{T-1}\sum_{j=1}^{N}P(O,i_{t}=i,i_{t+1}=j|\bar{\lambda})\\    &amp; \Rightarrow \gamma =-\sum_{t=1}^{T-1}P(O,i_{t}=i|\bar{\lambda})\end{split}\]</span></p><p>从而得到状态转移概率矩阵元素 <span class="math inline">\(a_{ij}\)</span> 的估计值为：</p><p><span class="math display">\[\hat{a}_{ij}=\frac{\sum_{t=1}^{T-1}P(O,i_{t}=i,i_{t+1}=j|\bar{\lambda})}{\sum_{t=1}^{T-1}P(O,i_{t}=i|\bar{\lambda})}\]</span></p><p>  <strong>(3) 估计观测概率矩阵 <span class="math inline">\(B\)</span></strong><br>  <span class="math inline">\(Q(\lambda,\bar{\lambda})\)</span> 中只有<span class="math inline">\(Q_{3}(B,\bar{\lambda})\)</span>与观测概率矩阵 <span class="math inline">\(B\)</span>有关，因此优化问题可以写成：</p><p><span class="math display">\[\begin{split}    &amp; \max_{B} \quad Q_{3}(B,\bar{\lambda}) \\    &amp; \space s.t. \quad \sum_{k=1}^{M}b_{j}(v_{k})=1 \\\end{split}\]</span></p><p>优化问题的拉格朗日函数：</p><p><span class="math display">\[\begin{split}    L(B,\gamma) &amp;= \sum_{I}\left( \sum_{t=1}^{T}\log(b_{i_t}(o_t))\right)P(O,I | \bar{\lambda})+\gamma\left( \sum_{k=1}^{M}b_{j}(v_{k})-1\right) \\    &amp;=\sum_{t=1}^{T}\sum_{j=1}^{N}\left(P(O,i_{t}=j|\bar{\lambda})\log{b_{j}(o_{t})} \right)+\gamma\left(\sum_{k=1}^{M}b_{j}(v_{k})-1 \right)\end{split}\]</span></p><p>由费马定理得：</p><p><span class="math display">\[\frac{\partial{L(B,\gamma)}}{\partial{b_{j}(v_{k})}}=\frac{\sum_{t=1}^{T}P(O,i_{t}=j|\bar{\lambda})I(o_{t}=v_{k})}{b_{j}(v_{k})}+\gamma=0,\quadj=1,\dotsb,N;k=1,\dotsb,M\]</span></p><p>得到：</p><p><span class="math display">\[\gammab_{j}(v_{k})=-\sum_{t=1}^{T}P(O,i_{t}=j|\bar{\lambda})I(o_{t}=v_{k})\]</span></p><p>两边同时对 <span class="math inline">\(k\)</span> 求和：</p><p><span class="math display">\[\begin{split}    &amp;\gamma\sum_{k=1}^{M}b_{j}(v_{k})=-\sum_{k=1}^{M}\sum_{t=1}^{T}P(O,i_{t}=j|\bar{\lambda})I(o_{t}=v_{k})\\    &amp; \Rightarrow \gamma = -\sum_{t=1}^{T}P(O,i_{t}=j|\bar{\lambda})\end{split}\]</span></p><p>从而得到观测概率矩阵元素的 <span class="math inline">\(b_{j}(k)\)</span> 的估计值为：</p><p><span class="math display">\[\hat{b}_{j}(k)=\frac{\sum_{t=1}^{T}P(O,i_{t}=j|\bar{\lambda})I(o_{t}=v_{k})}{\sum_{t=1}^{T}P(O,i_{t}=j|\bar{\lambda})}\]</span></p><h4 id="baum-welch-算法">Baum-Welch 算法</h4><p>  通过以上的推导，我们可以总结出无监督学习下隐马尔可夫参数估计的一种算法，其被称为Baum-Welch 算法，本质上是 EM 算法在隐马尔可夫模型学习中的具体实现。</p><p>  <strong>Baum-Welch 算法</strong><br>  输入：观测数据 <span class="math inline">\(O=(o_1,o_2,\dotsb,o_{T})\)</span>；<br>  输出：隐马尔可夫模型参数 <span class="math inline">\(\lambda=(A,B,\pi)\)</span>。<br>  (1) 初始化。对 <span class="math inline">\(n=0\)</span>，选取 <span class="math inline">\(a_{ij}^{(0)},b_{j}^{(0)}(k),\pi_{i}^{(0)}\)</span>，得到模型<span class="math inline">\(\lambda^{(0)}=(A^{(0)},B^{(0)},\pi^{(0)})\)</span>。<br>  (2) 递推。对 <span class="math inline">\(n=1,2,\dotsb\)</span>，</p><p><span class="math display">\[a_{ij}^{(n+1)}=\frac{\sum_{t=1}^{T-1}P(O,i_{t}=i,i_{t+1}=j|\lambda^{(n)})}{\sum_{t=1}^{T-1}P(O,i_{t}=i|\lambda^{(n)})}\]</span></p><p><span class="math display">\[b_{j}^{(n+1)}(k)=\frac{\sum_{t=1}^{T}P(O,i_{t}=j|\lambda^{(n)})I(o_{t}=v_{k})}{\sum_{t=1}^{T}P(O,i_{t}=j|\lambda^{(n)})}\]</span></p><p><span class="math display">\[\pi_{i}^{(n+1)}=\frac{P(O,i_1=i|\lambda^{(n)})}{P(O| \lambda^{(n)})}\]</span></p><p>  (3) 终止。得到模型参数 <span class="math inline">\(\lambda^{(n+1)}=(A^{(n+1)},B^{(n+1)},\pi^{(n+1)})\)</span></p><h2 id="解码问题-1">解码问题</h2><p>  解码问题主要是研究给定观测序列下最有可能出现的状态序列。已知模型参数<span class="math inline">\(\lambda=(A,B,\pi)\)</span> 和观测序列 <span class="math inline">\(O=(o_1,o_2,\dotsb,o_{T})\)</span>，求某一观测序列<span class="math inline">\(I=(i_1,i_2,\dotsb,i_{T})\)</span>使得条件概率 <span class="math inline">\(P(I | O)\)</span>最大。隐马尔可夫模型中的解码问题主要使用维特比算法。</p><h3 id="维特比算法">维特比算法</h3><p>  维特比算法实际是用动态规划(dynamicprogramming)解隐马尔可夫模型解码问题，即用动态规划求概率最大路径(最优路径)。这时一条路径对应着一个状态序列。维特比算法的思想可以用下图表示：</p><center><img src="https://s2.loli.net/2023/12/06/E87VbUDGMmWHFJp.jpg" width="60%" height="60%"><div data-align="center">Image3: 最优路径</div></center><p>  根据动态规划的原理，最优路径具有这样的特性：<strong>如果最优路径在<span class="math inline">\(t\)</span> 时刻通过结点 <span class="math inline">\(i_{t}^{*}\)</span>，那么这一路径从结点 <span class="math inline">\(i_{t}^{*}\)</span> 到终点 <span class="math inline">\(i_{T}^{*}\)</span> 的部分路径，对于从 <span class="math inline">\(i_{t}^{*}\)</span> 到 <span class="math inline">\(i_{T}^{*}\)</span>的所有可能的部分路径来说，必须是最优的。</strong>依据这一原理，我们只需要从时刻 <span class="math inline">\(t=1\)</span>开始，递推地计算在时刻 <span class="math inline">\(t\)</span> 状态为<span class="math inline">\(i\)</span>的各条部分路径的最大概率，直至得到时刻 <span class="math inline">\(t=T\)</span> 状态为 <span class="math inline">\(i\)</span> 的各条路径的最大概率。时刻 <span class="math inline">\(t=T\)</span> 的最大概率即为最优路径的概率 <span class="math inline">\(P^{*}\)</span>，最优路径的终结点 <span class="math inline">\(i_{T}^{*}\)</span>也同时得到。之后，为了找出最优路径的各个结点，从终结点 <span class="math inline">\(i_{T}^{*}\)</span> 开始，由后向前逐步求得结点<span class="math inline">\(i_{T-1}^{*},\dotsb,i_{1}^{*}\)</span>，得到最优路径<span class="math inline">\(I^{*}=(i_{1}^{*},i_{2}^{*},\dotsb,i_{T}^{*})\)</span>。这就是维特比算法。</p><p>  首先导入两个变量 <span class="math inline">\(\delta\)</span> 和<span class="math inline">\(\psi\)</span>。定义在时刻 <span class="math inline">\(t\)</span> 状态为 <span class="math inline">\(i\)</span> 的所有单个路径 <span class="math inline">\((i_1,i_2,\dotsb,i_{t})\)</span>中概率最大值为：</p><p><span class="math display">\[\delta_{t}(i)=\max_{i_1,i_2,\dotsb,i_{t-1}}P(i_{t}=i,i_{t-1},\dotsb,i_1;o_{t},\dotsb,o_1| \lambda),\quad i=1,2,\dotsb,N\]</span></p><p>  由定义可得变量 <span class="math inline">\(\delta\)</span>的递推公式：</p><p><span class="math display">\[\begin{split}    \delta_{t+1}(i) &amp;=\max_{i_1,i_2,\dotsb,i_{t}}P(i_{t+1}=i,i_{t},\dotsb,i_1;o_{t+1},\dotsb,o_1| \lambda) \\    &amp;= \max_{1 \leq j \leq N}[\delta_{t}a_{ji}]b_{i}(o_{t+1}),\quadi=1,2,\dotsb,N; \space t=1,2,\dotsb,T-1\end{split}\]</span></p><p>  定义在时刻 <span class="math inline">\(t\)</span> 状态为 <span class="math inline">\(i\)</span> 的所有单个路径 <span class="math inline">\((i_1,i_2,\dotsb,i_{t-1},i)\)</span>中概率最大的路径的第 <span class="math inline">\(t-1\)</span>个结点为</p><p><span class="math display">\[\psi_{t}(i)=\arg\max_{1 \leq j \leqN}[\delta_{t-1}(j)a_{ji}],\quad i=1,2,\dotsb,N\]</span></p><p>  <strong>维特比算法</strong><br>  输入：模型参数 <span class="math inline">\(\lambda=(A,B,\pi)\)</span>和观测 <span class="math inline">\(O=(o_1,o_2,\dotsb,o_{T})\)</span>；<br>  输出：最优路径 <span class="math inline">\(I^{*}=(i_{1}^{*},i_{2}^{*},\dotsb,i_{T}^{*})\)</span>.<br>  (1) 初始化</p><p><span class="math display">\[\delta_{1}(i)=\pi_{i}b_{i}(o_1),\quadi=1,2,\dotsb,N\]</span></p><p><span class="math display">\[\psi_{1}(i)=0,\quadi=1,2,\dotsb,N\]</span></p><p>  (2) 递推。对 <span class="math inline">\(t=2,3,\dotsb,T\)</span></p><p><span class="math display">\[\delta_{t}(i)=\max_{1 \leq j \leqN}[\delta_{t-1}(j)a_{ji}]b_{i}(o_{t}),\quad i=1,2,\dotsb,N\]</span></p><p><span class="math display">\[\psi_{t}(i)=\arg\max_{1 \leq j \leqN}[\delta_{t-1}(j)a_{ji}],\quad i=1,2,\dotsb,N\]</span></p><p>  (3) 终止</p><p><span class="math display">\[P^{*}=\max_{1 \leq i \leq N}\delta_{T}(i)\]</span></p><p><span class="math display">\[i_{T}^{*}=\arg\max_{1 \leq i \leqN}[\delta_{T}(i)]\]</span></p><p>  (4) 最优路径回溯。对 <span class="math inline">\(t=T-1,T-2,\dotsb,1\)</span></p><p><span class="math display">\[i_{t}^{*}=\psi_{t+1}(i_{t+1}^{*})\]</span></p><p>求得最优路径 <span class="math inline">\(I^{*}=(i_{1}^{*},i_{2}^{*},\dotsb,i_{T}^{*})\)</span>。</p><h2 id="参考">参考</h2><p><strong>[1] Book: 李航,统计学习方法(第2版)</strong><br><strong>[2] Book: 董平,机器学习中的统计思维(Python实现)</strong><br><strong>[3] Video: bilibili,简博士,隐马尔可夫系列</strong><br><strong>[4] Video: bilibili,shuhuai008,隐马尔可夫系列</strong></p>]]></content>
    
    
    <summary type="html">本节主要介绍隐马尔可夫模型的主要作用及理论推导。</summary>
    
    
    
    <category term="机器学习" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>矩阵分析-6.矩阵的等价与相似</title>
    <link href="http://example.com/2023/11/20/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-6-%E7%9F%A9%E9%98%B5%E7%9A%84%E7%AD%89%E4%BB%B7%E4%B8%8E%E7%9B%B8%E4%BC%BC/"/>
    <id>http://example.com/2023/11/20/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-6-%E7%9F%A9%E9%98%B5%E7%9A%84%E7%AD%89%E4%BB%B7%E4%B8%8E%E7%9B%B8%E4%BC%BC/</id>
    <published>2023-11-20T09:02:49.000Z</published>
    <updated>2023-11-20T09:34:36.756Z</updated>
    
    <content type="html"><![CDATA[<h1 id="矩阵的等价与相似">矩阵的等价与相似</h1><h2 id="矩阵等价">矩阵等价</h2><h3 id="定义">定义</h3><p>  设矩阵<span class="math inline">\(A,B \in \mathbb{F}^{m \timesn}\)</span>，若存在可逆矩阵 <span class="math inline">\(P \in\mathbb{F}^{n \times n}, Q \in \mathbb{F}^{m \times m}\)</span>，使得<span class="math inline">\(AP=QB\)</span>，则称矩阵<span class="math inline">\(A,B\)</span>等价。</p><p>[注]：由于<span class="math inline">\(AP=QB\)</span>，且<span class="math inline">\(Q\)</span>可逆，可得 <span class="math inline">\(Q^{-1}AP=B\)</span>. <strong>因此矩阵<span class="math inline">\(B\)</span>是由矩阵<span class="math inline">\(A\)</span>进行有限次初等变换后得到的新矩阵.</strong></p><h3 id="几何意义">几何意义</h3><p>  令：</p><p><span class="math display">\[P = \begin{bmatrix}    p_1,p_2,\dots,p_n\end{bmatrix}, p_{i} \in \mathbb{F}^{n}, i=1,2,\dots,n\]</span></p><p><span class="math display">\[Q = \begin{bmatrix}    q_1,q_2,\dots,q_{m}\end{bmatrix}, q_j \in \mathbb{F}^{m},i=1,2,\dots,m\]</span></p><p>  <span class="math inline">\(\because AP=QB\)</span>，故有：</p><p><span class="math display">\[A\begin{bmatrix}    p_1,p_2,\dots,p_n \\\end{bmatrix}=\begin{bmatrix}    q_1,q_2,\dots,q_m\end{bmatrix}B\]</span></p><p>  矩阵<span class="math inline">\(A \in \mathbb{F}^{m \timesn}\)</span>，可视为线性映射：</p><p><span class="math display">\[\begin{split}    \mathbb{F}^{n} &amp; \rightarrow \mathbb{F}^{m} \\     x &amp; \rightarrow y=Ax  \end{split}\]</span></p><p>  入口基：<span class="math inline">\(\begin{bmatrix}  p_1,p_2,\dots,p_n\end{bmatrix}\)</span>，出口基：<span class="math inline">\(\begin{bmatrix}  q_1,q_2,\dots,q_{m}\end{bmatrix}\)</span>，由线性映射的概念可以得到矩阵等价的几何意义是：<strong>线性映射<span class="math inline">\(A\)</span>在入口基<span class="math inline">\(P\)</span>和出口基<span class="math inline">\(Q\)</span>下的矩阵表示为<span class="math inline">\(B\)</span>.</strong></p>]]></content>
    
    
    <summary type="html">本节主要介绍矩阵等价与矩阵相似的概率，以及它们的几何意义。</summary>
    
    
    
    <category term="矩阵分析" scheme="http://example.com/categories/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90/"/>
    
    
  </entry>
  
  <entry>
    <title>机器学习-5.逻辑回归</title>
    <link href="http://example.com/2023/11/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-5-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <id>http://example.com/2023/11/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-5-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</id>
    <published>2023-11-15T12:52:01.000Z</published>
    <updated>2023-11-17T02:07:17.574Z</updated>
    
    <content type="html"><![CDATA[<h1 id="逻辑回归">逻辑回归</h1><p>  逻辑回归(LogisticRegression)是一种用于解决二分类问题的机器学习方法。虽然逻辑回归在名称中含有“回归”一词，但其实际上主要用于分类问题。逻辑回归的主要思想是将由线性回归所得的值通过一个逻辑函数映射到0和1之间的概率值，从而将线性回归模型转化为一个分类模型。逻辑回归最早是由统计学家和生物学家使用，用于建立生物学实验结果与概率的关系。后来，随着计算机科学的发展，逻辑回归成为机器学习领域的重要算法之一，被广泛用于分类问题。逻辑回归的优势在于简单易懂、计算效率高，特别适用于大规模数据集。然而，在处理复杂非线性关系的问题上，逻辑回归可能受到限制，这时候更复杂的模型（如支持向量机、深度学习等）可能更为适用。</p><h2 id="基本思想">基本思想</h2><p>  逻辑回归的基本思想是通过逻辑函数(也称为sigmoid函数)将线性回归的结果映射为一个概率值，然后利用概率值解决二分类问题。逻辑回归的基本思想中包含着三个主要因素：<strong>线性回归</strong>、<strong>逻辑函数</strong>、<strong>决策边界</strong>。<br>  线性回归很好理解，给定一个输入的特征向量 <span class="math inline">\(x=\begin{bmatrix}  x_1,x_2,\dots,x_n \\\end{bmatrix}^{T}\)</span>，以及权重向量 <span class="math inline">\(w=\begin{bmatrix}  w_1,w_2,\dots,w_n \\\end{bmatrix}^{T}\)</span>，以及偏置项 <span class="math inline">\(b\)</span>，可以计算线性回归结果：</p><p><span class="math display">\[z =w^{T}x+b=w_1x_1+w_2x_2+\dots+w_nx_n+b\]</span></p><p>  逻辑函数<span class="math inline">\(f\)</span>的作用是将线性回归的结果映射到一个概率值，即：</p><p><span class="math display">\[f: z \rightarrow p \in[0,1]\]</span></p><p>在实际中，我们的逻辑函数一般为<span class="math inline">\(sigmoid\)</span>函数，其函数形式为：</p><p><span class="math display">\[sigmoid(z)=\frac{1}{1+e^{-z}}\]</span></p><p>其函数图像为：</p><center><img src="https://s2.loli.net/2023/11/15/4rRHc3iUGn2FDKx.jpg" width="60%" height="60%"><div data-align="center">Image1: sigmoid函数图像</div></center><p>  sigmoid函数的主要优点如下：</p><ul><li><strong>输出范围为(0,1):</strong>sigmoid函数的输出范围在0和1之间，这与概率的范围一致。这使得逻辑回归的输出可以被解释为属于某个类别的概率。</li><li><strong>可导性:</strong>sigmoid函数是可导的，这使得使用梯度下降等优化算法来最小化损失函数成为可能。梯度下降等优化方法对于机器学习模型的训练非常重要，而Sigmoid函数的可导性使得模型参数可以通过梯度下降等优化方法进行有效地更新。<br></li><li><strong>单调递增性:</strong>sigmoid函数是单调递增的，这意味着输入变量的增加必然导致输出的增加。这一特性有助于模型学习输入特征与输出概率之间的关系，使得模型更容易收敛。</li><li><strong>数学上平滑:</strong>sigmoid函数的平滑性质有助于在优化过程中避免梯度爆炸或梯度消失的问题，这在深度学习等领域尤为重要。</li><li><strong>对异常值的鲁棒性:</strong>sigmoid函数在极端值上趋于饱和，对于一些异常值的影响相对较小。这有助于模型对于噪声或异常值的鲁棒性。</li></ul><p>  决策边界是指当我们通过逻辑函数得到概率值 <span class="math inline">\(p\)</span> 后，我们如何判别实例<span class="math inline">\(x\)</span>属于哪一个类别。当我们在面对的是二分类问题时，设<span class="math inline">\(y \in \{0,1\}\)</span> 表示实例<span class="math inline">\(x\)</span>的类别，概率值<span class="math inline">\(p\)</span>表示条件概率：</p><p><span class="math display">\[p = P(y=1 | x)\]</span></p><p>则我们选用的决策边界为：</p><p><span class="math display">\[\hat{y}= \left \{\begin{array}{rcl}1, &amp; {p &gt;0.5}\\0,&amp; {p \leq 0.5}\\\end{array} \right.\]</span></p><p>  其中，<span class="math inline">\(\hat{y}\)</span>为实例<span class="math inline">\(x\)</span>的预测类别。</p><h2 id="模型">模型</h2><p><strong>输入</strong></p><ul><li><strong>输入空间:</strong> <span class="math inline">\(\mathcal{X}\in \mathbb{R}^{n}\)</span>.<br></li><li><strong>输入实例:</strong> <span class="math inline">\(x =\begin{bmatrix}  x^{(1)},x^{(2)},\dots,x^{(n)} \\ \end{bmatrix}^T \in\mathcal{X}\)</span>.</li></ul><p>  其中，输入空间<span class="math inline">\(\mathcal{X}\)</span>为<span class="math inline">\(n\)</span>维实数空间的子集，输入实例<span class="math inline">\(x\)</span>为<span class="math inline">\(n\)</span>维特征向量。</p><p><strong>输出</strong><br>  由于我们只考虑二分类问题，因此实例点的类别只有正类与负类两种。</p><ul><li><strong>输出空间:</strong> <span class="math inline">\(\mathcal{Y} =\{ 0,1 \}\)</span><br></li><li><strong>输出实例:</strong> <span class="math inline">\(y \in\mathcal{Y}\)</span>.</li></ul><p>  其中，输出空间<span class="math inline">\(\mathcal{Y}\)</span>为只包含1，0两个元素的集合，1与0分别代表二分类问题中的正类与负类。输出实例<span class="math inline">\(y\)</span>代表相对应的输出实例<span class="math inline">\(x\)</span>的类别。</p><p><strong>数据集</strong><br>  逻辑回归的训练数据集<span class="math inline">\(T_{train}\)</span>为：</p><p><span class="math display">\[T_{train}=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N) \}\]</span></p><p>  其中，<span class="math inline">\(x_i \in\mathcal{X}=\mathbb{R}^{n},y_i \in\mathcal{Y}=\{0,1\},i=1,2,\dots,N\)</span>，<span class="math inline">\(N\)</span>表示训练数据的样本容量。</p><p><strong>模型</strong><br>  逻辑回归的模型形式表现为条件概率分布：</p><p><span class="math display">\[\begin{split}    P(Y=1|X) &amp;= \frac{1}{1+e^{-(w^{T}x+b)}} \\    P(Y=0|X) &amp;= \frac{e^{-(w^{T}x+b)}}{1+e^{-(w^{T}x+b)}}\end{split}\]</span></p>]]></content>
    
    
    <summary type="html">本节主要介绍逻辑回归模型。</summary>
    
    
    
    <category term="机器学习" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>机器学习-4.交叉熵与KL散度</title>
    <link href="http://example.com/2023/11/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-4-%E4%BA%A4%E5%8F%89%E7%86%B5%E4%B8%8EKL%E6%95%A3%E5%BA%A6/"/>
    <id>http://example.com/2023/11/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-4-%E4%BA%A4%E5%8F%89%E7%86%B5%E4%B8%8EKL%E6%95%A3%E5%BA%A6/</id>
    <published>2023-11-12T03:06:15.000Z</published>
    <updated>2023-11-14T12:55:44.553Z</updated>
    
    <content type="html"><![CDATA[<h1 id="交叉熵与kl散度">交叉熵与KL散度</h1><p>  在机器学习中，我们经常使用信息熵、交叉熵、KL散度等概率，例如在决策树中，我们使用基于信息熵的信息增益来构造树形结构；而交叉熵常被用于分类问题的损失函数；KL散度则被用于衡量两个分布之间的差异。本文将会介绍这些常用的概率，以便于我们今后学习相应的机器学习模型。</p><h2 id="信息熵-entropy">信息熵 (entropy)</h2><h3 id="熵">熵</h3><p>  在现实中，我们会接触到各种各样的信息，如何对信息进行量化便成为了一个重要的问题。信息论是应用数学的一个分支，由美国数学家香农提出并发展壮大，主要研究的是对一个事件包含信息的多少进行量化。<br>  信息论的基本思想是一个小概率事件发生了，要比大概率事件发生，提供的信息更多。在信息论中，我们认为事件的信息量具有以下三条性质：</p><ul><li>大概率事件所包含的信息量较小。<br></li><li>小概率事件所包含的信息量较大。</li><li>独立事件的信息量可以进行累加。</li></ul><p>  由以上三条性质，我们定义了某一事件<span class="math inline">\(X=x\)</span>的<strong>自信息量</strong>(self-information)为:</p><p><span class="math display">\[I(x)=-\log{P(x)}\]</span></p><p>  其中<span class="math inline">\(X\)</span>为随机变量，表示某一事件；<span class="math inline">\(x\)</span>为随机变量<span class="math inline">\(X\)</span>的取值。当上式中的<span class="math inline">\(\log\)</span>以2为底数时，<span class="math inline">\(I(x)\)</span>的单位是比特(bit)或者香农(shannons)；当<span class="math inline">\(\log\)</span>以2为底数时，<span class="math inline">\(I(x)\)</span>单位是奈特(nats)。这两个单位之间可以通过对数换底公式相互转换。<br>  自信息量表示单个事件的信息量。若我们已知事件<span class="math inline">\(X\)</span>服从某一概率分布<span class="math inline">\(P(X)\)</span>，我们可以使用<strong>香农熵</strong>(Shannonentropy)来对整个概率分布所包含的信息总量进行量化：</p><p><span class="math display">\[H(X)=\mathbb{E}_{X \simP}[I(x)]\]</span></p><p>  若随机变量<span class="math inline">\(X\)</span>为离散型随机变量，则熵可以写为求和形式：</p><p><span class="math display">\[H(X)=\sum_{i=1}^{N}P(x_i)I(x_i)=-\sum_{i=1}^{N}P(x_i)\log{P(x_i)}\]</span></p><p>  若随机变量<span class="math inline">\(X\)</span>为连续型随机变量，则熵可以写为积分形式：</p><p><span class="math display">\[H(X)=\int_{\mathcal{X}}P(x)I(x)dx=-\int_{\mathcal{X}}P(x)\log{P(x)}dx\]</span></p><h3 id="联合熵">联合熵</h3><p>  若有两个随机变量<span class="math inline">\(X,Y\)</span>，且服从某个联合分布<span class="math inline">\(P(X,Y)\)</span>，我们可以使用联合熵来对联合概率分布所包含的信息量进行量化：</p><p><span class="math display">\[H(X,Y)=-\sum_{x \in \mathcal{X}}\sum_{y\in \mathcal{Y}}P(x,y)\log{P(x,y)}\]</span></p><p>  以上给出的是<span class="math inline">\(X,Y\)</span>为离散型随机变量的联合熵，若<span class="math inline">\(X,Y\)</span>为连续型随机变量，则依照熵的形式，联合熵也可以写成积分形式：</p><p><span class="math display">\[H(X,Y)=-\int_{\mathcal{X}}\int_{\mathcal{Y}}P(x,y)\log{P(x,y)}dydx\]</span></p><h3 id="条件熵">条件熵</h3><p>  在数理统计中，我们还学习了条件概率分布，表示在某个事件发生后，另一个事件所发生的概率，在信息论中，用条件熵来表示条件概率分布所包含的信息。若有两个离散随机变量<span class="math inline">\(X,Y\)</span>，且已知联合概率分布<span class="math inline">\(P(X,Y)\)</span>，条件概率分布<span class="math inline">\(P(Y|X)\)</span>，则该条件分布的条件熵为：</p><p><span class="math display">\[H(Y|X)=-\sum_{x \in \mathcal{X}}\sum_{y\in \mathcal{Y}}P(x,y)\log{P(y|x)}\]</span></p><p>  当<span class="math inline">\(X,Y\)</span>为连续型随机变量时，上式可以写出积分形式：</p><p><span class="math display">\[H(Y|X)=-\int_{\mathcal{X}}\int_{\mathcal{Y}}P(x,y)\log{P(y|x)dydx}\]</span></p><h3 id="最大熵思想">最大熵思想</h3><p>  既然信息熵可以用来表示信息量的大小，人们自然希望找到包含信息量最大的概率分布。当我们的概率分布中存在未知参数时，可以使用<strong>最大化分布的熵</strong>的思想来估计未知参数，这类方法在机器学习中被称为最大熵学习，这里不展开讨论最大熵学习，有兴趣的读者可查阅相关资料进行了解。<br>  通过最大熵思想，我们有一个非常有意思的发现：<strong>若有定义在整个实数轴上的随机变量<span class="math inline">\(X\)</span>，其均值为<span class="math inline">\(\mu\)</span>，方差为<span class="math inline">\(\sigma^{2}\)</span>，当<span class="math inline">\(X \sim N(\mu,\sigma^2)\)</span>时，熵 <span class="math inline">\(H(X)\)</span> 最大.</strong>这个结论的证明放在目录，有兴趣的读者可自行阅读。高斯分布具有最大的信息熵，这也是为什么在现实生活中大量随机事件都服从高斯分布，因为自然界总是偏向于制造最大的不确定性，从而包含最多的信息，即最大的熵。</p><h2 id="交叉熵cross-entropy">交叉熵(cross entropy)</h2><p>  在机器学习中，我们常常需要比较两个分布之间的相似程度，例如当我们用一个估计的分布去近似真实分布时，我们自然希望这两个分布越相似越好。交叉熵便是衡量两个分布之间相似程度的一种度量方法，因此经常被用作机器学习模型的损失函数。<br>  为了理解交叉熵为什么能够度量两个分布的相似程度，我们借助贝叶斯统计的思想来进行解释。根据贝叶斯思想，对于某一个事件<span class="math inline">\(X\)</span>，我们会有一个先验的认知，即<span class="math inline">\(X\)</span>的先验分布<span class="math inline">\(P_0(x)\)</span>，在先验认知下，事件<span class="math inline">\(X\)</span>带给我们的信息量为：</p><p><span class="math display">\[I_{0}(x)=-\log{P_{0}(x)}\]</span></p><p>  假设事件<span class="math inline">\(X\)</span>的真实分布为<span class="math inline">\(P_{1}(x)\)</span>，则<strong>我们在主观认知下，通过客观事实所得到的事件<span class="math inline">\(X\)</span>的概率分布所包含的信息总量</strong>为：</p><p><span class="math display">\[H(P_0,P_1)=-\sum_{i=1}^{N}P_{1}(x)\log{P_{0}(x)}dx\]</span></p><p>  当<span class="math inline">\(X\)</span>为连续型随机变量时，其积分形式为：</p><p><span class="math display">\[H(P_{0},P_{1})=-\int_{\mathcal{X}}P_{1}(x)\log{P_{0}(x)}dx\]</span></p><p>  上式即为分布<span class="math inline">\(P_0,P_1\)</span>的<strong>交叉熵</strong>。若交叉熵较大，说明在已有主观先验认知下，事件<span class="math inline">\(X\)</span>的实际情况带给我们的信息量较大，说明我们的先验认知与实际情况差别较大，即<span class="math inline">\(P_{0}(x)\)</span>与<span class="math inline">\(P_{1}(x)\)</span>的相似度较低；若交叉熵较小，说明在已有主观先验认知下，事件<span class="math inline">\(X\)</span>的实际情况带给我们的信息量较小，说明我们的先验认知与实际情况差别较小，即<span class="math inline">\(P_{0}(x)\)</span>与<span class="math inline">\(P_{1}(x)\)</span>的相似度较高。<br>  在机器学习中，我们希望学习到的概率分布<span class="math inline">\(P_{m}\)</span>与训练数据所估计的真实分布<span class="math inline">\(P_{t}\)</span>足够相似，这时我们通常将分布<span class="math inline">\(P_{m}\)</span>与<span class="math inline">\(P_{t}\)</span>的交叉熵作为损失函数，例如逻辑回归模型，通过最小化交叉熵来调整<span class="math inline">\(P_{m}\)</span>，使得<span class="math inline">\(P_{m}\)</span>与真实分布<span class="math inline">\(P_{t}\)</span>足够相似。<br>  另外，有一个非常有意思的结论，<strong>最小化交叉熵实际上是等价于极大似然估计</strong>，这个结论的证明将会放在附录。</p><h2 id="kl散度kl-divergence">KL散度(KL Divergence)</h2><p>  上文介绍了交叉熵，其可以用来衡量两个分布的相似程度。但交叉熵存在一个问题，即若我们主观的先验认知与客观实际完全一致，即<span class="math inline">\(P_{0}=P_{1}\)</span>，此时我们实际上并没有得到任何信息，但交叉熵的计算结果为<span class="math inline">\(P_{0}\)</span>的信息熵，并不为<span class="math inline">\(0\)</span>。因此，我们可以转而考虑信息的增量，KL散度实际上就时在考虑信息熵的增量，我们先给出两个分布<span class="math inline">\(P_0,P_1\)</span>的KL散度的计算公式：</p><p><span class="math display">\[KL(P_1||P_0)=\mathbb{E}_{X \simP_1}[\log{\frac{P_{1}(x)}{P_{0}(x)}}]\]</span></p><p>  通过将上式进行调整，我们可以将<span class="math inline">\(P_{0}\)</span>与<span class="math inline">\(P_{1}\)</span>的KL散度写成<span class="math inline">\(P_{0}\)</span>与<span class="math inline">\(P_{1}\)</span>的交叉熵<span class="math inline">\(H(P_{0},P_{1})\)</span>与<span class="math inline">\(P_{0}\)</span>的信息熵之差：</p><p><span class="math display">\[KL(P_{1}||P_{0})=H(P_{0},P_{1})-H(P_{0})\]</span></p><p>  通过上式我们可以发现，当<span class="math inline">\(P_1\)</span>与<span class="math inline">\(P_0\)</span>的KL散度较大，说明在已有主观认知下，我们从客观事件获得信息增量较大，说明我们的主观认知与客观现实不一致，即<span class="math inline">\(P_1\)</span>与<span class="math inline">\(P_0\)</span>的相似度较低；当<span class="math inline">\(P_1\)</span>与<span class="math inline">\(P_0\)</span>的KL散度较小，说明在已有主观认知下，我们从客观事件获得信息增量较小，说明我们的主观认知与客观现实较为一致，即<span class="math inline">\(P_1\)</span>与<span class="math inline">\(P_0\)</span>的相似度较大；当<span class="math inline">\(P_1\)</span>与<span class="math inline">\(P_0\)</span>的KL散度等于0时，说明已有主观认知下，我们从客观事件中没有获得额外的信息，说明我们的主观认知与客观现实完全一致，即<span class="math inline">\(P_0=P_1\)</span>.<br>  KL散度衡量了一种信息增益，因此也被称为<strong>相对熵</strong>。在机器学习中我们同样可以使用KL散度作为损失函数。</p><h2 id="附录">附录</h2><h3 id="一-高斯分布具有最大的信息熵">(一) 高斯分布具有最大的信息熵</h3><p><strong>结论:</strong> 已知定义在整个实数轴上的连续型随机变量<span class="math inline">\(X\)</span>的均值为<span class="math inline">\(\mu\)</span>，方差为<span class="math inline">\(\sigma^{2}\)</span>，当<span class="math inline">\(X \sim N(\mu,\sigma^{2})\)</span>，<span class="math inline">\(X\)</span>的信息熵<span class="math inline">\(H(X)\)</span>最大。<br><strong>证明:</strong><br>  设随机变量<span class="math inline">\(X\)</span>的概率密度函数为<span class="math inline">\(p(x)\)</span>，由题意可知：</p><p><span class="math display">\[E(X)=\int_{-\infty}^{+\infty}xp(x)dx=\mu,\quadVar(X)=\int_{-\infty}^{+\infty}(x-\mu)^{2}p(x)dx=\sigma^{2}\]</span></p><p>  根据最大熵思想，可以得到如下一个带约束的优化问题：</p><p><span class="math display">\[\begin{split}    \max_{p(x)} \quad &amp;H(X)=-\int_{-\infty}^{+\infty}p(x)\ln{p(x)}dx  \\    s.t. \quad &amp; \int_{-\infty}^{+\infty}p(x)=1 \\    &amp; \int_{-\infty}^{+\infty}xp(x)dx=\mu  \\    &amp; \int_{-\infty}^{+\infty}(x-\mu)^{2}p(x)dx=\sigma^{2}\end{split}\]</span></p><p>  上述原问题的拉格朗日函数为：</p><p><span class="math display">\[\begin{split}    Q(p(x),\lambda_1,\lambda_2,\lambda_3) =&amp;-\int_{-\infty}^{+\infty}p(x)\ln{p(x)}dx+\lambda_1 \left(\int_{-\infty}^{+\infty}p(x)-1 \right) \\    &amp;+ \lambda_2 \left( \int_{-\infty}^{+\infty}xp(x)dx-\mu\right)+\lambda_3 \left(\int_{-\infty}^{+\infty}(x-\mu)^{2}p(x)dx-\sigma^{2} \right)\end{split}\]</span></p><p>  令<span class="math inline">\(\lambda=\begin{bmatrix}  \lambda_1,\lambda_2,\lambda_3\end{bmatrix}^{T}\)</span>，则原问题的无约束形式可以写为</p><p><span class="math display">\[\begin{split}     \max_{p(x)}\min_{\lambda} \quad &amp;Q(p(x),\lambda_1,\lambda_2,\lambda_3)  \\     \space s.t. \quad &amp; \lambda \ge 0\end{split}\]</span></p><p>  则原问题的对偶问题为：</p><p><span class="math display">\[\begin{split}     \min_{\lambda}\max_{p(x)} \quad &amp;Q(p(x),\lambda_1,\lambda_2,\lambda_3)  \\     \space s.t. \quad &amp; \lambda \ge 0\end{split}\]</span></p><p>  设<span class="math inline">\(p^{*}(x)\)</span>为原问题最优解，<span class="math inline">\(\lambda^{*}\)</span>为对偶问题最优解，由KKT条件得：</p><p><span class="math display">\[\frac{\partial Q}{\partial p(x)}|_{p(x)=p^{*}(x)}=-[\ln{p^{*}(x)+1}]+\lambda_1^{*}+\lambda_2^{*}x+\lambda_3^{*}(x-\mu)^{2}=0\]</span></p><p><span class="math display">\[\Rightarrowp^{*}(x)=e^{\lambda_1^{*}-1+\lambda_2^{*}x+\lambda_3^{*}(x-\mu)^{2}}\]</span></p><p>  之后对<span class="math inline">\(p^{*}(x)\)</span>做一些变形：</p><p><span class="math display">\[\begin{split}    p^{*}(x) &amp;=e^{\lambda_1^{*}-1+\lambda_2^{*}x+\lambda_3^{*}(x-\mu)^{2}}=e^{\lambda_1^{*}-1}e^{\lambda_3^{*}x^2+(\lambda_2^{*}-2\mu\lambda_3^{*})x+\mu^{2}\lambda_3^{*}}\\    &amp;=Ce^{\lambda_3^{*}[x^2+(\frac{\lambda_2^{*}}{\lambda_3^{*}}-2\mu)x+\mu^2]}= Ce^{\lambda_3^{*}[x-(\mu-\frac{\lambda_2^{*}}{2\lambda_3^{*}})]^{2}}\end{split}\]</span></p><p>  由密度函数<span class="math inline">\(p^{*}(x)\)</span>的非负性以及正则性可知：<span class="math inline">\(C&gt;0,\lambda_3^{*}&lt;0\)</span>；指数部分中的二次函数<span class="math inline">\([x-(\mu-\frac{\lambda_2^{*}}{2\lambda_3^{*}})]^{2}\)</span>表明 <span class="math inline">\(p^{*}(x)\)</span>的对称轴为 <span class="math inline">\(\mu-\frac{\lambda_2^{*}}{2\lambda_3^{*}}\)</span>，由于对称的密度函数，其对称轴一定等于均值可知：<span class="math inline">\(\lambda_2^{*}=0\)</span>。<br>  由于<span class="math inline">\(\lambda_3^{*}&gt;0\)</span>，可设<span class="math inline">\(\lambda_3^{*}=-\beta\)</span>，则<span class="math inline">\(p^{*}(x)\)</span>可化简为：</p><p><span class="math display">\[p^{*}(x)=Ce^{-\beta(x-\mu)^{2}}\]</span></p><p>  将<span class="math inline">\(p^{*}(x)\)</span>代入正则化约束得：</p><p><span class="math display">\[\begin{split}    1 &amp;=\int_{-\infty}^{+\infty}p^{*}(x)dx=C\int_{-\infty}^{+\infty}e^{-\beta(x-\mu)^{2}}dx\\    &amp;=C\int_{-\infty}^{+\infty}e^{-\betay^{2}}dy=\frac{C}{\sqrt{\beta}}\int_{0}^{+\infty}z^{-\frac{1}{2}}e^{-z}dz \\    &amp;=\frac{C}{\sqrt{\beta}}\Gamma \left( \frac{1}{2} \right) =C\sqrt{\frac{\pi}{\beta}}\end{split}\]</span></p><p>  从而得：<span class="math inline">\(C=\sqrt{\frac{\beta}{\pi}}\)</span>，再利用方差约束条件得：</p><p><span class="math display">\[\begin{split}    \sigma^{2} &amp;= \int_{-\infty}^{+\infty}(x-\mu)^2p^{*}(x)dx =C\int_{-\infty}^{+\infty}(x-\mu)^{2}e^{-\beta(x-\mu)^{2}}dx  \\    &amp;=2C\int_{0}^{+\infty}y^{2}e^{-\beta  y^{2}}dx = \frac{C}{\beta\sqrt{\beta}} \int_{0}^{+\infty} z^{\frac{1}{2}}e^{-z}dz \\    &amp;= \frac{C}{\beta \sqrt{\beta}} \Gamma \left( \frac{3}{2}\right) = \sqrt{\frac{\beta}{\pi}} \cdot \frac{1}{\beta \sqrt{\beta}}\cdot \frac{\sqrt{\pi}}{2}= \frac{1}{2\beta}\end{split}\]</span></p><p>  从而得：<span class="math inline">\(\beta =\frac{1}{2\sigma^{2}}\)</span>，联立：</p><p><span class="math display">\[\left \{\begin{array}{l}p^{*}(x)=Ce^{-\beta(x-\mu)^{2}}  \\C=\sqrt{\frac{\beta}{\pi}}  \\\beta = \frac{1}{2\sigma^{2}} \\\end{array} \right.\]</span></p><p>  解得：</p><p><span class="math display">\[p^{*}(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}}\]</span></p><p>  即当<span class="math inline">\(X \simN(\mu,\sigma^{2})\)</span>时，熵<span class="math inline">\(H(X)\)</span>最大，证毕.</p><h3 id="二-最小化交叉熵等价于极大似然估计">(二)最小化交叉熵等价于极大似然估计</h3><p><strong>结论：</strong> 假设我们从训练数据集<span class="math inline">\(T_{train}\)</span>中所得到的数据的经验分布为<span class="math inline">\(P_{t}(x)=\frac{1}{N}\)</span>(最大熵思想)，<span class="math inline">\(N\)</span>为训练数据的样本容量，我们所需要学习的模型为<span class="math inline">\(P_{m}(x;\theta)\)</span>，则有：</p><p><span class="math display">\[\min_{\theta}H(P_{t}(x),P_{m}(x;\theta)) \Leftrightarrow \max_{\theta} \prod_{x \inT} P_{m}(x;\theta)\]</span></p><p><strong>证明:</strong></p><p><span class="math display">\[\begin{split}    \max_{\theta} \prod_{x \in T} P_{m}(x;\theta) &amp; \Leftrightarrow\max_{\theta} \sum_{x \in T} \log{P_{m}(x;\theta)} \Leftrightarrow-\min_{\theta} \sum_{x \in T} \log{P_{m}(x;\theta)} \\    &amp; \Leftrightarrow -\min_{\theta} \sum_{i=1}^{N}\frac{1}{N}\log{P_{m}(x;\theta)} \Leftrightarrow \min_{\theta}\mathbb{E}_{X \sim P_{t}}[-\log{P_{m}(x;\theta)}]  \\    &amp; \Leftrightarrow \min_{\theta} H(P_{t}(x),P_{m}(x;\theta))\end{split}\]</span></p><p>  证毕.</p><h2 id="参考">参考</h2><p><strong>[1] Book:董平,机器学习中的统计思维(Python实现)</strong><br><strong>[2]Blog：知乎,康斯坦丁,一篇文章讲清楚交叉熵和KL散度</strong></p>]]></content>
    
    
    <summary type="html">本节主要介绍信息论中的熵、交叉熵以及KL散度的概念。</summary>
    
    
    
    <category term="机器学习" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>深度学习-2-Paper:生成对抗网络GANs——深度学习二十年间最酷的idea!</title>
    <link href="http://example.com/2023/11/06/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-2-Paper-%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9CGANs%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8D%81%E5%B9%B4%E9%97%B4%E6%9C%80%E9%85%B7%E7%9A%84idea/"/>
    <id>http://example.com/2023/11/06/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-2-Paper-%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9CGANs%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8D%81%E5%B9%B4%E9%97%B4%E6%9C%80%E9%85%B7%E7%9A%84idea/</id>
    <published>2023-11-06T12:27:07.000Z</published>
    <updated>2023-11-14T12:55:56.229Z</updated>
    
    <content type="html"><![CDATA[<h1 id="generative-adversarial-nets">Generative Adversarial Nets</h1><p>  生成对抗网络(GANs)是一种深度学习框架，由 lan Goodfellow和他的同事们于2014年提出，论文成果发表于人工智能顶会NIPS(NeuralInformation ProcessingSystems)。GANs被认为是深度学习领域的一项重大突破，其应用涵盖图像生成、图像修复、语音和文本合成、风格迁移和艺术创作、欺骗检测等多个领域。图灵奖得主，被誉为深度学习三巨头之一的Yann LeCunn 称赞GANs为 "the coolest idea in deep learning in the last 20years."。接下来，我们就来一起欣赏一下深度学习这二十年间最酷的idea！</p><p>  论文链接：https://arxiv.org/abs/1406.2661</p><h2 id="摘要">摘要</h2><p>  图1是 lan Goodfellow 在GANs的原始论文中所写的摘要全文：</p><center><img src="https://s2.loli.net/2023/11/06/fzbnJkFKo4cqOug.png" width="80%" height="80%"><div data-align="center">Image1: The Abstract of GANs</div></center><p>  摘要显示这篇论文提出了一种全新的基于对抗过程的生成模型框架。在这个框架中，存在着两个基于神经网络的模型：生成模型<span class="math inline">\(G\)</span>与判别模型<span class="math inline">\(D\)</span>。生成模型<span class="math inline">\(G\)</span>的作用是估计数据的真实分布；判别模型<span class="math inline">\(D\)</span>是用于判断所输入的样本来自于真实数据而非<span class="math inline">\(G\)</span>所生成的概率。生成模型<span class="math inline">\(G\)</span>的训练过程是最大化判别模型<span class="math inline">\(D\)</span>犯错的概率；判别模型<span class="math inline">\(D\)</span>的训练过程是最小化<span class="math inline">\(D\)</span>犯错的概率，整个GANs的训练过程可视为Minimax的二元博弈过程。通过理论分析发现，存在一个唯一的最优解，使得生成模型<span class="math inline">\(G\)</span>能够正确模拟训练数据的分布，同时判别模型所给出的概率几乎处处为<span class="math inline">\(\frac{1}{2}\)</span>，即几乎不能分辨所输入的样本是来自于真实数据分布还是生成模型<span class="math inline">\(G\)</span>。</p><h2 id="背景及主要思想">背景及主要思想</h2><h3 id="背景">背景</h3><p>  深度学习的目标是构建模型来表示在人工智能的应用中遇到的数据的概率分布，例如图像、音频、自然语言的语料等。在2014年之前，判别模型在这方面占据着主导，这些模型通常是利用反向传播算法、Dropout、ReLU等技术，直接学习一个从高维特征空间到实例类别的映射。与判别模型相比，生成模型的发展则有些相形见绌。这一方面是由于在与最大似然估计相关的策略中，有许多难以解决的概率计算问题；另一方面，判别模型在NLP任务中也难以利用分段线性单元的优势。</p><h3 id="主要思想">主要思想</h3><p>  本文的作者表示他们所提出的新的生成模型能够避开这些困难。生成对抗网络的主要思想可以概括为两个字——“对抗”，具体而言，在GANs框架的训练过程中，生成模型<span class="math inline">\(G\)</span>的训练目标是最大化判别模型犯错的概率，即希望由生成模型<span class="math inline">\(G\)</span>所生成的样本能够成功“骗过”判别模型；判别模型<span class="math inline">\(D\)</span>的训练目标是最小化自身犯错的概率，即希望判别模型的“鉴伪”能力越高越好。当我们将生成模型<span class="math inline">\(G\)</span>所生成的样本用于训练判别模型<span class="math inline">\(D\)</span>，并交替着训练这两个模型，便会引发这两个模型之间的“对抗”，它们为了达到自身的训练目标便会在对抗中提升各自的性能。最终，通过若干次训练，我们能得到一个性能非常好的生成模型<span class="math inline">\(G\)</span>，它所生成的样本与真实样本十分接近，以至于判别模型无法在所给的的参数量下分辨二者的区别，即生成模型所生成的样本几乎能够反映真实的数据分布。<br>  在GANs的论文中，lan Goodfellow用一个十分形象的比喻来说明生成对抗网络的基本思想。假设我们的目标是能够制造足够逼真的假钞，我们只需要找来两个队伍，一方是制造假钞的犯罪集团，另一方是警察队伍，犯罪集团的目标是制造假钞并在不被发现的情况下使用假钞；警察队伍的目标是鉴别假钞。这样，我们并不需要做太多的事情，只需要将这两方放在一起，让他们彼此对抗。在对抗的过程中，警察队伍鉴别假钞的能力会越来越强，犯罪集团所制造的假钞也会越来越逼真，最终我们便能得到足够逼真的假钞。</p><h2 id="模型构成">模型构成</h2><p>  在论文中，为了让生成模型<span class="math inline">\(G\)</span>能够学习到真实数据(训练数据)的分布<span class="math inline">\(p_{data}\)</span>，作者首先定义了输入噪声变量的先验分布<span class="math inline">\(p_{z}(z)\)</span>；生成模型<span class="math inline">\(G(z;\theta_{g})\)</span>由参数为<span class="math inline">\(\theta_{g}\)</span>的神经网络定义，其作用是将噪声变量<span class="math inline">\(z\)</span>映射到数据空间<span class="math inline">\(\mathcal{X}\)</span>:</p><p><span class="math display">\[G(z;\theta_{g}): z \rightarrowx\]</span></p><p>  由生成模型<span class="math inline">\(G(z;\theta_{g})\)</span>生成的数据<span class="math inline">\(x\)</span>的概率分布为<span class="math inline">\(p_{g}\)</span>。同时，作者定义了判别模型<span class="math inline">\(D(x;\theta_{d})\)</span>，判别模型<span class="math inline">\(D(x;\theta_{d})\)</span>由参数为<span class="math inline">\(\theta_{d}\)</span>的神经网络构成，其作用是给出输入数据<span class="math inline">\(x\)</span>是来自于真实数据分布<span class="math inline">\(p_{data}\)</span>而非生成模型<span class="math inline">\(G\)</span>的概率<span class="math inline">\(p\)</span>：</p><p><span class="math display">\[D(x;\theta_{d}): x \rightarrowp\]</span></p><p>  根据GANs的基本思想，在模型的训练过程中，生成模型<span class="math inline">\(G\)</span>的训练目标是最大化判别模型<span class="math inline">\(D\)</span>犯错的概率；判别模型<span class="math inline">\(D\)</span>的训练目标是最小化自身犯错的概率。其训练目标构成了一个minimax 的博弈过程。作者定义了训练的目标函数<span class="math inline">\(V(D,G)\)</span>，训练目标可以写为(1)式：</p><p><span class="math display">\[\begin{equation}\min_{G} \max_{D} V(D,G)=\mathbb{E}_{x \simp_{data}(x)}[\log{D(x)}]+\mathbb{E}_{z \sim p_{z}(z)}[\log{(1-D(G(z)))}]\end{equation}\]</span></p><p>  从目标函数可以得出，若固定<span class="math inline">\(G\)</span>，则判别模型<span class="math inline">\(D\)</span>为(2)式：</p><p><span class="math display">\[\begin{equation}    \begin{split}        D = \arg \max_{D} V(D,G) &amp;= \mathbb{E}_{x \simp_{data}(x)}[\log{D(x)}]+\mathbb{E}_{z \sim p_{z}(z)}[\log{(1-D(G(z)))}]\\        &amp;= \mathbb{E}_{x \sim p_{data}(x)}[\log{D(x)}]+\mathbb{E}_{x\sim p_{g}(x)}[\log{(1-D(x))}] \\    \end{split}\end{equation}\]</span></p><p>  根据(2)式，在训练过程中判别模型<span class="math inline">\(D\)</span>会调整参数<span class="math inline">\(\theta_{d}\)</span>，使得(2)式中的<span class="math inline">\(D(x)\)</span>较大，<span class="math inline">\(D(G(z))\)</span>较小，其含义是若判别模型<span class="math inline">\(D\)</span>的输入数据<span class="math inline">\(x\)</span>来自于真实的数据分布<span class="math inline">\(p_{data}\)</span>，则模型的输出概率值较大；若输入数据<span class="math inline">\(x\)</span>来自于生成模型定义的分布<span class="math inline">\(p_{g}\)</span>，则模型的输出概率值较小。(2)式实际上就是二分类问题中的交叉熵目标函数，通过(2)式的优化，可以得到一个分类性能更好的判别模型<span class="math inline">\(D\)</span>。<br>  若固定<span class="math inline">\(D\)</span>，则生成模型<span class="math inline">\(G\)</span>为(3)式：</p><p><span class="math display">\[\begin{equation}    \begin{split}        G &amp;= \arg \min_{G} V(D,G) = \mathbb{E}_{x \simp_{data}(x)}[\log{D(x)}]+\mathbb{E}_{z \sim p_{z}(z)}[\log{(1-D(G(z)))}]\\        &amp; \Leftrightarrow \arg \min_{G} \mathbb{E}_{z \simp_{z}(z)}[\log{(1-D(G(z)))}]=\mathbb{E}_{x \simp_{g}(x)}[\log{(1-D(x))}] \\    \end{split}\end{equation}\]</span></p><p>  根据(3)式，在训练过程中生成模型<span class="math inline">\(G\)</span>会调整参数<span class="math inline">\(\theta_{g}\)</span>，使得(3)式中的<span class="math inline">\(D(G(z))\)</span>较大，其含义是将从生成模型<span class="math inline">\(G\)</span>所定义的分布<span class="math inline">\(p_{g}\)</span>中给出的数据<span class="math inline">\(x\)</span>输入到判别模型<span class="math inline">\(D\)</span>中，判别模型输出的概率值较大，即判别模型误认为数据<span class="math inline">\(x\)</span>来自于真实的数据分布<span class="math inline">\(p_{data}\)</span>，这表明生成数据分布<span class="math inline">\(p_{g}\)</span>与真实数据分布<span class="math inline">\(p_{data}\)</span>足够相似，以至于当前的判别模型<span class="math inline">\(D\)</span>无法分辨这两个分布所产生的数据<span class="math inline">\(x\)</span>。</p><h2 id="优化算法">优化算法</h2><p>  lan Goodfellow 在GANs的原始论文中给出的目标函数的优化算法如下</p><center><img src="https://s2.loli.net/2023/11/07/TNJ4mGcanZewflz.png" width="80%" height="80%"><div data-align="center">Image2: 优化算法</div></center><p>  优化算法的基本思路是利用小批量随机梯度下降算法对目标函数进行优化。对于目标函数中的生成模型与判别模型，每次迭代时固定其中一个模型，利用SGD对另一个模型进行参数更新，彼此循环迭代，直至收敛。优化算法中需要特别注意的有以下几点：</p><ul><li>在刚开始迭代时，应该首先固定生成模型<span class="math inline">\(G\)</span>，对判别模型<span class="math inline">\(D\)</span>进行更新。这是因为在开始训练时，判别模型<span class="math inline">\(D\)</span>的参数是随机初始化的，其不具备对样本进行正确分类的能力，而判别模型分类的结果又会直接影响生成模型<span class="math inline">\(G\)</span>的训练，若首先更新生成模型<span class="math inline">\(G\)</span>，则生成模型一开始便有可能完全“骗过”判别模型，导致训练无法成功进行。<br></li><li>在训练过程中，每更新<span class="math inline">\(k\)</span>次判别模型<span class="math inline">\(D\)</span>,再更新1次生成模型<span class="math inline">\(G\)</span>。这会使得只要生成模型<span class="math inline">\(G\)</span>变化得足够缓慢，判别模型<span class="math inline">\(D\)</span>就会维持再其最优解附近。</li><li>在实际对生成模型<span class="math inline">\(G\)</span>的更新中，并不使用(3)式中的最小化 <span class="math inline">\(\log{(1-D(x))}\)</span>，而是最大化 <span class="math inline">\(\log{D(x)}\)</span>。这是因为在开始训练时，判别模型<span class="math inline">\(D\)</span>要强于生成模型<span class="math inline">\(G\)</span>，使得<span class="math inline">\(D(x)(x\sim p_{g})\)</span>的值较小，此时 <span class="math inline">\(\log{(1-D(x))}\)</span> 对<span class="math inline">\(D(x)\)</span>的梯度很小，训练会非常缓慢；而 <span class="math inline">\(\log{D(x)}\)</span> 在<span class="math inline">\(D(x)\)</span>较小时，梯度较大，更有利于参数更新。</li></ul><h2 id="理论分析">理论分析</h2><p>  lanGoodfellow在论文中对目标函数的理论最优解以及算法收敛性进行了分析，得出了非常有意义的结果。</p><h3 id="总体最优解">总体最优解</h3><p>  <strong>结论</strong>: 当<span class="math inline">\(p_{g}=p_{data}\)</span>时，目标函数达到总体最优。<br>  <strong>证明</strong>:<br>  训练目标：</p><p><span class="math display">\[\min_{G} \max_{D} V(D,G)=\mathbb{E}_{x\sim p_{data}(x)}[\log{D(x)}]+\mathbb{E}_{z \simp_{z}(z)}[\log{(1-D(G(z)))}]\]</span></p><p>  当固定生成模型<span class="math inline">\(G\)</span>时，<span class="math inline">\(D^{*}=\arg \max_{D} V(D,G)\)</span>，其中:</p><p><span class="math display">\[\begin{split}    V(D,G) &amp;= \mathbb{E}_{x \sim p_{data}}[\log{D(x)}]+\mathbb{E}_{x\sim p_{g}}[\log{(1-D(x))}] \\    &amp;= \int_{x} p_{data}(x)\log{D(x)} dx + \int_{x}p_{g}(x)\log{(1-D(x))} dx \\    &amp;= \int_{x} [p_{data}(x)\log{D(x)}+p_{g}(x)\log{(1-D(x))}] dx\end{split}\]</span></p><p><span class="math display">\[\begin{split}    \max_{D} V(D,G) &amp;= \max_{D} \int_{x}[p_{data}(x)\log{D(x)}+p_{g}(x)\log{(1-D(x))}] dx \\    &amp; \Leftrightarrow \max_{D} \spacep_{data}(x)\log{D(x)}+p_{g}(x)\log{(1-D(x))} \triangleq L(D)\end{split} \]</span></p><p><span class="math display">\[\frac{\partial L(D)}{\partialD}=\frac{p_{data}(x)}{D(x)}-\frac{p_{g}(x)}{1-D(x)}=0 \RightarrowD^{*}(x)=\frac{p_{data}(x)}{p_{data}(x)+p_{g}(x)}\]</span></p><p><span class="math display">\[\begin{split}    \max_{D} V(D,G) &amp;= V(D^{*},G) = \mathbb{E}_{x \simp_{data}}[\log{D^{*}(x)}]+\mathbb{E}_{x \sim p_{g}}[\log{(1-D^{*}(x))}]\\    &amp;= \int_x p_{data}(x)\log{\left(\frac{p_{data}(x)}{p_{data}(x)+p_{g}(x)} \right)} dx + \int_{x}p_{g}(x)\log{\left( \frac{p_{g}(x)}{p_{data}(x)+p_{g}(x)} \right)}dx \\    &amp;= -2\log{2}+\int_x p_{data}(x)\log{\left(\frac{p_{data}(x)}{(p_{data}(x)+p_{g}(x))/2} \right)} dx + \int_{x}p_{g}(x)\log{\left( \frac{p_{g}(x)}{(p_{data}(x)+p_{g}(x))/2}\right)}dx  \\    &amp;= -2\log{2}+ KL \left( p_{data}(x) ||\frac{p_{data}(x)+p_{g}(x)}{2} \right) + KL \left( p_{g}(x) ||\frac{p_{data}(x)+p_{g}(x)}{2} \right) \\    &amp;= -2\log{2}+2JS(p_{data}(x)||p_{g}(x))\end{split}\]</span></p><p>  完成对<span class="math inline">\(D\)</span>的最大化后，再对<span class="math inline">\(G\)</span>进行极小化：</p><p><span class="math display">\[\min_{G} \max_{D} V(D,G) \Leftrightarrow\min_{G} \space J(G)=-2\log{2}+2JS(p_{data}(x)||p_{g}(x))\]</span></p><p>  当 <span class="math inline">\(p_{g}(x)=p_{data}(x)\)</span>时，<span class="math inline">\(JS(p_{data}(x)||p_{g}(x))_{min}=0\)</span>，<span class="math inline">\(J(G)\)</span>达到最小，证毕。</p><h3 id="算法收敛性">算法收敛性</h3><p><strong>结论:</strong> 若生成模型<span class="math inline">\(G\)</span>和判别模型<span class="math inline">\(D\)</span>拥有足够的参数量，并且在上文优化算法的每一步中，在给定生成模型<span class="math inline">\(G\)</span>下，判别模型<span class="math inline">\(D\)</span>都达到了其最优解，从而有：</p><p><span class="math display">\[p_{g}= \arg\min_{p_{g}} \mathbb{E}_{x\sim p_{data}}[\log{D^{*}_{G}(x)}]+\mathbb{E}_{x \simp_{g}}[\log{(1-D^{*}_{G}(x))}]\]</span></p><p>利用上式对<span class="math inline">\(p_{g}\)</span>进行迭代更新，最终<span class="math inline">\(p_{g}\)</span>会收敛到<span class="math inline">\(p_{data}\)</span>.</p>]]></content>
    
    
    <summary type="html">本节主要介绍 Ian Goodfellow 于2014年提出的生成对抗网络模型框架。</summary>
    
    
    
    <category term="深度学习" scheme="http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>机器学习-3.硬间隔线性支持向量机</title>
    <link href="http://example.com/2023/10/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-3-%E7%A1%AC%E9%97%B4%E9%9A%94%E7%BA%BF%E6%80%A7%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    <id>http://example.com/2023/10/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-3-%E7%A1%AC%E9%97%B4%E9%9A%94%E7%BA%BF%E6%80%A7%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</id>
    <published>2023-10-24T12:45:55.000Z</published>
    <updated>2023-11-07T03:37:34.364Z</updated>
    
    <content type="html"><![CDATA[<h1 id="硬间隔线性支持向量机">硬间隔线性支持向量机</h1><p>  支持向量机(Support Vector Machine, SVM)是 <span class="math inline">\(Cortes\)</span>和<span class="math inline">\(Vapnik\)</span>于1995年在 <span class="math inline">\(Machine \space Learning\)</span>期刊上提出的分类模型，在自然语言处理、计算机视觉以及生物信息中有着重要的应用。支持向量机的模型形式与感知机相同，但有别于感知机模型的是，支持向量机学习通过添加约束条件，能够得到最优的分离超平面。支持向量机学习的目标是找到一个线性超平面，能够最大程度地分离训练数据集中不同类别的数据点，并且在分离过程中最大化间隔，即数据点与决策边界之间的距离最大化。<br>  根据处理的问题不同，支持向量机可大致分为硬间隔线性支持向量机、软间隔线性支持向量机，非线性支持向量机。本节主要介绍二分类问题下的硬间隔线性支持向量机。</p><h2 id="基本思想">基本思想</h2><p>  我们在之前已经介绍了感知机模型，它的模型形式是：</p><p><span class="math display">\[f(x) = sign(w^Tx+b)\]</span></p><p>  其中，<span class="math inline">\(x \in \mathbb{R}^n; w \in\mathbb{R}^n, b \in \mathbb{R}\)</span>为模型参数。通过设置初始参数，利用梯度下降算法，我们可以得到模型参数<span class="math inline">\(\hat{w},\hat{b}\)</span>.当我们设置不同的初始参数时，我们会得到不同的模型参数，即不同的分离超平面。现假设在二维情形下，我们得到了如图1所示的两个分离超平面<span class="math inline">\(S_1,S_2\)</span>。</p><center><img src="https://s2.loli.net/2023/10/24/Xe3R6PnIalTABcb.jpg" width="60%" height="60%"><div data-align="center">Image1: 感知机中分离超平面</div></center><p>  在感知机学习中，超平面<span class="math inline">\(S_1\)</span>与<span class="math inline">\(S_2\)</span>均可以将训练数据集完全正确分类，这两个分离超平面并没有优劣可分，实际上，感知机学习得到的分离超平面为无数个。<strong>现在的问题是：感知机学习得到的众多分离超平面中，哪一个分离超平面是最优的</strong>？为了回答这个问题，我们先来思考一下图1中分离超平面<span class="math inline">\(S_1\)</span>与<span class="math inline">\(S_2\)</span>哪一个更好。相信大多数读者在直觉上都会认为分离超平面<span class="math inline">\(S_2\)</span>要优于<span class="math inline">\(S_1\)</span>，这个结论是正确的。通过进一步地分析，我们可以发现，训练数据集中的实例点到分离超平面<span class="math inline">\(S_1\)</span>的最小距离要大于其到分离超平面<span class="math inline">\(S_2\)</span>的最小距离，这使得分离超平面<span class="math inline">\(S_1\)</span>的泛化能力要弱于分离超平面<span class="math inline">\(S_2\)</span>，一些轻微的噪声扰动便有可能使得实例点越过分离超平面<span class="math inline">\(S_1\)</span>，造成模型分类错误，如图1中的红色实例点所示，然而由于实例点距离分离超平面<span class="math inline">\(S_2\)</span>的距离相对较远，噪声扰动并不容易使得实例点越过超平面<span class="math inline">\(S_2\)</span>。因此，我们认为分离超平面<span class="math inline">\(S_2\)</span>要优于分离超平面<span class="math inline">\(S_1\)</span>。<br>  硬间隔支持向量机的基本思想便来源于以上的思考，其基本思想为：<strong>当训练数据集线性可分时，在特征空间中寻找一个超平面，其能够将训练数据中的实例点完全正确分类，同时最大程度远离训练数据中的示例点。</strong></p><h2 id="模型">模型</h2><p><strong>输入</strong></p><ul><li><strong>输入空间:</strong> <span class="math inline">\(\mathcal{X}\in \mathbb{R}^n\)</span>.<br></li><li><strong>输入实例:</strong> <span class="math inline">\(x =\begin{bmatrix}  x^{(1)},x^{(2)},\dots,x^{(n)} \\ \end{bmatrix}^T \in\mathcal{X}\)</span>.</li></ul><p>  其中，输入空间<span class="math inline">\(\mathcal{X}\)</span>为<span class="math inline">\(n\)</span>维实数空间的子集，输入实例<span class="math inline">\(x\)</span>为<span class="math inline">\(n\)</span>维特征向量。</p><p><strong>输出</strong><br>  由于我们只考虑二分类问题，因此实例点的类别只有正类与负类两种。</p><ul><li><strong>输出空间:</strong> <span class="math inline">\(\mathcal{Y} =\{ -1,+1 \}\)</span><br></li><li><strong>输出实例:</strong> <span class="math inline">\(y \in\mathcal{Y}\)</span>.</li></ul><p>  其中，输出空间<span class="math inline">\(\mathcal{Y}\)</span>为只包含+1，-1两个元素的集合，+1与-1分别代表二分类问题中的正类与负类。输出实例<span class="math inline">\(y\)</span>代表相对应的输出实例<span class="math inline">\(x\)</span>的类别。</p><p><strong>数据集</strong><br>  支持向量机的训练数据集<span class="math inline">\(T_{train}\)</span>为：</p><p><span class="math display">\[T_{train}=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N) \}\]</span></p><p>  其中，<span class="math inline">\(x_i \in\mathcal{X}=\mathbb{R}^{n},y_i \in\mathcal{Y}=\{+1,-1\},i=1,2,\dots,N\)</span>，<span class="math inline">\(N\)</span>表示训练数据的样本容量。<strong>需要注意的是，硬间隔线性支持向量机模型的前提假设为训练数据集<span class="math inline">\(T_{train}\)</span>是线性可分的</strong>。</p><p><strong>模型</strong><br>  支持向量机的模型形式与感知机相同，其模型形式为：</p><p><span class="math display">\[f(x) = sign(w^{T}x+b)\]</span></p><p>  其中，<span class="math inline">\(w\)</span>和<span class="math inline">\(b\)</span>为感知机模型的参数，<span class="math inline">\(w \in \mathbb{R}^{n}\)</span>为权重向量，<span class="math inline">\(b \in \mathbb{R}\)</span>为偏置。<span class="math inline">\(sign(\cdot)\)</span>为符号函数。硬间隔线性支持向量机为线性分类模型，属于判别模型。</p><p><strong>假设空间</strong><br>  硬间隔线性支持向量机模型的假设空间为：</p><p><span class="math display">\[\mathcal{H} = \{f \vert f(x)=w^{T}x+b\}\]</span></p><p>  与感知机模型相同，硬间隔线性支持向量机模型的假设空间实际上是特征空间中所有超平面的集合。</p><p><strong>参数空间</strong><br>  令<span class="math inline">\(\theta=(w,b)\)</span>，则模型的假设空间<span class="math inline">\(\mathcal{H}\)</span>等价于参数空间<span class="math inline">\(\Theta\)</span>：</p><p><span class="math display">\[\Theta=\{ \theta \vert \theta \in\mathbb{R}^{n+1} \}\]</span></p><h2 id="策略">策略</h2><p>  我们的目标是找到所有分离超平面中最优的一个超平面。首先，假设分离超平面为<span class="math inline">\(S=\{x \vert w^T x + b =0\}\)</span>，则实例点<span class="math inline">\(x_i\)</span>到超平面<span class="math inline">\(S\)</span>的距离为：</p><p><span class="math display">\[d(S,x_i)=\frac{|w^Tx_i+b|}{||w||}\]</span></p><p>  由于硬间隔线性支持向量机假设训练数据集是线性可分的，因此分离超平面<span class="math inline">\(S\)</span>能够将实例点完全正确分类。因此，当 <span class="math inline">\(w^Tx_i+b &gt; 0\)</span>时，有<span class="math inline">\(y_i=+1\)</span>；当<span class="math inline">\(w^Tx_i+b&lt;0\)</span>时，有<span class="math inline">\(y_i=-1\)</span>。则<span class="math inline">\(d(S,x_i)\)</span>同样可以表示为：</p><p><span class="math display">\[\gamma_i=\frac{y_i(w^Tx_i+b)}{||w||}\]</span></p><p>  我们称<span class="math inline">\(\gamma_i\)</span>为实例点<span class="math inline">\(x_i\)</span>到超平面<span class="math inline">\(S\)</span>的<strong>几何间隔</strong>。<br>  我们希望能够找到与训练数据集中的实例点距离最大的超平面，记 <span class="math inline">\(margin(T,S)\)</span> 表示数据集<span class="math inline">\(T\)</span>中的实例点到超平面<span class="math inline">\(S\)</span>的最小距离：</p><p><span class="math display">\[margin(T_{train},S)=\min_{i=1,\dots,N}d(S,x_i)=\min_{i=1,\dots,N} \gamma_i=\min_{i=1,\dots,N}\frac{y_i(w^Tx_i+b)}{||w||}\]</span></p><p>  我们使用 <span class="math inline">\(margin(T_{train},S)\)</span>来衡量训练数据集到超平面的距离，这种距离也称为<span class="math inline">\(hard-margin\)</span>，即硬间隔。由于训练数据集<span class="math inline">\(T_{train}\)</span>是给定的，因此 <span class="math inline">\(margin(T_{train},S)\)</span>实际上是由超平面的参数<span class="math inline">\(w,b\)</span>决定的，不同的分离超平面对应着不同的<span class="math inline">\(margin\)</span>距离，我们希望能够在所有分离超平面中找到与训练数据集距离最远的超平面，这个问题可以被描述为：</p><p><span class="math display">\[\max_{w,b}margin(T_{train},S)=\max_{w,b} \min_{i=1,\dots,N}\frac{y_i(w^Tx_i+b)}{||w||}\]</span></p><p>  综上所述，寻找最优分离超平面的优化问题可以描述为<strong>最小几何间隔最大化问题(1)</strong>：</p><p><span class="math display">\[\begin{equation}    \begin{split}    &amp; \max_{w,b} \min_{i=1,2,\dots,N} \quad\frac{1}{||w||}y_i(w^Tx_i+b) \\    &amp; \space s.t. \quad y_i(w^Tx_i+b) &gt; 0, \quad i=1,2,\dots,N\\    \end{split}\end{equation}\]</span></p><p>  其中称<span class="math inline">\(y_i(w^Tx_i+b)\)</span>为实例点<span class="math inline">\(x_i\)</span>到超平面<span class="math inline">\(S\)</span>的<strong>函数间隔</strong>；约束条件<span class="math inline">\(y_i(w^Tx_i+b) &gt;0\)</span>保证了该优化问题的可行解集为能够将训练数据集正确分类的超平面的集合。确定最优分离超平面需要找到最小几何间隔对应的实例点，这些实例点被称为<strong>支持向量</strong>。<br>  下面我们来对该优化问题做一些简化，通过分析我们可以得到：</p><p><span class="math display">\[\exists r &gt; 0, \space s.t. \space\min_{i=1,\dots,N} y_i(w^Tx_i+b)=r\]</span></p><p>  由于我们没有对<span class="math inline">\(w\)</span>的长度进行限制，则同一个超平面可能对应这不同的参数值<span class="math inline">\(w,b\)</span>，例如超平面<span class="math inline">\(S_1=\{x \vert w^Tx+b=0\}\)</span>与超平面<span class="math inline">\(S_2=\{x \vert(2w)^Tx+(2b)=0\}\)</span>在特征空间中表示同一个超平面。这样会造成，即使是同一个分离超平面，不同的参数值<span class="math inline">\(w,b\)</span>会对应不同的<span class="math inline">\(r\)</span>值。我们想要优化问题能够得到唯一一组确定的参数值<span class="math inline">\(w,b\)</span>，我们可以给定<span class="math inline">\(w\)</span>的模长，也可以固定<span class="math inline">\(r\)</span>值，为了简化优化问题，我们固定<span class="math inline">\(r=1\)</span>，此时优化问题(1)的约束条件可以转化为：</p><p><span class="math display">\[\min_{i=1,\dots,N} y_i(w^Tx_i+b)=1\Leftrightarrow y_i(w^Tx_i+b) \ge 1, i=1,2,\dots,N\]</span></p><p>  优化问题(1)中的目标函数可以转化为：</p><p><span class="math display">\[\max_{w,b} \min_{i=1,\dots,N}\frac{1}{||w||}y_i(w^Tx_i+b)=\max_{w,b}\frac{1}{||w||}\min_{i=1,\dots,N} y_i(w^Tx_i+b)=\max_{w,b} \frac{1}{||w||}\]</span></p><p>  再简最大化问题转化为常见的最小化问题：</p><p><span class="math display">\[\max_{w,b} \frac{1}{||w||}\Leftrightarrow \min_{w,b} ||w|| \Leftrightarrow \min_{w,b}\frac{1}{2}w^Tw\]</span></p><p>  综上所述，优化问题(1)可以转化为以下的优化问题(2)：</p><p><span class="math display">\[\begin{equation}    \begin{split}        &amp; \min_{w,b} \quad \frac{1}{2}w^Tw  \\        &amp; \space s.t. \quad 1-y_i(w^Tx_i+b) \leq 0, \quadi=1,2,\dots,N    \end{split}\end{equation}\]</span></p><p>  通过求解优化问题(2)，我们便可以得到最优的分离超平面参数<span class="math inline">\((w^{*},b^{*})\)</span>。可以证明在训练数据集线性可分的条件下，通过最小间隔最大化问题的求解，最优的分离超平面是唯一的。这部分证明将放在附录，有兴趣的读者可自行阅读。</p><center><img src="https://s2.loli.net/2023/10/25/DCZeJ5TOM93vtLS.jpg" width="60%" height="60%"><div data-align="center">Image2: 最优分离超平面</div></center><h2 id="算法">算法</h2><p>  通过前文的推导，我们得到的优化问题为问题(2)：</p><p><span class="math display">\[\begin{align*}        &amp; \min_{w,b} \frac{1}{2}w^Tw  \\        &amp; \space s.t. \space 1-y_i(w^Tx_i+b) \leq 0, \quadi=1,2,\dots,N \\    \end{align*}\]</span></p><p>  该优化问题为<strong>凸二次规划问题</strong>，有<span class="math inline">\(N\)</span>个约束条件。首先写出优化问题(2)的拉格朗日函数：</p><p><span class="math display">\[L(w,b,\lambda)=\frac{1}{2}w^Tw+\sum_{i=1}^{N}\lambda_i\left[ 1-y_i(w^Tx_i+b) \right]\]</span></p><p><span class="math display">\[\lambda = \begin{bmatrix}    \lambda_1,\lambda_2,\dots,\lambda_N\end{bmatrix}^T,\lambda_i \ge 0, i=1,2,\dots,N\]</span></p><p>  则优化问题(2)的无约束形式(3)为：</p><p><span class="math display">\[\begin{equation}    \begin{split}        &amp; \min_{w,b} \max_{\lambda} \quad L(w,b,\lambda)  \\        &amp; \space s.t. \quad \lambda_i \ge 0, \quad i=1,2,\dots,N \\    \end{split}\end{equation}\]</span></p><p>  则优化问题(2)的<strong>对偶问题</strong>(4)为：</p><p><span class="math display">\[\begin{equation}    \begin{split}        &amp; \max_{\lambda} \min_{w,b} \quad L(w,b,\lambda)  \\        &amp; \space s.t. \quad \lambda_i \ge 0, \quad i=1,2,\dots,N    \end{split}\end{equation}\]</span></p><p>  因为原始问题(2)为凸二次规划问题，其满足<strong>强对偶关系</strong>，即：</p><p><span class="math display">\[\min_{w,b} \max_{\lambda}L(w,b,\lambda)=\max_{\lambda} \max_{w,b} L(w,b,\lambda)\]</span></p><p>  首先来解决内部极大化问题：</p><p><span class="math display">\[\max_{w,b}L(w,b,\lambda)=\frac{1}{2}w^Tw+\sum_{i=1}^{N}\lambda_i \left[1-y_i(w^Tx_i+b) \right]\]</span></p><p>  由费马定理可得：</p><p><span class="math display">\[\left \{\begin{array}{lr}    \frac{\partial L(w,b,\lambda)}{\partial w}=0  \Rightarrow w =\sum_{i=1}^{N}\lambda_{i}y_ix_i  \\    \frac{\partial L(w,b,\lambda)}{\partial b}=0  \Rightarrow\sum_{i=1}^{N}\lambda_iy_i=0\end{array} \right.\]</span></p><p>  将其代入拉格朗日函数<span class="math inline">\(L(w,b,\lambda)\)</span>得：</p><p><span class="math display">\[\begin{align*}    L(w,b,\lambda) &amp;= \frac{1}{2} \left(\sum_{i=1}^{N}\lambda_{i}y_ix_i \right)^{T}\left(\sum_{i=1}^{N}\lambda_{i}y_ix_i \right)-\sum_{i=1}^{N}\lambda_iy_i\left( \sum_{i=1}^{N}\lambda_{i}y_ix_i\right)^{T}x_i-b\sum_{i=1}^{N}\lambda_iy_i+\sum_{i=1}^{N}\lambda_i  \\    &amp;=\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\lambda_i \lambda_jy_iy_j (x_i^{T}x_j)-\sum_{i=1}^{N}\sum_{j=1}^{N}\lambda_i \lambda_jy_iy_j (x_i^{T}x_j)+\sum_{i=1}^{N}\lambda_i  \\    &amp;= -\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\lambda_i \lambda_jy_iy_j (x_i^{T}x_j)+\sum_{i=1}^{N}\lambda_i  \\\end{align*}\]</span></p><p>  则对偶问题(4)可以化为以下优化问题(5)：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{\lambda} \quad &amp;\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\lambda_i \lambda_j y_iy_j(x_i^{T}x_j)-\sum_{i=1}^{N}\lambda_i \\        s.t. \quad &amp; \lambda_i \ge 0, \quad i=1,2,\space,N  \\        &amp; \sum_{i=1}^{N}\lambda_i y_i=0    \end{split}\end{equation}\]</span></p><p>  记 <span class="math inline">\(w^{*},b^{*}\)</span>为原始问题(2)的最优解，<span class="math inline">\(\lambda^{*}\)</span>为对偶问题(4)的最优解，其同时也是优化问题(5)的最优解。根据定理：<strong>原始问题及其对偶问题具备强对偶关系是原始问题与其对偶问题满足KKT条件的充要条件可知</strong>：</p><p><span class="math display">\[KKT条件:  \left \{\begin{array}{l}1-y_i(w^{*} \cdot x_i+b^{*}) \leq 0  \\\\\lambda_i^{*}(1-y_i(w^{*} \cdot x_i+b^{*}))=0 \\\\\frac{\partial L(w,b,\lambda^{*})}{\partial w}|_{w=w^{*}}=0  \Rightarrow w^{*}=\sum_{i=1}^{N}\lambda^{*}_{i}y_ix_i \\\\\frac{\partial L(w,b,\lambda^{*})}{\partial b} |_{b=b^{*}}=0 \Rightarrow\sum_{i=1}^{N}\lambda^{*}_{i}y_i=0 \\\\\lambda^{*} \ge 0 \\  \end{array} \right.\]</span></p><p>  其中，<span class="math inline">\(i=1,2,\dots,N\)</span>.以上关于凸二次规划、对偶关系、KKT条件等最优化知识会在附录中介绍，这里不多家阐述。<br>  由<span class="math inline">\(KKT\)</span>条件得到 <span class="math inline">\(w^{*}=\sum_{i=1}^{N}\lambda^{*}_{i}y_ix_i\)</span>，现在来思考<span class="math inline">\(b^{*}\)</span>如何用<span class="math inline">\(\lambda^{*}\)</span>来表示。由于 <span class="math inline">\(\lambda_i^{*}(1-y_i(w^{*} \cdotx_i+b^{*}))=0\)</span>，若 <span class="math inline">\(y_i(w^{*} \cdotx_i+b^{*}) &gt; 1\)</span>，则 <span class="math inline">\(\lambda_i^{*}=0\)</span>，即在图2中位于间隔边界<span class="math inline">\(H_1,H_2\)</span>两侧的实例点<span class="math inline">\(x_i\)</span>对确定最优超平面<span class="math inline">\(S\)</span>的参数<span class="math inline">\(w^{*},b^{*}\)</span>不起作用；若 <span class="math inline">\(y_i(w^{*} \cdot x_i+b^{*}) = 1\)</span>，则<span class="math inline">\(\lambda_i^{*}\)</span>不一定为0，即在图2中位于间隔边界上的实例点<span class="math inline">\(x_i\)</span>，也就是支持向量，对确定最优超平面<span class="math inline">\(S\)</span>的参数<span class="math inline">\(w^{*},b^{*}\)</span>起作用。此时可以解得：</p><p><span class="math display">\[b^{*}=y_j-\sum_{i=1}^{N}\lambda_i^{*}y_i(x_i^{T}\cdot x_j)\]</span></p><p>  其中实例点<span class="math inline">\(x_j\)</span>为某一个支持向量，由于<span class="math inline">\(b^{*}\)</span>的值是固定的，因此代入不同的支持向量，得到的<span class="math inline">\(b^{*}\)</span>的值是相同的，在实际计算中，我们一般在求解优化问题(5)得到的<span class="math inline">\(\lambda^{*}\)</span>中选择一个正值分量<span class="math inline">\(\lambda_j &gt;0\)</span>，使用其所对应的支持向量<span class="math inline">\(x_j\)</span>来求解<span class="math inline">\(b^{*}\)</span>。</p><h3 id="硬间隔支持向量机算法">硬间隔支持向量机算法</h3><p>  输入：训练数据集<span class="math inline">\(T_{train}=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}\)</span>，其中<span class="math inline">\(x_i \in \mathcal{X}=\mathbb{R}^{n}，y \in\mathcal{Y}=\{-1,+1\},i=1,2,\dots,N\)</span>.<br>  输出：最优分离超平面的参数<span class="math inline">\(w^{*},b^{*}\)</span>，以及相应的分类模型<span class="math inline">\(f(x)=sign(w^{*} \cdot x+b^{*})\)</span>.<br>  (1) 基于训练数据集<span class="math inline">\(T_{train}\)</span>构造凸二次规划问题(6)：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{\lambda} \quad &amp;\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\lambda_i \lambda_j y_iy_j(x_i^{T}x_j)-\sum_{i=1}^{N}\lambda_i \\        s.t. \quad &amp; \lambda_i \ge 0, \quad i=1,2,\space,N  \\        &amp; \sum_{i=1}^{N}\lambda_i y_i=0    \end{split}\end{equation}\]</span></p><p>  解得该优化问题的最优解为：<span class="math inline">\(\lambda^{*}=\begin{bmatrix}  \lambda_1^{*},\lambda_2^{*},\dots,\lambda_N^{*}\\ \end{bmatrix}^{T}\)</span></p><p>  (2) 在最优解<span class="math inline">\(\lambda^{*}\)</span>中选择一个正值分量<span class="math inline">\(\lambda_j &gt; 0\)</span>，取出其下标<span class="math inline">\(j\)</span>对应的数据<span class="math inline">\((x_j,y_j)\)</span>，基于<span class="math inline">\(KKT\)</span>条件求解最优超平面的参数<span class="math inline">\(w^{*},b^{*}\)</span>：</p><p><span class="math display">\[w^{*}=\sum_{i=1}^{N}\lambda_i^{*}y_ix_i\quadb^{*}=y_j-\sum_{i=1}^{N}\lambda_i^{*}y_i(x_i^{T} \cdot x_j)\]</span></p><p>  (3) 得到相应的分类模型：</p><p><span class="math display">\[f(x) = sign(w^{*} \cdot x +b^{*})\]</span></p><h2 id="基于python的算法实现">基于Python的算法实现</h2><p>  导入所需要的Python包：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> cvxopt<br><span class="hljs-keyword">from</span> sklearn.utils <span class="hljs-keyword">import</span> shuffle<br><span class="hljs-keyword">from</span> sklearn.svm <span class="hljs-keyword">import</span> LinearSVC<br></code></pre></td></tr></tbody></table></figure><h3 id="基于numpypandascvxopt等的算法实现">基于numpy,pandas,cvxopt等的算法实现</h3><p>  首先我们来生成我们的训练数据集，为了便于进行可视化，我们依然选择在二维情况下构建数据集。由于硬间隔线性支持向量机假设训练数据集是线性可分的，我利用均匀分布在区域<span class="math inline">\(S_{p}=\{ (x_1,x_2) \vert x_1 \in (1,2), x_2 \in(3,4)\}\)</span>产生了30个正类的实例点，在区域<span class="math inline">\(S_{n}= \{ (x_1,x_2) \vert x_1 \in (3,4), x_2 \in(1,2)\}\)</span>产生了20个负类的实例点。以下是数据生成的代码：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># data generation</span><br>np.random.seed(<span class="hljs-number">520</span>)<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">data_generation</span>(<span class="hljs-params">n,basepoint,ylabel</span>):<br>    x1 = np.random.random((n,))+np.array([basepoint[<span class="hljs-number">0</span>]]*n)<br>    x2 = np.random.random((n,))+np.array([basepoint[<span class="hljs-number">1</span>]]*n)<br>    y = np.array([ylabel]*n)<br>    data = pd.DataFrame({<span class="hljs-string">"x1"</span>:x1,<span class="hljs-string">"x2"</span>:x2,<span class="hljs-string">"y"</span>:y})<br>    <span class="hljs-keyword">return</span> data<br><br><span class="hljs-comment"># positive data</span><br>positivedata = data_generation(n=<span class="hljs-number">30</span>,basepoint=(<span class="hljs-number">1</span>,<span class="hljs-number">3</span>),ylabel=<span class="hljs-number">1</span>)<br><span class="hljs-comment"># negative data</span><br>negativedata = data_generation(n=<span class="hljs-number">20</span>,basepoint=(<span class="hljs-number">3</span>,<span class="hljs-number">1</span>),ylabel=-<span class="hljs-number">1</span>)<br><span class="hljs-comment"># train data</span><br>train_data = pd.concat([positivedata,negativedata])<br>train_data = shuffle(train_data)<br>train_data.index = <span class="hljs-built_in">range</span>(train_data.shape[<span class="hljs-number">0</span>])<br>train_data.head(<span class="hljs-number">5</span>)<br></code></pre></td></tr></tbody></table></figure><p>  这50个实例点构成训练数据集，使用以下代码画出训练数据的散点图:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># train data scatter plot</span><br>plt.scatter(x=positivedata[<span class="hljs-string">"x1"</span>],y=positivedata[<span class="hljs-string">"x2"</span>],marker=<span class="hljs-string">"o"</span>,c=<span class="hljs-string">"green"</span>,label=<span class="hljs-string">"positive data"</span>)<br>plt.scatter(x=negativedata[<span class="hljs-string">"x1"</span>],y=negativedata[<span class="hljs-string">"x2"</span>],marker=<span class="hljs-string">"x"</span>,c=<span class="hljs-string">"red"</span>,label=<span class="hljs-string">"negative data"</span>)<br>plt.xlabel(<span class="hljs-string">"x1"</span>)<br>plt.ylabel(<span class="hljs-string">"x2"</span>)<br>plt.xlim((<span class="hljs-number">0</span>,<span class="hljs-number">5</span>))<br>plt.ylim((<span class="hljs-number">0</span>,<span class="hljs-number">5</span>))<br>plt.xticks(np.arange(<span class="hljs-number">0</span>,<span class="hljs-number">5</span>,<span class="hljs-number">0.5</span>))<br>plt.yticks(np.arange(<span class="hljs-number">0</span>,<span class="hljs-number">5</span>,<span class="hljs-number">0.5</span>))<br>plt.legend()<br></code></pre></td></tr></tbody></table></figure><p>  图3为训练数据集的可视化结果：</p><center><img src="https://s2.loli.net/2023/10/31/eNkl2GVYi4mPy1v.png" width="60%" height="60%"><div data-align="center">Image3: 训练数据集</div></center><p>  从图3中可以很容易看出，训练数据集是线性可分的，满足硬间隔线性支持向量机的前提条件。下面我们来利用硬间隔线性支持向量机算法来求解最优超平面的参数<span class="math inline">\(w^{*},b^{*}\)</span>.<br>  为了使用Python的cvxopt包来求救凸二次规划问题(6)，需要将问题(6)转化为矩阵形式，记：</p><p><span class="math display">\[\lambda = \begin{bmatrix}    \lambda_1 \\    \lambda_2 \\    \vdots \\    \lambda_N \\\end{bmatrix},\quad y = \begin{bmatrix}    y_1 \\    y_2 \\    \vdots \\    y_N \\\end{bmatrix},\quad x = \begin{bmatrix}    x_1 \\    x_2 \\    \vdots \\    x_N \\\end{bmatrix}\]</span></p><p>  则：</p><p><span class="math display">\[x \odot y = \begin{bmatrix}    x_1y_1 \\    x_2y_2 \\    \vdots \\    x_Ny_N \\\end{bmatrix},\qquad Gram(x \odot y) = \begin{bmatrix}    (x_1y_1)^{T}(x_1y_1) &amp; (x_1y_1)^{T}(x_2y_2) &amp; \dots &amp;(x_1y_1)^{T}(x_Ny_N)  \\    (x_2y_2)^{T}(x_1y_1) &amp; (x_2y_2)^{T}(x_2y_2) &amp; \dots &amp;(x_2y_2)^{T}(x_Ny_N)  \\    \vdots &amp; \vdots &amp;  &amp; \vdots \\    (x_Ny_N)^{T}(x_1y_1) &amp; (x_Ny_N)^{T}(x_2y_2) &amp; \dots &amp;(x_Ny_N)^{T}(x_Ny_N)  \\\end{bmatrix}\]</span></p><p>  设：</p><p><span class="math display">\[P=Gram(x \odot y), \quadq=-1_{N}=\begin{bmatrix}    -1 \\    -1 \\    \vdots \\    -1\end{bmatrix}_{N \times 1}, \quad G = -I_{N}=\begin{bmatrix}    -1 &amp; 0 &amp; \dots &amp; 0 \\    0 &amp; -1 &amp; \dots &amp; 0 \\    \vdots &amp; \vdots &amp; &amp; \vdots \\    0 &amp; 0 &amp; \dots &amp; -1 \\\end{bmatrix}_{N \times N}\]</span></p><p><span class="math display">\[h = \begin{bmatrix}    0 \\    0 \\    \vdots \\    0\end{bmatrix}_{N \times 1}, \quad A = y^{T}, \quad b=0\]</span></p><p>  则凸二次规划问题(6)的矩阵形式为(7)：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{\lambda} \quad &amp; \frac{1}{2}\lambda^{T}P\lambda+q^{T}\lambda \\        s.t. \quad &amp;  G\lambda \leq h \\        &amp; A\lambda=b  \\    \end{split}\end{equation}\]</span></p><p>  利用硬间隔支持向量机算法来求解最优超平面的参数<span class="math inline">\(w^{*},b^{*}\)</span>，以下是相应的代码：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># data matrix</span><br>train_X = train_data.iloc[:,[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]].values<br>train_y = train_data.iloc[:,<span class="hljs-number">2</span>].values<br><br><span class="hljs-comment"># parameters solving</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">HardMargin_SVM</span>(<span class="hljs-params">X,y</span>):<br>    Y = np.array([y]*X.shape[<span class="hljs-number">1</span>]).T<br>    XdotY = X * Y<br>    n_samples = X.shape[<span class="hljs-number">0</span>]<br>    Gram_Matrix = np.zeros((n_samples,n_samples))<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_samples):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_samples):<br>            Gram_Matrix[i,j] = np.dot(XdotY[i],XdotY[j])<br>    <br>    P = matrix(Gram_Matrix).T<br>    q = matrix(-np.ones(n_samples))<br>    G = matrix(-np.eye(n_samples))<br>    h = matrix(np.zeros(n_samples))<br>    A = matrix(y.astype(<span class="hljs-built_in">float</span>)).T<br>    b = matrix([<span class="hljs-number">0.0</span>])<br><br>    lamda = np.array(solvers.qp(P,q,G,h,A,b)[<span class="hljs-string">'x'</span>])<br>    threshold = <span class="hljs-number">1e-5</span><br>    lamda[lamda &lt; threshold] = <span class="hljs-number">0</span><br>    <br>    w_hat = np.<span class="hljs-built_in">round</span>(np.<span class="hljs-built_in">sum</span>(lamda*XdotY,axis=<span class="hljs-number">0</span>),<span class="hljs-number">4</span>)<br><br>    positive_index = np.where(lamda&gt;<span class="hljs-number">0</span>)[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]<br>    SV_x = X[positive_index]<br>    SV_y = y[positive_index]<br>    GM_xwithSVx = np.zeros(n_samples)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_samples):<br>        GM_xwithSVx[i] = np.dot(X[i],SV_x)<br><br>    lamda = lamda.ravel()<br>    b_hat = <span class="hljs-built_in">round</span>(SV_y - np.dot(lamda,y * GM_xwithSVx),<span class="hljs-number">4</span>)<br><br>    <span class="hljs-keyword">return</span> [w_hat,b_hat],lamda<br><br>theta,lamda = HardMargin_SVM(X=train_X,y=train_y)<br></code></pre></td></tr></tbody></table></figure><p>  解得最优超平面的参数为：</p><p><span class="math display">\[w^{*}=\begin{bmatrix}    -0.8985 \\    0.6889 \\\end{bmatrix}, \quad b^{*}=0.4643\]</span></p><p>  画出最优分离超平面<span class="math inline">\(S^{*}=\{x \vert w^{*}\cdot x + b^{*}=0\}\)</span>：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># draw the Optimal Classification Hyperplane</span><br>w,b = theta[<span class="hljs-number">0</span>],theta[<span class="hljs-number">1</span>]<br>x1 = np.linspace(<span class="hljs-number">0</span>,<span class="hljs-number">5</span>,<span class="hljs-number">1000</span>)<br>x2 = -(w[<span class="hljs-number">0</span>]*x1+b)/w[<span class="hljs-number">1</span>]<br>plt.figure(figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">6</span>))<br>plt.scatter(x=positivedata[<span class="hljs-string">"x1"</span>],y=positivedata[<span class="hljs-string">"x2"</span>],marker=<span class="hljs-string">"o"</span>,c=<span class="hljs-string">"green"</span>,label=<span class="hljs-string">"positive data"</span>)<br>plt.scatter(x=negativedata[<span class="hljs-string">"x1"</span>],y=negativedata[<span class="hljs-string">"x2"</span>],marker=<span class="hljs-string">"x"</span>,c=<span class="hljs-string">"red"</span>,label=<span class="hljs-string">"negative data"</span>)<br>plt.plot(x1,x2,c=<span class="hljs-string">"blue"</span>,label=<span class="hljs-string">"Optimal Classification Hyperplane"</span>)<br>plt.xlabel(<span class="hljs-string">"x1"</span>)<br>plt.ylabel(<span class="hljs-string">"x2"</span>)<br>plt.xlim((<span class="hljs-number">0</span>,<span class="hljs-number">5</span>))<br>plt.ylim((<span class="hljs-number">0</span>,<span class="hljs-number">5</span>))<br>plt.xticks(np.arange(<span class="hljs-number">0</span>,<span class="hljs-number">5</span>,<span class="hljs-number">0.5</span>))<br>plt.yticks(np.arange(<span class="hljs-number">0</span>,<span class="hljs-number">5</span>,<span class="hljs-number">0.5</span>))<br>plt.legend()<br></code></pre></td></tr></tbody></table></figure><center><img src="https://s2.loli.net/2023/10/31/Pgsh97eGjJTlXfS.png" width="60%" height="60%"><div data-align="center">Image4: 最优分离超平面</div></center><p>  分类模型：</p><p><span class="math display">\[f(x)=sign(w^{*} \cdot x +b^{*})\]</span></p><h3 id="基于sklearn的算法实现">基于sklearn的算法实现</h3><p>  下面我们使用<code>sklearn</code>库来完成这个分类任务，我们依然使用前文生成的由30个正类实例点与20个负类实例点构成的训练数据集，使用<code>sklearn.svm</code>中的<code>LinearSVC</code>进行分类的代码为：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">svm = LinearSVC(penalty=<span class="hljs-string">"l2"</span>,C=<span class="hljs-number">1.0</span>,loss=<span class="hljs-string">'hinge'</span>)<br>svm.fit(train_X,train_y)<br>w,b = svm.coef_[<span class="hljs-number">0</span>],svm.intercept_[<span class="hljs-number">0</span>]<br></code></pre></td></tr></tbody></table></figure><p>  <code>LinearSVC</code>的一些主要参数为：</p><ul><li><code>penalty</code>:{'l1','l2'},default='l2'。设定惩罚项，SVC中默认为'l2'惩罚项，'l1'会导致稀疏的<code>coef_</code>向量。<br></li><li><code>loss</code>: {'hinge', 'squared_hinge'},default='squared_hinge'。设定损失函数，hinge是标准的SVM损失(如SVC类使用的)，而squared_hinge是hinge损失的平方。<br></li><li><code>dual</code>: bool,default=True。选择算法来解决对偶或原始优化问题。 当n_samples&gt;n_features时，首选dual = False。<br></li><li><code>tol</code>: float, default=1e-4。设置停止的条件。</li><li><code>C</code>: float, default=1.0。正则化参数。正则化的强度与C成反比。必须严格设置为正的。</li></ul><p>  得到的最优分类超平面的参数为：</p><p><span class="math display">\[w^{*}=\begin{bmatrix}    -0.8240 \\    0.7860 \\\end{bmatrix}, \quad b^{*}=0.0349\]</span></p><p>  画出最优分离超平面<span class="math inline">\(S^{*}=\{x \vert w^{*}\cdot x + b^{*}=0\}\)</span>：</p><center><img src="https://s2.loli.net/2023/11/01/j2cTdOPCIbWKHEV.png" width="60%" height="60%"><div data-align="center">Image4: 最优分离超平面</div></center><h2 id="附录">附录</h2><h3 id="关于凸优化的相关知识">关于凸优化的相关知识</h3><h4 id="基础概念">基础概念</h4><p><strong>仿射集</strong><br>  设<span class="math inline">\(C\)</span>为某一集合，若 <span class="math inline">\(\forall x_1,x_2 \in C, \theta \in \mathbb{R},\theta x_1+(1-\theta)x_2 \in C\)</span>，则称集合<span class="math inline">\(C\)</span>为仿射集。</p><p><strong>仿射函数</strong><br>  设有映射 <span class="math inline">\(f: \mathbb{R}^{n} \rightarrow\mathbb{R}^{m}\)</span>，若 <span class="math inline">\(f(x)=Ax+b, A \in\mathbb{R}^{m \times n}, b \in \mathbb{R}^{m}\)</span>，则称映射<span class="math inline">\(f\)</span>为仿射函数。</p><p><strong>凸集</strong><br>  设<span class="math inline">\(C\)</span>为某一集合，若 <span class="math inline">\(\forall x_1,x_2 \in C, \theta \in [0,1], \thetax_1+(1-\theta)x_2 \in C\)</span>，则称集合<span class="math inline">\(C\)</span>为凸集。</p><p><strong>凸函数</strong><br>  一个函数<span class="math inline">\(f(x)\)</span>被称为凸函数，如果它的定义域 <span class="math inline">\(dom f\)</span> 为凸集，并且对 <span class="math inline">\(\forall x_1,x_2 \in dom f, \alpha \in[0,1]\)</span>，有以下不等式成立：</p><p><span class="math display">\[f[\alpha x_1+(1-\alpha) x_2] \leq \alphaf(x_1)+(1-\alpha)f(x_2)\]</span></p><p>  <strong>凸函数的一阶条件</strong><br>  设函数<span class="math inline">\(f(x)\)</span><strong>一阶可微</strong>，<span class="math inline">\(x \in \mathbb{R}^{n}\)</span>，则<span class="math inline">\(f(x)\)</span>为凸函数的<strong>充要条件</strong>为：<span class="math inline">\(dom \space f\)</span>为凸集，且对<span class="math inline">\(\forall x,y \in dom \spacef\)</span>，有以下不等式成立：</p><p><span class="math display">\[f(y) \ge f(x)+\nablaf^{T}(x)(y-x)\]</span></p><p>  <strong>凸函数的二阶条件</strong><br>  设函数<span class="math inline">\(f(x)\)</span><strong>二阶可微</strong>，<span class="math inline">\(x \in \mathbb{R}^{n}\)</span>，则<span class="math inline">\(f(x)\)</span>为凸函数的<strong>充要条件</strong>为：<span class="math inline">\(dom \space f\)</span>为凸集，且对<span class="math inline">\(\forall x \in dom \space f\)</span>，有<span class="math inline">\(\nabla^{2}f(x) \succeq 0\)</span>，即<span class="math inline">\(Hessain\)</span>矩阵半正定。</p><p><strong>最优化问题</strong><br>  最优化问题的基本形式(7)：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{x} \quad &amp; f(x) \\        s.t. \quad &amp; m_{i}(x) \leq 0,\quad i=1,2,\dots,M \\        &amp; n_{j}(x) = 0,\quad j=1,2,\dots,N    \end{split}\end{equation}\]</span></p><ul><li><span class="math inline">\(x=\begin{bmatrix}  x_1,x_2,\dots,x_n\end{bmatrix}^{T},x \in \mathbb{R}^{n}\)</span>称为<strong>优化变量(Optimization Variable)</strong>.<br></li><li><span class="math inline">\(f: \mathbb{R}^{n} \rightarrow\mathbb{R}\)</span>，称为<strong>目标函数(ObjectiveFunction)</strong>.<br></li><li><span class="math inline">\(m_i: \mathbb{R}^{n} \rightarrow\mathbb{R}\)</span>，称为<strong>不等式约束(InequalityConstraint)</strong>.<br></li><li><span class="math inline">\(n_j: \mathbb{R}^{n} \rightarrow\mathbb{R}\)</span>，称为<strong>等式约束(EqualityConstraint)</strong>.<br></li><li><span class="math inline">\(D = \left( dom \space f \right) \bigcap\{x \vert m_{i}(x) \leq 0,i=1,2,\dots,M\} \bigcap \{x \vert h_{j}(x) =0,j=1,2,\dots,N\}\)</span>，称为最优化问题的<strong>域(Domain)</strong>，<span class="math inline">\(x \in D\)</span> 称为<strong>可行解(FeasibleSolution)</strong>.</li><li><span class="math inline">\(p^{*} = \inf \{ f(x) \vert x \in D\}\)</span>，称为最优化问题的<strong>最优值(Optimum)</strong>.<br></li><li><span class="math inline">\(f(x^{*})=p^{*}\)</span>，称<span class="math inline">\(x^{*}\)</span>为最优化问题的<strong>最优解(OptimalSolution)</strong>.<br></li><li><span class="math inline">\(X_{opt} = \{ x \vert x \in D,f(x)=p^{*}\}\)</span>，称为最优化问题的<strong>最优解集(Optima Set)</strong>.</li></ul><p><strong>凸优化问题</strong><br>  若在优化问题(7)中，目标函数<span class="math inline">\(f(x)\)</span>为凸函数，不等式约束<span class="math inline">\(m_i(x)\)</span>为凸函数，等式约束<span class="math inline">\(n_j(x)\)</span>为仿射函数，则称该优化问题为凸优化问题。</p><h4 id="对偶关系">对偶关系</h4><p><strong>拉格朗日函数</strong><br>  原问题(7)的拉格朗日函数为：</p><p><span class="math display">\[L(x,\lambda,\eta)=f(x)+\sum_{i=1}^{M}\lambda_im_i(x)+\sum_{j=1}^{N}\eta_j n_j(x)\]</span></p><p><span class="math display">\[\lambda_i \ge 0, \quadi=1,2,\dots,M\]</span></p><p><strong>原问题的无约束形式</strong><br>  原问题(7)的无约束形式(8)为：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{x} \max_{\lambda,\eta} \quad &amp; L(x,\lambda,\eta)  \\        s.t. \quad &amp; \lambda_i \ge 0, \quad i=1,2,\dots,M \\    \end{split}\end{equation}\]</span></p><p>  原问题(7)与其无约束形式(8)是等价的，现证明这个结论。<br><strong>证明:</strong><br>  令<span class="math inline">\(h(x) = \max_{\lambda,\eta}L(x,\lambda,\eta)\)</span>,<br>  若优化变量<span class="math inline">\(x\)</span>不满足某个不等式约束<span class="math inline">\(m_i(x)\)</span>，即 <span class="math inline">\(\exists i, m_i(x) &gt; 0\)</span>,则有：</p><p><span class="math display">\[h(x)=\max_{\lambda,\eta}L(x,\lambda,\eta) \rightarrow +\infty\]</span></p><p>  若优化变量<span class="math inline">\(x\)</span>不满足某个等式约束<span class="math inline">\(n_j(x)\)</span>，即 <span class="math inline">\(\exists j,n_j(x) \ne 0\)</span>，则有：</p><p><span class="math display">\[h(x)=\max_{\lambda,\eta}L(x,\lambda,\eta) \rightarrow +\infty\]</span></p><p>  若优化变量<span class="math inline">\(x\)</span>满足所有的不等式约束<span class="math inline">\(m_i(x)\)</span>与等式约束<span class="math inline">\(n_j(x)\)</span>,即 <span class="math inline">\(\forall i,j, m_i(x) \leq 0,n_j(x)=0\)</span>，则有:</p><p><span class="math display">\[h(x) = \max_{\lambda,\eta}L(x,\lambda,\eta) &lt; +\infty\]</span></p><p><span class="math display">\[\lambda_i = 0, \quadi=1,2,\dots,M\]</span></p><p>  设集合<span class="math inline">\(S_1,S_2\)</span>分别为：</p><p><span class="math display">\[S_1 = \{ x \vert \exists i,m_i(x) &gt; 0\} \cup \{ x \vert \exists j, n_j(x) \ne 0 \}\]</span></p><p><span class="math display">\[S_2 = \{ x \vert \forall i,j, m_i(x)\leq 0,n_j(x) = 0 \}\]</span></p><p>  有:</p><p><span class="math display">\[S_1 \cup S_2 = \mathbb{R}^{n}, S_1 \capS_2 = \emptyset\]</span></p><p>  则无约束问题(8)可以写为</p><p><span class="math display">\[\min_{x} \max_{\lambda,\eta}L(x,\lambda,\eta) \Leftrightarrow \min_{x} h(x) = \left \{\begin{array}{l}+\infty, &amp; {x \in S_1}\\c(x)&lt;+\infty,&amp; {x \in S_2}\\\end{array} \right.\]</span></p><p><span class="math display">\[\Rightarrow \min_{x} \max_{\lambda,\eta}L(x,\lambda,\eta) \Leftrightarrow \min_{x \in S_2} h(x)=\min_{x \in S_2}f(x)\]</span></p><p>  故原问题(7)与其无约束形式(8)等价。<br>  证毕.</p><p><strong>对偶问题</strong><br>  原问题(7)的拉格朗日对偶问题(9)为：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \max_{\lambda,\eta} \min_{x} \quad &amp; L(x,\lambda,\eta) \\        s.t. \quad &amp; \lambda_i \ge 0, \quad i = 1,2,\dots,M    \end{split}\end{equation}\]</span></p><p><strong>弱对偶关系</strong><br>  原问题(7)与其对偶问题(9)满足弱对偶关系：</p><p><span class="math display">\[\max_{\lambda,\eta} \min_{x}L(x,\lambda,\eta) \leq \min_{x} \max_{\lambda,\eta}L(x,\lambda,\eta)\]</span></p><p><strong>证明:</strong><br>  令：</p><p><span class="math display">\[A(\lambda,\eta)=\min_{x}L(x,\lambda,\eta),\quad B(x)=\max_{\lambda,\eta}L(x,\lambda,\eta)\]</span></p><p><span class="math display">\[\because \min_{x}L(x,\lambda,\eta) \leqL(x,\lambda,\eta) \leq \max_{\lambda,\eta}L(x,\lambda,\eta)\]</span></p><p><span class="math display">\[\Rightarrow A(\lambda,\eta) \leqB(x),\quad \forall \lambda,\eta, \forall x\]</span></p><p><span class="math display">\[\Rightarrow \max_{\lambda,\eta}A(\lambda,\eta) \leq \min_{x} B(x)\]</span></p><p>  即：</p><p><span class="math display">\[\max_{\lambda,\eta} \min_{x}L(x,\lambda,\eta) \leq \min_{x} \max_{\lambda,\eta}L(x,\lambda,\eta)\]</span></p><p>  证毕.</p><p><strong>强对偶关系</strong><br>  若原问题(7)与其对偶问题(9)满足：</p><p><span class="math display">\[\max_{\lambda,\eta} \min_{x}L(x,\lambda,\eta) = \min_{x} \max_{\lambda,\eta}L(x,\lambda,\eta)\]</span></p><p>  则称原问题(7)与其对偶问题(9)满足强对偶关系。</p><p><strong>强对偶关系的几何理解</strong><br>  设原问题为仅有不等式约束的优化问题(10)：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{x} \quad &amp; f(x) \\        s.t. \quad &amp; m_{i}(x) \leq 0, \quad i=1,2,\dots,M \\    \end{split}\end{equation}\]</span></p><p>  原问题(10)的拉格朗日函数为：</p><p><span class="math display">\[L(x,\lambda)=f(x)+\sum_{i=1}^{M}\lambda_im_i(x)\]</span></p><p>  若原问题与其对偶问题满足强对偶关系，则有：</p><p><span class="math display">\[\min_{x} \max_{\lambda} L(x,\lambda) =\max_{\lambda} \min_{x} L(x,\lambda)\]</span></p><p>  <strong>鞍点的定义:</strong> 若 <span class="math inline">\(\exists(\hat{w},\hat{z})\)</span>，使得：</p><p><span class="math display">\[\sup_{z} \inf_{w} f(w,z)=f(\hat{w},\hat{z})= \inf_{w} \sup_{z} f(w,z)\]</span></p><p>  则称 <span class="math inline">\((\hat{w},\hat{z})\)</span> 为<span class="math inline">\(f(w,z)\)</span> 的鞍点。由于 <span class="math inline">\(f(\hat{w},z)= \inf_{w} f(w,z),f(w,\hat{z})=sup_{z} f(w,z)\)</span>，则以上等式也可以写成：</p><p><span class="math display">\[\sup_{z}f(\hat{w},z)=f(\hat{w},\hat{z})=\inf_{w} f(w,\hat{z})\]</span></p><center><img src="https://s2.loli.net/2023/10/29/lH3OtvE7UN9A15m.jpg" width="60%" height="60%"><div data-align="center">Image: 鞍点</div></center><p>  结合鞍点以及强对偶关系的概率，我们可以得出结论：<strong>原问题的拉格朗日函数存在鞍点是原问题与对偶问题满足强对偶关系的充要条件，且鞍点即为原问题与对偶问题的最优解。</strong></p><h4 id="kkt条件">KKT条件</h4><p>  下面来介绍如何判断原问题与对偶问题是否满足强对偶关系，以及如何求出相应的最优解。首先来介绍<span class="math inline">\(Slater\)</span>条件。<br><strong><span class="math inline">\(Slater\)</span>条件</strong><br>  若原问题(7)是凸问题，同时 <span class="math inline">\(\exists x \inrelint(D)\)</span>，使得约束满足：</p><p><span class="math display">\[\begin{split}    &amp; m_{i}(x) &lt; 0, \quad i=1,2,\dots,M \\    &amp; n_{j}(x) = 0, \quad j=1,2,\dots,N \\\end{split}\]</span></p><p>  则原问题与对偶问题满足强对偶关系。</p><ul><li>注：<span class="math inline">\(relint(D)\)</span>表示原始凸问题的域的相对内部，即域内除了边界点以外的所有点。</li></ul><p>  <span class="math inline">\(Slater\)</span>条件是一个<strong>充分不必要条件</strong>，若满足<span class="math inline">\(Slater\)</span>条件，则强对偶一定成立，不满足<span class="math inline">\(Slater\)</span>条件，强对偶也可能成立。大多数凸优化问题均满足<span class="math inline">\(Slater\)</span>条件，即有强对偶性。<br>  若我们已知原问题与对偶问题满足强对偶关系，如何求解出原问题以及对偶问题的最优解？下面我们来介绍凸优化中一个非常经典的理论——KKT条件。</p><p><strong>KKT条件</strong><br>  设<span class="math inline">\(x^{*}\)</span>为原始问题(7)的最优解，<span class="math inline">\(\lambda^{*},\eta^{*}\)</span>为对偶问题(9)的最优解，且原始问题与对偶问题满足强对偶关系，则有以下四组条件成立：</p><p><span class="math display">\[KKT条件:  \left \{\begin{array}{l}m_{i}(x^{*}) \leq 0, h_{j}(x^{*}) = 0 &amp;&amp;(primal \spacefeasibility)  \\\\\lambda^{*} \ge 0 &amp;&amp;(dual \space feasibility)\\\\\lambda^{*}m_{i}(x^{*})=0 &amp;&amp;(complementary \space slackness)\\\\\frac{\partial L(x,\lambda^{*},\eta^{*})}{\partial x} \vert_{x=x^{*}}=0&amp;&amp;(stationarity)\\\end{array} \right.\]</span></p><p>  原问题、对偶问题的可行性条件，稳定性条件都很好理解，我们主要来推导一下互补松弛条件是如何得到的。<br>  <strong>证明:</strong><br>  记 <span class="math inline">\(p^{*}\)</span>为原问题(7)的最优值，<span class="math inline">\(d^{*}\)</span>为对偶问题(9)的最优值，即：</p><p><span class="math display">\[p^{*}= \min_{x}f(x)=f(x^{*})\]</span></p><p><span class="math display">\[d^{*}=\max_{\lambda,\eta} \min_{x}L(x,\lambda,\eta) \triangleq \max_{\lambda,\eta}g(\lambda,\eta)=g(\lambda^{*},\eta^{*})\]</span></p><p>  通过分析可得：</p><p><span class="math display">\[\begin{align*}    d^{*} &amp;= \max_{\lambda,\eta} \min_{x} L(x,\lambda,\eta) =\min_{x} \max_{\lambda,\eta} L(x,\lambda,\eta) = \min_{x}L(x,\lambda^{*},\eta^{*}) \\    &amp;\leq L(x^{*},\lambda^{*},\eta^{*}) =f(x^{*})+\sum_{i=1}^{M}\lambda_{i}^{*}m_{i}(x^{*}) +\sum_{j=1}^{N}\eta_{j}^{*}n_{j}(x^{*}) \\    &amp;\leq f(x^{*}) = p^{*}\end{align*}\]</span></p><p>  由原问题与对偶问题满足强对偶关系可知：<span class="math inline">\(p^{*}=d^{*}\)</span>，则以上式子中的小于等于号均取等号，故有：</p><p><span class="math display">\[\sum_{i=1}^{M}\lambda_{i}^{*}m_{i}(x^{*})=0\Rightarrow \lambda^{*}m_{i}(x^{*})=0, \forall i\]</span></p><p>  互补松弛条件成立，证毕.</p><p>  对于<strong>一般的原问题</strong>，KKT条件是 <span class="math inline">\(x^{*},\lambda^{*},\eta^{*}\)</span>为最优解的<strong>必要条件</strong>，即只要 <span class="math inline">\(x^{*},\lambda^{*},\eta^{*}\)</span>为原问题及对偶问题的最优解，则一定满足KKT条件。<br>  对于<strong>原问题为凸问题</strong>，KKT条件是 <span class="math inline">\(x^{*},\lambda^{*},\eta^{*}\)</span>为最优解的<strong>充要条件</strong>，即只要 <span class="math inline">\(x^{*},\lambda^{*},\eta^{*}\)</span>满足KKT条件，则其一定为原问题及对偶问题的最优解，反过来，只要 <span class="math inline">\(x^{*},\lambda^{*},\eta^{*}\)</span>为原问题及对偶问题的最优解，则其一定满足KKT条件。</p><h4 id="凸二次规划">凸二次规划</h4><p>  凸二次规划问题的基本形式为：</p><p><span class="math display">\[\begin{equation}    \begin{split}        \min_{x} \quad &amp; \frac{1}{2} x^{T}Px+q^{T}x+r \\        s.t. \quad &amp;  Gx \leq h \\        &amp; Ax=b  \\    \end{split}\end{equation}\]</span></p><p>  其中，<span class="math inline">\(x \in \mathbb{R}^{n}, P \inS_{+}^{n}\)</span>，为对称正定矩阵；<span class="math inline">\(q,r \in\mathbb{R}^{n}; G \in \mathbb{R}^{M \times n}, h \in \mathbb{R}^{M}; A\in \mathbb{R}^{N \times n}, b \in \mathbb{R}^{N}\)</span>.<br>  结论：<strong>凸二次规划问题满足强对偶关系。</strong></p><h3 id="硬间隔线性支持向量机的解是唯一的">硬间隔线性支持向量机的解是唯一的</h3><p><strong>结论:</strong> 如果训练数据集<span class="math inline">\(T_{train}\)</span>完全线性可分，通过最小间隔最大化问题的求解，存在唯一的分离超平面可以将所有样本点完全分开。<br><strong>证明:</strong><br>  (1) 存在性的证明：<br>  由于训练数据集<span class="math inline">\(T_{train}\)</span>完全线性可分，由线性可分的定义可知存在分离超平面<span class="math inline">\(S = \{x \vert w \cdot x + b = 0\}\)</span>可将所有样本点完全正确分类。存在性得证。<br>  (2) 唯一性的证明：<br>  利用反证法来进行证明。假设优化问题(2)存在两个不同的最优解，分别记为<span class="math inline">\(w_{1}^{*},b_{1}^{*}\)</span>和<span class="math inline">\(w_{2}^{*},b_{2}^{*}\)</span>，意味着两个权值向量所对应的模都是最小值，记为<span class="math inline">\(a\)</span>：</p><p><span class="math display">\[||w_{1}^{*}||=||w_{2}^{*}||=a\]</span></p><p>  根据这两组参数可以构造一组新的参数：</p><p><span class="math display">\[w=\frac{w_{1}^{*}+w_{2}^{*}}{2}, \quadb=\frac{b_{1}^{*}+b_{2}^{*}}{2}\]</span></p><p>  新构造的参数<span class="math inline">\(w\)</span>的模长一定满足：</p><p><span class="math display">\[||w||= || \frac{w_{1}^{*}+w_{2}^{*}}{2}|| \ge a\]</span></p><p>  另一方面，由模长的三角不等式可得：</p><p><span class="math display">\[||\frac{w_{1}^{*}+w_{2}^{*}}{2}|| =||\frac{1}{2}w_{1}^{*}+\frac{1}{2}w_{2}^{*}|| \leq\frac{1}{2}||w_{1}^{*}||+\frac{1}{2}||w_{2}^{*}||=a\]</span></p><p>  从而有：</p><p><span class="math display">\[|| \frac{w_{1}^{*}+w_{2}^{*}}{2}||=a\]</span></p><p>  即：</p><p><span class="math display">\[||w||=\frac{1}{2}||w_{1}^{*}||+\frac{1}{2}||w_{2}^{*}||\]</span></p><p>  由此发现，<span class="math inline">\(w_{1}^{*}\)</span>和<span class="math inline">\(w_{2}^{*}\)</span>在同一直线上：</p><p><span class="math display">\[w_{1}^{*}=\pm w_{2}^{*}\]</span></p><p>  当<span class="math inline">\(w_{1}^{*}=w_{2}^{*}\)</span>时，与假设矛盾；当<span class="math inline">\(w_{1}^{*}=-w_{2}^{*}\)</span>时，违背了权重向量时非零向量的前提。唯一性得证。<br>  证毕.</p><h2 id="参考">参考</h2><p><strong>[1] Book: 李航,统计学习方法(第2版)</strong><br><strong>[2] Book: 董平,机器学习中的统计思维(Python实现)</strong><br><strong>[3] Video: bilibili,简博士,支持向量机系列</strong><br><strong>[4] Video: bilibili,shuhuai008,支持向量机系列</strong><br><strong>[5] Video:bilibili,欧拉的费米子,凸优化理论-中科大凌青</strong><br><strong>[6] Blog:知乎,Lauer,[凸优化笔记6]-拉格朗日对偶、KKT条件</strong></p>]]></content>
    
    
    <summary type="html">本节主要介绍数据集线性可分条件下的支持向量机模型。</summary>
    
    
    
    <category term="机器学习" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>机器学习-2.感知机</title>
    <link href="http://example.com/2023/10/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
    <id>http://example.com/2023/10/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-%E6%84%9F%E7%9F%A5%E6%9C%BA/</id>
    <published>2023-10-08T08:15:17.000Z</published>
    <updated>2024-02-02T17:30:29.764Z</updated>
    
    <content type="html"><![CDATA[<h1 id="感知机与线性判别分析">感知机与线性判别分析</h1><h2 id="感知机">感知机</h2><p>  感知机(perception)是一个处理二分类问题的线性分类模型。感知机模型旨在寻找一个合适的超平面，将特征空间中的示例进行正确分类。为了找到这个超平面，需要设定基于误分类的损失函数，并利用梯度下降方法对损失函数进行极小化，求得感知机模型。感知机学习算法具有简单与易于实现的优点，分为原始形式和对偶形式。<br>  感知机模型由<span class="math inline">\(Rosenblatt\)</span>于1957年提出，尽管感知机在处理复杂问题方面存在局限性，但它为神经网络和深度学习的发展铺平了道路。感知机模型的思想和学习算法为后来更复杂的神经网络提供了基础，尤其是多层感知机和其他深度学习模型。感知机的历史地位在机器学习领域被广泛认可，被视为神经网络的早期里程碑。</p><h3 id="基本思想">基本思想</h3><p>  感知机的基本思想为：在<span class="math inline">\(n\)</span>维特征空间中，寻找一个能够使训练数据集中的正实例点与负实例点完全分开的超平面，对于新的实例，利用该超平面进行分类。为了找到这个超平面，感知机设置了以误分类样本数为基础的损失函数，通过梯度下降算法最小化损失函数，更新超平面，最终使得所有的训练实例均被正确分类，此时便找到了分类所需的超平面。感知机的基本思想可以用如下图1来描述：</p><center><img src="https://s2.loli.net/2023/10/08/r9CDXyRLPI5kdN6.jpg" width="60%" height="60%"><div data-align="center">Image1: 感知机的基本思想</div></center><h3 id="模型">模型</h3><p><strong>输入</strong></p><ul><li><strong>输入空间:</strong> <span class="math inline">\(\mathcal{X} =\mathbb{R}^{n}\)</span><br></li><li><strong>输入实例:</strong> <span class="math inline">\(x =\begin{bmatrix}  x^{(1)},x^{(2)},\dots,x^{(n)} \\ \end{bmatrix} \in\mathcal{X}\)</span></li></ul><p>  其中<span class="math inline">\(\mathcal{X}\)</span>代表<span class="math inline">\(n\)</span>维实数空间，输入实例<span class="math inline">\(x\)</span>为<span class="math inline">\(n\)</span>维特征向量。</p><p><strong>输出</strong></p><ul><li><strong>输出空间:</strong> <span class="math inline">\(\mathcal{Y} =\{-1,+1\}\)</span><br></li><li><strong>输出实例:</strong> <span class="math inline">\(y \in\mathcal{Y}\)</span></li></ul><p>  其中输出空间<span class="math inline">\(\mathcal{Y}\)</span>只包含+1和-1的一个集合，+1与-1分别代表二分类问题中的正类与负类。输出实例<span class="math inline">\(y\)</span>代表输出实例<span class="math inline">\(x\)</span>的类别。</p><p><strong>数据集</strong><br>  感知机模型的训练数据集<span class="math inline">\(T_{train}\)</span>为：</p><p><span class="math display">\[T_{train}=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}\]</span></p><p>  其中，<span class="math inline">\(x_i \in\mathcal{X}=\mathbb{R}^{n},y_i \in\mathcal{Y}=\{+1,-1\},i=1,2,\dots,N\)</span>，<span class="math inline">\(N\)</span>表示训练数据的样本容量。</p><p><strong>模型</strong><br>  设输入空间<span class="math inline">\(\mathcal{X}\)</span>到输出空间<span class="math inline">\(\mathcal{Y}\)</span>的映射为：</p><p><span class="math display">\[f(x)=sign(w^{T}x+b)\]</span></p><p>  称模型<span class="math inline">\(f\)</span>为感知机，其中，<span class="math inline">\(w\)</span>和<span class="math inline">\(b\)</span>为感知机模型的参数，<span class="math inline">\(w \in \mathbb{R}^{n}\)</span>为权重向量，<span class="math inline">\(b \in \mathbb{R}\)</span>为偏置。<span class="math inline">\(sign(\cdot)\)</span>为符号函数，即：</p><p><span class="math display">\[sign(x)= \left \{\begin{array}{rcl}+1, &amp; x \ge 0\\-1, &amp; x &lt; 0  \\\end{array} \right.\]</span></p><p>  感知机是一种线性分类模型，属于判别模型。</p><p><strong>假设空间</strong><br>  感知机模型的假设空间<span class="math inline">\(\mathcal{H}\)</span>为：</p><p><span class="math display">\[\mathcal{H}=\{f \vertf(x)=w^{T}x+b\}\]</span></p><p>  感知机模型的假设空间实际上特征空间中超平面的集合。</p><p><strong>参数空间</strong><br>  令<span class="math inline">\(\theta=(w,b)\)</span>，则模型的假设空间<span class="math inline">\(\mathcal{H}\)</span>等价于参数空间<span class="math inline">\(\Theta\)</span>：</p><p><span class="math display">\[\Theta=\{\theta \vert \theta \in\mathbb{R}^{n+1}\}\]</span></p><h3 id="策略">策略</h3><p>  感知机学习的基本假设是训练数据集是线性可分的，这里先给出数据集线性可分性的定义。</p><p><strong>数据集的线性可分性</strong><br>  给定一个数据集<span class="math inline">\(T\)</span>：</p><p><span class="math display">\[T=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}\]</span></p><p>其中，<span class="math inline">\(x_i \in\mathcal{X}=\mathbb{R}^{n},y_i \in\mathcal{Y}=\{+1,-1\},i=1,2,\dots,N\)</span>，如果存在某个超平面<span class="math inline">\(S\)</span>能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，即<span class="math inline">\(\exists \theta=(w,b) \in\mathbb{R}^{n+1}\)</span>，对所有 <span class="math inline">\(y_i=+1\)</span> 的实例<span class="math inline">\(i\)</span>，有 <span class="math inline">\(w^{T}x_i+b &gt; 0\)</span> ；对所有 <span class="math inline">\(y_i=-1\)</span> 的实例<span class="math inline">\(i\)</span>，有 <span class="math inline">\(w^{T}x_i+b &lt; 0\)</span>，则称数据集<span class="math inline">\(T\)</span>为线性可分数据集(linearly separable dataset)；否则，称数据集<span class="math inline">\(T\)</span>线性不可分。</p><p><strong>损失函数</strong>  为了寻找到合适的超平面，需要设置损失函数，一个很自然的想法是将损失函数设置为误分类点的个数，对于误分类数据<span class="math inline">\((x_i,y_i)\)</span>有：</p><p><span class="math display">\[-y_i(w^{T}x_i+b) &gt; 0\]</span></p><p>这是因为误分类的两种情况:<span class="math inline">\(w^{T}x_i+b &gt;0,y_i=-1\)</span>或<span class="math inline">\(w^{T}x_i+b &lt;0,y_i=+1\)</span>均可由上式表示，则损失函数可以表示为：</p><p><span class="math display">\[L(x,y)=\sum_{i=1}^{N}I_{\{-y_i(w^{T}x_i+b)&gt;0\}}\]</span></p><p><span class="math display">\[I_{\{-y_i(w^{T}x_i+b)&gt;0\}}=\left \{\begin{array}{rcl}1, &amp; {-y_i(w^{T}x_i+b)&gt;0}\\0, &amp; {-y_i(w^{T}x_i+b) \leq 0}\\\end{array} \right.\]</span></p><p>  这样设置损失函数的思路非常直观，但是，这样的损失函数不是参数<span class="math inline">\(w,b\)</span>的连续可导函数，不易进行优化，因此我们一般不将损失函数设置为这种形式。<br>  <strong>感知机所采用的损失函数是误分类点到超平面的总距离</strong>，其与误分类点的个数相关，当损失函数降至0时，意味着没有误分类点了，我们也就找到了可以将训练数据集完全正确分类的超平面了。  为此，首先给出输入空间<span class="math inline">\(\mathcal{X}\)</span>中任意一点<span class="math inline">\(x_0\)</span>到超平面<span class="math inline">\(S=\{x \vert w^{T}x+b=0\}\)</span>的距离：</p><p><span class="math display">\[d(x_0,S)=\frac{|w^{T}x_0+b|}{|| w ||_{2}}\]</span></p><p>  关于<span class="math inline">\(n\)</span>维空间中点到超平面的距离公式的证明可参见附录，这里不多做赘述。对于误分类点<span class="math inline">\(x_i\)</span>，其到初始超平面<span class="math inline">\(S\)</span>的距离为：</p><p><span class="math display">\[d(x_i,S)=\frac{-y_i(w^{T}x_i+b)}{||w||_2}\]</span></p><p>  假设在初始超平面<span class="math inline">\(S\)</span>的分类下，误分类点的集合为<span class="math inline">\(M\)</span>，则误分类点到超平面的总距离为：</p><p><span class="math display">\[d(M,S)=-\frac{1}{||w||_2}\sum_{x_i \inM}y_i(w^{T}x_i+b)\]</span></p><p>  在设定损失函数时，我们可以不考虑<span class="math inline">\(\frac{1}{||w||_2}\)</span>，因为其既不影响损失函数的正负，也不影响感知机算法的最终结果。这样，对于线性可分的训练数据集：</p><p><span class="math display">\[T_{train}=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}\]</span></p><p>  感知机学习的损失函数一般设定为：</p><p><span class="math display">\[L(w,b)=-\sum_{x_i \inM}y_i(w^{T}x_i+b)\]</span></p><p>  损失函数<span class="math inline">\(L(w,b)\)</span>是参数<span class="math inline">\(w,b\)</span>的连续可导函数，便于进行优化。当损失函数减小时，误分类点的数量也会减小，当<span class="math inline">\(L(w,b)=0\)</span>时，意味着训练数据集中所有的实例都被正确分类了，此时得到的超平面即是我们要寻找的可以完全正确分类的超平面。</p><h3 id="算法">算法</h3><p>  感知机算法分为原始形似与对偶形式，这里分别来介绍这两种形式。</p><h4 id="原始形式">原始形式</h4><p>  在策略里我们已经确定了感知机学习的损失函数，我们现在要做的便是最小化损失函数，即：</p><p><span class="math display">\[\min_{w,b} -\sum_{x_i \inM}y_i(w^{T}x_i+b)\]</span></p><p>  我们采用随机梯度下降法(stochastic gradientdescent)来优化损失函数。<br>  首先，随机选取一个初始超平面<span class="math inline">\(S_0=\{x \vertw_0^{T}x+b_0=0\}\)</span>，此时误分类点的集合为<span class="math inline">\(M_0\)</span>，则损失函数<span class="math inline">\(L(w,b)\)</span>关于参数<span class="math inline">\(w_0,b_0\)</span>的梯度为：</p><p><span class="math display">\[\nabla_{w_0}L(w_0,b_0)=-\sum_{x_i \inM_0}y_ix_i\]</span></p><p><span class="math display">\[\nabla_{b_0}L(w_0,b_0)=-\sum_{x_i \inM_0}y_i\]</span></p><p>  在<span class="math inline">\(M_0\)</span>随机选取一个误分类点<span class="math inline">\((x_0,y_0)\)</span>，对<span class="math inline">\(w_0,b_0\)</span>进行更新：</p><p><span class="math display">\[w_1 \leftarrow w_0+\etay_ix_i\]</span></p><p><span class="math display">\[b_1 \leftarrow b_0+\eta y_i\]</span></p><p>  其中，<span class="math inline">\(\eta\)</span>为梯度下降的步长，在机器学习中也称为学习率(learningrate)。更新后我们会得到一个新的超平面<span class="math inline">\(S_1=\{x\vert w_1^{T} x +b_1=0\}\)</span>，此时损失函数的值会减小。这样通过迭代可以期待损失函数<span class="math inline">\(L(w,b)\)</span>不断减小，直到为0，此时误分类集合<span class="math inline">\(M=\varnothing\)</span>，我们便找到了可以对训练集完全正确分类的超平面<span class="math inline">\(S\)</span>.<br>  综上所述，我们写出感知机算法的原始形式。</p><h5 id="感知机学习算法的原始形式">感知机学习算法的原始形式</h5><p>  输入：训练数据集<span class="math inline">\(T_{train}=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}\)</span>，其中<span class="math inline">\(x_i \in \mathcal{X}=\mathbb{R}^{n}，y \in\mathcal{Y}=\{-1,+1\},i=1,2,\dots,N\)</span>；学习率<span class="math inline">\(\eta (0 &lt; \eta \leq 1)\)</span>.<br>  输出：<span class="math inline">\(w,b\)</span>：感知机模型 <span class="math inline">\(f(x)=sign(w^{T}x+b)\)</span><br>  (1) 随机选取初始参数<span class="math inline">\(w_0,b_0\)</span>；<br>  (2) 在训练数据集中选取一个数据<span class="math inline">\((x_i,y_i)\)</span>；<br>  (3) <span class="math inline">\(if \space y_i(w_0^{T}x_i+b_0) \leq0:\)</span></p><p><span class="math display">\[w_1 \leftarrow w_0+\etay_ix_i\]</span></p><p><span class="math display">\[b_1 \leftarrow b_0+\eta y_i\]</span></p><p>  (4) 转至 (2)，直至训练集中没有误分类点。</p><p>  这种学习算法直观上有如下解释：<strong>当一个实例点被误分类，即位于分离超平面的错误一侧时，则调整<span class="math inline">\(w,b\)</span>的值，使超平面向该误分类点的一侧移动，以减小误分类点与超平面间的距离，直至超平面越过该误分类点使其被正确分类。</strong></p><h4 id="对偶形式">对偶形式</h4><p>  在感知机算法的原始形式中，如果实例点<span class="math inline">\((x_i,y_i)\)</span>是误分类点，则可以用该点更新参数，即：</p><p><span class="math display">\[w \leftarrow w + \etay_ix_i\]</span></p><p><span class="math display">\[b \leftarrow b + \eta y_i\]</span></p><p>  现在设想一下，如果在参数更新的过程中，点<span class="math inline">\((x_i,y_i)\)</span>被误分类了<span class="math inline">\(n_i\)</span>次，则在从初始参数<span class="math inline">\(w_0,b_0\)</span>到最终参数<span class="math inline">\(w,b\)</span>中，点<span class="math inline">\((x_i,y_i)\)</span>贡献的增量为<span class="math inline">\(n_i \eta y_ix_i\)</span>和<span class="math inline">\(n_i \eta y_i\)</span>.<br>  假设在迭代过程中，训练集中的每一个实例点<span class="math inline">\((x_i,y_i)\)</span>被误分类的次数为<span class="math inline">\(n_i,i=1,2,\dots,N\)</span>，取初始的参数向量为零向量，则通过迭代最终学习到的次数可以表示为：</p><p><span class="math display">\[w=\sum_{i=1}^{N}\alpha_iy_ix_i\]</span></p><p><span class="math display">\[b =\sum_{i=1}^{N}\alpha_iy_i\]</span></p><p>  其中，<span class="math inline">\(\alpha_i=n_i\eta\)</span>，若<span class="math inline">\(\eta=1\)</span>，则<span class="math inline">\(\alpha_i\)</span>表示训练集中的实例点<span class="math inline">\((x_i,y_i)\)</span>由于被误分类而用于更新参数的次数。<br>  在使用对偶算法进行迭代时，若<span class="math inline">\((x_i,y_i)\)</span>被误分类，则我们便在其对应的<span class="math inline">\(\alpha_i\)</span>上加上增量<span class="math inline">\(\eta\)</span>，从而得到新的超平面，循环迭代过程，直至没有误分类点为止。  综上所述，我们可以写出感知机算法的对偶形式。</p><h5 id="感知机学习算法的对偶形式">感知机学习算法的对偶形式</h5><p>  输入：训练数据集<span class="math inline">\(T_{train}=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}\)</span>，其中<span class="math inline">\(x_i \in \mathcal{X}=\mathbb{R}^{n}，y \in\mathcal{Y}=\{-1,+1\},i=1,2,\dots,N\)</span>；学习率<span class="math inline">\(\eta (0 &lt; \eta \leq 1)\)</span>.<br>  输出：<span class="math inline">\(\alpha\)</span>：感知机模型 <span class="math inline">\(f(x)=sign \left( (\sum_{i=1}^{N}\alpha_iy_ix_i)\cdot x+(\sum_{i=1}^{N}\alpha_iy_i) \right),\alpha=\begin{bmatrix}  \alpha_1, \alpha_2, \dots, \alpha_N \\\end{bmatrix}^T\)</span><br>  (1) <span class="math inline">\(\alpha \leftarrow 0\)</span>;<br>  (2) 在训练数据集<span class="math inline">\(T_{train}\)</span>中选取实例点<span class="math inline">\((x_j,y_j)\)</span>;<br>  (3) <span class="math inline">\(if \space y_i \left(\sum_{i=1}^{N}\alpha_iy_ix_i \cdot x_j+\sum_{i=1}^{N}\alpha_iy_i \right)\leq 0：\)</span></p><p><span class="math display">\[\alpha_i \leftarrow \alpha_i +\eta\]</span></p><p>  (4) 转至(2)直到没有误分类数据。</p><p>  值得注意的是，在对偶算法的每次迭代中，训练实例仅以内积的形式出现。为了方便计算，可以预先将训练数据中的实例间的内积计算出来，并以矩阵的形式储存，这个矩阵便是<span class="math inline">\(Gram\)</span>矩阵：</p><p><span class="math display">\[G=[x_i \cdot x_j]_{N \timesN}\]</span></p><p>  可以证明感知机算法是收敛的，即经过有限次迭代可以得到一个能将训练数据集完全正确分类的分离超平面。这部分证明放在附录里，感兴趣的读者可自行阅读。</p><h3 id="感知机算法的实例及python实现">感知机算法的实例及Python实现</h3><p>  首先，我们需要创造一个线性可分的二分类数据。为了便于可视化，设输入实例<span class="math inline">\(x \in \mathcal{X}=\mathbb{R}^2，y \in \mathcal{Y}=\{-1,1\}\)</span>，以下是用于产生数据的Python代码：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Data Generation</span><br>np.random.seed(<span class="hljs-number">1314</span>)<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">data_generation</span>(<span class="hljs-params">n,basepoint,ylabel</span>):<br>    x1 = np.random.random((n,))+np.array([basepoint[<span class="hljs-number">0</span>]]*n)<br>    x2 = np.random.random((n,))+np.array([basepoint[<span class="hljs-number">1</span>]]*n)<br>    y = np.array([ylabel]*n)<br>    data = pd.DataFrame({<span class="hljs-string">"x1"</span>:x1,<span class="hljs-string">"x2"</span>:x2,<span class="hljs-string">"y"</span>:y})<br>    <span class="hljs-keyword">return</span> data<br><br><span class="hljs-comment"># positive data</span><br>positivedata = data_generation(n=<span class="hljs-number">30</span>,basepoint=(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>),ylabel=<span class="hljs-number">1</span>)<br><span class="hljs-comment"># negative data</span><br>negativedata = data_generation(n=<span class="hljs-number">20</span>,basepoint=(<span class="hljs-number">2</span>,<span class="hljs-number">1</span>),ylabel=-<span class="hljs-number">1</span>)<br><span class="hljs-comment"># train data</span><br>train_data = pd.concat([positivedata,negativedata])<br>train_data = shuffle(train_data)<br>train_data.index = <span class="hljs-built_in">range</span>(train_data.shape[<span class="hljs-number">0</span>])<br>train_data.head(<span class="hljs-number">5</span>)<br></code></pre></td></tr></tbody></table></figure><p>  在这段代码里，我利用均匀分布在<span class="math inline">\(\{x \vertx_1 \in (1,2);x_2 \in (2,3)\}\)</span>里产生了30个正类实例，在<span class="math inline">\(\{x \vert x_1 \in (2,3);x_2 \in(1,2)\}\)</span>里产生了20个负类实例。得到的实例数据如下表1：</p><style>.center {  width: auto;  display: table;  margin-left: auto;  margin-right: auto;}</style><p align="center"><font face="黑体" size="2.">table1 训练数据(部分)</font></p><div class="center"><table><thead><tr class="header"><th style="text-align: center;">index</th><th style="text-align: center;">x1</th><th style="text-align: center;">x2</th><th style="text-align: center;">y</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">0</td><td style="text-align: center;">1.749421</td><td style="text-align: center;">2.193847</td><td style="text-align: center;">1</td></tr><tr class="even"><td style="text-align: center;">1</td><td style="text-align: center;">2.057071</td><td style="text-align: center;">1.983485</td><td style="text-align: center;">-1</td></tr><tr class="odd"><td style="text-align: center;">2</td><td style="text-align: center;">2.830457</td><td style="text-align: center;">1.026819</td><td style="text-align: center;">-1</td></tr><tr class="even"><td style="text-align: center;">3</td><td style="text-align: center;">1.437378</td><td style="text-align: center;">2.345612</td><td style="text-align: center;">1</td></tr><tr class="odd"><td style="text-align: center;">4</td><td style="text-align: center;">2.730692</td><td style="text-align: center;">1.493602</td><td style="text-align: center;">-1</td></tr></tbody></table></div><p>  利用以下代码将训练数据进行可视化：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># train data scatter plot</span><br>plt.scatter(x=positivedata[<span class="hljs-string">"x1"</span>],y=positivedata[<span class="hljs-string">"x2"</span>],marker=<span class="hljs-string">"o"</span>,c=<span class="hljs-string">"green"</span>,label=<span class="hljs-string">"positive data"</span>)<br>plt.scatter(x=negativedata[<span class="hljs-string">"x1"</span>],y=negativedata[<span class="hljs-string">"x2"</span>],marker=<span class="hljs-string">"x"</span>,c=<span class="hljs-string">"red"</span>,label=<span class="hljs-string">"negative data"</span>)<br>plt.xlabel(<span class="hljs-string">"x1"</span>)<br>plt.ylabel(<span class="hljs-string">"x2"</span>)<br>plt.xlim((<span class="hljs-number">0</span>,<span class="hljs-number">4</span>))<br>plt.ylim((<span class="hljs-number">0</span>,<span class="hljs-number">4</span>))<br>plt.xticks(np.arange(<span class="hljs-number">0</span>,<span class="hljs-number">4</span>,<span class="hljs-number">0.5</span>))<br>plt.yticks(np.arange(<span class="hljs-number">0</span>,<span class="hljs-number">4</span>,<span class="hljs-number">0.5</span>))<br>plt.legend()<br></code></pre></td></tr></tbody></table></figure><p>  得到的训练数据散点图为下图2：</p><center><img src="https://s2.loli.net/2023/10/12/b3KfhBve5VYl6Sw.png" width="60%" height="60%"><div data-align="center">Image2: 训练数据散点图</div></center><p>  从图中我们可以看出，训练数据是线性可分的，满足感知机学习的假设。<br>  之后我们可以根据感知机学习的原始算法来求解分离超平面的参数<span class="math inline">\(w,b\)</span>，参数求解的代码如下：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">param_solving</span>(<span class="hljs-params">train_data,lr,start_w</span>):<br>    <span class="hljs-comment"># initial parameter</span><br>    w = np.array(start_w) <span class="hljs-comment"># w = [w1,w2,b]</span><br>    x = train_data[[<span class="hljs-string">"x1"</span>,<span class="hljs-string">"x2"</span>]].values<br>    onecolumn = np.ones((train_data.shape[<span class="hljs-number">0</span>],))<br>    X = np.column_stack((x,onecolumn))<br>    y = train_data[<span class="hljs-string">"y"</span>].values<br>    k = <span class="hljs-number">0</span><br>    inter_data = {<span class="hljs-string">"k"</span>:[],<span class="hljs-string">"Parameter"</span>:[],<span class="hljs-string">"Loss Function"</span>:[],<span class="hljs-string">"Misclassifications"</span>:[],<span class="hljs-string">"Point Used for Update"</span>:[]}<br>    <span class="hljs-comment"># stochastic gradient descent</span><br>    <span class="hljs-keyword">while</span> np.<span class="hljs-built_in">any</span>((np.dot(X,w)*y&lt;<span class="hljs-number">0</span>) | (np.dot(X,w)*y==<span class="hljs-number">0</span>)):<br>        k += <span class="hljs-number">1</span><br>        falseindex = np.where((np.dot(X,w)*y&lt;<span class="hljs-number">0</span>) | (np.dot(X,w)*y==<span class="hljs-number">0</span>))<br><br>        <span class="hljs-comment"># storing information about the iterative process</span><br>        <span class="hljs-keyword">if</span> k%<span class="hljs-number">5</span> == <span class="hljs-number">0</span>:<br>            inter_data[<span class="hljs-string">"k"</span>].append(k)<br>            inter_data[<span class="hljs-string">"Parameter"</span>].append(w.<span class="hljs-built_in">round</span>(<span class="hljs-number">4</span>))<br>            falsedata = train_data.iloc[falseindex[<span class="hljs-number">0</span>],:]<br>            x_falseclassified = falsedata[[<span class="hljs-string">"x1"</span>,<span class="hljs-string">"x2"</span>]].values<br>            X_falseclassified = np.column_stack((x_falseclassified,np.ones((falsedata.shape[<span class="hljs-number">0</span>],))))<br>            y_falseclassified = falsedata[<span class="hljs-string">"y"</span>].values<br>            loss = -<span class="hljs-built_in">sum</span>(np.dot(X_falseclassified,w)*y_falseclassified)<br>            inter_data[<span class="hljs-string">"Loss Function"</span>].append(loss.<span class="hljs-built_in">round</span>(<span class="hljs-number">4</span>))<br>            inter_data[<span class="hljs-string">"Misclassifications"</span>].append(falseindex[<span class="hljs-number">0</span>])<br>            inter_data[<span class="hljs-string">"Point Used for Update"</span>].append(falseindex[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>])<br>        <br>        x_forupdate = train_data.iloc[falseindex[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>],[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]].values<br>        y_forupdate = train_data.iloc[falseindex[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>],[<span class="hljs-number">2</span>]].values<br>        delta = np.append(lr*y_forupdate*x_forupdate,lr*y_forupdate)<br>        w = w + delta<br>    inter_information = pd.DataFrame(inter_data)<br><br>    result_data = {}<br>    result_data[<span class="hljs-string">"final parameters"</span>] = w.<span class="hljs-built_in">round</span>(<span class="hljs-number">4</span>)<br>    result_data[<span class="hljs-string">"interative information"</span>] = inter_information<br>    result_data[<span class="hljs-string">"number of iterations"</span>] = k<br> <br>    <span class="hljs-keyword">return</span> result_data<br></code></pre></td></tr></tbody></table></figure><p>  若我们将初始的参数均设置为0，则我们最终得到的分离超平面参数为：</p><p><span class="math display">\[w,b = \begin{bmatrix}    -0.1117,0.1103 \\\end{bmatrix}^T,0.01\]</span></p><p>  总共的迭代次数为<span class="math inline">\(k=85\)</span>，以下表是迭代过程中的一些信息：</p><style>.center {  width: auto;  display: table;  margin-left: auto;  margin-right: auto;}</style><p align="center"><font face="黑体" size="2.">table2 迭代过程信息(部分)</font></p><div class="center"><table style="width:100%;"><thead><tr class="header"><th style="text-align: center;">k</th><th style="text-align: center;">Parameter</th><th style="text-align: center;">Loss Function</th><th style="text-align: center;">Misclassifications</th><th style="text-align: center;">Point Used for Update</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">5</td><td style="text-align: center;">[-0.0062, 0.0042, 0.0]</td><td style="text-align: center;">0.0161</td><td style="text-align: center;">[0, 5, 7, 9, 24, 28, 36, 43, 45,49]</td><td style="text-align: center;">0</td></tr><tr class="even"><td style="text-align: center;">10</td><td style="text-align: center;">[0.0052, 0.0304, 0.01]</td><td style="text-align: center;">1.3912</td><td style="text-align: center;">[ 1, 2, 4, 10, 12, 13, 15, 18, 22, 29,30, 32, 33, 37, 38, 39, 40,42, 44, 46]</td><td style="text-align: center;">1</td></tr><tr class="odd"><td style="text-align: center;">15</td><td style="text-align: center;">[-0.0215, 0.0147, 0.0]</td><td style="text-align: center;">0.0564</td><td style="text-align: center;">[0, 5, 7, 9, 24, 28, 36, 43, 45,49]</td><td style="text-align: center;">0</td></tr><tr class="even"><td style="text-align: center;">20</td><td style="text-align: center;">[-0.0102, 0.0409, 0.01]</td><td style="text-align: center;">0.9020</td><td style="text-align: center;">[ 1, 2, 4, 10, 12, 13, 15, 18, 22, 29,30, 32, 33, 37, 38, 39, 40,42, 44, 46]</td><td style="text-align: center;">1</td></tr><tr class="odd"><td style="text-align: center;">25</td><td style="text-align: center;">[-0.0369, 0.0252, 0.0]</td><td style="text-align: center;">0.0967</td><td style="text-align: center;">[0, 5, 7, 9, 24, 28, 36, 43, 45,49]</td><td style="text-align: center;">0</td></tr></tbody></table></div><p>  表中每一列的含义为：</p><ul><li><strong>k:</strong> 表示第<span class="math inline">\(k\)</span>次更新参数前。<br></li><li><strong>Parameter:</strong> 表示第<span class="math inline">\(k\)</span>次更新参数前参数向量<span class="math inline">\(\hat{w}=(w^T,b)^T\)</span>.<br></li><li><strong>Loss Function:</strong> 表示第<span class="math inline">\(k\)</span>次更新参数前损失函数<span class="math inline">\(L(w,b)\)</span>的值。<br></li><li><strong>Misclassifications:</strong> 表示第<span class="math inline">\(k\)</span>次更新参数前训练数据中被误分类的示例点的索引。<br></li><li><strong>Point Used for Update:</strong> 表示第<span class="math inline">\(k\)</span>次更新所用的误分类示例点的索引。</li></ul><p>  利用以下代码画出分类超平面：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">result = param_solving(train_data=train_data,lr=<span class="hljs-number">0.01</span>,start_w=[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>])<br>w = result[<span class="hljs-string">"final parameters"</span>]<br>x1_line = np.linspace(<span class="hljs-number">0</span>,<span class="hljs-number">4</span>,<span class="hljs-number">1000</span>)<br>x2_line = (-w[<span class="hljs-number">0</span>]*x1_line - w[<span class="hljs-number">2</span>])/w[<span class="hljs-number">1</span>]<br>plt.scatter(x=positivedata[<span class="hljs-string">"x1"</span>],y=positivedata[<span class="hljs-string">"x2"</span>],marker=<span class="hljs-string">"o"</span>,c=<span class="hljs-string">"green"</span>,label=<span class="hljs-string">"positive data"</span>)<br>plt.scatter(x=negativedata[<span class="hljs-string">"x1"</span>],y=negativedata[<span class="hljs-string">"x2"</span>],marker=<span class="hljs-string">"x"</span>,c=<span class="hljs-string">"red"</span>,label=<span class="hljs-string">"negative data"</span>)<br>plt.plot(x1_line,x2_line,c=<span class="hljs-string">"blue"</span>,label=<span class="hljs-string">"Classification Hyperplane"</span>)<br>plt.xlim((<span class="hljs-number">0</span>,<span class="hljs-number">4</span>))<br>plt.ylim((<span class="hljs-number">0</span>,<span class="hljs-number">4</span>))<br>plt.xticks(np.arange(<span class="hljs-number">0</span>,<span class="hljs-number">4</span>,<span class="hljs-number">0.5</span>))<br>plt.yticks(np.arange(<span class="hljs-number">0</span>,<span class="hljs-number">4</span>,<span class="hljs-number">0.5</span>))<br>plt.xlabel(<span class="hljs-string">"x1"</span>)<br>plt.ylabel(<span class="hljs-string">"x2"</span>)<br>plt.legend()<br></code></pre></td></tr></tbody></table></figure><p>  得到的分类超平面如下图所示：</p><center><img src="https://s2.loli.net/2023/10/12/8tjLUPw4aqFr29O.png" width="60%" height="60%"><div data-align="center">Image3: 分类超平面</div></center><p>  如果我们改变初始参数的值，我们可能会得到不同的分类超平面，比如我们将初始参数设置为<span class="math inline">\(w=\begin{bmatrix}  1,1 \\\end{bmatrix}^T,b=1\)</span>，则我们最终得到的分类超平面如下图4所示：</p><center><img src="https://s2.loli.net/2023/10/12/Hmcf6eJ13yoFWwh.png" width="60%" height="60%"><div data-align="center">Image4: 分类超平面</div></center><p>  此时，分类超平面的参数为：</p><p><span class="math display">\[w,b = \begin{bmatrix}    -0.2049,-0.0153 \\\end{bmatrix}^T,0.45\]</span></p><h2 id="附录">附录</h2><h3 id="空间中点到超平面的距离">空间中点到超平面的距离</h3><p><strong>结论</strong>   设<span class="math inline">\(n\)</span>维空间中存在某超平面<span class="math inline">\(S=\{x \vert w^{T}x+b=0;x,w \in \mathbb{R}^n,b \in\mathbb{R}\}\)</span>，已知点<span class="math inline">\(x_0 \in\mathbb{R}^{n},x_0 \notin S\)</span>，则点<span class="math inline">\(x_0\)</span>到超平面<span class="math inline">\(S\)</span>的距离为：</p><p><span class="math display">\[d(x_0,S)=\frac{|w^{T}x_0+b|}{||w||}\]</span></p><p><strong>证明:</strong></p><center><img src="https://s2.loli.net/2023/10/11/mKVMIlyDUipd45k.jpg" width="60%" height="60%"><div data-align="center">Image6: 点到超平面的距离</div></center><p>  设<span class="math inline">\(x_1\)</span>为<span class="math inline">\(x_0\)</span>在超平面<span class="math inline">\(S\)</span>上的投影，<span class="math inline">\(x_2\)</span>为<span class="math inline">\(S\)</span>上另外任意一点，向量<span class="math inline">\(\overrightarrow{x_0x_1}\)</span>与向量<span class="math inline">\(\overrightarrow{x_0x_2}\)</span>的夹角为<span class="math inline">\(\theta\)</span>，则有：</p><p><span class="math display">\[d(x_0,S)=||\overrightarrow{x_0x_1}||=||\overrightarrow{x_0x_2}||\cos{\theta}\]</span></p><p>  由向量夹角公式可知：</p><p><span class="math display">\[\cos{\theta}=\frac{\overrightarrow{x_0x_1} \cdot\overrightarrow{x_0x_2}}{||\overrightarrow{x_0x_1}|| \cdot||\overrightarrow{x_0x_2}||}\]</span></p><p>  将其代入到<span class="math inline">\(d(x_0,S)\)</span>表达式中可得：</p><p><span class="math display">\[d(x_0,S)=\frac{\overrightarrow{x_0x_1}\cdot \overrightarrow{x_0x_2}}{||\overrightarrow{x_0x_1}||}\]</span></p><p>  由于<span class="math inline">\(x_1,x_2\)</span>是超平面<span class="math inline">\(S\)</span>上的点，故有：</p><p><span class="math display">\[w^{T}x_1+b=0\]</span></p><p><span class="math display">\[w^{T}x_2+b=0\]</span></p><p><span class="math display">\[\Rightarrow w^{T}(x_1-x_2)=0 \Rightarroww \cdot \overrightarrow{x_2x_1}=0\]</span></p><p>  由于<span class="math inline">\(\overrightarrow{x_0x_1}\)</span>与<span class="math inline">\(\overrightarrow{x_2x_1}\)</span>垂直，因而有：</p><p><span class="math display">\[\overrightarrow{x_0x_1} \cdot\overrightarrow{x_2x_1}=0\]</span></p><p>  由于法向量<span class="math inline">\(w\)</span>与向量<span class="math inline">\(\overrightarrow{x_0x_1}\)</span>均垂直于超平面<span class="math inline">\(S\)</span>，故可设<span class="math inline">\(\overrightarrow{x_0x_1}=kw,k \in\mathbb{R}\)</span>，将其代入<span class="math inline">\(d(x_0,S)\)</span>可得：</p><p><span class="math display">\[d(x_0,S)=\frac{|k| \cdot |w \cdot\overrightarrow{x_0x_2}|}{|k| \cdot ||w||}=\frac{|w \cdot(x_2-x_0)|}{||w||}\]</span></p><p>  由于<span class="math inline">\(w \cdot x_2 + b=0\)</span>可得<span class="math inline">\(w \cdot x_2 = -b\)</span>，代入上式可得：</p><p><span class="math display">\[d(x_0,S)=\frac{|-b-w \cdotx_0|}{||w||}=\frac{|w^{T}x_0+b|}{||w||}\]</span></p><p>  证毕.</p><h3 id="感知机算法收敛性证明">感知机算法收敛性证明</h3><p>  现在证明，对于线性可分数据集，感知机学习算法原始形式收敛，即经过有限次迭代可以得到一个将训练数据集完全正确划分的分离超平面及感知机模型。<br>  为了便于叙述和推导，将偏置<span class="math inline">\(b\)</span>并入权重向量<span class="math inline">\(w\)</span>，记作<span class="math inline">\(\hat{w}=(w^T,b)^T\)</span>，同样也将输入向量<span class="math inline">\(x\)</span>进行扩充，加进常数1，记作<span class="math inline">\(\hat{x}=(x^T,1)^T\)</span>。这样，<span class="math inline">\(\hat{x} \in \mathbb{R}^{n+1},\hat{w} \in\mathbb{R}^{n+1}\)</span>。显然，<span class="math inline">\(\hat{w}^T\hat{x}=w^{T}x+b\)</span>。</p><h4 id="novikoff定理"><span class="math inline">\(Novikoff\)</span>定理</h4><p><strong>结论</strong><br>  设训练数据集<span class="math inline">\(T_{train}=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}\)</span>是线性可分的，其中<span class="math inline">\(x_i \in \mathcal{X} = \mathbb{R}^{n},y_i \in\mathcal{Y} = \{-1,+1\},i=1,2,\dots,N\)</span>，则有：<br>  (1) 存在超平面<span class="math inline">\(S=\{x \vert\hat{w}_{opt}^Tx=w_{opt}^Tx+b_{opt}=0,||\hat{w}_{opt}||=1\}\)</span>可将训练数据集<span class="math inline">\(T_{train}\)</span>完全正确分类；且<span class="math inline">\(\exists \gamma &gt;0\)</span>，对所有的<span class="math inline">\(x_i,i=1,2,\dots,N\)</span>，有</p><p><span class="math display">\[y_i(\hat{w}_{opt}^{T}\hat{x}_i)=y_i(w_{opt}^{T}x_i+b_{opt})\ge \gamma\]</span></p><p>  (2) 令 <span class="math inline">\(R=\max \{ ||\hat{x_i}|| \space\vert i=1,2,\dots,N\}\)</span>，则感知机学习的原始算法在训练数据集<span class="math inline">\(T_{train}\)</span>上的误分类次数<span class="math inline">\(k\)</span>满足不等式：</p><p><span class="math display">\[k \leq \left( \frac{R}{\gamma}\right)^2\]</span></p><p>  <span class="math inline">\(Novikoff\)</span>定理说明感知机学习算法能够经过有限次迭代得到一个可以将训练数据集完全正确分类的超平面，即算法收敛。下面，给出这个定理的证明。</p><p><strong>证明:</strong><br>  首先来证明(1):<br>  由于训练数据集<span class="math inline">\(T_{train}\)</span>是线性可分的，由数据集线性可分的定义可知，一定存在某个超平面能够将训练数据集完全正确分类，不妨设这个超平面为<span class="math inline">\(S=\{x \vert \hat{w}_{opt}^Tx=w_{opt}^Tx+b_{opt}=0,||\hat{w}_{opt}||=1\}\)</span>，则对于训练数据集中的每个实例，若<span class="math inline">\(y_i = +1\)</span>,则<span class="math inline">\(\hat{w}_{opt}^Tx_i &gt; 0\)</span>；若<span class="math inline">\(y_i=-1\)</span>，则<span class="math inline">\(\hat{w}_{opt}^Tx_i &lt;0\)</span>，因此有以下不等式成立：</p><p><span class="math display">\[y_i(\hat{w}_{opt}^{T}\hat{x}_i)=y_i(w_{opt}^{T}x_i+b_{opt})&gt; 0\]</span></p><p>  故存在：</p><p><span class="math display">\[\gamma = \min_{i}\{y_i(w_{opt}^{T}x_i+b_{opt})\}\]</span></p><p>  使得：</p><p><span class="math display">\[y_i(\hat{w}_{opt}^{T}\hat{x}_i)=y_i(w_{opt}^{T}x_i+b_{opt})\ge \gamma\]</span></p><p>  再来证明(2):</p><p>  不妨设感知机算法的初始权重向量<span class="math inline">\(\hat{w}_{0}=0\)</span>，如果发现一个实例被误分类，则更新一次权重。令<span class="math inline">\(\hat{w}_{k-1}\)</span>是发现的第<span class="math inline">\(i\)</span>个误分类实例之前的权重向量，即：</p><p><span class="math display">\[\hat{w}_{k-1}=(w_{k-1}^{T},b_{k-1})^T\]</span></p><p>  判断训练集中的实例<span class="math inline">\((x_i,y_i)\)</span>被误分类的条件为：</p><p><span class="math display">\[y_i(\hat{w}_{opt}^{T}\hat{x}_i)=y_i(w_{opt}^{T}x_i+b_{opt})\leq 0\]</span></p><p>  若实例<span class="math inline">\((x_i,y_i)\)</span>是被<span class="math inline">\(\hat{w}_{k-1}=(w_{k-1}^{T},b_{k-1})^T\)</span>误分类的数据，则<span class="math inline">\(w\)</span>和<span class="math inline">\(b\)</span>的更新过程为：</p><p><span class="math display">\[w_{k} \leftarrow w_{k-1} + \etay_ix_i\]</span></p><p><span class="math display">\[b_{k} \leftarrow b_{k-1} + \etay_i\]</span></p><p>  则：</p><p><span class="math display">\[\hat{w}_{k}= \hat{w}_{k-1}+\eta y_i\hat{x}_i\]</span></p><p>  由此可得：</p><p><span class="math display">\[\begin{align*}    \hat{w}_{k} \cdot \hat{w}_{opt} &amp;= \hat{w}_{k-1} \cdot\hat{w}_{opt}+\eta (y_i \hat{w}_{opt}^T \hat{x}_i) \\    &amp; \ge \hat{w}_{k-1} \cdot \hat{w}_{opt} + \eta \gamma  \\    &amp; \ge \hat{w}_{k-2} \cdot \hat{w}_{opt} + 2\eta \gamma  \\      &amp; \space \vdots  \\    &amp; \ge k \eta \gamma \\\end{align*}\]</span></p><p>  同时：</p><p><span class="math display">\[\begin{align*}    ||\hat{w}_{k}||^{2} &amp;= ||\hat{w}_{k-1}+\eta y_i \hat{x}_i||^2 \\    &amp;= ||\hat{w}_{k-1}||^2 + 2 \eta (y_i \hat{w}_{k-1}^T\hat{x}_i)+\eta^2 ||\hat{x}_i||^2 \\    &amp; \leq ||\hat{w}_{k-1}||^{2} + \eta^2||\hat{x_i}||^2 \\    &amp; \leq ||\hat{w}_{k-1}||^{2} + \eta^2R^2 \\    &amp; \leq ||\hat{w}_{k-2}||^{2} + 2\eta^2R^2 \\      &amp; \space \vdots \\    &amp; \leq k \eta^2R^2\end{align*}\]</span></p><p>  由内积不等式可知：</p><p><span class="math display">\[k \eta \gamma \leq \hat{w}_k \cdot\hat{w}_{opt} \leq ||\hat{w}_k||\space ||\hat{w}_{opt}|| \leq \sqrt{k}\eta R\]</span></p><p><span class="math display">\[\Rightarrow k \leq \left(\frac{R}{\gamma} \right)^2\]</span></p><p>  证毕.</p><h2 id="参考">参考</h2><p><strong>[1] Book: 李航.(2019).统计学习方法</strong><br><strong>[2] Video: bilibili,简博士,感知机系列</strong><br><strong>[3] Blog: 知乎,夜魔刀,点到超平面距离的公式推导</strong></p>]]></content>
    
    
    <summary type="html">本节主要介绍机器学习中的感知机模型及Python实现。</summary>
    
    
    
    <category term="机器学习" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>矩阵分析-5.线性映射</title>
    <link href="http://example.com/2023/10/04/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-5-%E7%BA%BF%E6%80%A7%E6%98%A0%E5%B0%84/"/>
    <id>http://example.com/2023/10/04/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-5-%E7%BA%BF%E6%80%A7%E6%98%A0%E5%B0%84/</id>
    <published>2023-10-04T12:51:17.000Z</published>
    <updated>2023-10-05T09:35:24.539Z</updated>
    
    <content type="html"><![CDATA[<h1 id="线性映射">线性映射</h1><h2 id="线性映射的定义">线性映射的定义</h2><p>  设<span class="math inline">\(V_1,V_2\)</span>是数域<span class="math inline">\(\mathbb{F}\)</span>上的线性空间，有映射 <span class="math inline">\(\sigma: V_1 \rightarrow V_2\)</span>，如果<span class="math inline">\(\sigma\)</span>满足：<br>(1) <strong>加法关系:</strong> 对<span class="math inline">\(\foralle_1,e_2 \in V_1, \sigma(e_1+e_2) = \sigma(e_1)+\sigma(e_2) \inV_2\)</span>.<br>(2) <strong>数乘关系:</strong> d对<span class="math inline">\(\forall e\in V_1, k \in \mathbb{F}, \sigma(ek)=\sigma(e)k\)</span><br>则称映射<span class="math inline">\(\sigma\)</span>是<span class="math inline">\(V_1\)</span>到<span class="math inline">\(V_2\)</span>的<strong>线性映射</strong>。特别地，若有<span class="math inline">\(V_1=V_2=V\)</span>，则称映射<span class="math inline">\(\sigma\)</span>为<span class="math inline">\(V\)</span>上的<strong>线性变换</strong>。</p><p><strong>注:</strong> 若线性映射<span class="math inline">\(\sigma:V_1 \rightarrow V_2\)</span> 是可逆映射(一一映射)，则称<span class="math inline">\(\sigma\)</span>为<strong>线性同构</strong>。</p><h2 id="线性映射的实例">线性映射的实例</h2><h3 id="例1-线性与非线性映射">例1 线性与非线性映射</h3><p><strong>非线性映射的实例</strong>   设线性空间<span class="math inline">\(V_1,V_2=\mathbb{R}^2\)</span>，有映射：</p><p><span class="math display">\[\mathcal{A}: \begin{bmatrix}    x_1 \\    x_2 \\\end{bmatrix} \mapsto \begin{bmatrix}    x_1+x_2 \\    x_1x_2 \\\end{bmatrix}\]</span></p><p>则映射<span class="math inline">\(\mathcal{A}\)</span>为<span class="math inline">\(V_1\)</span>到<span class="math inline">\(V_2\)</span>的非线性映射。<br><strong>证明:</strong><br>  取<span class="math inline">\(e_1,e_2 \in V_1\)</span>，其中:</p><p><span class="math display">\[e_1=e_2=\begin{bmatrix}    1 \\    1 \\\end{bmatrix},e_1+e_2=\begin{bmatrix}    2 \\    2 \\\end{bmatrix}\]</span></p><p><span class="math display">\[\mathcal{A}(e_1+e_2)=\begin{bmatrix}    2+2 \\    2 \times 2 \\\end{bmatrix}=\begin{bmatrix}    4 \\    4 \\\end{bmatrix}\]</span></p><p><span class="math display">\[\mathcal{A}(e_1)+\mathcal{A}(e_1)=\begin{bmatrix}    1+1 \\    1 \times 1 \\\end{bmatrix}+\begin{bmatrix}    1+1 \\    1 \times 1 \\\end{bmatrix}=\begin{bmatrix}    4 \\    2 \\\end{bmatrix}\]</span></p><p>  <span class="math inline">\(\because \mathcal{A}(e_1+e_2) \ne\mathcal{A}(e_1)+\mathcal{A}(e_1)\)</span>，故映射<span class="math inline">\(\mathcal{A}\)</span>不满足加法关系，其不是<span class="math inline">\(V_1\)</span>到<span class="math inline">\(V_2\)</span>上的线性映射。</p><p><strong>线性映射的实例</strong><br>  设线性空间<span class="math inline">\(V_1=\mathbb{R}^3,V_2=\mathbb{R}^2\)</span>，有映射：</p><p><span class="math display">\[\mathcal{B}: \begin{bmatrix}    x_1 \\    x_2 \\    x_3 \\\end{bmatrix} \mapsto \begin{bmatrix}    x_1+x_2-x_3 \\    \frac{1}{2}x_1-3x_2\end{bmatrix}\]</span></p><p>则线性映射<span class="math inline">\(\mathcal{B}\)</span>为<span class="math inline">\(V_1\)</span>到<span class="math inline">\(V_2\)</span>上的线性映射。<br><strong>证明:</strong><br>  (1) 先验证映射<span class="math inline">\(\mathcal{B}\)</span>满足线性映射的加法关系:<br>  设<span class="math inline">\(\forall \alpha,\beta \in V_1, \alpha =\begin{bmatrix}  \alpha_1 ,\alpha_2,\alpha_3 \end{bmatrix}^T,\beta =\begin{bmatrix}  \beta_1,\beta_2,\beta_3 \end{bmatrix}^T\)</span></p><p><span class="math display">\[\mathcal{B}(\alpha)=\begin{bmatrix}    \alpha_1+\alpha_2-\alpha_3 \\    \frac{1}{2}\alpha_1-3\alpha_2 \\\end{bmatrix},\mathcal{B}(\beta)=\begin{bmatrix}    \beta_1+\beta_2-\beta_3 \\    \frac{1}{2}\beta_1-3\beta_2 \\\end{bmatrix}\]</span></p><p><span class="math display">\[\mathcal{B}(\alpha)+\mathcal{B}(\beta)=\begin{bmatrix}    \alpha_1+\beta_1+\alpha_2+\beta_2-(\alpha_3+\beta_3) \\    \frac{1}{2}(\alpha_1+\beta_1)-3(\alpha_2+\beta_2) \\\end{bmatrix}\]</span></p><p><span class="math display">\[\alpha+\beta=\begin{bmatrix}    \alpha_1+\beta_1 \\    \alpha_2+\beta_2 \\    \alpha_3+\beta_3 \\\end{bmatrix},\mathcal{B}(\alpha+\beta)=\begin{bmatrix}    \alpha_1+\beta_1+\alpha_2+\beta_2-(\alpha_3+\beta_3) \\    \frac{1}{2}(\alpha_1+\beta_1)-3(\alpha_2+\beta_2) \\\end{bmatrix}\]</span></p><p><span class="math display">\[\Rightarrow\mathcal{B}(\alpha+\beta)=\mathcal{B}(\alpha)+\mathcal{B}(\beta)\]</span></p><p>  故映射<span class="math inline">\(\mathcal{B}\)</span>满足线性映射的加法关系。<br>  (2) 再验证映射<span class="math inline">\(\mathcal{B}\)</span>满足线性映射的数乘关系:<br>  设<span class="math inline">\(\forall \alpha \in V_1,k \in\mathbb{F},\alpha = \begin{bmatrix}  \alpha_1 ,\alpha_2,\alpha_3 \\\end{bmatrix}^T\)</span>,有:</p><p><span class="math display">\[\alpha k = \begin{bmatrix}    \alpha_1 k \\    \alpha_2 k \\    \alpha_3 k \\\end{bmatrix}, \mathcal{B}(\alpha k)=\begin{bmatrix}    \alpha_1 k+\alpha_2 k-\alpha_3 k \\    \frac{1}{2}\alpha_1 k-3\alpha_2 k \\\end{bmatrix}=\begin{bmatrix}    (\alpha_1+\alpha_2-\alpha_3)k \\    (\frac{1}{2}\alpha_1-3\alpha_2) k \\\end{bmatrix}\]</span></p><p><span class="math display">\[\mathcal{B}(\alpha)=\begin{bmatrix}    \alpha_1+\alpha_2-\alpha_3 \\    \frac{1}{2}\alpha_1-3\alpha_2 \\\end{bmatrix},\mathcal{B}(\alpha)k=\begin{bmatrix}    (\alpha_1+\alpha_2-\alpha_3)k \\    (\frac{1}{2}\alpha_1-3\alpha_2) k \\\end{bmatrix}\]</span></p><p><span class="math display">\[\Rightarrow \mathcal{B}(\alphak)=\mathcal{B}(\alpha)k\]</span></p><p>  故映射<span class="math inline">\(\mathcal{B}\)</span>满足线性映射的数乘关系。<br>  综上所述，映射<span class="math inline">\(\mathcal{B}\)</span>为<span class="math inline">\(V_1\)</span>到<span class="math inline">\(V_2\)</span>上的线性映射。</p><h3 id="例2-矩阵与标准线性空间之间的线性映射的等同性">例2矩阵与标准线性空间之间的线性映射的等同性</h3><p>  给定矩阵 <span class="math inline">\(A \in \mathbb{F}^{m \times n},x \in \mathbb{F}^n\)</span>，则矩阵<span class="math inline">\(A\)</span>可以作为线性映射<span class="math inline">\(\sigma_{A}\)</span>：</p><p><span class="math display">\[\begin{align*}    \sigma_{A}: &amp;\mathbb{F}^{n} \rightarrow \mathbb{F}^{m} \\    &amp;x \mapsto y=Ax\end{align*}\]</span></p><p>  若我们已知有线性映射 <span class="math inline">\(\sigma:\mathbb{F}^{n} \rightarrow\mathbb{F}^{m}\)</span>，如例1中的线性映射<span class="math inline">\(\mathcal{B}\)</span>，能否求得相应的矩阵<span class="math inline">\(A\)</span>?<br><strong>解:</strong><br>  记标准线性空间<span class="math inline">\(\mathbb{F}^n\)</span>的标准基为：<span class="math inline">\(e_1,e_2,\dots,e_n\)</span>，可以构造矩阵：</p><p><span class="math display">\[\sigma(\begin{bmatrix}    e_1,e_2,\dots,e_n \\\end{bmatrix})=\begin{bmatrix}    \sigma(e_1),\sigma(e_2),\dots,\sigma(e_n) \\\end{bmatrix} \triangleq A \in \mathbb{F}^{n \times m}\]</span></p><p>  对<span class="math inline">\(\forall x \in\mathbb{F}^{n}\)</span>，将<span class="math inline">\(x\)</span>沿着标准基展开：</p><p><span class="math display">\[x = \begin{bmatrix}    e_1,e_2,\dots,e_n \\\end{bmatrix}x=e_1x_1+e_2x_2,\dots,e_nx_n\]</span></p><p><span class="math display">\[\begin{align*}    \sigma(x)&amp;=\sigma(e_1x_1+e_2x_2,\dots,e_nx_n)=\sigma(e_1)x_1+\sigma(e_2)x_2+\dots+\sigma(e_n)x_n\\    &amp;= \begin{bmatrix}    \sigma(e_1),\sigma(e_2),\dots,\sigma(e_n) \\\end{bmatrix}x=Ax\end{align*}\]</span></p><p>  因此矩阵<span class="math inline">\(A\)</span>与线性映射<span class="math inline">\(\sigma\)</span>具有等同性.</p><h2 id="线性映射的矩阵表示">线性映射的矩阵表示</h2><h3 id="定义">定义</h3><p>  设有标准线性空间<span class="math inline">\(V=\mathbb{F}^n,W=\mathbb{F}^m\)</span>，给定线性映射：</p><p><span class="math display">\[\begin{align*}    \sigma: &amp;V \rightarrow W \\    &amp; v \mapsto w\end{align*}\]</span></p><p>  选取<span class="math inline">\(V\)</span>的基向量<span class="math inline">\(v_1,v_2,\dots,v_n\)</span>，作为<strong>入口基</strong>，<span class="math inline">\(W\)</span>的基向量<span class="math inline">\(w_1,w_2,\dots,w_m\)</span>作为<strong>出口基</strong>，记<span class="math inline">\(V\)</span>中第<span class="math inline">\(j\)</span>个入口基向量<span class="math inline">\(v_j\)</span>在<span class="math inline">\(W\)</span>中的象<span class="math inline">\(\sigma(v_j)\)</span>在出口基下的坐标表示为<span class="math inline">\(a_j = \begin{bmatrix}  a_{1j},a_{2j},\dots,a_{mj}\\ \end{bmatrix}^T\)</span>，即：</p><p><span class="math display">\[\sigma(v_j)=\begin{bmatrix}    w_1,w_2,\dots,w_n \\\end{bmatrix}\begin{bmatrix}    a_{1j} \\    a_{2j} \\    \vdots \\    a_{mj} \\\end{bmatrix}\]</span></p><p>  将所有入口基的象在出口基下的坐标拼成矩阵<span class="math inline">\(A\)</span>:</p><p><span class="math display">\[A=\begin{bmatrix}    \sigma(v_1),\sigma(v_2),\dots,\sigma(v_n) \\\end{bmatrix}=\begin{bmatrix}    a_{11} &amp; a_{21} &amp; \dots &amp; a_{1n} \\    a_{21} &amp; a_{22} &amp; \dots &amp; a_{2n} \\    \vdots &amp; \vdots &amp;  &amp; \vdots \\    a_{m1} &amp; a_{m2} &amp; \dots &amp; a_{mn} \\\end{bmatrix}\]</span></p><p>  则有：</p><p><span class="math display">\[\sigma(\begin{bmatrix}    v_1,v_2,\dots,v_n\end{bmatrix})=\begin{bmatrix}    w_1,w_2,\dots,w_m\end{bmatrix}A\]</span></p><p>  则称矩阵<span class="math inline">\(A\)</span>为线性映射<span class="math inline">\(\sigma\)</span>在入口基<span class="math inline">\(v_i,i=1,\dots,n\)</span>和出口基<span class="math inline">\(w_j,j=1,\dots,m\)</span>下的矩阵表示。</p><p><strong>注:</strong></p><p><span class="math display">\[\color{green} \begin{bmatrix}    线性 \\    映射 \\\end{bmatrix}\begin{bmatrix}    入口基 \\    矩阵  \\\end{bmatrix}=\begin{bmatrix}    出口基 \\    矩阵  \\\end{bmatrix}\begin{bmatrix}    线性映射 \\    矩阵表示  \\\end{bmatrix}\]</span></p><h3 id="定理线性映射的坐标计算">定理(线性映射的坐标计算)</h3><p>  设有<span class="math inline">\(n\)</span>维线性空间<span class="math inline">\(V\)</span>和<span class="math inline">\(m\)</span>维线性空间<span class="math inline">\(W\)</span>，<span class="math inline">\(v_1,v_2,\dots,v_n\)</span>为<span class="math inline">\(V\)</span>的一组基向量，<span class="math inline">\(w_1,w_2,\dots,w_m\)</span>为<span class="math inline">\(W\)</span>的一组基向量，映射<span class="math inline">\(\sigma\)</span>为<span class="math inline">\(V\)</span>到<span class="math inline">\(W\)</span>上的线性映射，矩阵<span class="math inline">\(A\)</span>为线性映射<span class="math inline">\(\sigma\)</span>在入口基<span class="math inline">\(\{v_i \vert i=1,2,\dots,n\}\)</span>与出口基<span class="math inline">\(\{w_i \verti=1,2,\dots,m\}\)</span>下的矩阵表示。<br>  给定<span class="math inline">\(\forall v \inV\)</span>，其沿着基向量组展开的坐标为<span class="math inline">\(x\)</span>，即：</p><p><span class="math display">\[v=\begin{bmatrix}    v_1,v_2,\dots,v_n \\\end{bmatrix}x=\begin{bmatrix}    v_1,v_2,\dots,v_n \\\end{bmatrix}\begin{bmatrix}    x_1 \\    x_2 \\    \vdots \\    x_m \\\end{bmatrix}\]</span></p><p>则经过线性映射<span class="math inline">\(\sigma\)</span>后，<span class="math inline">\(\sigma(v) \inW\)</span>在沿着基向量组展开的坐标为<span class="math inline">\(Ax\)</span>，即：</p><p><span class="math display">\[\sigma(v) = \begin{bmatrix}    w_1,w_2,\dots,w_m \\\end{bmatrix}Ax\]</span></p><p><strong>证明:</strong><br>  由题意可知：</p><p><span class="math display">\[v=\begin{bmatrix}    v_1,v_2,\dots,v_n \\\end{bmatrix}\begin{bmatrix}    x_1 \\    x_2 \\    \vdots \\    x_m \\\end{bmatrix}=v_1x_1+v_2x_2+\dots+v_nx_n\]</span></p><p>  则有</p><p><span class="math display">\[\begin{align*}    \sigma(v)&amp;=\sigma(v_1x_1+v_2x_2+\dots+v_nx_n)=\sigma(v_1)x_1+\sigma(v_2)x_2+\dots+\sigma(v_n)x_n\\    &amp;=\begin{bmatrix}        \sigma(v_1),\sigma(v_2),\dots,\sigma(v_n) \\    \end{bmatrix}=(\begin{bmatrix}        w_1,w_2,\dots,w_m    \end{bmatrix}A)x \\    &amp;=\begin{bmatrix}        w_1,w_2,\dots,w_m    \end{bmatrix}(Ax)\end{align*}\]</span></p><p>  故<span class="math inline">\(\sigma(v)\)</span>沿着基向量组<span class="math inline">\(\{w_i \verti=1,2,\dots,m\}\)</span>展开后的坐标为<span class="math inline">\(Ax\)</span>.</p><p><span style="color: green;">结论：基向量组将抽象的线性空间映射为标准线性空间，在基向量组的表示下，原本抽象线性空间之间的线性映射可以被表示为具体的矩阵。</span></p><center><img src="https://s2.loli.net/2023/10/05/GTHiFnjpR4v2CQr.jpg    " width="60%" height="60%"><div data-align="center">Image1: 线性映射的矩阵表示</div></center><h2 id="线性映射矩阵表示的实例">线性映射矩阵表示的实例</h2><h3 id="例1-微分算子的矩阵表示">例1 微分算子的矩阵表示</h3><p>  设线性空间<span class="math inline">\(V=\mathbb{R}_{4}[x],W=\mathbb{R}_{3}[x]\)</span>(<span class="math inline">\(\mathbb{R}_{n}[x]\)</span>表示<span class="math inline">\(n\)</span>维多项式函数空间)，微分算子<span class="math inline">\(\sigma\)</span>为<span class="math inline">\(V\)</span>到<span class="math inline">\(W\)</span>上的线性映射，求<span class="math inline">\(\sigma\)</span>在标准基向量组下的矩阵表示.</p><p><strong>解:</strong><br>  <span class="math inline">\(V\)</span>的标准基向量组为：<span class="math inline">\(\{1,x,x^2,x^3\}\)</span>(入口基)，<span class="math inline">\(W\)</span>的标准基向量组为<span class="math inline">\(\{1,x,x^2\}\)</span>，则有：</p><p><span class="math display">\[\begin{align*}    \sigma(\begin{bmatrix}        1,x,x^2,x^3 \\    \end{bmatrix})&amp;=\begin{bmatrix}        \sigma(1),\sigma(x),\sigma(x^2),\sigma(x^3) \\    \end{bmatrix} \\    &amp;=\begin{bmatrix}        0,1,2x,3x^2 \\    \end{bmatrix}=\begin{bmatrix}        1,x,x^2 \\    \end{bmatrix}A\end{align*}\]</span></p><p><span class="math display">\[A=\begin{bmatrix}    0 &amp; 1 &amp; 0 &amp; 0 \\    0 &amp; 0 &amp; 2 &amp; 0 \\    0 &amp; 0 &amp; 0 &amp; 3 \\\end{bmatrix}\]</span></p><p>  故矩阵<span class="math inline">\(A\)</span>即为微分算子<span class="math inline">\(\sigma\)</span>在<span class="math inline">\(V\)</span>与<span class="math inline">\(W\)</span>的标准基向量组下的矩阵表示.</p><p><strong>应用:</strong></p><p>  <span class="math inline">\(v = \frac{1}{2}x^3+5x \inV\)</span>，将<span class="math inline">\(v\)</span>沿着<span class="math inline">\(V\)</span>的标准基向量组<span class="math inline">\(\{1,x,x^2,x^3\}\)</span>展开得其坐标:</p><p><span class="math display">\[v=\frac{1}{2}x^3+5x=\begin{bmatrix}    1,x,x^2,x^3 \\\end{bmatrix}\begin{bmatrix}    0 \\    5 \\    0 \\    \frac{1}{2} \\\end{bmatrix}\]</span></p><p>  则<span class="math inline">\(\sigma(v)\)</span>在<span class="math inline">\(W\)</span>的标准基向量组<span class="math inline">\(\{1,x,x^2\}\)</span>下的坐标表示为:</p><p><span class="math display">\[A\begin{bmatrix}    0 \\    5 \\    0 \\    \frac{1}{2} \\\end{bmatrix}=\begin{bmatrix}    0 &amp; 1 &amp; 0 &amp; 0 \\    0 &amp; 0 &amp; 2 &amp; 0 \\    0 &amp; 0 &amp; 0 &amp; 3 \\\end{bmatrix}\begin{bmatrix}    0 \\    5 \\    0 \\    \frac{1}{2} \\\end{bmatrix}=\begin{bmatrix}    5 \\    0 \\    \frac{3}{2} \\\end{bmatrix}\]</span></p><p>  故对<span class="math inline">\(v\)</span>求微分的结果为：</p><p><span class="math display">\[\sigma(v)=\begin{bmatrix}    1,x,x^2,x^3 \\\end{bmatrix}\begin{bmatrix}    5 \\    0 \\    \frac{3}{2} \\\end{bmatrix}=\frac{3}{2}x^2+5\]</span></p><h3 id="例2-旋转变换的矩阵表示">例2 旋转变换的矩阵表示</h3><p>  欧几里得空间中的某一物体绕固定轴旋转<span class="math inline">\(\theta\)</span>，求该变换的矩阵表示。<br><strong>解:</strong><br>  设<span class="math inline">\(V=W\)</span>为欧几里得空间，映射<span class="math inline">\(\sigma\)</span>为旋转变换，易知<span class="math inline">\(\sigma\)</span>为线性变换.  设欧几里得空间中的一组标准正交基向量为<span class="math inline">\(e_1,e_2,e_3\)</span>，</p><p><span class="math display">\[e_1=\begin{bmatrix}    1 \\    0 \\    0 \\\end{bmatrix}, e_2=\begin{bmatrix}    0 \\    1 \\    0 \\\end{bmatrix}, e_3=\begin{bmatrix}    0 \\    0 \\    1 \\\end{bmatrix}\]</span></p><p>  其中<span class="math inline">\(e_3\)</span>为旋转变换所固定的轴，则<span class="math inline">\(e_1,e_2\)</span>为与旋转轴所垂直的平面的一组正交基，可以<span class="math inline">\(e_1,e_2,e_3\)</span>的方向为<span class="math inline">\(x,y,z\)</span>轴建立坐标系. 向量组<span class="math inline">\(e_1,e_2,e_3\)</span>既为入口基也为出口基.  旋转变换<span class="math inline">\(\sigma\)</span>作用于基向量组<span class="math inline">\(e_1,e_2,e_3\)</span>时，<span class="math inline">\(e_3\)</span>并不会改变，<span class="math inline">\(e_1,e_2\)</span>在其所在的平面上旋转<span class="math inline">\(\theta\)</span>，旋转变换可用图2表示。</p><center><img src="https://s2.loli.net/2023/10/05/hU9ixmNgfXRtWze.jpg    " width="40%" height="40%"><div data-align="center">Image2: 旋转变换</div></center><p>  由几何知识可得：</p><p><span class="math display">\[\sigma(e_1)=\begin{bmatrix}    cos\theta \\    sin\theta \\    0 \\\end{bmatrix},\sigma(e_2)=\begin{bmatrix}    -sin\theta \\    cos\theta \\    0 \\\end{bmatrix},\sigma(e_3)=\begin{bmatrix}    0 \\    0 \\    1 \\\end{bmatrix}\]</span></p><p>  故有:</p><p><span class="math display">\[\begin{align*}    \sigma(\begin{bmatrix}    e_1,e_2,e_3 \\\end{bmatrix})&amp;=\begin{bmatrix}    \sigma(e_1),\sigma(e_2),\sigma(e_3)\end{bmatrix}=\begin{bmatrix}    cos\theta &amp; -sin\theta &amp; 0 \\    sin\theta &amp; cos\theta &amp; 0 \\    0 &amp; 0 &amp; 1 \\\end{bmatrix} \\    &amp;=\begin{bmatrix}        1 &amp; 0 &amp; 0 \\        0 &amp; 1 &amp; 0 \\        0 &amp; 0 &amp; 1 \\    \end{bmatrix}\begin{bmatrix}    cos\theta &amp; -sin\theta &amp; 0 \\    sin\theta &amp; cos\theta &amp; 0 \\    0 &amp; 0 &amp; 1 \\\end{bmatrix}=\begin{bmatrix}    e_1,e_2,e_3 \\\end{bmatrix}A\end{align*}\]</span></p><p><span class="math display">\[A=\begin{bmatrix}    cos\theta &amp; -sin\theta &amp; 0 \\    sin\theta &amp; cos\theta &amp; 0 \\    0 &amp; 0 &amp; 1 \\\end{bmatrix}\]</span></p><p>  矩阵<span class="math inline">\(A\)</span>即为欧几里得空间中旋转变换<span class="math inline">\(\sigma\)</span>在标准正交基下的矩阵表示.</p><h3 id="例3-镜面反射的矩阵表示">例3 镜面反射的矩阵表示</h3><p>  欧几里得空间中的某一物体对固定平面进行镜面反射，求该变换的矩阵表示.<br><strong>解:</strong><br>  设<span class="math inline">\(V=M\)</span>为欧几里得空间，映射<span class="math inline">\(\sigma\)</span>为镜面反射，易知<span class="math inline">\(\sigma\)</span>为线性变换.  设欧几里得空间中的一组标准正交基向量为<span class="math inline">\(e_1,e_2,e_3\)</span>，</p><p><span class="math display">\[e_1=\begin{bmatrix}    1 \\    0 \\    0 \\\end{bmatrix}, e_2=\begin{bmatrix}    0 \\    1 \\    0 \\\end{bmatrix}, e_3=\begin{bmatrix}    0 \\    0 \\    1 \\\end{bmatrix}\]</span></p><p>  其中，<span class="math inline">\(e_1,e_2\)</span>所在的平面即为进行镜面反射所依赖的平面，<span class="math inline">\(e_3\)</span>为垂直于该平面的一个基向量。可以<span class="math inline">\(e_1,e_2,e_3\)</span>的方向为<span class="math inline">\(x,y,z\)</span>轴建立坐标系. 向量组<span class="math inline">\(e_1,e_2,e_3\)</span>既为入口基也为出口基.<br>  镜面反射<span class="math inline">\(\sigma\)</span>作用于基向量组<span class="math inline">\(e_1,e_2,e_3\)</span>时，<span class="math inline">\(e_1,e_2\)</span>并不会改变，<span class="math inline">\(e_3\)</span>变为相反方向，镜面反射可用图3表示。</p><center><img src="https://s2.loli.net/2023/10/05/oSeUqtnj5I74Kyf.jpg    " width="40%" height="40%"><div data-align="center">Image3: 镜面反射</div></center><p>  由几何知识可知：</p><p><span class="math display">\[\sigma(e_1)=\begin{bmatrix}    1 \\    0 \\    0 \\\end{bmatrix},\sigma(e_2)=\begin{bmatrix}    0 \\    1 \\    0 \\\end{bmatrix},\sigma(e_3)=\begin{bmatrix}    0 \\    0 \\    -1 \\\end{bmatrix}\]</span></p><p>  故有:</p><p><span class="math display">\[\begin{align*}    \sigma(\begin{bmatrix}    e_1,e_2,e_3 \\\end{bmatrix})&amp;=\begin{bmatrix}    \sigma(e_1),\sigma(e_2),\sigma(e_3)\end{bmatrix}=\begin{bmatrix}    1 &amp; 0 &amp; 0 \\    0 &amp; 1 &amp; 0 \\    0 &amp; 0 &amp; -1 \\\end{bmatrix} \\    &amp;=\begin{bmatrix}        1 &amp; 0 &amp; 0 \\        0 &amp; 1 &amp; 0 \\        0 &amp; 0 &amp; 1 \\    \end{bmatrix}\begin{bmatrix}    1 &amp; 0 &amp; 0 \\    0 &amp; 1 &amp; 0 \\    0 &amp; 0 &amp; -1 \\\end{bmatrix}=\begin{bmatrix}    e_1,e_2,e_3 \\\end{bmatrix}A\end{align*}\]</span></p><p><span class="math display">\[A=\begin{bmatrix}    1 &amp; 0 &amp; 0 \\    0 &amp; 1 &amp; 0 \\    0 &amp; 0 &amp; -1 \\\end{bmatrix}\]</span></p><p>  矩阵<span class="math inline">\(A\)</span>即为欧几里得空间中镜面反射<span class="math inline">\(\sigma\)</span>在标准正交基下的矩阵表示.</p>]]></content>
    
    
    <summary type="html">本节主要介绍线性映射的概率，以及如何用矩阵表示线性映射。</summary>
    
    
    
    <category term="矩阵分析" scheme="http://example.com/categories/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90/"/>
    
    
  </entry>
  
</feed>
