<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>喵老师&#39;s Blog</title>
  
  <subtitle>喵老师很忙~</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2023-09-18T07:49:16.637Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>喵老师</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>矩阵分析-4.子空间</title>
    <link href="http://example.com/2023/09/07/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-4-%E5%AD%90%E7%A9%BA%E9%97%B4/"/>
    <id>http://example.com/2023/09/07/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-4-%E5%AD%90%E7%A9%BA%E9%97%B4/</id>
    <published>2023-09-07T15:22:39.000Z</published>
    <updated>2023-09-18T07:49:16.637Z</updated>
    
    <content type="html"><![CDATA[<h1 id="子空间">子空间</h1><h2 id="子空间的定义">子空间的定义</h2><p>  设<span class="math inline">\(V\)</span>是数域<span class="math inline">\(\mathbb{F}\)</span>上的线性空间，<span class="math inline">\(W \in V\)</span>是非空子集，若<span class="math inline">\(W\)</span>有以下两个属性:<br>(1) <strong>对加法封闭</strong>：<span class="math inline">\(\forall\alpha,\beta \in W, \alpha+\beta \in W\)</span>.<br>(2) <strong>对数乘封闭</strong>：<span class="math inline">\(\forall k\in \mathbb{F}, \alpha \in W, \alpha k \in W\)</span>.<br>则称集合<span class="math inline">\(W\)</span>是线性空间<span class="math inline">\(V\)</span>的子空间.</p><p>注：子空间<span class="math inline">\(W\)</span>本身按<span class="math inline">\(V\)</span>中定义的加法与数乘也构成线性空间.</p><h2 id="子空间的实例">子空间的实例</h2><h3 id="二维空间的子空间">二维空间的子空间</h3><p>  <span class="math inline">\(\color{green}{结论:二维空间的子空间是任意经过原点的直线.}\)</span><br>  <strong>证明:</strong><br>  设<span class="math inline">\(V = \mathbb{R}^{2}\)</span>，<span class="math inline">\(V\)</span>表示二维平面.<br>  二维平面中经过原点的某一直线：<span class="math inline">\(W = \{x\vert \theta^{T} x=0, x \in V\}, \theta \in\mathbb{R}^{2}\)</span>.<br>  验证直线<span class="math inline">\(W\)</span>是二维平面<span class="math inline">\(V\)</span>的子空间：<br>  (1) 对<span class="math inline">\(\forall x_1,x_2 \in W,\theta^{T}(x_1+x_2)= \theta^{T}x_1+\theta^{T}x_2=0\)</span>,且<span class="math inline">\(x_1+x_2 \in V,\)</span><br>  <span class="math inline">\(\Rightarrow x_1+x_2 \inW\)</span>，即<span class="math inline">\(W\)</span>对加法封闭.<br>  (2) 对<span class="math inline">\(\forall \beta \in \mathbb{R},\forall x \in W, \theta^{T}(\betax)=\beta(\theta^{T}x)=0\)</span>，且<span class="math inline">\(\beta x\in V,\)</span><br>  <span class="math inline">\(\Rightarrow \beta x \inW\)</span>，即<span class="math inline">\(W\)</span>对数乘封闭.<br>  综上所述：直线<span class="math inline">\(W\)</span>是二维空间<span class="math inline">\(V\)</span>的子空间.</p><p>  <strong>注：二维空间中不经过原点的直线不是其子空间。</strong><br>  <strong>证明:</strong><br>  设<span class="math inline">\(V=\mathbb{R}^2\)</span>，<span class="math inline">\(V\)</span>表示二维平面.<br>  二维平面中不经过原点的某一直线：<span class="math inline">\(W=\{x\vert \theta^{T}x+b=0,b \neq 0,x \in V \}, \theta \in\mathbb{R}^{2}.\)</span><br>  验证直线<span class="math inline">\(W\)</span>不是二维平面<span class="math inline">\(V\)</span>的子空间：<br>  (1) 对<span class="math inline">\(\forall x_1,x_2 \in W, x_1,x_2 \neq0, \theta^{T}(x_1+x_2)+b = (\theta^{T}x_1+b)+\theta^{T}x_2=\theta^{T}x_2\neq 0.\)</span><br>  又<span class="math inline">\(\because x_1+x_2 \in V \Rightarrowx_1+x_2 \notin W \Rightarrow\)</span> 直线<span class="math inline">\(W\)</span>不满足对加法封闭，故其不是二维空间<span class="math inline">\(V\)</span>的子空间.</p><p>  对于二维空间的子空间，我们也可以借助图像来直观理解：</p><center><img src="https://s2.loli.net/2023/09/17/6eWufsxXb3FpVYh.jpg" width="40%" height="40%"><div data-align="center">Image1: 二维空间的子空间</div></center><p>  从图中我们可以发现，经过原点的直线<span class="math inline">\(W_1\)</span>上任意两个向量<span class="math inline">\(x_1,x_2\)</span>的和仍位于直线<span class="math inline">\(W_1\)</span>上，说明<span class="math inline">\(W_1\)</span>对加法封闭，同时<span class="math inline">\(W_1\)</span>上的向量伸缩后仍位于直线<span class="math inline">\(W_1\)</span>上，说明<span class="math inline">\(W_1\)</span>对数乘封闭。故经过原点的直线<span class="math inline">\(W_1\)</span>是二维空间<span class="math inline">\(V\)</span>的子空间.<br>  不经过原点的直线<span class="math inline">\(W_2\)</span>上的任意两个向量<span class="math inline">\(y_1,y_2\)</span>，这两个向量的和<span class="math inline">\(y_3\)</span>不位于直线<span class="math inline">\(W_2\)</span>上，故直线<span class="math inline">\(W_2\)</span>对加法不封闭，其不是二维空间<span class="math inline">\(V\)</span>的子空间.</p>]]></content>
    
    
    <summary type="html">本节主要介绍子空间的概念、相应的实例，以及子空间的交与和。</summary>
    
    
    
    <category term="矩阵分析" scheme="http://example.com/categories/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90/"/>
    
    
  </entry>
  
  <entry>
    <title>矩阵分析-3.基与坐标</title>
    <link href="http://example.com/2023/08/27/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-3-%E5%9F%BA%E4%B8%8E%E5%9D%90%E6%A0%87/"/>
    <id>http://example.com/2023/08/27/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-3-%E5%9F%BA%E4%B8%8E%E5%9D%90%E6%A0%87/</id>
    <published>2023-08-27T15:56:42.000Z</published>
    <updated>2023-08-31T17:39:27.521Z</updated>
    
    <content type="html"><![CDATA[<h1 id="基与坐标">基与坐标</h1><p>  在前面的章节，我们介绍了线性空间的概念，线性空间是一个抽象的概念，但在实际应用中，出于对向量运算的需求，我们通常更需要标准线性空间中的向量。为了将抽象线性空间中的向量映射到标准线性空间，我们引入了基向量的概念。抽象向量沿着基向量展开后得到坐标向量，我们用坐标向量来表示映射后的抽象向量。</p><h2 id="有限维线性空间基坐标">有限维线性空间基坐标</h2><p>  设<span class="math inline">\(V\)</span>是数域<span class="math inline">\(\mathbb{F}\)</span>上的线性空间，若有正整数<span class="math inline">\(n\)</span>，及<span class="math inline">\(V\)</span>中的向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_n\}\)</span>，使得:</p><ol type="1"><li><strong>线性无关性</strong>: 向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_n \}\)</span>为线性无关向量组.<br></li><li><strong>线性生成性</strong>: <span class="math inline">\(\forall\alpha \in V\)</span>，均可由向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_n \}\)</span>线性表示.</li></ol><p><span class="math display">\[\alpha = \alpha_{1} k_1 + \alpha_{2} k_2+ \dots + \alpha_{n} k_n = \begin{bmatrix}    \alpha_1,\alpha_2,\dots,\alpha_n \\\end{bmatrix} \begin{bmatrix}    k_1 \\    k_2 \\    \vdots \\    k_n \\\end{bmatrix} = Ak\]</span></p><p>  则称<span class="math inline">\(V\)</span>为<span class="math inline">\(n\)</span>维线性空间，向量组<span class="math inline">\(\{ \alpha_1,\alpha_2,\dots,\alpha_n\}\)</span>称为<span class="math inline">\(V\)</span>中的一个基(坐标系)，<span class="math inline">\(k \in \mathbb{F}^n\)</span>称为<span class="math inline">\(\alpha \in V\)</span>，沿着该基的坐标向量.</p><p>注：</p><p><span class="math display">\[\color{green} \begin{bmatrix}    抽 \\    象 \\    向 \\    量 \\\end{bmatrix} = \begin{bmatrix}    基矩阵 \\\end{bmatrix} \begin{bmatrix}    坐 \\    标 \\    向 \\    量 \\\end{bmatrix}\]</span></p><h2 id="关于基向量组的定理">关于基向量组的定理</h2><p><strong>定理1(基向量个数的唯一性)</strong> 设向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_m\}\)</span>及<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_s\}\)</span>分别是线性空间<span class="math inline">\(V\)</span>的两个基，则有<span class="math inline">\(m=s\)</span>.<br><strong>证明:</strong><br>  从线性空间中任意取<span class="math inline">\(p\)</span>个向量组成一个向量组<span class="math inline">\(\{v_1,v_2,\dots,v_p\}\)</span>，要求<span class="math inline">\(m \leq p, s \leq p\)</span>.<br>  由基向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_m\}\)</span>的定义可知：<br>  1. 向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_m\}\)</span>为线性无关向量组.<br>  2. 对<span class="math inline">\(\forall v \in \{v_1,v_2,\dots,v_p\},v\)</span>均可由向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_m\}\)</span>线性表示.<br>  <span class="math inline">\(\Rightarrow\)</span>向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_m\}\)</span>是向量组<span class="math inline">\(\{v_1,v_2,\dots,v_p\}\)</span>的极大线性无关组.<br>  同理可知：向量组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_s\}\)</span>也为向量组<span class="math inline">\(\{v_1,v_2,\dots,v_p\}\)</span>的极大线性无关组.<br>  由向量组的极大线性无关组中向量个数的唯一性可知: <span class="math inline">\(m=s\)</span>.</p><p><strong>定理2</strong> <span class="math inline">\(\color{green}{基实现了抽象线性空间到标准线性空间的一一映射.}\)</span><br><strong>证明:</strong><br>  设<span class="math inline">\(V\)</span>为<span class="math inline">\(n\)</span>维线性空间，向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_n\}\)</span>是<span class="math inline">\(V\)</span>的一个基.<br>  设由基向量组实现的映射为:<br><span class="math display">\[\sigma: V \longrightarrow\mathbb{F}^{n}\]</span></p><p><span class="math display">\[v \longmapsto k = \begin{bmatrix}    k_1 \\    k_2 \\    \vdots \\    k_n \\\end{bmatrix}\]</span></p><p>  <span class="math inline">\(k \in \mathbb{F}^{n}\)</span>是<span class="math inline">\(v\)</span>在基下的坐标向量.<br>  现需要验证映射<span class="math inline">\(\sigma\)</span>满足一一映射的两个条件.<br>  (1) 验证对<span class="math inline">\(\forall k \in \mathbb{F}^n,\exists v \in V\)</span>，使得<span class="math inline">\(v=\alpha_1k_1+\alpha_2k_2+\dots+\alpha_nk_n\)</span>.<br>  <span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_n\} \inV\)</span>. 由线性空间<span class="math inline">\(V\)</span>对加法与数乘封闭的性质可知:<br>  <span class="math inline">\(\exists v \in V\)</span>使得<span class="math inline">\(v =\alpha_1k_1+\alpha_2k_2+\dots+\alpha_nk_n\)</span>.<br>  (2) 验证若<span class="math inline">\(\sigma(v)=\sigma(v_0)=k\)</span>，则有<span class="math inline">\(v=v_0\)</span>.<br>  <span class="math inline">\(\sigma(v)=k \Leftrightarrowv=\alpha_1k_1+\alpha_2k_2+\dots+\alpha_nk_n\)</span>.<br>  <span class="math inline">\(\sigma(v_0)=k \Leftrightarrowv_0=\alpha_1k_1+\alpha_2k_2+\dots+\alpha_nk_n\)</span>.<br>  <span class="math inline">\(\Rightarrow v_0 = v\)</span>.<br>  综上所述映射<span class="math inline">\(\sigma\)</span>为一一映射.</p><p><strong>注：一一映射的定义</strong><br>  设映射<span class="math inline">\(\sigma: S_1 \rightarrowS_2\)</span>满足:<br>  (1)满射：对<span class="math inline">\(\forall s_2 \in S_2, \existss_1 \in S_1, \sigma(s_1)=s_2 .\)</span><br>  (2)单射：若<span class="math inline">\(\sigma(s_1)=\sigma(s^{*}_{1})\)</span>,则<span class="math inline">\(s_1=s^{*}_{1}.\)</span><br>  则称映射<span class="math inline">\(\sigma\)</span>为集合<span class="math inline">\(S_1\)</span>与<span class="math inline">\(S_2\)</span>之间的一一映射.</p><h2 id="标准线性空间mathbbrn的标准基与一般基">标准线性空间<span class="math inline">\(\mathbb{R}^n\)</span>的标准基与一般基</h2><h3 id="标准基">标准基</h3><p>  标准线性空间<span class="math inline">\(\mathbb{R}^n\)</span>的标准基：</p><p><span class="math display">\[e_1=\begin{bmatrix}    1 \\    0 \\    \vdots \\    0 \\\end{bmatrix}, e_2 = \begin{bmatrix}    0 \\    1 \\    \vdots \\    0\end{bmatrix}, \dots, e_n = \begin{bmatrix}    0 \\    0 \\    \vdots \\    1\end{bmatrix}\]</span></p><p><strong>证明:</strong><br>  (1)先证明标准基向量组的线性无关性：<br>  令 <span class="math inline">\(I_n =\begin{bmatrix}  e_1,e_2,\dots,e_n\end{bmatrix}\)</span>，有线性方程组<span class="math inline">\(I_n x =0\)</span>.<br>  该方程组仅有<span class="math inline">\(x=0\)</span>唯一解，故标准基向量组<span class="math inline">\(\{e_1,e_2,\dots,e_n\}\)</span>线性无关.<br>  (2)再证标准基向量组的线性生成性：<br>  对<span class="math inline">\(\forall v \in\mathbb{R}^n\)</span>，判断线性方程组<span class="math inline">\(I_n x =v\)</span>是否有解.<br>  <span class="math inline">\(I_n x = v \Rightarrow x = v\Rightarrow\)</span>方程有解<span class="math inline">\(\Rightarrowv\)</span>可由标准基向量组<span class="math inline">\(\{e_1,e_2,\dots,e_n\}\)</span>线性表示.<br>  综上所述，向量组<span class="math inline">\(\{e_1,e_2,\dots,e_n\}\)</span>可作为标准线性空间<span class="math inline">\(\mathbb{R}^n\)</span>的基向量组.</p><h3 id="一般基">一般基</h3><p>  <span class="math inline">\(\alpha_1,\alpha_2,\dots,\alpha_n \in\mathbb{R}^n\)</span>，向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_n\}\)</span>构成标准线性空间<span class="math inline">\(\mathbb{R}^n\)</span>的一组基的充要条件为：向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_n\}\)</span>的秩为<span class="math inline">\(n\)</span>.</p><p><strong>证明:</strong><br>  向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_n\}\)</span>的秩为<span class="math inline">\(n\)</span> <span class="math inline">\(\Rightarrow\)</span> 向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_n\}\)</span>线性无关<br>  令<span class="math inline">\(A=\begin{bmatrix}  \alpha_1,\alpha_2,\dots,\alpha_n\end{bmatrix}\)</span>，有<span class="math inline">\(rank(A) =n\)</span>.<br>  对<span class="math inline">\(\forall \beta \in\mathbb{R}^n\)</span>，判断矩阵方程<span class="math inline">\(Ax=\beta\)</span>是否有解.<br>  <span class="math inline">\(\becauserank(A)=rank([A,b])=n\)</span>，<span class="math inline">\(\therefore\)</span>方程<span class="math inline">\(Ax=\beta\)</span>有解.</p><p><span class="math inline">\(\color{green}{注：线性方程组 Ax=\beta的几何语言：在n维线性空间中，将向量\beta沿着矩阵A的列向量组所构成的基展开.}\)</span></p><h2 id="多项式函数空间作为线性空间的基">多项式函数空间作为线性空间的基</h2><p>  在第一节我们已经说明函数空间<span class="math inline">\(V=\mathcal{F}(I,\mathbb{F}^n)\)</span>可以作为线性空间。多项式是函数的一种形式，我们可以定义以多项式为元素的线性空间：</p><p><span class="math display">\[\mathbb{F}[x]=\{以x为自变量，以数域\mathbb{F}中的数为系数的多项式\}=\{f=a_0+a_1x+a_2x^2+\dots\vert a_i \in \mathbb{F}, i =1,2,\dots\}\]</span></p><p>  通过分析我们可以得知这是一个无限维的线性空间，这里我们不讨论无限维线性空间的基，通过对<span class="math inline">\(x\)</span>的次数添加限制，我们可以将这个线性空间变为有限维：</p><p><span class="math display">\[\mathbb{F}_n[x]=\{以x为自变量，以数域\mathbb{F}中的数为系数,次数小于n的多项式\}=\{f=a_0+a_1x++\dots+a_{n-1}x^{n-1}\vert a_i \in \mathbb{F}, i =1,2,\dots,n-1\}\]</span></p><p>  <span class="math inline">\(\mathbb{F}_n[x]\)</span>是一个<span class="math inline">\(n\)</span>维线性空间，取<span class="math inline">\(\mathbb{F}=\mathbb{R}\)</span>，接下来我们来讨论<span class="math inline">\(\mathbb{R}_n[x]\)</span>的基向量组。<br>  在高等代数中，我们知道多项式函数空间中的元素与标准线性空间中的元素一一对应，对<span class="math inline">\(\forall f \in \mathbb{R}_n[x]\)</span>，有：</p><p><span class="math display">\[f=a_0+a_1x++\dots+a_{n-1}x^{n-1}=\begin{bmatrix}    1,x,\dots,x^{n-1} \\\end{bmatrix}\begin{bmatrix}    a_0 \\    a_1 \\    \vdots \\    a_{n-1} \\\end{bmatrix}, f \rightarrow \begin{bmatrix}    a_0 \\    a_1 \\    \vdots \\    a_{n-1} \\\end{bmatrix}\]</span></p><p>  可以取<span class="math inline">\(\{1,x,\dots,x^{n-1}\}\)</span>为<span class="math inline">\(\mathbb{R}_n[x]\)</span>的一组基，以下是证明<span class="math inline">\(\{1,x,\dots,x^{n-1}\}\)</span>可以作为<span class="math inline">\(\mathbb{R}_n[x]\)</span>的基.<br><strong>证明:</strong><br>  (1)线性无关性<br>  若<span class="math inline">\(a_0+a_1x+\dots+a_{n-1}x^{n-1}=0\)</span>，带入<span class="math inline">\(x=1,2,\dots,n\)</span>，得：</p><p><span class="math display">\[\begin{bmatrix}    1^{0}&amp;1^{1}&amp;\dots&amp;1^{n-1} \\    2^{0}&amp;2^{1}&amp;\dots&amp;2^{n-1} \\    \vdots&amp;\vdots&amp;&amp;\vdots \\    n^{0}&amp;n^{1}&amp;\dots&amp;n^{n-1} \\\end{bmatrix}\begin{bmatrix}    a_0 \\    a_1 \\    \vdots \\    a_{n-1} \\\end{bmatrix}=\begin{bmatrix}    0 \\    0 \\    \vdots \\    0\end{bmatrix}\]</span></p><p>  令<span class="math inline">\(A=\begin{bmatrix}  1^{0}&amp;1^{1}&amp;\dots&amp;1^{n-1}\\  2^{0}&amp;2^{1}&amp;\dots&amp;2^{n-1}\\  \vdots&amp;\vdots&amp;&amp;\vdots\\  n^{0}&amp;n^{1}&amp;\dots&amp;n^{n-1} \\\end{bmatrix},a=\begin{bmatrix}  a_0 \\  a_1 \\  \vdots \\  a_{n-1} \\\end{bmatrix}\)</span>，<span class="math inline">\(det(A)\)</span>为范德蒙行列式,且<span class="math inline">\(det(A)\neq 0\)</span>，故<span class="math inline">\(rank(A)=n\)</span>.<br>  <span class="math inline">\(rank(A)=n \Rightarrow\)</span> 方程<span class="math inline">\(Aa=0\)</span>只有零解,即 <span class="math inline">\(a_i=0, i=1,2,\dots,n-1\)</span>.<br>  故<span class="math inline">\(\{1,x,\dots,x^{n-1}\}\)</span>线性无关.<br>  (2)线性生成性<br>  由多项式的定义可知，<span class="math inline">\(\mathbb{R}_n[x]\)</span>中的元素均可由<span class="math inline">\(\{1,x,\dots,x^{n-1}\}\)</span>线性表示.<br>  综上所述：<span class="math inline">\(\{1,x,\dots,x^{n-1}\}\)</span>可以作为<span class="math inline">\(\mathbb{R}_n[x]\)</span>的一组基.</p><p>注：<span class="math inline">\(\{1,x,\dots,x^{n-1}\}\)</span>对应于<span class="math inline">\(\mathbb{R}^n\)</span>中的标准基<br>  设<span class="math inline">\(\sigma\)</span>为以<span class="math inline">\(\{1,x,\dots,x^{n-1}\}\)</span>为基时，从<span class="math inline">\(\mathbb{R}_n[x]\)</span>到<span class="math inline">\(\mathbb{R}^n\)</span>的映射，有：</p><p><span class="math display">\[1=\begin{bmatrix}    1,x,\dots,x^{n-1} \\\end{bmatrix}\begin{bmatrix}    1 \\    0 \\    \vdots \\    0 \\\end{bmatrix}, \sigma: 1 \rightarrow \begin{bmatrix}    1 \\    0 \\    \vdots \\    0 \\\end{bmatrix}\]</span></p><p><span class="math display">\[x=\begin{bmatrix}    1,x,\dots,x^{n-1} \\\end{bmatrix}\begin{bmatrix}    0 \\    1 \\    \vdots \\    0 \\\end{bmatrix}, \sigma: x \rightarrow \begin{bmatrix}    0 \\    1 \\    \vdots \\    0 \\\end{bmatrix}\]</span></p><p><span class="math display">\[\vdots\]</span></p><p><span class="math display">\[x^{n-1}=\begin{bmatrix}    1,x,\dots,x^{n-1} \\\end{bmatrix}\begin{bmatrix}    0 \\    0 \\    \vdots \\    1 \\\end{bmatrix}, \sigma: x^{n-1} \rightarrow \begin{bmatrix}    0 \\    0 \\    \vdots \\    1 \\\end{bmatrix}\]</span></p>]]></content>
    
    
    <summary type="html">本节主要介绍线性空间中的基向量以及坐标向量的概念。</summary>
    
    
    
    <category term="矩阵分析" scheme="http://example.com/categories/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90/"/>
    
    
  </entry>
  
  <entry>
    <title>深度学习-1.使用Pytorch搭建深度学习模型的基本框架——以COVID-19 Cases Prediction为例</title>
    <link href="http://example.com/2023/08/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-1.%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A1%86%E6%9E%B6%E2%80%94%E2%80%94%E4%BB%A5COVID-19%20Cases%20Prediction%E4%B8%BA%E4%BE%8B/"/>
    <id>http://example.com/2023/08/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-1.%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BA%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A1%86%E6%9E%B6%E2%80%94%E2%80%94%E4%BB%A5COVID-19%20Cases%20Prediction%E4%B8%BA%E4%BE%8B/</id>
    <published>2023-08-14T05:43:29.000Z</published>
    <updated>2023-08-15T17:32:44.757Z</updated>
    
    <content type="html"><![CDATA[<h1 id="使用pytorch搭建深度学习模型的基本框架">使用Pytorch搭建深度学习模型的基本框架</h1><p>  自2012年Alexnet在Imagenet图像分类竞赛中一鸣惊人，以神经网络算法为主体的深度学习技术在人工智能领域兴起，而后诸如卷积神经网络(CNN)，残差网络(Resnet)，生成式对抗网络(GAN)，自注意力模型(Transformer)等众多性能强大的算法模型被提出，使得人工智能领域的研究与应用进入了一个蓬勃发展的阶段。2016年，DeepMind推出围棋人工智能AlphaGo，其以4:1战胜围棋世界冠军李世石，让世人认识到了深度学习技术的强大。2023年，基于Transformer的LLM模型ChatGPT横空出世，这场来自AI领域的技术革命第一次距离我们如此之近。无人知晓深度学习未来能给人类世界带来多大的变革，它能否最终实现强人工智能，带领我们进入科幻电影中的世界，但至少目前，深度学习仍牢牢占据学术与工业界研究的主流。<br>  在众多用于实现深度学习技术的框架中，Pytorch与TensorFlow目前被使用得最为广泛，而Pyotrch以其简洁的语法与强大的生态颇受学者的青睐，成为目前学术研究最流行的深度学习框架，本节会以COVID-19Cases为例，使用Pytorch搭建一个深度学习模型。这是一个非常简单的回归任务，在完成这个任务的过程中，我们将会了解使用Pytorch搭建深度学习模型的流程、各种函数的作用以及训练模型的步骤。</p><h2 id="task-description">Task Description</h2><ul><li><p><strong>Objectives</strong></p><ul><li>Solve a regression problem with deep neural networks(DNN)<br></li><li>Understand basic DNN training steps.</li><li>Get familiar with PyTorch.</li></ul></li><li><p><strong>Task</strong><br>  <strong>COVID-19 Cases Prediction:</strong> Given survey results inthe past 5 days in a specific states in U.S., then predict thepercentage of new tested positive cases in the 5th day.</p></li><li><p><strong>Data</strong></p><ul><li>Training Data: 2699 samples<br></li><li>Testing Data: 1078 samples<br></li><li>Feature Infactors(117):<ul><li>States(37, encoded to one-hot vector)</li><li>COVID-like illness(4*5)<ul><li>cli、ili ...</li></ul></li><li>Behavior Indicators(8*5)<ul><li>wearing_mask、travel_outside_state ...</li></ul></li><li>Mental Health Indicators(3*5)<ul><li>anxious、depressed ...</li></ul></li><li>Tested Positive Cases(1*5)<ul><li>tested_positive(<strong>this is what we want topredict</strong>)</li></ul></li></ul></li></ul></li></ul><h2 id="download-data">Download Data</h2><p>  本案例的数据集来自于Kaggle，可以访问以下网站下载数据集.</p><ul><li>Data Source:https://www.kaggle.com/competitions/ml2022spring-hw1/overview</li></ul><h2 id="import-packages">Import Packages</h2><p>  在开始任务前首先导入我吗需要使用到的Python库</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Numerical Operations</span><br><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># Reading/Writing Data</span><br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> csv<br><br><span class="hljs-comment"># For Progress Bar</span><br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><br><span class="hljs-comment"># Pytorch</span><br><span class="hljs-keyword">import</span> torch <br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset, DataLoader, random_split<br><br><span class="hljs-comment"># For plotting learning curve</span><br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br></code></pre></td></tr></tbody></table></figure><p>  一些库的主要功能：</p><ul><li><code>tqdm</code>:用于在循环中显示进度条，以增强用户对程序运行进度的可视化体验。主要功能有<strong>进度条显示、时间估算、定制输出、支持多种数据结构、并发安全.</strong><br></li><li><code>torch.nn</code>:用于构建神经网络模型。它提供了各种用于构建神经网络层、损失函数、优化器等的类和函数，使用户能够方便地创建、训练和部署各种类型的神经网络模型。主要功能有<strong>神经网络层的构建、损失函数的定义、优化器、自定义模型、数据转换层、模型的保存和加载.</strong></li><li><code>torch.utils.data</code>:用于处理和管理数据加载、预处理以及批量处理等任务。它提供了一组工具和类，帮助用户有效地加载、处理和传输数据到神经网络模型中，从而方便地进行训练、验证和测试。主要功能有<strong>数据加载与管理、数据预处理、数据批处理、并行加载、迭代加载.</strong><br></li><li><code>torch.utils.tensorboard</code>:用于在训练过程中可视化模型训练和性能指标，可以帮助深度学习研究人员和工程师实时监视、分析和优化他们的模型训练过程。主要功能有<strong>训练过程的可视化、模型结构的可视化、嵌入向量的可视化、分布的可视化、图像和音频的可视化.</strong></li></ul><h2 id="set-random-seed">Set Random Seed</h2><p>  由于深度学习的训练过程包含一定的随机性，例如网络参数初始化、随机梯度下降、Dropout等，在学术研究中为了使得结果可以复现，我们通常需要事先设置随机数种子以固定模型的随机性，其代码如下:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">same_seed</span>(<span class="hljs-params">seed</span>): <br>    <span class="hljs-comment"># Fixes random number generator seeds for reproducibility.</span><br>    torch.backends.cudnn.deterministic = <span class="hljs-literal">True</span><br>    torch.backends.cudnn.benchmark = <span class="hljs-literal">False</span><br>    np.random.seed(seed)<br>    torch.manual_seed(seed)<br>    <span class="hljs-keyword">if</span> torch.cuda.is_available():<br>        torch.cuda.manual_seed(seed)<br>        torch.cuda.manual_seed_all(seed)<br></code></pre></td></tr></tbody></table></figure><p>  一些主要函数的功能：</p><ul><li><code>torch.bankends.cudnn.deterministic</code>:用于控制在使用CUDA进行深度学习训练时是否启用确定性计算。在深度学习中，训练过程中使用CUDA可以显著加速计算，但由于浮点数的不精确性和优化算法的随机性，相同的代码在不同的运行环境中可能会产生不同的结果。但需要注意的是，启用确定性计算会带来一定的计算效率损失，因为一些优化策略可能会被禁用，从而降低了性能。</li><li><code>torch.backends.cudnn.benchmark</code>: 用于控制CuDNN（CUDADeep Neural Networklibrary，NVIDIA深度神经网络库）在使用CUDA加速时是否自动寻找最适合当前硬件环境的优化算法。当设置为True时，将会让程序在开始时花费一点额外时间，为整个网络的每个卷积层搜索最适合它的卷积实现算法，进而实现网络的加速，然而，不同的卷积算法可能在计算精度和数值稳定性方面有微小差异，这可能导致每次前向传播的结果略微不同。当设置为False时，CuDNN不再搜寻最佳算法，而是选择一个固定的卷积算法。这个固定的算法在相同的输入数据和参数情况下产生相同的输出，因为它不受运行时的微小变化影响。<br></li><li><code>torch.manual_seed()</code>:用于为CPU设置随机数生成器的种子，从而控制生成的随机数序列。</li><li><code>torch.cuda.is_available()</code>:用于检查当前系统是否支持CUDA，以及是否安装了可用的GPU设备。</li><li><code>torch.cuda.manual_seed()</code>:为GPU设置随机数种子，从而控制生成的随机数序列。</li><li><code>torch.cuda.manual_seed_all()</code>:如果使用的是多GPU模型，其可以设置用于在所有GPU上生成随机数的种子。</li></ul><h2 id="dataset">Dataset</h2><p>  在PyTorch中，我们需要将原始数据集定义为一个Dataset实例，定义Dataset实例的作用是将数据加载和预处理封装成一个可迭代的对象，以便在训练过程中有效地加载和使用数据。其代码如下：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">COVID19Dataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-string">'''</span><br><span class="hljs-string">    x: Features.</span><br><span class="hljs-string">    y: Targets, if none, do prediction.</span><br><span class="hljs-string">    '''</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, x, y=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-keyword">if</span> y <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            self.y = y<br>        <span class="hljs-keyword">else</span>:<br>            self.y = torch.FloatTensor(y)<br>        self.x = torch.FloatTensor(x)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        <span class="hljs-keyword">if</span> self.y <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">return</span> self.x[idx]<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">return</span> self.x[idx], self.y[idx]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.x)<br></code></pre></td></tr></tbody></table></figure><p>  Dataset类中一些函数的功能：</p><ul><li><code>__init__()</code>:Dataset类的构造函数，用于初始化数据集的属性和参数。</li><li><code>__getitem__()</code>:用于根据索引获取数据集中的一个样本。在训练过程中，DataLoader会通过迭代访问数据集，调用__getitem__来获取样本。</li><li><code>__len__()</code>:返回数据集中样本的数量，通常通过获取数据的长度来实现。</li></ul><h2 id="feature-selection">Feature Selection</h2><p>  原始数据集包含众多的特征，但如果将所有的特征作为训练数据，可能会造成训练时间过长，模型过拟合等问题。实际上特征之间往往存在多重共线性，对于这个任务，我们并不需要数量非常庞大的特征，这时我们需要进行特征工程方面的工作，这里不展开解释特征工程的方法，有兴趣可以自行查阅相关资料。总得来说，当我们的数据集包含众多特征时，我们需要保有进行特征工程的意识。在本案例为了简单起见，省略掉了特征筛选的过程，以人为设置的特征作为筛选结果。<br></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">select_feat</span>(<span class="hljs-params">train_data, valid_data, test_data, select_all=<span class="hljs-literal">True</span></span>):<br>    <span class="hljs-string">'''Selects useful features to perform regression'''</span><br>    y_train, y_valid = train_data[:,-<span class="hljs-number">1</span>], valid_data[:,-<span class="hljs-number">1</span>]<br>    raw_x_train, raw_x_valid, raw_x_test = train_data[:,:-<span class="hljs-number">1</span>], valid_data[:,:-<span class="hljs-number">1</span>], test_data<br><br>    <span class="hljs-keyword">if</span> select_all:<br>        feat_idx = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(raw_x_train.shape[<span class="hljs-number">1</span>]))<br>    <span class="hljs-keyword">else</span>:<br>        feat_idx = [<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>] <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> Select suitable feature columns.</span><br>        <br>    <span class="hljs-keyword">return</span> raw_x_train[:,feat_idx], raw_x_valid[:,feat_idx], raw_x_test[:,feat_idx], y_train, y_valid<br></code></pre></td></tr></tbody></table></figure><p></p><h2 id="dataloader">Dataloader</h2><p>  在定义了Dataset实例后，我们通常需要将Dataset实例传递给Dataloader类。Dataloader是一个迭代器，用于加载和处理训练数据，其可以将数据集划分成小批量的数据，并可以自动进行数据预处理、洗牌和GPU加速等操作。在定义Dataloader时，我们需要确定TrainingData、Testing Data、代码如下：<br></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Set seed for reproducibility</span><br>same_seed(config[<span class="hljs-string">'seed'</span>])<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_valid_split</span>(<span class="hljs-params">data_set, valid_ratio, seed</span>):<br>    <span class="hljs-comment">#Split provided training data into training set and validation set</span><br>    valid_set_size = <span class="hljs-built_in">int</span>(valid_ratio * <span class="hljs-built_in">len</span>(data_set)) <br>    train_set_size = <span class="hljs-built_in">len</span>(data_set) - valid_set_size<br>    train_set, valid_set = random_split(data_set, [train_set_size, valid_set_size], generator=torch.Generator().manual_seed(seed))<br>    <span class="hljs-keyword">return</span> np.array(train_set), np.array(valid_set)<br><br><span class="hljs-comment"># train_data size: 2699 x 118 (id + 37 states + 16 features x 5 days) </span><br><span class="hljs-comment"># test_data size: 1078 x 117 (without last day's positive rate)</span><br>train_data, test_data = pd.read_csv(<span class="hljs-string">'./covid.train.csv'</span>).values, pd.read_csv(<span class="hljs-string">'./covid.test.csv'</span>).values<br>train_data, valid_data = train_valid_split(train_data, config[<span class="hljs-string">'valid_ratio'</span>], config[<span class="hljs-string">'seed'</span>])<br><br><span class="hljs-comment"># Print out the data size.</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f"""train_data size: <span class="hljs-subst">{train_data.shape}</span> </span><br><span class="hljs-string">valid_data size: <span class="hljs-subst">{valid_data.shape}</span> </span><br><span class="hljs-string">test_data size: <span class="hljs-subst">{test_data.shape}</span>"""</span>)<br><br><span class="hljs-comment"># Select features</span><br>x_train, x_valid, x_test, y_train, y_valid = select_feat(train_data, valid_data, test_data, config[<span class="hljs-string">'select_all'</span>])<br><br><span class="hljs-comment"># Print out the number of features.</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f'number of features: <span class="hljs-subst">{x_train.shape[<span class="hljs-number">1</span>]}</span>'</span>)<br><br>train_dataset, valid_dataset, test_dataset = COVID19Dataset(x_train, y_train),COVID19Dataset(x_valid, y_valid), COVID19Dataset(x_test)<br><br><span class="hljs-comment"># Pytorch data loader loads pytorch dataset into batches.</span><br>train_loader = DataLoader(train_dataset, batch_size=config[<span class="hljs-string">'batch_size'</span>], shuffle=<span class="hljs-literal">True</span>, pin_memory=<span class="hljs-literal">True</span>)<br>valid_loader = DataLoader(valid_dataset, batch_size=config[<span class="hljs-string">'batch_size'</span>], shuffle=<span class="hljs-literal">True</span>, pin_memory=<span class="hljs-literal">True</span>)<br>test_loader = DataLoader(test_dataset, batch_size=config[<span class="hljs-string">'batch_size'</span>], shuffle=<span class="hljs-literal">False</span>, pin_memory=<span class="hljs-literal">True</span>)  <br></code></pre></td></tr></tbody></table></figure><p></p><p>  在这段代码中，我们首先通常前文定义的<code>same_seed()</code>函数固定随机数种子，使得之后的一系列操作的结果可复现。然后我们定义了<code>train_valid_split()</code>函数，这个函数用于将TrainingData进行再次划分，分为真正用于训练的数据与用于检验模型泛化能力的数据，通过<code>valid_ratio</code>参数，我们可以设置traindata和validdata的划分比例。定义好函数后，我们读取原始csv文件，得到原始的TrainingData和TestingData，利用<code>train_valid_split()</code>函数得到划分好的traindata与validdata。此时，这些数据集仍包含着所有的特征，我们需要通过前文定义的<code>select_feat()</code>函数进行特征筛选，得到正在用于本次任务的数据。最后我们使用这些数据定义Dataset实例，并将定义好的Dataset传递给<code>Dataloader</code>用于之后训练模型。</p><p>  <code>Dataloader</code>的一些主要参数及其作用：</p><ul><li><code>dataset</code>:指定要加载的Dataset实例，这是DataLoader的必需参数，用于提供数据样本。</li><li><code>batch_size</code>:指定每个批次中包含的样本数量。将数据划分成小批次可以在训练时提高内存的使用效率。</li><li><code>shuffle</code>:设置为True时，在每次返回一批次前会对数据进行随机洗牌。这有助于提高模型的泛化能力。默认值为False。</li><li><code>pin_memory</code>：如果在GPU上训练，可以将此参数设置为True，以便在加载数据时将数据置于CUDA固定内存中，从而加速数据传输。</li><li><code>drop_last</code>：如果数据样本数量不能整除批次大小，设置为True时，会丢弃最后一个不完整的批次。默认值为False。</li><li><code>num_workers</code>：指定用于数据加载的并行线程数。通过并行加载数据可以加快速度，特别是在数据集较大时。</li></ul><h2 id="neural-network-model">Neural Network Model</h2><p>  准备好数据后，我们便需要构建本次任务所用的深度学习模型，这里我构建了一个简单的三层前馈神经网络模型，这个神经网络包含一个输入层，一个隐藏层，一个输出层，其代码如下:<br></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">My_Model</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_dim</span>):<br>        <span class="hljs-built_in">super</span>(My_Model, self).__init__()<br>        <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> modify model's structure, be aware of dimensions. </span><br>        self.layers = nn.Sequential(<br>            nn.Linear(input_dim, <span class="hljs-number">16</span>),<br>            nn.ReLU(),<br>            nn.Linear(<span class="hljs-number">16</span>, <span class="hljs-number">8</span>),<br>            nn.ReLU(),<br>            nn.Linear(<span class="hljs-number">8</span>, <span class="hljs-number">1</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.layers(x)<br>        x = x.squeeze(<span class="hljs-number">1</span>) <span class="hljs-comment"># (B, 1) -&gt; (B)</span><br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></tbody></table></figure><p></p><p>  一些函数及类的主要功能：</p><ul><li><p><code>nn.Module</code>:提供了一种组织和管理神经网络组件的方式，使得模型的构建、参数管理和前向传播等过程更加简洁和可控。我们在实际应用中定义的模型<code>My_Model</code>需要继承<code>nn.Module</code>类。<br></p></li><li><p><code>nn.Sequrntial()</code>:PyTorch中的一个模型容器，用于按顺序组合多个神经网络模块，从而构建一个序列式的神经网络模型。它可以在不需要自定义模型类的情况下，方便地定义简单的神经网络结构。<br></p></li><li><p><code>nn.Linear()</code>:用于定义线性变换（全连接层）。它将输入数据与权重矩阵相乘，并添加一个偏置，从而实现线性变换。nn.Linear()主要用于神经网络中的全连接层，将输入特征映射到输出特征。  除了使用<code>nn.Linear()</code>进行线性神经网络层，我们还可以根据任务的不同使用诸如卷积神经网络层<code>nn.convXd</code>、循环神经网络层<code>nn.RNN()</code>等。</p></li><li><p><code>nn.ReLU()</code>: 用于实现激活函数 ReLU（Rectified LinearActivation）。ReLU是深度学习中常用的激活函数之一，它对输入进行非线性变换，将负值变为零，保持正值不变。ReLU激活函数在神经网络中引入非线性性质，有助于模型学习复杂的特征和表示。<br>  除了ReLU之外，还有一些常用的激活函数，例如<code>nn.Sigmoid()</code>(Sigmoid函数)、<code>nn.Tanh()</code>(双曲正切激活函数)。我们可以根据训练数据的特点，选择合适的激活函数，或者通过实验确定最佳激活函数。<br></p></li><li><p><code>forward()</code>:用于将训练数据通过网络进行前向传播。</p></li></ul><h2 id="training-loop">Training Loop</h2><p>  在准备好数据与模型后，下一步就是训练模型，训练模型的主要流程如下：<br></p><figure class="highlight markdown"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs markdown">设置损失函数<br>定义优化器<br>定义tensorboard<br>总训练轮次、当前训练轮次、当前最佳误差、无效训练次数等参数初始化<br><br>for 每一训练轮次 in 总训练轮次：<br><span class="hljs-code">    设置模型为训练模式</span><br><span class="hljs-code">    定义一个空的训练误差列表</span><br><span class="hljs-code">    将训练数据传递给tqdm</span><br><span class="hljs-code">    for 每一批次特征数据、标签数据 in tqdm(训练数据):</span><br><span class="hljs-code">        初始化优化器梯度值</span><br><span class="hljs-code">        将数据移动到GPU中</span><br><span class="hljs-code">        将特征数据输入模型，通过正向传播得到标签数据的预测值</span><br><span class="hljs-code">        通过标签数据的预测值与真实值得到误差</span><br><span class="hljs-code">        误差反向传播，得到误差对于网络参数的梯度</span><br><span class="hljs-code">        利用梯度下降算法更新网络参数</span><br><span class="hljs-code">        step += 1</span><br><span class="hljs-code">        将本批次的误差添加到训练误差列表中</span><br><span class="hljs-code">        自定义训练进度条内容</span><br><span class="hljs-code">    通过对训练误差列表求平均得到这一轮次的平均训练误差</span><br><span class="hljs-code">    将setp与平均训练误差添加到tensorboard中</span><br><span class="hljs-code">    将模型设置为评估模式</span><br><span class="hljs-code">    定义一个空的评估误差列表</span><br><span class="hljs-code">    for 每一批次特征数据、标签数据 in 评估数据:</span><br><span class="hljs-code">        将数据移动到GPU中</span><br><span class="hljs-code">        with 初始化优化器梯度值:</span><br><span class="hljs-code">            将特征数据输入当前模型，通过正向传播得到标签数据的预测值</span><br><span class="hljs-code">            通过标签数据的预测值与真实值得到误差</span><br><span class="hljs-code">        将本批次的误差添加到评估误差列表</span><br><span class="hljs-code">    通过对评估误差列表求平均得到这一轮次的平均评估误差</span><br><span class="hljs-code">    print(当前训练轮次/平均训练误差/平均评估误差)</span><br><span class="hljs-code">    将setp与平均评估误差添加到tensorboard中</span><br><span class="hljs-code">    if 平均评估误差 &lt; 当前最佳误差:</span><br><span class="hljs-code">        将当前最佳误差设置为平均评估误差</span><br><span class="hljs-code">        保存当前模型</span><br><span class="hljs-code">        初始化无效训练次数</span><br><span class="hljs-code">    else:</span><br><span class="hljs-code">        无效训练次数 += 1</span><br><span class="hljs-code">    if 无效训练次数 &gt; 所设置的无效训练次数上限:</span><br><span class="hljs-code">        停止训练</span><br></code></pre></td></tr></tbody></table></figure><p></p><p>  这一流程的代码如下：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">trainer</span>(<span class="hljs-params">train_loader, valid_loader, model, config, device</span>):<br><br>    criterion = nn.MSELoss(reduction=<span class="hljs-string">'mean'</span>) <span class="hljs-comment"># Define your loss function, do not modify this.</span><br><br>    <span class="hljs-comment"># Define your optimization algorithm. </span><br>    optimizer = torch.optim.SGD(model.parameters(), lr=config[<span class="hljs-string">'learning_rate'</span>], momentum=<span class="hljs-number">0.9</span>) <br><br>    writer = SummaryWriter() <span class="hljs-comment"># Writer of tensoboard.</span><br><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.isdir(<span class="hljs-string">'./models'</span>):<br>        os.mkdir(<span class="hljs-string">'./models'</span>) <span class="hljs-comment"># Create directory of saving models.</span><br><br>    n_epochs, best_loss, step, early_stop_count = config[<span class="hljs-string">'n_epochs'</span>], math.inf, <span class="hljs-number">0</span>, <span class="hljs-number">0</span><br><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_epochs):<br>        model.train() <span class="hljs-comment"># Set your model to train mode.</span><br>        loss_record = []<br><br>        <span class="hljs-comment"># tqdm is a package to visualize your training progress.</span><br>        train_pbar = tqdm(train_loader, position=<span class="hljs-number">0</span>, leave=<span class="hljs-literal">True</span>)<br><br>        <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> train_pbar:<br>            optimizer.zero_grad()               <span class="hljs-comment"># Set gradient to zero.</span><br>            x, y = x.to(device), y.to(device)   <span class="hljs-comment"># Move your data to device. </span><br>            pred = model(x)             <br>            loss = criterion(pred, y)<br>            loss.backward()                     <span class="hljs-comment"># Compute gradient(backpropagation).</span><br>            optimizer.step()                    <span class="hljs-comment"># Update parameters.</span><br>            step += <span class="hljs-number">1</span><br>            loss_record.append(loss.detach().item())<br>            <br>            <span class="hljs-comment"># Display current epoch number and loss on tqdm progress bar.</span><br>            train_pbar.set_description(<span class="hljs-string">f'Epoch [<span class="hljs-subst">{epoch+<span class="hljs-number">1</span>}</span>/<span class="hljs-subst">{n_epochs}</span>]'</span>)<br>            train_pbar.set_postfix({<span class="hljs-string">'loss'</span>: loss.detach().item()})<br><br>        mean_train_loss = <span class="hljs-built_in">sum</span>(loss_record)/<span class="hljs-built_in">len</span>(loss_record)<br>        writer.add_scalar(<span class="hljs-string">'Loss/train'</span>, mean_train_loss, step)<br><br>        model.<span class="hljs-built_in">eval</span>() <span class="hljs-comment"># Set your model to evaluation mode.</span><br>        loss_record = []<br>        <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> valid_loader:<br>            x, y = x.to(device), y.to(device)<br>            <span class="hljs-keyword">with</span> torch.no_grad():<br>                pred = model(x)<br>                loss = criterion(pred, y)<br><br>            loss_record.append(loss.item())<br>            <br>        mean_valid_loss = <span class="hljs-built_in">sum</span>(loss_record)/<span class="hljs-built_in">len</span>(loss_record)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f'Epoch [<span class="hljs-subst">{epoch+<span class="hljs-number">1</span>}</span>/<span class="hljs-subst">{n_epochs}</span>]: Train loss: <span class="hljs-subst">{mean_train_loss:<span class="hljs-number">.4</span>f}</span>, Valid loss: <span class="hljs-subst">{mean_valid_loss:<span class="hljs-number">.4</span>f}</span>'</span>)<br>        writer.add_scalar(<span class="hljs-string">'Loss/valid'</span>, mean_valid_loss, step)<br><br>        <span class="hljs-keyword">if</span> mean_valid_loss &lt; best_loss:<br>            best_loss = mean_valid_loss<br>            torch.save(model.state_dict(), config[<span class="hljs-string">'save_path'</span>]) <span class="hljs-comment"># Save your best model</span><br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">'Saving model with loss {:.3f}...'</span>.<span class="hljs-built_in">format</span>(best_loss))<br>            early_stop_count = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">else</span>: <br>            early_stop_count += <span class="hljs-number">1</span><br><br>        <span class="hljs-keyword">if</span> early_stop_count &gt;= config[<span class="hljs-string">'early_stop'</span>]:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">'\nModel is not improving, so we halt the training session.'</span>)<br>            <span class="hljs-keyword">return</span><br></code></pre></td></tr></tbody></table></figure><p>  训练流程中一些函数的作用及参数:</p><ul><li><code>nn.MSELoss()</code>: PyTorch中的一个损失函数模块，用于计算均方误差损失。   常用的一些损失函数:交叉熵损失<code>nn.CrossEntropyLoss()</code>，负对数似然损失<code>nn.NLLLoss()</code>，KL散度损失<code>nn.KLDivLoss()</code>等，可以更具任务特点与所用模型选择合适的损失函数，例如均方误差损失多用于回归任务，交叉熵损失多用于分类任务，KL散度损失一般用于GAN模型。<br></li><li><code>torch.optim.SGD()</code>: 用于实现随机梯度下降（StochasticGradient Descent，SGD）优化算法。SGD是深度学习中最基本和常用的优化算法之一，用于调整模型的参数以最小化损失函数。其重要参数:<ul><li><code>params</code>：这是一个模型参数的可迭代对象，指定了需要进行优化的参数。一般通过<code>model.parameters()</code>来获取模型中的参数列表。</li><li><code>lr</code>：学习率，控制参数更新的步长。它决定了每次参数更新的幅度，过大可能导致不稳定的训练，过小可能导致收敛速度缓慢。<br></li><li><code>momentum</code>: 动量，用于加速梯度下降过程。设置一个介于 0 到1之间的值，代表在更新参数时考虑前一次的动量。较大的动量值可以帮助跳出局部最小值。</li></ul></li></ul><p>  除开基础的SGD优化方法，深度学习中还有Adam<code>torch.optim.Adam()</code>，RMSprop<code>torch.optim.RMSprop()</code>，Adagrad<code>torch.optim.Adagrad()</code>等优化算法，它们各种适应不同的数据特点。想进一步了解深度学习中的优化算法可以查阅相关资料。</p><p>  完成模型训练的流程后，下一步便是设置训练步骤中所需要的超参数。</p><h2 id="configurations">Configurations</h2><p>  设置我们在整个任务中所需要用到的参数：<br></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">device = <span class="hljs-string">'cuda'</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">'cpu'</span><br>config = {<br>    <span class="hljs-string">'seed'</span>: <span class="hljs-number">5201314</span>,      <span class="hljs-comment"># random seed</span><br>    <span class="hljs-string">'select_all'</span>: <span class="hljs-literal">True</span>,   <span class="hljs-comment"># Whether to use all features.</span><br>    <span class="hljs-string">'valid_ratio'</span>: <span class="hljs-number">0.2</span>,   <span class="hljs-comment"># validation_size = train_size * valid_ratio</span><br>    <span class="hljs-string">'n_epochs'</span>: <span class="hljs-number">3000</span>,     <span class="hljs-comment"># Number of epochs.            </span><br>    <span class="hljs-string">'batch_size'</span>: <span class="hljs-number">256</span>, <br>    <span class="hljs-string">'learning_rate'</span>: <span class="hljs-number">1e-5</span>,              <br>    <span class="hljs-string">'early_stop'</span>: <span class="hljs-number">400</span>,    <span class="hljs-comment"># If model has not improved for this many consecutive epochs, stop training.     </span><br>    <span class="hljs-string">'save_path'</span>: <span class="hljs-string">'./models/model.ckpt'</span>  <span class="hljs-comment"># Your model will be saved here.</span><br>}<br></code></pre></td></tr></tbody></table></figure><p></p><h2 id="start-training">Start training!</h2><p>  完事具备，开始训练我们的模型吧！<br></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">model = My_Model(input_dim=x_train.shape[<span class="hljs-number">1</span>]).to(device) <span class="hljs-comment"># put your model and data on the same computation device.</span><br>trainer(train_loader, valid_loader, model, config, device)<br></code></pre></td></tr></tbody></table></figure><p></p><p>  由于深度学习模型的参数量一般较大，训练可能会花费一定的时间，待训练完成后我们便得到了已更新好参数的神经网络模型。</p><h2 id="plot-learning-curves-with-tensorboard">Plot learning curves withtensorboard</h2><p>  通过使用<code>tensorboard</code>，我们可以得到损失曲线与学习曲线，便于我们更好地理解模型训练的过程。<br></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">%reload_ext tensorboard<br>%tensorboard --logdir=./runs/<br></code></pre></td></tr></tbody></table></figure><p></p><h2 id="testing">Testing</h2><p>  最后，我们可以将TestingData输入到已经更新好参数的模型，得到相应标签数据的预测值，我们可以通过比较真实值与预测值之间的差异评价模型训练的结果。<br></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">test_loader, model, device</span>):<br>    model.<span class="hljs-built_in">eval</span>() <span class="hljs-comment"># Set your model to evaluation mode.</span><br>    preds = []<br>    <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> tqdm(test_loader):<br>        x = x.to(device)                        <br>        <span class="hljs-keyword">with</span> torch.no_grad():                   <br>            pred = model(x)                     <br>            preds.append(pred.detach().cpu())   <br>    preds = torch.cat(preds, dim=<span class="hljs-number">0</span>).numpy()  <br>    <span class="hljs-keyword">return</span> preds<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">save_pred</span>(<span class="hljs-params">preds, file</span>):<br>    <span class="hljs-string">''' Save predictions to specified file '''</span><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file, <span class="hljs-string">'w'</span>) <span class="hljs-keyword">as</span> fp:<br>        writer = csv.writer(fp)<br>        writer.writerow([<span class="hljs-string">'id'</span>, <span class="hljs-string">'tested_positive'</span>])<br>        <span class="hljs-keyword">for</span> i, p <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(preds):<br>            writer.writerow([i, p])<br><br>model = My_Model(input_dim=x_train.shape[<span class="hljs-number">1</span>]).to(device)<br>model.load_state_dict(torch.load(config[<span class="hljs-string">'save_path'</span>]))<br>preds = predict(test_loader, model, device) <br>save_pred(preds, <span class="hljs-string">'pred.csv'</span>)      <br></code></pre></td></tr></tbody></table></figure><p></p><p>  预测的结果保存在<code>pred.csv</code>文件中。</p><h2 id="reference">Reference</h2><p>https://www.bilibili.com/video/BV1Wv411h7kN?p=11&amp;vd_source=234cf2ac075a1558881a6956450ddf89</p>]]></content>
    
    
    <summary type="html">本节主要介绍使用Pytorch完成深度学习模型训练的基本框架及算法步骤。</summary>
    
    
    
    <category term="深度学习" scheme="http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>最优化理论-2.仿射集与仿射包</title>
    <link href="http://example.com/2023/08/08/%E6%9C%80%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA-2-%E4%BB%BF%E5%B0%84%E9%9B%86%E4%B8%8E%E4%BB%BF%E5%B0%84%E5%8C%85/"/>
    <id>http://example.com/2023/08/08/%E6%9C%80%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA-2-%E4%BB%BF%E5%B0%84%E9%9B%86%E4%B8%8E%E4%BB%BF%E5%B0%84%E5%8C%85/</id>
    <published>2023-08-08T14:31:22.000Z</published>
    <updated>2023-08-14T06:10:23.415Z</updated>
    
    <content type="html"><![CDATA[<h1 id="仿射集与仿射包">仿射集与仿射包</h1><p>  上节讨论了最优化问题的基本形式，其数学表达为：<br><span class="math display">\[min \space\space f_0(x)\]</span></p><p><span class="math display">\[s.t. \space\space f_i(x) \leq b_i,i=1,\dots,m\]</span></p><p>  其中 <span class="math inline">\(x \in \mathbb{R}^n, x =\begin{bmatrix}  x_1,x_2,\dots,x_n \\ \end{bmatrix}^T\)</span>.<br>  最优化问题可以分为凸优化与非凸优化，本博客会首先从凸优化开始讨论。要准确认识何为"凸"，我们需要学习仿射集、凸集、凸函数等概念，接下来我们先从仿射集的概念开始学习。</p><h2 id="仿射集">仿射集</h2><p>  设<span class="math inline">\(x_1,x_2 \in\mathbb{R}^n\)</span>，且<span class="math inline">\(x_1 \neqx_2\)</span>，<br>  经过<span class="math inline">\(x_1,x_2\)</span>两点的直线<span class="math inline">\(y\)</span>的表达式为：<span class="math inline">\(y=\theta x_1 + (1-\theta)x_2, \theta \in\mathbb{R}.\)</span><br>  经过<span class="math inline">\(x_1,x_2\)</span>两点的线段<span class="math inline">\(y\)</span>的表达式为：<span class="math inline">\(y=\theta x_1 + (1-\theta)x_2, \theta \in[0,1].\)</span></p><h3 id="仿射集的定义与性质">仿射集的定义与性质</h3><p><strong>定义</strong><br>  一个集合<span class="math inline">\(C\)</span>是仿射集，若<span class="math inline">\(\forall x_1,x_2 \in C ,\theta \in \mathbb{R},y=\theta x_1 + (1-\theta)x_2 \in C.\)</span></p><ul><li>注1：从定义可以得知，若经过集合<span class="math inline">\(C\)</span>内任意两点<span class="math inline">\(x_1,x_2\)</span>的直线为在集合<span class="math inline">\(C\)</span>内，则集合<span class="math inline">\(C\)</span>为仿射集。<br></li><li>注2：由仿射集的定义可知：在二维空间中，整个二维空间构成一个仿射集，但二维空间中的某一区域则不是仿射集；二维空间中的直线是一个仿射集，但线段不是仿射集。</li></ul><p><strong>性质1：仿射集对仿射组合封闭.</strong><br>  设<span class="math inline">\(\forall x_1,x_2,\dots,x_k \in C,\theta_1,\theta_2,\dots,\theta_k \in \mathbb{R}, \theta_1 + \theta_2 +\dots + \theta_k = 1\)</span>，定义<strong>仿射组合:</strong><br><span class="math display">\[\theta_1x_1+\theta_2x_2+\dots+\theta_kx_k\]</span></p><p>若集合<span class="math inline">\(C\)</span>为仿射集，则仿射组合<span class="math inline">\(\theta_1x_1+\theta_2x_2+\dots+\theta_kx_k\in C\)</span>.</p><p><strong>证明:</strong><br>  借助定义，使用数学归纳法来证明该性质：<br>  当<span class="math inline">\(k=2\)</span>时，由定义1可知：<br><span class="math display">\[集合C是仿射集 \Rightarrow \forall x_1,x_2\in C,\theta \in \mathbb{R},\theta x_1+(1-\theta)x_2 \in C\]</span></p><p>  当<span class="math inline">\(k=t\)</span>时，假设该结论仍然成立，即：</p><p><span class="math display">\[集合C是仿射集 \Rightarrow \forallx_1,x_2,\dots,x_t \in C,\theta_1,\theta_2,\dots,\theta_t \in \mathbb{R},\sum_{i=1}^{t}\theta_i=1, 有\sum_{i=1}^{t}\theta_ix_i \in C\]</span></p><p>  当<span class="math inline">\(k=t+1\)</span>时，需要证明以下结论成立：</p><p><span class="math display">\[集合C是仿射集 \Rightarrow \forallx_1,x_2,\dots,x_{t+1} \in C,\theta_1,\theta_2,\dots,\theta_{t+1} \in\mathbb{R}, \sum_{i=1}^{t+1}\theta_i=1, 有\sum_{i=1}^{t+1}\theta_ix_i\in C\]</span></p><p>  已知集合<span class="math inline">\(C\)</span>是仿射集，<span class="math inline">\(\forall x_1,x_2,\dots,x_{t+1} \inC,\theta_1,\theta_2,\dots,\theta_{t+1} \in \mathbb{R},\sum_{i=1}^{t+1}\theta_i=1\)</span>,<br>  令 <span class="math inline">\(\beta_i =\theta_i/\sum_{i=1}^{t}\theta_i, i=1,\dots t\)</span>，则有<span class="math inline">\(\sum_{i=1}^{t}\beta_i = 1\)</span>.<br>  由前文的结论可知：<span class="math inline">\(\sum_{i=1}^{t}\beta_ix_i\in C\)</span>.<br>  令<span class="math inline">\(y = \sum_{i=1}^{t}{\beta_ix_i} \in C,x_{t+1} \in C\)</span>，由前文的结论可知: <span class="math inline">\((\sum_{i=1}^{t}\theta_i)y +(1-\sum_{i=1}^{t}\theta_i)x_{t+1} \in C\)</span>.<br>  即: <span class="math inline">\(\sum_{i=1}^{t+1}\theta_ix_i \inC\)</span>，证毕.</p><p><strong>性质2：与仿射集相关的子空间也是仿射集.</strong><br>  设集合<span class="math inline">\(C\)</span>是仿射集，对<span class="math inline">\(\forall x_0 \in C\)</span>，令<span class="math inline">\(V=C-x_0= \{ x-x_0 \vert x \in C\}\)</span>，称集合<span class="math inline">\(V\)</span>为与<span class="math inline">\(C\)</span>相关的子空间.<br>  集合<span class="math inline">\(V\)</span>有以下性质：<br>  (1) <span class="math inline">\(V\)</span>为仿射集.<br>  (2) 记<span class="math inline">\(v_1,v_2 \in V\)</span>，则有<span class="math inline">\(\alpha v_1+\beta v_2 \in V, \alpha,\beta \in\mathbb{R}.\)</span><br>  (3) 集合V一定包含原点<span class="math inline">\(\mathbf{0}\)</span>.</p><p><strong>证明:</strong><br>  记<span class="math inline">\(v_1 ,v_2 \in V, \theta_1,\theta_2 \in\mathbb{R},\theta_1+\theta_2 = 1.\)</span><br>  设<span class="math inline">\(v_1 = x_1 - x_0, v_2 = x_2 - x_0,x_1,x_2 \in C.\)</span><br>  <span class="math inline">\(\theta_1v_1 + \theta_2v_2 =\theta_1(x_1-x_0)+\theta_2(x_2-x_0) = \theta_1x_1 + \theta_2x_2 -x_0\)</span><br>  <span class="math inline">\(\because C\)</span>是仿射集，且<span class="math inline">\(\theta_1+\theta_2=1\)</span>，<span class="math inline">\(\therefore \theta_1x_1+\theta_2x_2 \inC\)</span>，令<span class="math inline">\(x_3 = \theta_1x_1+\theta_2x_2,x_3 \in C\)</span>.<br>  则 <span class="math inline">\(\theta_1v_1+\theta_2v_2 =\theta_1x_1+\theta_2x_2 - x_0 = x_3 - x_0 \in V, \theta_1 + \theta_2 =1.\)</span><br>  故集合<span class="math inline">\(V\)</span>是仿射集，性质(1)证毕.<br>  设<span class="math inline">\(\alpha, \beta \in\mathbb{R},\)</span><br>  <span class="math inline">\(\alpha v_1 + \beta v_2 \in V\Leftrightarrow \alpha v_1 + \beta v_2 +x_0 = x, x \in C \Leftrightarrow\alpha v_1 + \beta v_2 + x_0 \in C.\)</span><br>  <span class="math inline">\(\alpha v_1 + \beta v_2 +x_0 =\alpha(v_1+x_0)+\beta(v_2+x_0)+(1-\alpha-\beta)x_0=\alpha x_1 + \betax_2 + (1-\alpha-\beta)x_0.\)</span><br>  由仿射集的性质1可知: 仿射组合 <span class="math inline">\(\alpha x_1 +\beta x_2 + (1-\alpha-\beta)x_0 \in C.\)</span><br>  故有<span class="math inline">\(\alpha v_1 + \beta v_2 \inV\)</span>，性质(2)证毕.<br>  由集合V的定义易证性质(3)成立.</p><h3 id="仿射集的实例">仿射集的实例</h3><p><strong>例：线性方程组的解集是仿射集</strong></p><p>  设有线性方程组 <span class="math inline">\(Ax=b, A \in\mathbb{R}^{m\times n}, b \in \mathbb{R}^m, x\in \mathbb{R}^n. 其解集C=\{ x \vert Ax=b \}\)</span>，则集合<span class="math inline">\(C\)</span>是仿射集.</p><p><strong>证明:</strong><br>  对<span class="math inline">\(\forall x_1,x_2 \in C\)</span>，有 <span class="math inline">\(Ax_1=b, Ax_2 = b.\)</span><br>  若 <span class="math inline">\(\theta x_1 + (1-\theta)x_2 \in C,\theta \in \mathbb{R}\)</span>，则集合<span class="math inline">\(C\)</span>是仿射集.<br>  <span class="math inline">\(\theta x_1 + (1-\theta)x_2 \in C\Leftrightarrow \theta x_1 + (1-\theta)x_2 = x, x \in C \LeftrightarrowA(\theta x_1 + (1-\theta)x_2) = Ax = b.\)</span><br>  <span class="math inline">\(A(\theta x_1 + (1-\theta)x_2) = \thetaAx_1 + (1-\theta)Ax_2 = \theta b + (1-\theta)b = b\)</span><br>  故有<span class="math inline">\(\theta x_1 + (1-\theta)x_2 \inC\)</span>，解集<span class="math inline">\(C\)</span>是仿射集.</p><ul><li><strong>注：与线性方程组解集C相关的子空间V是系数矩阵A的核</strong></li></ul><p><strong>证明:</strong><br>  设集合 <span class="math inline">\(V= \{ x-x_0 \vert x \in C \},\forall x_0 \in C\)</span>，则集合V是与C相关的子空间.<br>  由<span class="math inline">\(Ax = b \Rightarrow V= \{ x-x_0 \vertAx=b \}\)</span>.<br>  由<span class="math inline">\(Ax_0 = b \Rightarrow V = \{ x-x_0 \vertAx=Ax_0 \} = \{ x-x_0 \vert A(x-x_0)=0 \}.\)</span><br>  令<span class="math inline">\(y = x-x_0， V= \{ y \vert Ay = 0\}\)</span>，即<span class="math inline">\(V= ker \spaceA\)</span>，证毕.</p><h2 id="仿射包">仿射包</h2><h3 id="仿射包的定义">仿射包的定义</h3><p>  对任意集合<span class="math inline">\(C\)</span>，记：</p><p><span class="math display">\[aff \space C = \{\theta_1x_1+\dots+\theta_kx_k \vert \forall x_1,\dots,x_k \in C,\theta_i \in \mathbb{R}, \sum_{i=1}^{k}\theta_i=1 \}\]</span></p><p>则集合<span class="math inline">\(aff \space C\)</span>称为集合<span class="math inline">\(C\)</span>的仿射包.</p><p><font color="green">注：仿射包<span class="math inline">\(aff\spaceC\)</span>是集合<span class="math inline">\(C\)</span>所能构造的最小的仿射集，是集合<span class="math inline">\(C\)</span>中元素的仿射组合的集合.</font></p><h3 id="仿射包的实例">仿射包的实例</h3><p><strong>例：二维空间中的仿射包</strong></p><p>  设集合<span class="math inline">\(V\)</span>为二维平面，<span class="math inline">\(x_1,x_2,x_3\)</span>为二维平面中不在同一条直线上的三个点，<span class="math inline">\(L\)</span>为经过点<span class="math inline">\(x_1,x_2\)</span>的直线，如下图所示：</p><center><img src="https://s2.loli.net/2023/08/13/mh4IpMxoERZ3sy9.png" width="75%"><div data-align="center">Image1: 二维平面及其元素</div></center><p><br></p><p>  设集合<span class="math inline">\(C_1 = \{ x_1,x_2\}\)</span>，则集合<span class="math inline">\(C_1\)</span>的仿射包<span class="math inline">\(aff \space C_1 = \{ \theta_1x_1+\theta_2x_2 \vertx_1,x_2 \in C_1, \theta_1,\theta_2 \in \mathbb{R}, \theta_1+\theta_2=1\}\)</span>，仿射包<span class="math inline">\(aff \spaceC_1\)</span>即为直线<span class="math inline">\(L\)</span>.<br>  设集合<span class="math inline">\(C_2 = \{ x_1,x_2,x_3\}\)</span>，则集合<span class="math inline">\(C_2\)</span>的仿射包<span class="math inline">\(aff \space C_2 = \{\theta_1x_1+\theta_2x_2+\theta_3x_3 \vert x_1,x_2,x_3 \in C_2,\theta_1,\theta_2,\theta_3 \in \mathbb{R}, \theta_1+\theta_2+\theta_3 =1 \}\)</span>，仿射包<span class="math inline">\(aff \spaceC_2\)</span>即为二维平面<span class="math inline">\(V\)</span>.</p>]]></content>
    
    
    <summary type="html">本节主要介绍仿射集与仿射包的概念。</summary>
    
    
    
    <category term="最优化理论" scheme="http://example.com/categories/%E6%9C%80%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA/"/>
    
    
  </entry>
  
  <entry>
    <title>最优化理论-1.最优化问题的基本形式</title>
    <link href="http://example.com/2023/08/03/%E6%9C%80%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA-1-%E6%9C%80%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%BD%A2%E5%BC%8F/"/>
    <id>http://example.com/2023/08/03/%E6%9C%80%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA-1-%E6%9C%80%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%BD%A2%E5%BC%8F/</id>
    <published>2023-08-02T17:02:57.000Z</published>
    <updated>2023-08-28T06:10:37.724Z</updated>
    
    <content type="html"><![CDATA[<h1>最优化问题的基本形式</h1><h2 id="最优化问题的定义">最优化问题的定义</h2><p>  从一个可行解的集合$S$中，求出针对某一问题的最优的解$X^*$。</p><h2 id="最优化问题的基本形式">最优化问题的基本形式</h2><p>  最优化问题的基本形式为：<br>$$min \space\space f_0(x)$$</p><p>$$s.t. \space\space f_i(x) \leq b_i, i=1,\dots,m$$</p><p>  $x=\begin{bmatrix}<br>x_1,x_2,\dots,x_n \<br>\end{bmatrix},x\in \mathbb{R}^n$ 称为优化变量(Optimization Varriable).<br>  $f_0: \mathbb{R}^n \rightarrow \mathbb{R}$，称为目标函数(Objective Function).<br>  $f_i: \mathbb{R}^n \rightarrow \mathbb{R}$，称为不等式约束(Inequality Constraint).<br>  $S = {x \vert f_i(x) \leq b_i, i=1,\dots,m}$，称为可行解(Feasible Set).<br>  $X^{* }$称为最优解(Optimal)，如果$ X^{* } $满足：$ \forall x \in S $ 均有 $ f_0(X^{*}) \leq f_0(x) $.</p><ul><li>注：有时最优解不止一个，最优解的集合称为最优解集。</li></ul><h2 id="最优化问题的分类">最优化问题的分类</h2><p>  根据最优化问题的目标函数、可行解的不同，可以将最优化问题分为多种类别，以下是一些比较常见的分类：</p><ul><li><p><strong>线性规划/非线性规划</strong><br>  若目标函数$f_0$与不等式约束$f_i$均为线性函数，则该最优化问题称为线性规划问题，否则即为非线性规划问题。<br>  函数$f$为线性函数的充要条件：$f(\alpha x + \beta y) = \alpha f(x) + \beta f(y)$.</p></li><li><p><strong>凸优化/非凸优化</strong><br>  若目标函数$f_0$为凸函数，且可行解$S$为凸集，则该最优化问题称为凸优化问题。<br>  集合$C$为凸集的充要条件：$\forall x_1,x_2 \in C, \forall \theta \in \mathbb{R}, \theta x_1 + (1-\theta)x_2 \in C$.<br>  函数$f$为凸函数的充要条件: $dom f$为凸集，且$\forall x_1,x_2 \in dom f, 0 \leq \theta \leq 1$，有以下不等式成立：<br>$$f(\theta x_1 + (1-\theta)x_2) \leq \theta f(x_1) + (1-\theta)f(x_2)$$</p></li><li><p><strong>连续变量优化/离散变量优化</strong><br>  若可行解$S$是连续的，则该最优化问题为连续变量优化问题；若可行解$S$是离散的，则该最优化问题称为离散变量优化问题。<br>  离散变量优化问题中比较常见的为可行解均为整数的整数变量优化。</p></li><li><p><strong>单目标优化/多目标优化</strong><br>  若目标函数$f_0$是唯一的，则为单目标优化问题；若有多个目标函数$f_{0}^{1},f_{0}^{2},\dots,f_{0}^{r}$，则为多目标优化问题。</p></li><li><p><strong>无约束问题/等式约束问题/不等式约束问题</strong><br>  若最优化问题没有约束条件，只有目标函数，则称为无约束问题；若最优化问题的约束条件是一些等式，则称为等式约束问题；若最优化问题的约束条件是一些不等式，则称为不等式约束问题。</p></li></ul>]]></content>
    
    
    <summary type="html">本节主要介绍最优化问题的基本形式以及其分类。</summary>
    
    
    
    <category term="最优化理论" scheme="http://example.com/categories/%E6%9C%80%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA/"/>
    
    
  </entry>
  
  <entry>
    <title>矩阵分析-2.向量组与线性相关性</title>
    <link href="http://example.com/2023/08/01/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-2-%E5%90%91%E9%87%8F%E7%BB%84%E4%B8%8E%E7%BA%BF%E6%80%A7%E7%9B%B8%E5%85%B3%E6%80%A7/"/>
    <id>http://example.com/2023/08/01/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-2-%E5%90%91%E9%87%8F%E7%BB%84%E4%B8%8E%E7%BA%BF%E6%80%A7%E7%9B%B8%E5%85%B3%E6%80%A7/</id>
    <published>2023-07-31T16:31:12.000Z</published>
    <updated>2023-08-02T18:06:30.319Z</updated>
    
    <content type="html"><![CDATA[<h1 id="向量组与线性相关性">向量组与线性相关性</h1><h2 id="向量组">向量组</h2><h3 id="向量组的定义">向量组的定义</h3><p>  设<span class="math inline">\(V\)</span>是数域<span class="math inline">\(\mathbb{F}\)</span>上的线性空间，<span class="math inline">\(V\)</span>中有限序列<span class="math inline">\(\alpha_1,\alpha_2,\dots,\alpha_n\)</span>称为<span class="math inline">\(V\)</span>中的一个向量组，记为向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_n\}\)</span>。</p><h3 id="向量组所拼成的抽象矩阵">向量组所拼成的抽象矩阵</h3><p>  若矩阵<span class="math inline">\(A\)</span>中的每一列是由向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_n\}\)</span>所构成的，则称矩阵<span class="math inline">\(A\)</span>是由向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_n\}\)</span>所拼成的抽象矩阵，记为：</p><p><span class="math display">\[A = \begin{bmatrix}    \alpha_1,\alpha_2,\dots,\alpha_n \\\end{bmatrix}\]</span></p><h2 id="向量组的线性相关性">向量组的线性相关性</h2><h3 id="线性相关与线性无关的定义">线性相关与线性无关的定义</h3><p>  向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>称为线性相关，如果存在不全为零的<span class="math inline">\(P\)</span>个数<span class="math inline">\(k_i \in\mathbb{F}, i=1,2,\dots,p\)</span>，使得：</p><p><span class="math display">\[\begin{equation}\\alpha_1k_1+\alpha_2k_2+\dots+\alpha_pk_p=0\end{equation}\]</span></p><p>  向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>称为线性无关，如果任意不全为零的<span class="math inline">\(P\)</span>个数<span class="math inline">\(k_i \in\mathbb{F}, i=1,2,\dots,p\)</span>，使得：</p><p><span class="math display">\[\begin{equation}\\alpha_1k_1+\alpha_2k_2+\dots+\alpha_pk_p \ne 0\end{equation}\]</span></p><ul><li><strong>注1</strong> 线性无关的另外一种表述：<br>  向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>称为线性无关，若<span class="math inline">\(\alpha_1k_1+\alpha_2k_2+\dots+\alpha_pk_p =0\)</span> 当且仅当 <span class="math inline">\(k_i=0,i=1,\dots,p\)</span> 时成立。</li></ul><h3 id="线性相关性的矩阵表述">线性相关性的矩阵表述</h3><p>  设向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_n\}\)</span>所拼成的抽象矩阵为<span class="math inline">\(A\)</span>，向量<span class="math inline">\(x =\begin{bmatrix}  x_1,x_2,\dots,x_p\end{bmatrix}^T\)</span>，有齐次线性方程：</p><p><span class="math display">\[\begin{equation}    Ax=\begin{bmatrix}    \alpha_1,\alpha_2,\dots,\alpha_p \\\end{bmatrix}\begin{bmatrix}    x_1 \\    x_2 \\    \vdots \\    x_p\end{bmatrix}=0\end{equation}\]</span></p><p><span style="color: green;">向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>线性相关<span class="math inline">\(\Leftrightarrow\)</span>齐次线性方程(3)有非零解。</span><br><span style="color: green;">向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>线性无关<span class="math inline">\(\Leftrightarrow\)</span>齐次线性方程(3)只有零解。</span></p><h2 id="向量组之间的线性表示">向量组之间的线性表示</h2><h3 id="线性表示的定义">线性表示的定义</h3><p>  设有向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>，<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_q\}\)</span>以及向量<span class="math inline">\(\beta\)</span>。<br>  称向量<span class="math inline">\(\beta\)</span>可由向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>线性表示，如果<span class="math inline">\(\exists k_1,k_2,\dots,k_p \in\mathbb{F}\)</span>，使得：<br><span class="math display">\[\begin{equation}   \beta = \alpha_1k_1+\alpha_2k_2+\dots+\alpha_pk_p\end{equation}\]</span></p><p>  称向量组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_q\}\)</span>可由向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>线性表示，如果每个<span class="math inline">\(\beta_i\)</span>均可由向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>线性表示。</p><h3 id="线性表示的矩阵表述">线性表示的矩阵表述</h3><p>  设向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>所拼成的抽象矩阵为<span class="math inline">\(A\)</span>，向量组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_q\}\)</span>所拼成的抽象矩阵为<span class="math inline">\(B\)</span>,有非齐次线性方程：<br><span class="math display">\[\begin{equation}    Ax=\begin{bmatrix}        \alpha_1,\alpha_2,\dots,\alpha_p    \end{bmatrix}\begin{bmatrix}        x_1 \\        x_2 \\        \vdots \\        x_p    \end{bmatrix} = \beta\end{equation}\]</span></p><p><span class="math display">\[\begin{equation}    AX= \begin{bmatrix}        \alpha_1,\alpha_2,\dots,\alpha_p    \end{bmatrix}\begin{bmatrix}        x_{11} &amp; x_{12} &amp; \dotsb &amp; x_{1q} \\        x_{21} &amp; x_{22} &amp; \dotsb &amp; x_{2q} \\        \vdots &amp; \vdots &amp;        &amp; \vdots \\        x_{p1} &amp; x_{p2} &amp; \dotsb &amp; x_{pq} \\    \end{bmatrix} = \begin{bmatrix}        \beta_1,\beta_2,\dots,\beta_q    \end{bmatrix}=B\end{equation}\]</span></p><p><span style="color: green;">向量<span class="math inline">\(\beta\)</span>可由向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>线性表示<span class="math inline">\(\Leftrightarrow\)</span>非齐次线性方程(5)有解。</span><br><span style="color: green;">向量组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_q\}\)</span>可由向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>线性表示<span class="math inline">\(\Leftrightarrow\)</span>非齐次线性方程组(6)有解。</span></p><h3 id="线性表示的传递性">线性表示的传递性</h3><p>  设有三个向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>、<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_q\}\)</span>、<span class="math inline">\(\{v_1,v_2,\dots,v_t\}\)</span>，其所拼成抽象矩阵分别为<span class="math inline">\(A、B、C\)</span>。<br>  若向量组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_q\}\)</span>可由向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>线性表示，而向量组<span class="math inline">\(\{v_1,v_2,\dots,v_t\}\)</span>可由向量组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_q\}\)</span>线性表示，则向量组<span class="math inline">\(\{v_1,v_2,\dots,v_t\}\)</span>可由向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>线性表示。</p><p><strong>证明：</strong><br>向量组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_q\}\)</span>可由向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>线性表示<span class="math inline">\(\Rightarrow\)</span> 矩阵方程 <span class="math inline">\(AX_{p\times q}=B\)</span> 有解<br>向量组<span class="math inline">\(\{v_1,v_2,\dots,v_t\}\)</span>可由向量组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_q\}\)</span>线性表示<span class="math inline">\(\Rightarrow\)</span> 矩阵方程 <span class="math inline">\(BY_{q\times t}=C\)</span> 有解</p><p><span class="math display">\[\left\{             \begin{array}{lr}             AX_{p\times q}=B \\             BY_{q \times t}=C             \end{array}\right. \Rightarrow AXY=C \Rightarrow 矩阵方程AZ_{p \times t} = C有解，Z_{p \times t}=XY\]</span></p><p>故向量组<span class="math inline">\(\{v_1,v_2,\dots,v_t\}\)</span>可由向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>线性表示。</p><h2 id="向量组的极大线性无关组">向量组的极大线性无关组</h2><h3 id="极大线性无关组的定义">极大线性无关组的定义</h3><p>  设向量组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_s\}\)</span>是向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>的子组，即<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_s\} \subseteq\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>。子组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_s\}\)</span>称为向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>的极大线性无关组，若其满足：<br>1. 子组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_s\}\)</span>线性无关。<br>2. 若向量组<span class="math inline">\(\{v_1,v_2,\dots,v_t\}\)</span>也是向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>的子组，且<span class="math inline">\(s &lt; t\)</span>，则子组<span class="math inline">\(\{v_1,v_2,\dots,v_t\}\)</span>线性相关。</p><ul><li><strong>注：“极大性”的另一种说法：“生成性”。</strong></li></ul><p>  若子组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_s\}\)</span>线性无关，且<span class="math inline">\(\forall \alpha_i \in\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>，<span class="math inline">\(\alpha_i\)</span>都可由子组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_s\}\)</span>线性表示，则子组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_s\}\)</span>是向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>的极大线性无关组。</p><p><span class="math display">\[\color{green} 极大线性无关组\Leftrightarrow 线性无关生成组\]</span></p><h3 id="向量个数的唯一性">向量个数的唯一性</h3><p><strong>定理：</strong>向量组的极大线性无关组所含向量的个数是唯一的。<br><strong>证明：</strong><br>  设有向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>，向量组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_s\}\)</span>与<span class="math inline">\(\{v_1,v_2,\dots,v_t\}\)</span>均是其极大线性无关组，<br>  要证明 $ s=t $，可以利用反证法，假设 <span class="math inline">\(s&lt; t\)</span>，<br>  设 <span class="math inline">\(A =\begin{bmatrix}  \alpha_1,\alpha_2,\dots,\alpha_p\end{bmatrix}\)</span>，<span class="math inline">\(B=\begin{bmatrix}  \beta_1,\beta_2,\dots,\beta_s\end{bmatrix}\)</span>，<span class="math inline">\(C =\begin{bmatrix}  v_1,v_2,\dots,v_t \end{bmatrix}\)</span>，<br>  <span class="math inline">\(\because\)</span> 向量组<span class="math inline">\(\{\alpha_1,\alpha_2,\dots,\alpha_p\}\)</span>可由其极大线性无关组<span class="math inline">\(\{\beta_1,\beta_2,\dots,\beta_s\}\)</span>、<span class="math inline">\(\{v_1,v_2,\dots,v_t\}\)</span>线性表示，<br>  <span class="math inline">\(\therefore\)</span> 矩阵方程 <span class="math inline">\(BX=A, AY=C\)</span>均有解，<br>  <span class="math inline">\(\Rightarrow\)</span> 矩阵方程 <span class="math inline">\(BZ=C\)</span> 有解，其中<span class="math inline">\(Z_{s \times t}=XY\)</span>，<br>  $ s &lt; t$ <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(rank(Z) &lt; t\)</span> <span class="math inline">\(\Rightarrow\)</span> 矩阵方程<span class="math inline">\(ZW= \textbf{0}\)</span>有无穷多解 <span class="math inline">\(\Rightarrow\)</span> 矩阵方程<span class="math inline">\(ZW= \textbf{0}\)</span>必有非零解，<br>  设矩阵方程 <span class="math inline">\(ZW= \textbf{0}\)</span>的非零解为<span class="math inline">\(W\)</span>，将矩阵方程 <span class="math inline">\(BZ=C\)</span> 的两边同时乘以<span class="math inline">\(W\)</span>，<br>  有 <span class="math inline">\(B(ZW) = CW, \becauseZW=\textbf{0}\)</span>， <span class="math inline">\(\therefore CW=\textbf{0}\)</span>，<br>  与向量组<span class="math inline">\(\{v_1,v_2,\dots,v_t\}\)</span>线性无关矛盾，<br>  <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(s \nless t\)</span>，同理可证 <span class="math inline">\(s \ngtr t\)</span>，故有 <span class="math inline">\(s = t\)</span></p><h2 id="向量组的秩">向量组的秩</h2><p>  <span style="color: green;">向量组的秩等于向量组的极大线性无关组所含向量的个数。</span></p>]]></content>
    
    
    <summary type="html">本节主要介绍向量组以及其线性相关性的概念。</summary>
    
    
    
    <category term="矩阵分析" scheme="http://example.com/categories/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90/"/>
    
    
  </entry>
  
  <entry>
    <title>最优化理论-0.引言</title>
    <link href="http://example.com/2023/07/28/%E6%9C%80%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA-0-%E5%BC%95%E8%A8%80/"/>
    <id>http://example.com/2023/07/28/%E6%9C%80%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA-0-%E5%BC%95%E8%A8%80/</id>
    <published>2023-07-27T16:17:50.000Z</published>
    <updated>2023-07-30T16:51:04.079Z</updated>
    
    <content type="html"><![CDATA[<h1 id="引言">引言</h1><p>  最优化理论(OptimizationTheory)是一门研究在给定条件下如何寻找最优解的学科。这个学科涵盖了广泛的数学方法和算法，用于解决机器学习、经济学、工程学等领域的各种实际问题。<br>  如今，人工智能与机器学习领域的研究十分火热，而最优化理论是在其中起着至关重要的作用的一门数学学科。在统计机器学习中，很多问题的实质都能用一个最优化模型表示，例如线性判别分析(LinearDiscriminantAnalysis)的思想是将样本从高维空间投影到低维空间，使得不同类别样本之间的距离尽可能大，同一类别样本之间的距离尽可能小，从而完成样本分类的目标。支持向量机(SupportVectorMachine)的思想是在特征空间中找到一个超平面，使得不同类别的样本尽可能远离该超平面，从而实现分类。这些思想都可以用最优化模型表示，且都是一个凸优化问题，可以利用凸优化的方法得到最优解。在深度学习中，我们要通过最小化损失函数得到神经网络的权重参数，这实际上也是一个最优化问题。由于损失函数大多是非凸函数，我们通常使用梯度下降算法来求得近似最优解。<br>  总得来说，最优化理论在当今是一门非常有用的学科，笔者认为对于统计与数据科学领域的学生来说，学好最优化理论是未来开展研究的基础。</p><h2 id="最优化理论的主要内容">最优化理论的主要内容</h2><p>  最优化理论的主要内容包括以下几个方面：</p><ul><li><strong>最优化问题的表述</strong>:最优化问题通常由目标函数和约束条件构成。目标函数是需要最大化或最小化的函数，而约束条件是问题的限制条件。<br></li><li><strong>最优解的定义</strong>:最优解是指满足约束条件的使得目标函数取得最大值或最小值的变量值。<br></li><li><strong>凸优化</strong>:凸优化是一类特殊的最优化问题，其中目标函数是凸函数，约束条件是凸集。凸优化问题具有良好的性质，可以高效地求解。<br></li><li><strong>等式约束与不等式约束</strong>:最优化问题的约束条件可以是等式约束，也可以是不等式约束。等式约束将变量限制在一个子空间内，而不等式约束则将变量限制在一个半空间内。<br></li><li><strong>无约束优化</strong>:在无约束优化问题中，只需考虑目标函数的最大化或最小化，没有额外的约束条件。<br></li><li><strong>数值优化方法</strong>:为了求解最优化问题，需要使用各种数值优化方法。常见的数值优化方法包括梯度下降法、牛顿法、共轭梯度法、拟牛顿法等。<br></li><li><strong>条件极值与全局极值</strong>:最优化问题可能存在多个极值点，包括局部极值和全局极值。局部极值是在某个特定区域内的最优解，而全局极值是在整个定义域内的最优解。<br></li><li><strong>敏感性分析</strong>:最优化理论还涉及敏感性分析，即研究目标函数和约束条件中参数的微小变化对最优解的影响。<br></li><li><strong>最优化理论在实际问题中的应用</strong>:最优化理论广泛应用于各个领域，如机器学习中的模型训练、工程中的设计优化、经济学中的资源分配问题等。</li></ul><p>  最优化问题可分为凸优化问题与非凸优化问题，本系列首先会以凸优化为主展开讨论，之后再补充非凸优化的相关方法。</p><h2 id="相关学习资料">相关学习资料</h2><p>  本系列主要参考了以下学习资料:</p><ul><li><a href="https://www.bilibili.com/video/BV19M411T7S7/?vd_source=234cf2ac075a1558881a6956450ddf89">Video:《凸优化》- 凌青 中科大</a><br></li><li><a href="https://github.com/SXUSongJH/Book">Book1:《最优化：建模、算法与理论》- 刘浩洋等</a><br></li><li><a href="https://github.com/SXUSongJH/Book">Book2：《ConvexAnalysis》- Stephen Boyd</a></li></ul><p>  本系列首先会参考凌青老师的课程，介绍凸优化的基础知识，然后以另外两本书作为补充，介绍一些课程中没有提及的知识以及非凸优化的方法。</p>]]></content>
    
    
    <summary type="html">本节主要最优化理论系列的主要内容，以及本博客所用到的学习资料。</summary>
    
    
    
    <category term="最优化理论" scheme="http://example.com/categories/%E6%9C%80%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA/"/>
    
    
  </entry>
  
  <entry>
    <title>矩阵分析-1.线性空间</title>
    <link href="http://example.com/2023/07/26/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-1-%E7%BA%BF%E6%80%A7%E7%A9%BA%E9%97%B4/"/>
    <id>http://example.com/2023/07/26/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-1-%E7%BA%BF%E6%80%A7%E7%A9%BA%E9%97%B4/</id>
    <published>2023-07-25T16:27:03.000Z</published>
    <updated>2023-07-26T18:34:05.851Z</updated>
    
    <content type="html"><![CDATA[<h1 id="线性空间">线性空间</h1><h2 id="加法与数乘">加法与数乘</h2><p><strong>加法的定义</strong><br>  给定非空集合V和数域<span class="math inline">\(\mathbb{F}\)</span>，若存在映射：<br><span class="math display">\[\sigma: V \times V \rightarrowV\]</span></p><p><span class="math display">\[(\alpha,\beta) \mapsto\sigma(\alpha,\beta)\]</span></p><p>即对V中任意元素<span class="math inline">\(\alpha\)</span>，<span class="math inline">\(\beta\)</span>，在集合V中都存在唯一元素<span class="math inline">\(\gamma\)</span>,使得<span class="math inline">\(\gamma=\alpha+\beta \in V\)</span>，则称映射<span class="math inline">\(\sigma\)</span>为集合V上的加法。</p><p><strong>数乘的定义</strong><br>  给定非空集合V和数域<span class="math inline">\(\mathbb{F}\)</span>，若存在映射： <span class="math display">\[\sigma: V \times \mathbb{F} \rightarrowV\]</span></p><p><span class="math display">\[(\alpha,k) \mapsto\sigma(\alpha,k)\]</span></p><p>即对集合V中的任意元素<span class="math inline">\(\alpha\)</span>和数域<span class="math inline">\(\mathbb{F}\)</span>中任意元素k，在集合V中都存在唯一元素<span class="math inline">\(\gamma\)</span>，使得<span class="math inline">\(\gamma=\alpha k \in V\)</span>，则称映射<span class="math inline">\(\sigma\)</span>为集合V上的数乘。</p><ul><li><p><strong>注1：关于映射的符号</strong><br>  设映射<span class="math inline">\(\sigma\)</span>为正弦函数<span class="math inline">\(sin(*)\)</span><br>  “<span class="math inline">\(\rightarrow\)</span>”表示将集合映射到集合。例如<span class="math inline">\(\sigma: R \rightarrow[-1,1]\)</span>，表示将实数集R映射到集合[-1,1]。<br>  “<span class="math inline">\(\mapsto\)</span>”表示将元素映射到元素。例如 <span class="math inline">\(\sigma: \pi \mapsto 0\)</span>，表示将元素<span class="math inline">\(\pi\)</span>映射到元素0。</p></li><li><p><strong>注2：集合的笛卡尔积</strong><br>  集合<span class="math inline">\(S_1\)</span>和<span class="math inline">\(S_2\)</span>的笛卡尔积的数学表示为： <span class="math display">\[S_1 \times S_1 = \{\begin{bmatrix}s_1\\s_2\\\end{bmatrix} \mid s_1 \in S_1,s_1 \in S_2\}\]</span></p></li><li><p><strong>注3：域的定义及常用的域</strong><br>  域的定义：包含加法与乘法的，满足通常运算规则的代数结构称为域。<br>  常用的域：<span class="math inline">\(\mathbb{Q}\)</span>(有理数域)、<span class="math inline">\(\mathbb{R}\)</span>(实数域)、<span class="math inline">\(\mathbb{C}\)</span>(复数域)等。</p></li></ul><h2 id="通常的运算规则">通常的运算规则</h2><ol type="1"><li><strong>加法交换律</strong><br>  已知非空集合V，对 <span class="math inline">\(\forall v_1,v_2 \inV\)</span>，有 <span class="math inline">\(v_1+v_2=v_2+v_1\)</span>。<br></li><li><strong>加法结合律</strong><br>  已知非空集合V，对 <span class="math inline">\(\forall v_1,v_2,v_3 \inV\)</span>，有 <span class="math inline">\((v_1+v_2)+v_3=v_1+(v_2+v_3)\)</span>。<br></li><li><strong>加法零元素</strong><br>  已知非空集合V，对 <span class="math inline">\(\forall v \in V, \existse \in V\)</span>，使得 <span class="math inline">\(e+v=v\)</span>。<br></li><li><strong>加法逆元素</strong><br>  已知非空集合V，对 <span class="math inline">\(\forall v \in V, \existsa \in V\)</span>，使得 <span class="math inline">\(v+a=e\)</span>，记<span class="math inline">\(a=-v\)</span>。<br></li><li><strong>数乘分配律</strong><br>  已知非空集合V和数域<span class="math inline">\(\mathbb{F}\)</span>，对<span class="math inline">\(\forall v_1,v_2 \in V, k \in\mathbb{F}\)</span>，有 <span class="math inline">\((v_1+v_2)k=v_1k+v_2k\)</span>。<br>  已知非空集合V和数域<span class="math inline">\(\mathbb{F}\)</span>，对<span class="math inline">\(\forall v \in V;k_1,k_2 \in\mathbb{F}\)</span>，有 <span class="math inline">\(v(k_1+k_2)=vk_1+vk_2\)</span>。<br></li><li><strong>数乘结合律</strong><br>  已知非空集合V和数域<span class="math inline">\(\mathbb{F}\)</span>，对<span class="math inline">\(\forall v \in V;k,l \in\mathbb{F}\)</span>，有 <span class="math inline">\((vk)l=v(kl)\)</span>。<br></li><li><strong>数乘单位元素</strong><br>  已知非空集合V和数域<span class="math inline">\(\mathbb{F}\)</span>，对<span class="math inline">\(\forall v \in V,\exists 1 \in\mathbb{F}\)</span>，使得 <span class="math inline">\(v1=v\)</span>。</li></ol><h2 id="线性空间的定义">线性空间的定义</h2><p>  <font color="#008000">若集合V满足<b>加法</b>与<b>数乘</b>两种运算，且这两种运算满足<b>通常的运算规则</b>，则称<b>集合V</b>关于此加法和数乘是<b>数域<span class="math inline">\(\mathbb{F}\)</span></b>上的线性空间。</font>一般也把这种线性空间称为向量空间，集合V中的元素称为向量。</p><h2 id="线性空间的具体实例">线性空间的具体实例</h2><h3 id="例1数域mathbbf上的标准线性空间mathbbfn">例1：数域<span class="math inline">\(\mathbb{F}\)</span>上的标准线性空间<span class="math inline">\(\mathbb{F}^n\)</span></h3><p>  已知数域<span class="math inline">\(\mathbb{F}\)</span>,设<br><span class="math display">\[V := \mathbb{F}^n=\mathbb{F} \times\mathbb{F} \times \dots \times \mathbb{F}=\{\begin{bmatrix}  v_1\\  v_2\\  \vdots\\  v_n\\\end{bmatrix} \mid v_i \in \mathbb{F},i=1, \dots,n\}\]</span></p><p>  对 <span class="math inline">\(\forall \alpha,\beta \in V, k \in\mathbb{F}\)</span>，<br>  定义集合V上的加法运算：<br><span class="math display">\[\alpha + \beta=\begin{bmatrix}  \alpha_1\\  \alpha_2\\  \vdots\\  \alpha_n\\\end{bmatrix}+\begin{bmatrix}  \beta_1\\  \beta_2\\  \vdots\\  \beta_n\\\end{bmatrix}=\begin{bmatrix}  \alpha_1+\beta_1\\  \alpha_2+\beta_2\\  \vdots\\  \alpha_n+\beta_n\\\end{bmatrix} \in V\]</span></p><p>  定义集合V上的数乘运算：<br><span class="math display">\[\alpha \cdot k=\begin{bmatrix}  \alpha_1\\  \alpha_2\\  \vdots\\  \alpha_n\\\end{bmatrix} \cdot k = \begin{bmatrix}  \alpha_1 k \\  \alpha_2 k \\  \vdots \\  \alpha_n k \\\end{bmatrix} \in V\]</span></p><p>  易证此加法与数乘满足通常的运算法则，此时称集合V为数域<span class="math inline">\(\mathbb{F}\)</span>上的n维标准线性空间，记为<span class="math inline">\(\mathbb{F}^n\)</span>。<br>  一些常用数域上的n维标准线性空间：<span class="math inline">\(\mathbb{R}^n\)</span>(实数域)、<span class="math inline">\(\mathbb{C}^n\)</span>(复数域)等。</p><h3 id="例2欧几里得空间作为线性空间">例2：欧几里得空间作为线性空间</h3>  令数域<span class="math inline">\(\mathbb{F}=\mathbb{R}\)</span>，集合<span class="math inline">\(V=\{欧几里得空间中的全体有向线段\}\)</span>。(当两条有向线段经过平移能够重叠时，则把这两条线段算做一条线段)<br>  定义集合V上的加法运算：向量运算的平行四边形法则。<br>  定义集合V上的数乘运算：向量同向或反向伸缩。<br><center class="half"><img src="https://s2.loli.net/2023/07/27/GNvpJQHgdrOLMRA.png" width="50%"><img src="https://s2.loli.net/2023/07/27/lFhM6DNEmVkRW7c.png" width="50%"><p>Image 1 向量的平行四边形法则            Image 2 向量的伸缩</p></center><p>  易证此加法与数乘满足通常的运算法则，则集合V是线性空间，说明欧几里得空间可以作为线性空间。</p><h3 id="例3函数空间作为线性空间">例3：函数空间作为线性空间</h3><p>  已知数域<span class="math inline">\(\mathbb{F}\)</span>，集合<span class="math inline">\(V=\mathcal{F}(I,\mathbb{F}^n)\)</span>。集合V为函数空间，V中的元素是以数域<span class="math inline">\(\mathbb{F}\)</span>中的区间<span class="math inline">\(I\)</span>为定义域，具有n个分量的n维向量值函数。例如：<br><span class="math display">\[f=\begin{bmatrix}  f_1(x) \\  f_2(x) \\\end{bmatrix}=\begin{bmatrix}  sin(x) \\  \frac{1}{2}x^3 \\\end{bmatrix}, f \in \mathcal{F}([-1,1],\mathbb{R}^2) \]</span></p><p>  对 <span class="math inline">\(\forall f,g \in\mathcal{F}(I,\mathbb{F}^n),k \in \mathbb{F}\)</span>，<br>  定义集合V上的加法运算：<br><span class="math display">\[f+g = \begin{bmatrix}  f_1(x) \\  f_2(x) \\  \vdots \\  f_n(x) \\\end{bmatrix}+\begin{bmatrix}  g_1(x) \\  g_2(x) \\  \vdots \\  g_n(x) \\\end{bmatrix} = \begin{bmatrix}  f_1(x)+g_1(x) \\  f_2(x)+g_2(x) \\  \vdots \\  f_n(x)+g_n(x) \\\end{bmatrix} \in \mathcal{F}(I,\mathbb{F}^n)\]</span></p><p>  定义集合V上的数乘运算：<br><span class="math display">\[f \cdot k = \begin{bmatrix}  f_1(x) \\  f_2(x) \\  \vdots \\  f_n(x) \\\end{bmatrix} \cdot k = \begin{bmatrix}  kf_1(x) \\  kf_2(x) \\  \vdots \\  kf_n(x) \\\end{bmatrix} \in \mathcal{F}(I,\mathbb{F}^n)\]</span></p><p>  易证此加法与数乘满足通常的运算法则，则集合V是线性空间，说明函数空间可以作为线性空间。</p>]]></content>
    
    
    <summary type="html">本节主要介绍线性空间的定义，以及一些具体实例。</summary>
    
    
    
    <category term="矩阵分析" scheme="http://example.com/categories/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90/"/>
    
    
  </entry>
  
  <entry>
    <title>矩阵分析-0.引言</title>
    <link href="http://example.com/2023/07/16/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-0.%E5%BC%95%E8%A8%80/"/>
    <id>http://example.com/2023/07/16/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90-0.%E5%BC%95%E8%A8%80/</id>
    <published>2023-07-15T16:38:23.000Z</published>
    <updated>2023-08-08T09:53:13.101Z</updated>
    
    <content type="html"><![CDATA[<h1 id="引言">引言</h1><p>  矩阵分析是笔者开启的第一个博客系列，之所以想把矩阵理论作为起点，是因为在统计与数据科学领域，矩阵是最基本也是最重要的数学工具，当我们在讨论高维空间、随机向量、多元正态分布这些统计领域最基本的概念时，我们都离不开矩阵这个数学工具。掌握矩阵理论，能够帮助我们更好地分析、处理高维空间中数学问题。</p><h2 id="矩阵分析的研究内容">矩阵分析的研究内容</h2><p>  与线性代数偏向于计算不同，矩阵分析的研究内容更加偏向于分析。矩阵分析的研究内容是非常广泛的，很难做到面面俱到，本博客的文章主要是参考一些主流的矩阵理论著作以及学习资料。以下是本系列打算讨论的一些重点内容：<br>1.<strong>线性空间</strong>：在研究问题时，我们通常把对象限定在某一个空间中，而线性空间便是代数中最基本的空间。从线性空间出发，我们会认识向量组、基、子空间等概念。<br>2.<strong>线性映射</strong>：当我们需要将一个线性空间中的对象映射到另一个线性空间时，我们便需要用到线性映射这个方法。基于线性映射，我们会讨论几何中的旋转变换、镜面反射等操作的矩阵表示。<br>3.<strong>矩阵等价与相似</strong>：矩阵的等价与相似是我们在线性代数中非常熟悉的概念，在矩阵分析中，我们将借助线性映射的概念进一步理解等价与相似的几何意义。另外，通过引入多项式矩阵以及Smith型、Jordan标准型等概念，我们将能够把较为复杂的矩阵相似问题转化为简单的矩阵等价问题。<br>4.<strong>内积</strong>：内积是解析几何中一个非常重要的概念，内积赋予空间向量以度量，使得我们可以定义范数、距离和角度等概念，从而建立内积空间的结构。内积空间是函数空间、向量空间和张量空间的基础。<br>5.<strong>矩阵微分</strong>：矩阵微积分是矩阵理论的重要组成部分。它对矩阵的导数、积分和微分方程进行了系统的研究，为解决矩阵和向量值函数的微分问题提供了理论基础。矩阵微分在机器学习、控制论等领域有广泛应用。<br>6.<strong>矩阵分解</strong>：矩阵分解是将一个矩阵拆解为多个简单矩阵或特殊形式矩阵的过程。矩阵分解在线性方程组求解、特征值计算、数据降维等方面有非常重要的作用。我们将会学习一些常见的矩阵分解技术，如LU分解、QR分解、奇异值分解等。</p><p>  以上是我打算在矩阵分析系列前期讨论的一些内容，当然，矩阵理论博大精深，远非一朝一夕能够掌握理解的，随着本人学习的深入，一些新的内容会陆续补充到这个系列中。</p><h2 id="矩阵分析的应用场景">矩阵分析的应用场景</h2><p>  矩阵理论在物理、控制、计算机等领域有着广泛的应用场景，由于本人是统计与数据科学领域的学生，所以我主要介绍一些矩阵理论在本领域的一些应用场景：</p><ul><li><strong>主成分分析（PCA）</strong>：PCA是一种常用的降维技术，它将高维数据转换为低维空间，同时最大程度保留原始数据的方差。PCA的核心是对数据协方差矩阵进行特征值分解，从而得到主成分。<br></li><li><strong>特征提取</strong>：在机器学习中，矩阵理论常用于特征提取。通过将数据矩阵进行降维、转换或者分解，可以得到更具有表征能力的特征，从而提高模型的性能。<br></li><li><strong>多元正态分布</strong>：在多元统计分析中，多元正态分布是一个重要的概率分布模型，用于描述多维随机变量的联合分布。矩阵理论提供了对多元正态分布的理论和应用支持，包括协方差矩阵、特征值分解、条件分布等。<br></li><li><strong>线性回归和广义线性模型</strong>：线性回归和广义线性模型是数据科学中常用的建模方法。它们使用矩阵来描述变量之间的线性关系，并通过矩阵求解技术来拟合模型和估计参数。<br></li><li><strong>时间序列分析</strong>：时间序列分析中，矩阵理论被用于处理多维时间序列数据，如协方差矩阵估计、谱分析等。<br></li><li><strong>神经网络</strong>：深度学习中的神经网络可以用矩阵表示网络的权重和输入输出。矩阵运算在神经网络的前向传播和反向传播过程中发挥着关键作用，实现模型的训练和优化。</li></ul><h2 id="愿景">愿景</h2><p>  矩阵分析是我开始写的第一个博客系列，我希望这个博客作为我学习笔记的同时，也能为读者解决遇到的问题。在我的想法中，我希望这个博客能一直处于更新状态，每当自己在矩阵理论方面有新的收获，便能把它记录在这里，愿自己能够一直坚持下来！</p><h2 id="相关学习资料">相关学习资料</h2><p>  矩阵理论有非常多的著作与学习资料，本博客主要参考的资料有：</p><ul><li><a href="https://www.bilibili.com/video/BV1Em4y1r7ss/?spm_id_from=333.337.search-card.all.click&amp;vd_source=234cf2ac075a1558881a6956450ddf89">Video1：《矩阵论》-严质彬 哈工大</a><br></li><li><a href="https://www.bilibili.com/video/BV1b4411j7V3/?spm_id_from=333.337.search-card.all.click&amp;vd_source=234cf2ac075a1558881a6956450ddf89">Video2：《数据分析、信号处理和机器学习中的矩阵方法》-Gilbert Strang MIT</a><br></li><li><a href="https://github.com/SXUSongJH/Book">Book1：《矩阵分析与应用》(第二版)-张贤达</a><br></li><li><a href="https://github.com/SXUSongJH/Book">Book2：《MatrixAnalysis》(Sencond Edition) - Roger A.Horn</a></li></ul><p>  在本系列的前期，将主要参考严质彬老师的课程。对于课程中没有涉及的内容，后期将借助其它几个资料进行补充。</p>]]></content>
    
    
    <summary type="html">本节介绍矩阵分析的研究内容，相关应用场景，以及本博客所用的学习资料。</summary>
    
    
    
    <category term="矩阵分析" scheme="http://example.com/categories/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90/"/>
    
    
  </entry>
  
  <entry>
    <title>这才是猛男该弹的曲子！！！</title>
    <link href="http://example.com/2023/07/15/%E8%BF%99%E6%89%8D%E6%98%AF%E7%8C%9B%E7%94%B7%E8%AF%A5%E5%90%AC%E7%9A%84%E6%9B%B2%E5%AD%90/"/>
    <id>http://example.com/2023/07/15/%E8%BF%99%E6%89%8D%E6%98%AF%E7%8C%9B%E7%94%B7%E8%AF%A5%E5%90%AC%E7%9A%84%E6%9B%B2%E5%AD%90/</id>
    <published>2023-07-14T17:23:18.000Z</published>
    <updated>2023-07-17T15:56:22.834Z</updated>
    
    <content type="html"><![CDATA[<p>  无语，最近对于睫毛弯弯这个曲子特着迷，吉他老师在教我光辉岁月时，我满脑子都想的是睫毛弯弯<span class="github-emoji"><span>😂</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f602.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>小时候咋没发现这个旋律这么好听。不行，我早晚得出一期这首歌的弹唱<span class="github-emoji"><span>🎶</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f3b6.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></p><center><iframe src="//player.bilibili.com/player.html?aid=384390831&amp;bvid=BV1fZ4y1t7db&amp;cid=731251859&amp;page=1" width="300" height="200" title="【吉他独奏】睫毛弯弯(王心凌)" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"></iframe></center>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;  无语，最近对于睫毛弯弯这个曲子特着迷，吉他老师在教我光辉岁月时，我满脑子都想的是睫毛弯弯&lt;span class=&quot;github-emoji&quot;&gt;&lt;span&gt;😂&lt;/span&gt;&lt;img src=&quot;https://github.githubassets.com/images/</summary>
      
    
    
    
    <category term="Daily" scheme="http://example.com/categories/Daily/"/>
    
    
  </entry>
  
</feed>
